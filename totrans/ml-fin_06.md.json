["```py\nencoding_dim = 32\n```", "```py\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\n```", "```py\ninput_img = Input(shape=(784,))\n\nencoded = Dense(encoding_dim, activation='relu')(input_img)\n\ndecoded = Dense(784, activation='sigmoid')(encoded)\n```", "```py\nautoencoder = Model(input_img, decoded)\n```", "```py\nfrom keras.utils import plot_model\nplot_model(autoencoder, to_file='model.png', show_shapes=True) plt.figure(figsize=(10,10))\nplt.imshow(plt.imread('model.png'))\n```", "```py\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n```", "```py\nautoencoder.fit(X_train_flat, X_train_flat,epochs=50,batch_size=256,shuffle=True,validation_data=(X_test_flat, X_test_flat))\n```", "```py\noriginal = np.expand_dims(X_test_flat[0],0)\n```", "```py\nseven = autoencoder.predict(original)\n```", "```py\nseven = seven.reshape(1,28,28)\noriginal = original.reshape(1,28,28)\n```", "```py\nfig = plt.figure(figsize=(7, 10))\na=fig.add_subplot(1,2,1)\na.set_title('Original')\nimgplot = plt.imshow(original[0,:,:])\n\nb=fig.add_subplot(1,2,2)\nb.set_title('Autoencoder')\nimgplot = plt.imshow(seven[0,:,:])\n```", "```py\ndf = pd.read_csv('../input/creditcard.csv')\ndf = df.drop('Time',axis=1)\n```", "```py\nX = df.drop('Class',axis=1).values\ny = df['Class'].values\n```", "```py\nX -= X.min(axis=0)\nX /= X.max(axis=0)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.1)\n```", "```py\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\n```", "```py\ndata_in = Input(shape=(29,))\nencoded = Dense(12,activation='tanh')(data_in)\ndecoded = Dense(29,activation='sigmoid')(encoded)\nautoencoder = Model(data_in,decoded)\n```", "```py\nautoencoder.compile(optimizer='adam',loss='mean_squared_error')\n```", "```py\nautoencoder.fit(X_train,X_train,epochs = 20,batch_size=128,validation_data=(X_test,X_test))\n```", "```py\npred = autoencoder.predict(X_test[0:10])\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwidth = 0.8\n\nprediction   = pred[9]\ntrue_value    = X_test[9]\n\nindices = np.arange(len(prediction))\n\nfig = plt.figure(figsize=(10,7))\n\nplt.bar(indices, prediction, width=width, color='b', label='Predicted Value')\n\nplt.bar([i+0.25*width for i in indices], true_value, width=0.5*width, color='r', alpha=0.5, label='True Value')\n\nplt.xticks(indices+width/2., ['V{}'.format(i) for i in range(len(prediction))] )\n\nplt.legend()\n\nplt.show()\n```", "```py\nencoder = Model(data_in,encoded)\n```", "```py\nenc = encoder.predict(X_test)\n```", "```py\nfrom sklearn.manifold import TSNE\ntsne = TSNE(verbose=1,n_iter=5000)\nres = tsne.fit_transform(enc)\n```", "```py\nfig = plt.figure(figsize=(10,7))\nscatter =plt.scatter(res[:,0],res[:,1],c=y_test, cmap='coolwarm', s=0.6)\nscatter.axes.get_xaxis().set_visible(False)\nscatter.axes.get_yaxis().set_visible(False)\n```", "```py\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Lambda\nfrom keras import backend as K\nfrom keras import metrics\n```", "```py\nbatch_size = 100\noriginal_dim = 784\nlatent_dim = 32\nintermediate_dim = 256\nepochs = 50\n```", "```py\nx = Input(shape=(original_dim,))\nh = Dense(intermediate_dim, activation='relu')(x)\nz_mean = Dense(latent_dim)(h)\nz_log_var = Dense(latent_dim)(h)\n```", "```py\ndef sampling(args):\n    z_mean, z_log_var = args                                  #1\n    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,stddev=1.0)                     #2\n    return z_mean + K.exp(z_log_var / 2) * epsilon            #3\n```", "```py\nz = Lambda(sampling)([z_mean, z_log_var])\n```", "```py\ndecoder_h = Dense(intermediate_dim, activation='relu')(z)\nx_decoded = Dense(original_dim, activation='sigmoid')decoder_mean(h_decoded)\n```", "```py\nreconstruction_loss = original_dim * metrics.binary_crossentropy(x, x_decoded)\n```", "```py\nkl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n```", "```py\nvae_loss = K.mean(reconstruction_loss + kl_loss)\n```", "```py\nvae = Model(x, x_decoded)\n```", "```py\nvae.add_loss(vae_loss)\n```", "```py\nvae.compile(optimizer='rmsprop')\n```", "```py\n\t\tvae.fit(X_train_flat,\n\t\t\tshuffle=True,\n\t\t\tepochs=epochs,\n\t\t\tbatch_size=batch_size,\n\t\t\tvalidation_data=(X_test_flat, None))\n\n```", "```py\none_seven = X_test_flat[0]\n```", "```py\none_seven = np.expand_dims(one_seven,0)\none_seven = one_seven.repeat(4,axis=0)\n```", "```py\ns = vae.predict(one_seven)\n```", "```py\ns= s.reshape(4,28,28)\n```", "```py\nfig=plt.figure(figsize=(8, 8))\ncolumns = 2\nrows = 2\nfor i in range(1, columns*rows +1):\n    img = s[i-1]\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img)\nplt.show()\n```", "```py\noriginal_dim = 29\nlatent_dim = 6\nintermediate_dim = 16\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, RepeatVector\n\nmodel = Sequential()                                            #1\nmodel.add(LSTM(latent_dim, input_shape=(maxlen, nb_features)))  #2\nmodel.add(RepeatVector(maxlen))                                 #3\nmodel.add(LSTM(nb_features, return_sequences=True))             #4\n```", "```py\nfrom keras.models import Model, Sequential\n```", "```py\nfrom keras.layers import Input, Dense, Dropout, Flatten\nfrom keras.layers import LeakyReLU, Reshape\nfrom keras.layers import Conv2D, UpSampling2D\n```", "```py\nfrom keras.optimizers import Adam\n```", "```py\nfrom keras.initializers import RandomNormal\n```", "```py\ngenerator = Sequential()                                       #1 \n\ngenerator.add(Dense(128*7*7, input_dim=latent_dim, kernel_initializer=RandomNormal(stddev=0.02)))   #2\n\ngenerator.add(LeakyReLU(0.2))                                  #3\ngenerator.add(Reshape((128, 7, 7)))                            #4\ngenerator.add(UpSampling2D(size=(2, 2)))                       #5\n\ngenerator.add(Conv2D(64,kernel_size=(5, 5),padding='same'))    #6\n\ngenerator.add(LeakyReLU(0.2))                                  #7\ngenerator.add(UpSampling2D(size=(2, 2)))                       #8\n\ngenerator.add(Conv2D(1, kernel_size=(5, 5),padding='same', activation='tanh'))                    #9\n\nadam = Adam(lr=0.0002, beta_1=0.5)\ngenerator.compile(loss='binary_crossentropy', optimizer=adam) #10\n```", "```py\n#Discriminator\ndiscriminator = Sequential()\ndiscriminator.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same', input_shape=(1, 28, 28),kernel_initializer=RandomNormal(stddev=0.02)))                                               #1\n\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Dropout(0.3))\ndiscriminator.add(Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding='same'))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Dropout(0.3))                          #2\ndiscriminator.add(Flatten())\ndiscriminator.add(Dense(1, activation='sigmoid'))\ndiscriminator.compile(loss='binary_crossentropy', optimizer=adam)\n```", "```py\ndiscriminator.trainable = False                         #1\nganInput = Input(shape=(latent_dim,))                   #2\nx = generator(ganInput)                                 #3\nganOutput = discriminator(x)                            #4\ngan = Model(inputs=ganInput, outputs=ganOutput)         #5\ngan.compile(loss='binary_crossentropy', optimizer=adam) #6\n```", "```py\nepochs=50\nbatchSize=128\nbatchCount = X_train.shape[0] // batchSize                     #1\n\nfor e in range(1, epochs+1):                                   #2\n    print('-'*15, 'Epoch %d' % e, '-'*15)\n    for _ in tqdm(range(batchCount)):                          #3\n\n        noise = np.random.normal(0, 1, size=[batchSize, latent_dim]) #4\n        imageBatch = X_train[np.random.randint(0, X_train.shape[0],size=batchSize)] #5\n\n        generatedImages = generator.predict(noise)             #6\n        X = np.concatenate([imageBatch, generatedImages])      #7\n\n        yDis = np.zeros(2*batchSize)                           #8\n        yDis[:batchSize] = 0.9 \n\n        labelNoise = np.random.random(yDis.shape)              #9\n        yDis += 0.05 * labelNoise + 0.05\n\n        discriminator.trainable = True                         #10\n        dloss = discriminator.train_on_batch(X, yDis)          #11\n\n        noise = np.random.normal(0, 1, size=[batchSize, latent_dim]) #12\n        yGen = np.ones(batchSize)                              #13\n        discriminator.trainable = False                        #14\n        gloss = gan.train_on_batch(noise, yGen)                #15\n\n    dLosses.append(dloss)                                      #16\n    gLosses.append(gloss)\n```", "```py\nlatent_dim=10\ndata_dim=29\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=latent_dim))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Dense(32, input_dim=latent_dim))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Dense(data_dim,activation='tanh'))\n```", "```py\nnoise = Input(shape=(latent_dim,))\nimg = model(noise)\n\ngenerator = Model(noise, img)\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(31,input_dim=data_dim))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(16,input_dim=data_dim))\nmodel.add(LeakyReLU(alpha=0.2))\n```", "```py\nimg = Input(shape=(data_dim,))                               #1\nfeatures = model(img)                                        #2\nvalid = Dense(1, activation=\"sigmoid\")(features)             #3\nlabel = Dense(num_classes+1, activation=\"softmax\")(features) #4\n\ndiscriminator = Model(img, [valid, label])                   #5\n```", "```py\n\t\toptimizer = Adam(0.0002, 0.5) #1\n\t\tdiscriminator.compile(loss=['binary_crossentropy',\n\t\t\t\t\t\t\t\t\t'categorical_crossentropy'], #2\n\t\t\t\t\t\t\t\t\tloss_weights=[0.5, 0.5], #3\n\t\t\t\t\t\t\t\t\toptimizer=optimizer, #4\n\t\t\t\t\t\t\t\t\tmetrics=['accuracy']) #5\n\n```", "```py\nnoise = Input(shape=(latent_dim,))               #1\nimg = generator(noise)                           #2\ndiscriminator.trainable = False                  #3\nvalid,_ = discriminator(img)                     #4\ncombined = Model(noise , valid)                  #5\ncombined.compile(loss=['binary_crossentropy'],optimizer=optimizer)\n```", "```py\n\tdef train(X_train,y_train,\n\t\t\t\tX_test,y_test,\n\t\t\t\tgenerator,discriminator,\n\t\t\t\tcombined,\n\t\t\t\tnum_classes,\n\t\t\t\tepochs,\n\t\t\t\tbatch_size=128):\n\n    f1_progress = []                                             #1\n    half_batch = int(batch_size / 2)                             #2\n\n    cw1 = {0: 1, 1: 1}                                           #3\n    cw2 = {i: num_classes / half_batch for i in range(num_classes)}\n    cw2[num_classes] = 1 / half_batch\n\n    for epoch in range(epochs):\n\n        idx = np.random.randint(0, X_train.shape[0], half_batch) #4\n        imgs = X_train[idx]\n\n        noise = np.random.normal(0, 1, (half_batch, 10))         #5\n        gen_imgs = generator.predict(noise)\n\n        valid = np.ones((half_batch, 1))                           #6\n        fake = np.zeros((half_batch, 1))\n\n        labels = to_categorical(y_train[idx], num_classes=num_classes+1)                                         #7\n\n        fake_labels = np.full((half_batch, 1),num_classes)         #8\n        fake_labels = to_categorical(fake_labels,num_classes=num_classes+1)\n        d_loss_real = discriminator.train_on_batch(imgs, [valid, labels],class_weight=[cw1, cw2])  #9\n        d_loss_fake = discriminator.train_on_batch(gen_imgs, [fake, fake_labels],class_weight=[cw1, cw2])  #10\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)            #11\n\n        noise = np.random.normal(0, 1, (batch_size, 10))           #12\n        validity = np.ones((batch_size, 1))\n        g_loss = combined.train_on_batch(noise, validity, class_weight=[cw1, cw2]) #13\n\n        print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, g_loss))  #14\n\n        if epoch % 10 == 0:                                       #15\n            _,y_pred = discriminator.predict(X_test,batch_size=batch_size)\n            y_pred = np.argmax(y_pred[:,:-1],axis=1)\n\n            f1 = f1_score(y_test,y_pred)\n            print('Epoch: {}, F1: {:.5f}'.format(epoch,f1))\n            f1_progress.append(f1)\n\n    return f1_progress\n```", "```py\nf1_p = train(X_res,y_res,X_test,y_test,generator,discriminator,combined,num_classes=2,epochs=5000, batch_size=128)\n```", "```py\nfig = plt.figure(figsize=(10,7))\nplt.plot(f1_p)\nplt.xlabel('10 Epochs')\nplt.ylabel('F1 Score Validation')\n```"]