- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text Data for Trading – Sentiment Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the first of three chapters dedicated to extracting signals for algorithmic
    trading strategies from text data using **natural language processing** (**NLP**)
    and **machine learning** (**ML**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Text data is very rich in content but highly unstructured, so it requires more
    preprocessing to enable an ML algorithm to extract relevant information. A key
    challenge consists of converting text into a numerical format without losing its
    meaning. We will cover several techniques capable of capturing the nuances of
    language so that they can be used as input for ML algorithms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce fundamental **feature extraction** techniques
    that focus on individual semantic units, that is, words or short groups of words
    called **tokens**. We will show how to represent documents as vectors of token
    counts by creating a document-term matrix and then proceed to use it as input
    for **news classification** and **sentiment analysis**. We will also introduce
    the naive Bayes algorithm, which is popular for this purpose.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: In the following two chapters, we build on these techniques and use ML algorithms
    such as topic modeling and word-vector embeddings to capture the information contained
    in a broader context.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular in this chapter, we will cover the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: What the fundamental NLP workflow looks like
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build a multilingual feature extraction pipeline using spaCy and TextBlob
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing NLP tasks such as **part-of-speech** (**POS**) tagging or named entity
    recognition
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting tokens to numbers using the document-term matrix
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying text using the naive Bayes model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to perform sentiment analysis
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: ML with text data – from language to features
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text data can be extremely valuable given how much information humans communicate
    and store using natural language. The diverse set of data sources relevant to
    financial investments range from formal documents like company statements, contracts,
    and patents, to news, opinion, and analyst research or commentary, to various
    types of social media postings or messages.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Numerous and diverse text data samples are available online to explore the use
    of NLP algorithms, many of which are listed among the resources included in this
    chapter's `README` file on GitHub. For a comprehensive introduction, see Jurafsky
    and Martin (2008).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: To realize the potential value of text data, we'll introduce the specialized
    NLP techniques and the most effective Python libraries, outline key challenges
    particular to working with language data, introduce critical elements of the NLP
    workflow, and highlight NLP applications relevant for algorithmic trading.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Key challenges of working with text data
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The conversion of unstructured text into a machine-readable format requires
    careful preprocessing to preserve the valuable semantic aspects of the data. How
    humans comprehend the content of language is not fully understood and improving
    machines' ability to understand language remains an area of very active research.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 将非结构化文本转换为机器可读格式需要仔细的预处理，以保留数据的宝贵语义方面。人类如何理解语言的内容尚不完全清楚，改进机器理解语言的能力仍然是一个非常活跃的研究领域。
- en: 'NLP is particularly challenging because the effective use of text data for
    ML requires an understanding of the inner workings of language as well as knowledge
    about the world to which it refers. Key challenges include the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）特别具有挑战性，因为有效利用文本数据进行机器学习需要理解语言的内在工作原理以及它所指的世界的知识。 主要挑战包括以下内容：
- en: Ambiguity due to **polysemy**, that is, a word or phrase having different meanings
    depending on context ("Local High School Dropouts Cut in Half")
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于**多义性**而产生的歧义，即一个词或短语根据上下文具有不同的含义（“Local High School Dropouts Cut in Half”）
- en: The nonstandard and **evolving use** of language, especially on social media
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在社交媒体上尤其是非标准和**不断发展的**语言使用
- en: The use of **idioms** like "throw in the towel"
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用诸如“throw in the towel”这样的**习语**
- en: Tricky **entity names** like "Where is A Bug's Life playing?"
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像“Where is A Bug's Life playing?”这样的棘手的**实体名称**
- en: 'Knowledge of the world: "Mary and Sue are sisters" versus "Mary and Sue are
    mothers"'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对世界的了解：“Mary and Sue are sisters”与“Mary and Sue are mothers”
- en: The NLP workflow
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理工作流程
- en: A key goal for using ML from text data for algorithmic trading is to extract
    signals from documents. A document is an individual sample from a relevant text
    data source, for example, a company report, a headline, a news article, or a tweet.
    A corpus, in turn, is a collection of documents.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本数据中使用机器学习进行算法交易的一个关键目标是从文档中提取信号。 文档是来自相关文本数据源的单个样本，例如公司报告、标题、新闻文章或推文。 语料库，反过来，是文档的集合。
- en: '*Figure 14.1* lays out the **key steps** to convert documents into a dataset
    that can be used to train a supervised ML algorithm capable of making actionable
    predictions:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14.1* 概述了将文档转换为可以用于训练具有可操作预测能力的监督机器学习算法的数据集的**关键步骤**：'
- en: '![](img/B15439_14_01.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_01.png)'
- en: 'Figure 14.1: The NLP workflow'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1：自然语言处理工作流程
- en: '**Fundamental techniques** extract text features as isolated semantic units
    called tokens and use rules and dictionaries to annotate them with linguistic
    and semantic information. The bag-of-words model uses token frequency to model
    documents as token vectors, which leads to the document-term matrix that is frequently
    used for text classification, retrieval, or summarization.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**基本技术** 提取文本特征作为被称为标记的孤立语义单元，并使用规则和字典对它们进行语言和语义信息的标注。 词袋模型使用令牌频率将文档建模为令牌向量，这导致经常用于文本分类、检索或摘要的文档-术语矩阵。'
- en: '**Advanced approaches** rely on ML to refine basic features such as tokens
    and produce richer document models. These include topic models that reflect the
    joint usage of tokens across documents and word-vector models that aim to capture
    the context of token usage.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**高级方法** 依赖于机器学习来改进基本特征，例如令牌，并生成更丰富的文档模型。 这些包括反映跨文档使用令牌的联合的主题模型和旨在捕获令牌使用上下文的单词向量模型。'
- en: 'We will review key decisions at each step of the workflow and the related tradeoffs
    in more detail before illustrating their implementation using the spaCy library
    in the next section. The following table summarizes the key tasks of an NLP pipeline:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将详细审查工作流程每一步的关键决策以及相关的权衡，并在示例中使用spaCy库说明它们的实现。 以下表格总结了自然语言处理管道的关键任务：
- en: '| Feature | Description |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 描述 |'
- en: '| Tokenization | Segment text into words, punctuation marks, and so on. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 分词 | 将文本分割成单词、标点符号等。 |'
- en: '| Part-of-speech tagging | Assign word types to tokens, such as a verb or noun.
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 词性标注 | 为令牌分配词类型，如动词或名词。 |'
- en: '| Dependency parsing | Label syntactic token dependencies, like subject <=>
    object. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 依存句法分析 | 标记句法令牌依赖关系，如主语<=>宾语。 |'
- en: '| Stemming and lemmatization | Assign the base forms of words: "was" => "be",
    "rats" => "rat". |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 词干提取和词形还原 | 分配单词的基本形式：“was” => “be”，“rats” => “rat”。 |'
- en: '| Sentence boundary detection | Find and segment individual sentences. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 句子边界检测 | 找到并分割单个句子。 |'
- en: '| Named entity recognition | Label "real-world" objects, such as people, companies,
    or locations. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 命名实体识别 | 标记“真实世界”对象，如人物、公司或地点。 |'
- en: '| Similarity | Evaluate the similarity of words, text spans, and documents.
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 相似性 | 评估单词、文本跨度和文档的相似性。 |'
- en: Parsing and tokenizing text data – selecting the vocabulary
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解析和标记文本数据 - 选择词汇表
- en: A token is an instance of a sequence of characters in a given document and is
    considered a semantic unit. The vocabulary is the set of tokens contained in a
    corpus deemed relevant for further processing; tokens not in the vocabulary will
    be ignored.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 标记是给定文档中字符序列的实例，并被认为是一个语义单位。词汇是被认为对进一步处理相关的语料库中包含的标记的集合；不在词汇中的标记将被忽略。
- en: The **goal**, of course, is to extract tokens that most accurately reflect the
    document's meaning. The **key tradeoff** at this step is the choice of a larger
    vocabulary to better reflect the text source at the expense of more features and
    higher model complexity (discussed as the *curse of dimensionality* in *Chapter
    13*, *Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning*).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，**目标**是提取最能准确反映文档含义的标记。在这一步的**关键折衷**是选择更大的词汇量，以更好地反映文本来源，代价是增加更多的特征和更高的模型复杂性（在*第13章*，*使用无监督学习评估数据驱动的风险因素和资产配置*中讨论为*维度诅咒*）。
- en: Basic choices in this regard concern the treatment of punctuation and capitalization,
    the use of spelling correction, and whether to exclude very frequent so-called
    "stop words" (such as "and" or "the") as meaningless noise.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面的基本选择涉及标点和大写的处理，拼写校正的使用，以及是否排除非常频繁的所谓“停用词”（如“and”或“the”）作为无意义的噪音。
- en: In addition, we need to decide whether to include groupings of *n* individual
    tokens called *n***-grams** as semantic units (an individual token is also called
    *unigram*). An example of a two-gram (or *bigram*) is "New York", whereas "New
    York City" is a three-gram (or *trigram*). The decision can rely on dictionaries
    or a comparison of the relative frequencies of the individual and joint usage.
    There are more unique combinations of tokens than unigrams, hence adding *n*-grams
    will drive up the number of features and risks adding noise unless filtered for
    by frequency.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们需要决定是否将由*n*个单独的标记组成的*n***-gram**作为语义单位（一个单独的标记也称为*unigram*）包含在内。一个二元组（或*bigram*）的例子是“纽约”，而“纽约市”是一个三元组（或*trigram*）。这个决定可以依赖于词典或者对个体和联合使用的相对频率进行比较。与unigrams相比，标记的唯一组合更多，因此添加*n*-grams将增加特征数量，并且除非按频率进行过滤，否则会添加噪音。
- en: Linguistic annotation – relationships among tokens
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言学标注 - 标记之间的关系
- en: Linguistic annotations include the application of **syntactic and grammatical
    rules** to identify the boundary of a sentence despite ambiguous punctuation,
    and a token's role and relationships in a sentence for POS tagging and dependency
    parsing. It also permits the identification of common root forms for stemming
    and lemmatization to group together related words.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 语言学标注包括将**句法和语法规则**应用于识别句子边界，尽管标点符号模糊不清，以及词标注和依赖解析中的一个标记的角色和关系。它还允许识别词根的常见形式以进行词干提取和词形还原，以将相关单词分组在一起。
- en: 'The following are some key concepts related to annotations:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是与标注相关的一些关键概念：
- en: '**Stemming** uses simple rules to remove common endings such as *s*, *ly*,
    *ing*, or *ed* from a token and reduce it to its stem or root form.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取**使用简单的规则从标记中删除常见的结尾，比如*s*、*ly*、*ing*或*ed*，并将其减少到其词干或根形式。'
- en: '**Lemmatization** uses more sophisticated rules to derive the canonical root
    (**lemma**) of a word. It can detect irregular common roots such as "better" and
    "best" and more effectively condenses the vocabulary but is slower than stemming.
    Both approaches simplify the vocabulary at the expense of semantic nuances.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词形还原**使用更复杂的规则来推导单词的规范根（**lemma**）。它可以检测到不规则的常见根，比如“better”和“best”，并更有效地压缩词汇，但比词干提取慢。这两种方法都是以语义细微差别为代价来简化词汇。'
- en: '**POS** annotations help disambiguate tokens based on their function (for example,
    when a verb and noun have the same form), which increases the vocabulary but may
    capture meaningful distinctions.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**POS**标注有助于根据它们的功能消除标记的歧义（例如，当动词和名词具有相同的形式时），这会增加词汇量，但可能捕捉到有意义的区别。'
- en: '**Dependency parsing** identifies hierarchical relationships among tokens and
    is commonly used for translation. It is important for interactive applications
    that require more advanced language understanding, such as chatbots.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖解析**识别标记之间的分层关系，通常用于翻译。对于需要更高级语言理解的交互应用程序，比如聊天机器人，这一点至关重要。'
- en: Semantic annotation – from entities to knowledge graphs
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义注释 - 从实体到知识图
- en: '**Named-entity recognition** (**NER**) aims at identifying tokens that represent
    objects of interest, such as persons, countries, or companies. It can be further
    developed into a **knowledge graph** that captures semantic and hierarchical relationships
    among such entities. It is a critical ingredient for applications that, for example,
    aim at predicting the impact of news events on sentiment.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**命名实体识别**（**NER**）旨在识别表示感兴趣对象的标记，如人物、国家或公司。它可以进一步发展成捕捉这些实体之间语义和层次关系的**知识图**。这对于那些旨在预测新闻事件对情绪影响的应用至关重要。'
- en: Labeling – assigning outcomes for predictive modeling
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标签化 - 为预测建模分配结果
- en: Many NLP applications learn to predict outcomes based on meaningful information
    extracted from the text. Supervised learning requires labels to teach the algorithm
    the true input-output relationship. With text data, establishing this relationship
    may be challenging and require explicit data modeling and collection.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 许多自然语言处理应用程序通过从文本中提取的有意义信息来学习预测结果。监督学习需要标签来教会算法真实的输入输出关系。在文本数据中，建立这种关系可能具有挑战性，并且可能需要显式的数据建模和收集。
- en: Examples include decisions on how to quantify the sentiment implicit in a text
    document such as an email, transcribed interview, or tweet with respect to a new
    domain, or which aspects of a research document or news report should be assigned
    a specific outcome.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 示例包括如何量化文本文档（例如电子邮件、转录的采访或推文）中隐含的情感，与新领域相关的，或者应该分配特定结果的研究文档或新闻报告的哪些方面。
- en: Applications
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用
- en: 'The use of ML with text data for trading relies on extracting meaningful information
    in the form of features that help predict future price movements. Applications
    range from the exploitation of the short-term market impact of news to the longer-term
    fundamental analysis of the drivers of asset valuation. Examples include the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本数据进行交易的机器学习依赖于提取有意义的信息以形成有助于预测未来价格走势的特征。应用范围从利用新闻的短期市场影响到对资产估值驱动因素的长期基本分析。例如：
- en: The evaluation of product review sentiment to assess a company's competitive
    position or industry trends
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估产品评论情感以评估公司的竞争地位或行业趋势
- en: The detection of anomalies in credit contracts to predict the probability or
    impact of a default
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测信贷合同中的异常以预测违约的概率或影响
- en: The prediction of news impact in terms of direction, magnitude, and affected
    entities
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测新闻影响的方向、幅度和受影响的实体
- en: JP Morgan, for instance, developed a predictive model based on 250,000 analyst
    reports that outperformed several benchmark indices and produced uncorrelated
    signals relative to sentiment factors formed from consensus EPS and recommendation
    changes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，摩根大通公司基于25万份分析师报告开发了一个预测模型，该模型的表现优于多个基准指数，并且相对于从共识EPS和推荐变化中形成的情绪因素产生了不相关的信号。
- en: From text to tokens – the NLP pipeline
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文本到标记 - 自然语言处理流程
- en: In this section, we will demonstrate how to construct an NLP pipeline using
    the open-source Python library spaCy. The textacy library builds on spaCy and
    provides easy access to spaCy attributes and additional functionality.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何使用开源Python库spaCy构建一个NLP流程。textacy库基于spaCy构建，并提供了易于访问的spaCy属性和额外功能。
- en: Refer to the notebook `nlp_pipeline_with_spaCy` for the following code samples,
    installation instruction, and additional details.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅笔记本`nlp_pipeline_with_spaCy`以获取以下代码示例、安装说明和更多详细信息。
- en: NLP pipeline with spaCy and textacy
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用spaCy和textacy的自然语言处理流程
- en: spaCy is a widely used Python library with a comprehensive feature set for fast
    text processing in multiple languages. The usage of the tokenization and annotation
    engines requires the installation of language models. The features we will use
    in this chapter only require the small models; the larger models also include
    word vectors that we will cover in *Chapter 16*, *Word Embeddings for Earnings
    Calls and SEC Filings*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy是一个广泛使用的Python库，具有多语言快速文本处理的综合功能集。使用标记化和注释引擎需要安装语言模型。本章中我们将使用的功能仅需要小型模型；较大的模型还包括我们将在*第16章*中介绍的词向量。
- en: 'With the library installed and linked, we can instantiate a spaCy language
    model and then apply it to the document. The result is a `Doc` object that tokenizes
    the text and processes it according to configurable pipeline components that by
    default consist of a tagger, a parser, and a named-entity recognizer:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 安装并链接库后，我们可以实例化一个spaCy语言模型，然后将其应用于文档。结果是一个`Doc`对象，它对文本进行标记化和处理，根据默认的可配置流水线组件进行处理，这些组件通常包括标记器、解析器和命名实体识别器：
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s illustrate the pipeline using a simple sentence:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个简单的句子来说明流水线：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parsing, tokenizing, and annotating a sentence
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解析、标记和注释一个句子
- en: 'The parsed document content is iterable, and each element has numerous attributes
    produced by the processing pipeline. The next sample illustrates how to access
    the following attributes:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 解析后的文档内容是可迭代的，每个元素都有由处理流程生成的许多属性。下一个示例演示了如何访问以下属性：
- en: '`.text`: The original word text'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.text`: 原始词文本'
- en: '`.lemma_`: The word root'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.lemma_`: 词的词根'
- en: '`.pos_`: A basic POS tag'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.pos_`: 基本词性标记'
- en: '`.tag_`: The detailed POS tag'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.tag_`: 详细的词性标记'
- en: '`.dep_`: The syntactic relationship or dependency between tokens'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.dep_`: 标记词间的句法关系或依赖性'
- en: '`.shape_`: The shape of the word, in terms of capitalization, punctuation,
    and digits'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.shape_`: 词的形状，以大写、标点和数字为准'
- en: '`.is alpha`: Checks whether the token is alphanumeric'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.is alpha`: 检查标记是否为字母数字'
- en: '`.is stop`: Checks whether the token is on a list of common words for the given
    language'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.is stop`: 检查标记是否在给定语言的常用词列表中'
- en: 'We iterate over each token and assign its attributes to a `pd.DataFrame`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迭代处理每个标记，并将其属性分配给一个`pd.DataFrame`：
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This produces the following result:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下结果：
- en: '| text | lemma | pos | tag | dep | shape | is_alpha | is_stop |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| text | lemma | pos | tag | dep | shape | is_alpha | is_stop |'
- en: '| Apple | apple | PROPN | NNP | nsubj | Xxxxx | TRUE | FALSE |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Apple | apple | PROPN | NNP | nsubj | Xxxxx | TRUE | FALSE |'
- en: '| is | be | VERB | VBZ | aux | xx | TRUE | TRUE |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| is | be | VERB | VBZ | aux | xx | TRUE | TRUE |'
- en: '| looking | look | VERB | VBG | ROOT | xxxx | TRUE | FALSE |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| looking | look | VERB | VBG | ROOT | xxxx | TRUE | FALSE |'
- en: '| at | at | ADP | IN | prep | xx | TRUE | TRUE |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| at | at | ADP | IN | prep | xx | TRUE | TRUE |'
- en: '| buying | buy | VERB | VBG | pcomp | xxxx | TRUE | FALSE |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| buying | buy | VERB | VBG | pcomp | xxxx | TRUE | FALSE |'
- en: '| U.K. | u.k. | PROPN | NNP | compound | X.X. | FALSE | FALSE |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| U.K. | u.k. | PROPN | NNP | compound | X.X. | FALSE | FALSE |'
- en: '| startup | startup | NOUN | NN | dobj | xxxx | TRUE | FALSE |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| startup | startup | NOUN | NN | dobj | xxxx | TRUE | FALSE |'
- en: '| for | for | ADP | IN | prep | xxx | TRUE | TRUE |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| for | for | ADP | IN | prep | xxx | TRUE | TRUE |'
- en: '| $ | $ | SYM | $ | quantmod | $ | FALSE | FALSE |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| $ | $ | SYM | $ | quantmod | $ | FALSE | FALSE |'
- en: '| 1 | 1 | NUM | CD | compound | d | FALSE | FALSE |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | NUM | CD | compound | d | FALSE | FALSE |'
- en: '| billion | billion | NUM | CD | pobj | xxxx | TRUE | FALSE |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| billion | billion | NUM | CD | pobj | xxxx | TRUE | FALSE |'
- en: 'We can visualize the syntactic dependency in a browser or notebook using the
    following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下方法在浏览器或笔记本中可视化句法依赖：
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code allows us to obtain a dependency tree like the following
    one:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使我们能够获得如下的依赖树：
- en: '![](img/B15439_14_02.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_02.png)'
- en: 'Figure 14.2: spaCy dependency tree'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2：spaCy依赖树
- en: 'We can get additional insight into the meaning of attributes using `spacy.explain()`,
    such as the following, for example:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`spacy.explain()`获取属性含义的额外见解，例如：
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Batch-processing documents
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批处理文档
- en: 'We will now read a larger set of 2,225 BBC News articles (see GitHub for the
    data source details) that belong to five categories and are stored in individual
    text files. We do the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将读取一个更大的数据集，包含2,225篇BBC新闻文章（详见GitHub获取数据源细节），这些文章分属五个类别，并存储在单独的文本文件中。我们执行以下操作：
- en: Call the `.glob()` method of the `pathlib` module's `Path` object.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`pathlib`模块的`Path`对象的`.glob()`方法。
- en: Iterate over the resulting list of paths.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代处理结果列表中的路径。
- en: Read all lines of the news article excluding the heading in the first line.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取新闻文章中除了第一行标题之外的所有行。
- en: 'Append the cleaned result to a list:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将清理后的结果附加到列表中：
- en: '[PRE5]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Sentence boundary detection
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 句子边界检测
- en: 'We will illustrate sentence detection by calling the NLP object on the first
    of the articles:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 调用NLP对象对文章的第一句进行句子检测：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: spaCy computes sentence boundaries from the syntactic parse tree so that punctuation
    and capitalization play an important but not decisive role. As a result, boundaries
    will coincide with clause boundaries, even for poorly punctuated text.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy根据句法分析树计算句子边界，因此标点符号和大写字母起着重要但不决定性的作用。因此，边界将与从句边界重合，即使是标点不良的文本也是如此。
- en: 'We can access the parsed sentences using the `.sents` attribute:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `.sents` 属性访问解析后的句子：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Named entity recognition
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: 'spaCy enables named entity recognition using the `.ent_type_` attribute:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 通过 `.ent_type_` 属性启用了命名实体识别：
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Textacy makes access to the named entities that appear in the first article
    easy:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Textacy 让访问第一篇文章中出现的命名实体变得很容易：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: N-grams
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N-grams
- en: N-grams combine *n* consecutive tokens. This can be useful for the bag-of-words
    model because, depending on the textual context, treating (for example) "data
    scientist" as a single token may be more meaningful than the two distinct tokens
    "data" and "scientist".
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: N-grams 结合了 *n* 个连续的标记。这对于词袋模型可能是有用的，因为根据文本上下文，将（例如）"数据科学家" 视为单个标记可能比将 "数据"
    和 "科学家" 两个不同的标记更有意义。
- en: 'Textacy makes it easy to view the `ngrams` of a given length `n` occurring
    at least `min_freq` times:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Textacy 使查看至少出现 `min_freq` 次的给定长度 `n` 的 `ngrams` 变得很容易：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: spaCy's streaming API
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: spaCy 的流式 API
- en: 'To pass a larger number of documents through the processing pipeline, we can
    use spaCy''s streaming API as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过处理管道传递更多的文档，我们可以使用 spaCy 的流式 API 如下所示：
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Multi-language NLP
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多语言自然语言处理
- en: spaCy includes trained language models for English, German, Spanish, Portuguese,
    French, Italian, and Dutch, as well as a multi-language model for named-entity
    recognition. Cross-language usage is straightforward since the API does not change.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 包含了针对英语、德语、西班牙语、葡萄牙语、法语、意大利语和荷兰语的经过训练的语言模型，以及一个用于命名实体识别的多语言模型。由于 API 不变，跨语言使用很简单。
- en: 'We will illustrate the Spanish language model using a parallel corpus of TED
    talk subtitles (see the GitHub repo for data source references). For this purpose,
    we instantiate both language models:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 TED 演讲字幕的平行语料库来说明西班牙语言模型（请参阅数据来源参考的 GitHub 存储库）。为此，我们实例化两个语言模型：
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We read small corresponding text samples in each model:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在每个模型中读取相应的小文本样本：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Sentence boundary detection uses the same logic but finds a different breakdown:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 句子边界检测使用相同的逻辑，但找到了不同的分解：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'POS tagging also works in the same way:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注也是以相同的方式工作：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This produces the following table:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下表格：
- en: '| Token | POS Tag | Meaning | Token | POS Tag | Meaning |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Token | POS 标记 | 意思 | Token | POS 标记 | 意思 |'
- en: '| There | ADV | adverb | Existe | VERB | verb |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| There | ADV | 副词 | Existe | VERB | 动词 |'
- en: '| s | VERB | verb | una | DET | determiner |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| s | VERB | 动词 | una | DET | 限定词 |'
- en: '| a | DET | determiner | estrecha | ADJ | adjective |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| a | DET | 限定词 | estrecha | ADJ | 形容词 |'
- en: '| tight | ADJ | adjective | y | CONJ | conjunction |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| tight | ADJ | 形容词 | y | CONJ | 连词 |'
- en: '| and | CCONJ | coordinating conjunction | sorprendente | ADJ | adjective |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| and | CCONJ | 并列连词 | sorprendente | ADJ | 形容词 |'
- en: The next section illustrates how to use the parsed and annotated tokens to build
    a document-term matrix that can be used for text classification.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将说明如何使用解析和注释的标记构建可以用于文本分类的文档-术语矩阵。
- en: NLP with TextBlob
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TextBlob 的自然语言处理
- en: TextBlob is a Python library that provides a simple API for common NLP tasks
    and builds on the  **Natural Language Toolkit** (**NLTK**) and the Pattern web
    mining libraries. TextBlob facilitates POS tagging, noun phrase extraction, sentiment
    analysis, classification, and translation, among others.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: TextBlob 是一个提供简单 API 用于常见自然语言处理任务的 Python 库，它构建在 **自然语言工具包** (**NLTK**) 和 Pattern
    网络挖掘库的基础上。TextBlob 提供了词性标注、名词短语提取、情感分析、分类和翻译等功能。
- en: 'To illustrate the use of TextBlob, we sample a BBC Sport article with the headline
    "Robinson ready for difficult task". Similar to spaCy and other libraries, the
    first step is to pass the document through a pipeline represented by the `TextBlob`
    object to assign the annotations required for various tasks (see the notebook
    `nlp_with_textblob`):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要说明 TextBlob 的用法，我们采样了一篇标题为 "Robinson ready for difficult task" 的 BBC Sport
    文章。与 spaCy 和其他库类似，第一步是通过由 `TextBlob` 对象表示的管道将文档传递，以分配所需任务的注释（请参阅笔记本 `nlp_with_textblob`）：
- en: '[PRE16]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Stemming
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词干提取
- en: 'To perform stemming, we instantiate the `SnowballStemmer` from the NTLK library,
    call its `.stem()` method on each token, and display tokens that were modified
    as a result:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行词干提取，我们从 NTLK 库实例化 `SnowballStemmer`，对每个标记调用其 `.stem()` 方法，并显示因此而被修改的标记：
- en: '[PRE17]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Sentiment polarity and subjectivity
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 情感极性和主观性
- en: TextBlob provides polarity and subjectivity estimates for parsed documents using
    dictionaries provided by the Pattern library. These dictionaries lexicon-map adjectives
    frequently found in product reviews to sentiment polarity scores, ranging from
    -1 to +1 (negative ↔ positive) and a similar subjectivity score (objective ↔ subjective).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: TextBlob 提供使用 Pattern 库提供的字典为解析的文档提供极性和主观度估计。这些字典将产品评论中频繁出现的形容词与情感极性分数进行词典映射，分数范围从
    -1 到 +1（负面 ↔ 正面），以及类似的主观度分数（客观 ↔ 主观）。
- en: 'The `.sentiment` attribute provides the average for each score over the relevant
    tokens, whereas the `.sentiment_assessments` attribute lists the underlying values
    for each token (see the notebook):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`.sentiment` 属性为每个分数提供相关标记的平均值，而 `.sentiment_assessments` 属性则列出了每个标记的基础值（请参阅笔记本）：'
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Counting tokens – the document-term matrix
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算标记 - 文档-词汇矩阵
- en: In this section, we first introduce how the bag-of-words model converts text
    data into a numeric vector space representations. The goal is to approximate document
    similarity by their distance in that space. We then proceed to illustrate how
    to create a document-term matrix using the sklearn library.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍词袋模型如何将文本数据转换为数值向量空间表示。目标是通过它们在该空间中的距离来近似文档的相似性。然后，我们继续说明如何使用 sklearn
    库创建文档-词汇矩阵。
- en: The bag-of-words model
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词袋模型
- en: The bag-of-words model represents a document based on the frequency of the terms
    or tokens it contains. Each document becomes a vector with one entry for each
    token in the vocabulary that reflects the token's relevance to the document.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型基于文档包含的术语或标记的频率来表示文档。每个文档变成一个向量，其中每个标记在词汇表中对应一个条目，反映了该标记对文档的相关性。
- en: Creating the document-term matrix
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建文档-词汇矩阵
- en: The document-term matrix is straightforward to compute given the vocabulary.
    However, it is also a crude simplification because it abstracts from word order
    and grammatical relationships. Nonetheless, it often achieves good results in
    text classification quickly and, thus, provides a very useful starting point.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 给定词汇表，文档-词汇矩阵很容易计算。然而，它也是一种粗糙的简化，因为它抽象了单词顺序和语法关系。尽管如此，它通常能够快速在文本分类中取得良好的结果，因此为非常有用的起点。
- en: The left panel of *Figure 14.3* illustrates how this document model converts
    text data into a matrix with numerical entries where each row corresponds to a
    document and each column to a token in the vocabulary. The resulting matrix is
    usually both very high-dimensional and sparse, that is, it contains many zero
    entries because most documents only contain a small fraction of the overall vocabulary.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.3* 的左侧面板说明了这种文档模型如何将文本数据转换为一个矩阵，其中每行对应一个文档，每列对应词汇表中的一个标记。由此产生的矩阵通常是非常高维和稀疏的，即它包含许多零条目，因为大多数文档只包含整体词汇的一小部分。'
- en: '![](img/B15439_14_03.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_03.png)'
- en: 'Figure 14.3: Document-term matrix and cosine similarity'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3：文档-词汇矩阵和余弦相似性
- en: There are several ways to weigh a token's vector entry to capture its relevance
    to the document. We will illustrate how to use sklearn to use binary flags that
    indicate presence or absence, counts, and weighted counts that account for differences
    in term frequencies across all documents in the corpus.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以对一个标记的向量条目进行加权，以捕捉其与文档的相关性。我们将演示如何使用 sklearn 使用二进制标志表示存在或不存在、计数以及考虑到语料库中所有文档中术语频率差异的加权计数。
- en: Measuring the similarity of documents
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量文档的相似性
- en: The representation of documents as word vectors assigns to each document a location
    in the vector space created by the vocabulary. Interpreting the vector entries
    as Cartesian coordinates in this space, we can use the angle between two vectors
    to measure their similarity because vectors that point in the same direction contain
    the same terms with the same frequency weights.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 将文档表示为词向量将给每个文档分配一个位置，该位置位于由词汇表创建的向量空间中。解释该空间中的向量条目为笛卡尔坐标，我们可以使用两个向量之间的角度来测量它们的相似性，因为指向相同方向的向量包含具有相同频率权重的相同术语。
- en: The right panel of the preceding figure illustrates—simplified in two dimensions—the
    calculation of the distance between a document represented by a vector *d*[1]
    and a query vector (either a set of search terms or another document) represented
    by the vector *q*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图的右侧面板在二维中简化地说明了一个由向量 *d*[1] 表示的文档与由向量 *q* 表示的查询向量（可以是一组搜索词或另一个文档）之间距离的计算。
- en: The **cosine similarity** equals the cosine of the angle between the two vectors.
    It translates the size of the angle into a number in the range [0, 1] since all
    vector entries are non-negative token weights. A value of 1 implies that both
    documents are identical with respect to their token weights, whereas a value of
    0 implies that the two documents only contain distinct tokens.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**余弦相似度** 等于两个向量之间角度的余弦值。它将角度大小转换为范围为 [0, 1] 的数字，因为所有向量条目都是非负的标记权重。值为 1 意味着两个文档在其标记权重方面完全相同，而值为
    0 意味着两个文档仅包含不同的标记。'
- en: As shown in the figure, the cosine of the angle is equal to the dot product
    of the vectors, that is, the sum product of their coordinates, divided by the
    product of the lengths, measured by the Euclidean norms, of each vector.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，角度的余弦等于向量的点积，即它们坐标的和积除以它们各自向量的欧几里德范数的乘积。
- en: Document-term matrix with scikit-learn
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 创建文档 - 词项矩阵
- en: The scikit-learn preprocessing module offers two tools to create a document-term
    matrix. `CountVectorizer` uses binary or absolute counts to measure the **term
    frequency** (**TF**) *tf*(*d, t*) for each document *d* and token *t*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 预处理模块提供了两个工具来创建文档 - 词项矩阵。`CountVectorizer` 使用二进制或绝对计数来测量每个文档 *d*
    和标记 *t* 的 **词频** (**TF**) *tf*(*d, t*)。
- en: '`TfidfVectorizer`, by contrast, weighs the (absolute) term frequency by the
    **inverse document frequency** (**IDF**). As a result, a term that appears in
    more documents will receive a lower weight than a token with the same frequency
    for a given document but with a lower frequency across all documents. More specifically,
    using the default settings, the *tf-idf*(*d*, *t*) entries for the document-term
    matrix are computed as *tf-idf(d, t) = tf(d, t)* x *idf(t)* with:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`TfidfVectorizer` 相比之下，通过**逆文档频率**（**IDF**）加权（绝对）词频。因此，在更多文档中出现的词语将比在给定文档中具有相同频率但在所有文档中频率较低的标记接收到较低的权重。更具体地说，使用默认设置，文档
    - 词项矩阵的 *tf-idf*(*d*, *t*) 条目被计算为 *tf-idf(d, t) = tf(d, t)* x *idf(t)* ，其中：'
- en: '![](img/B15439_14_001.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_001.png)'
- en: where *n*[d] is the number of documents and *df*(*d*, *t*) the document frequency
    of term *t*. The resulting TF-IDF vectors for each document are normalized with
    respect to their absolute or squared totals (see the sklearn documentation for
    details). The TF-IDF measure was originally used in information retrieval to rank
    search engine results and has subsequently proven useful for text classification
    and clustering.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *n*[d] 是文档数，*df*(*d*, *t*) 是词项 *t* 的文档频率。每个文档的结果 TF-IDF 向量都针对它们的绝对或平方总数进行了归一化（有关详细信息，请参阅
    sklearn 文档）。TF-IDF 度量最初用于信息检索以对搜索引擎结果进行排名，并随后被证明对于文本分类和聚类非常有用。
- en: Both tools use the same interface and perform tokenization and further optional
    preprocessing of a list of documents before vectorizing the text by generating
    token counts to populate the document-term matrix.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个工具使用相同的接口，并在向量化文本之前对文档列表进行分词和进一步可选的预处理，通过生成标记计数来填充文档 - 词项矩阵。
- en: 'Key parameters that affect the size of the vocabulary include the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 影响词汇表大小的关键参数包括以下内容：
- en: '`stop_words`: Uses a built-in or user-provided list of (frequent) words to
    exclude'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stop_words`: 使用内置或用户提供的（频繁）词语列表来排除词语'
- en: '`ngram_range`: Includes *n*-grams in a range of *n* defined by a tuple of (*n*[min],
    *n*[max])'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_range`: 包括 *n*-gram，在由元组定义的 *n* 范围内的 *n*'
- en: '`lowercase`: Converts characters accordingly (the default is `True`)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lowercase`: 相应地转换字符（默认值为 `True`）'
- en: '`min_df`/`max_df`: Ignores words that appear in less/more (`int`) or are present
    in a smaller/larger share of documents (if `float` [0.0,1.0])'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_df`/`max_df`: 忽略出现在较少/较多（`int`）的文档中的词语，或者出现在较小/较大比例的文档中（如果是 `float` [0.0,1.0]）'
- en: '`max_features`: Limits the number of tokens in vocabulary accordingly'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`: 限制词汇表中的标记数量'
- en: '`binary`: Sets non-zero counts to 1 (`True`)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary`: 将非零计数设置为 1（`True`）'
- en: See the notebook `document_term_matrix` for the following code samples and additional
    details. We are again using the 2,225 BBC news articles for illustration.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 查看笔记本 `document_term_matrix` 以获取以下代码示例和更多详细信息。我们再次使用 2,225 篇 BBC 新闻文章作为示例。
- en: Using CountVectorizer
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 CountVectorizer
- en: 'The notebook contains an interactive visualization that explores the impact
    of the `min_df` and `max_df` settings on the size of the vocabulary. We read the
    articles into a DataFrame, set `CountVectorizer` to produce binary flags and use
    all tokens, and call its `.fit_transform()` method to produce a document-term
    matrix:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中包含一个交互式可视化，探索 `min_df` 和 `max_df` 设置对词汇量大小的影响。我们将文章读入 DataFrame，设置 `CountVectorizer`
    以生成二进制标志并使用所有词项，并调用其 `.fit_transform()` 方法以生成文档-词项矩阵：
- en: '[PRE19]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The output is a `scipy.sparse` matrix in row format that efficiently stores
    a small share (<0.7 percent) of the 445,870 non-zero entries in the 2,225 (document)
    rows and 29,275 (token) columns.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个以行格式存储的 `scipy.sparse` 矩阵，有效地存储了 2,225（文档）行和 29,275（词项）列中的 445,870 个非零条目中的一小部分（<0.7%）。
- en: Visualizing the vocabulary distribution
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可视化词汇分布
- en: The visualization in *Figure 14.4* shows that requiring tokens to appear in
    at least 1 percent and less than 50 percent of documents restricts the vocabulary
    to around 10 percent of the almost 30,000 tokens.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 14.4*中的可视化显示，要求词项出现在至少 1% 且少于 50% 的文档中，将词汇限制在近 30000 个词项中的约 10% 左右。
- en: 'This leaves a mode of slightly over 100 unique tokens per document, as shown
    in the left panel of the following plot. The right panel shows the document frequency
    histogram for the remaining tokens:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这样留下了每个文档略多于 100 个唯一词项的模式，如下图左侧面板所示。右侧面板显示了剩余词项的文档频率直方图：
- en: '![](img/B15439_14_04.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_04.png)'
- en: 'Figure 14.4: The distributions of unique tokens and number of tokens per document'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4：唯一词项和每个文档的词项数量的分布
- en: Finding the most similar documents
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查找最相似的文档
- en: '`CountVectorizer` result lets us find the most similar documents using the
    `pdist()` functions for pairwise distances provided by the `scipy.spatial.distance`
    module. It returns a condensed distance matrix with entries corresponding to the
    upper triangle of a square matrix. We use `np.triu_indices()` to translate the
    index that minimizes the distance to the row and column indices that in turn correspond
    to the closest token vectors:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer` 的结果使我们能够使用由 `scipy.spatial.distance` 模块提供的 pairwise 距离的 `pdist()`
    函数找到最相似的文档。它返回一个压缩的距离矩阵，其条目对应于正方形矩阵的上三角形。我们使用 `np.triu_indices()` 将最小距离的索引转换为相应于最接近的词项向量的行和列索引：'
- en: '[PRE20]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Articles 6 and 245 are closest by cosine similarity, due to the fact that they
    share 38 tokens out of a combined vocabulary of 303 (see notebook). The following
    table summarizes these two articles and demonstrates the limited ability of similarity
    measures based on word counts to identify deeper semantic similarity:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 文章 6 和 245 在余弦相似性上最接近，因为它们共享 303 个词汇中的 38 个词项（见笔记本）。以下表格总结了这两篇文章，并展示了基于词数的相似度测量对于识别更深层语义相似性的有限能力：
- en: '|  | Article 6 | Article 245 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | 文章 6 | 文章 245 |'
- en: '| Topic | Business | Business |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 商业 | 商业 |'
- en: '| Heading | Jobs growth still slow in the US | Ebbers ''aware'' of WorldCom
    fraud |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 美国就业增长仍然缓慢 | Ebbers 对 WorldCom 的欺诈案件有所了解 |'
- en: '| Body | The US created fewer jobs than expected in January, but a fall in
    jobseekers pushed the unemployment rate to its lowest level in three years. According
    to Labor Department figures, US firms added only 146,000 jobs in January. | Former
    WorldCom boss Bernie Ebbers was directly involved in the $11bn financial fraud
    at the firm, his closest associate has told a US court. Giving evidence in the
    criminal trial of Mr Ebbers, ex-finance chief Scott Sullivan implicated his colleague
    in the accounting scandal at the firm. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 正文 | 美国在一月份创造的工作岗位少于预期，但求职者的减少使失业率降至三年来的最低水平。根据劳工部的数据，美国企业在一月份仅增加了 146,000
    个工作岗位。 | 前世界通信公司总裁 Bernie Ebbers 在该公司的 110 亿美元金融欺诈案中直接参与其中，他最亲密的同事在美国法院告诉说。在 Ebbers
    先生的刑事审判中作证，前财务主管 Scott Sullivan 暗示了他的同事在该公司的会计丑闻中的牵连。 |'
- en: 'Both `CountVectorizer` and `TfidfVectorizer` can be used with spaCy, for example,
    to perform lemmatization and exclude certain characters during tokenization:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer` 和 `TfidfVectorizer` 都可以与 spaCy 一起使用，例如，执行词形还原并在词项化过程中排除某些字符：'
- en: '[PRE21]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: See the notebook for additional detail and more examples.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 参见笔记本以获取更多细节和更多示例。
- en: TfidfTransformer and TfidfVectorizer
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TfidfTransformer 和 TfidfVectorizer
- en: '`TfidfTransformer` computes the TF-IDF weights from a document-term matrix
    of token counts like the one produced by `CountVectorizer`.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`TfidfTransformer` 从文档-词项矩阵中计算 TF-IDF 权重，类似于 `CountVectorizer` 生成的矩阵。'
- en: '`TfidfVectorizer` performs both computations in a single step. It adds a few
    parameters to the `CountVectorizer` API that controls the smoothing behavior.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`TfidfVectorizer` 在一个步骤中执行这两个计算。它添加了一些参数到 `CountVectorizer` API，用于控制平滑行为。'
- en: 'The TFIDF computation works as follows for a small text sample:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个小的文本样本，TFIDF 计算如下：
- en: '[PRE22]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We compute the term frequency as before:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像以前一样计算术语频率：
- en: '[PRE23]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The document frequency is the number of documents containing the token:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 文档频率是包含该标记的文档数：
- en: '[PRE24]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The TF-IDF weights are the ratio of these values:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 权重是这些值的比率：
- en: '[PRE25]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The effect of smoothing
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 平滑的效果
- en: 'To avoid zero division, `TfidfVectorizer` uses smoothing for document and term
    frequencies:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免零除法，`TfidfVectorizer` 使用平滑处理文档和术语频率：
- en: '`smooth_idf`: Adds one to document frequency, as if an extra document contained
    every token in the vocabulary, to prevent zero divisions'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`smooth_idf`: 将文档频率加一，就像额外的文档包含词汇表中的每个标记一样，以防止零除法'
- en: '`sublinear_tf`: Applies sublinear tf scaling, that is, replaces tf with 1 +
    log(tf)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sublinear_tf`: 应用亚线性 tf 缩放，即用 1 + log(tf) 替换 tf'
- en: 'In combination with normed weights, the results differ slightly:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 与规范化权重相结合，结果略有不同：
- en: '[PRE26]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Summarizing news articles using TfidfVectorizer
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 TfidfVectorizer 摘要新闻文章
- en: Due to their ability to assign meaningful token weights, TF-IDF vectors are
    also used to summarize text data. For example, Reddit's `autotldr` function is
    based on a similar algorithm. See the notebook for an example using the BBC articles.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们能够分配有意义的标记权重，TF-IDF 向量也用于总结文本数据。例如，Reddit 的 `autotldr` 函数基于类似的算法。请参阅笔记本以查看使用
    BBC 文章的示例。
- en: Key lessons instead of lessons learned
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键教训而不是已学到的教训
- en: The large number of techniques and options to process natural language for use
    in ML models corresponds to the complex nature of this highly unstructured data
    source. The engineering of good language features is both challenging and rewarding,
    and arguably the most important step in unlocking the semantic value hidden in
    text data.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 处理自然语言以在 ML 模型中使用的技术和选项数量庞大，这对应于这个高度非结构化数据源的复杂性质。构建良好的语言特征既具有挑战性又具有回报，可以说是揭示文本数据中隐藏的语义价值的最重要步骤。
- en: In practice, experience helps to select transformations that remove the noise
    rather than the signal, but it will likely remain necessary to cross-validate
    and compare the performance of different combinations of preprocessing choices.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，经验有助于选择消除噪声而不是信号的变换，但很可能仍然需要交叉验证和比较不同预处理选择组合的性能。
- en: NLP for trading
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交易的自然语言处理
- en: Once text data has been converted into numerical features using the NLP techniques
    discussed in the previous sections, text classification works just like any other
    classification task.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文本数据使用前面讨论的自然语言处理技术转换为数值特征，文本分类就像任何其他分类任务一样。
- en: In this section, we will apply these preprocessing techniques to news articles,
    product reviews, and Twitter data and teach various classifiers to predict discrete
    news categories, review scores, and sentiment polarity.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将这些预处理技术应用于新闻文章、产品评论和 Twitter 数据，并教授各种分类器以预测离散的新闻类别、评论分数和情感极性。
- en: First, we will introduce the naive Bayes model, a probabilistic classification
    algorithm that works well with the text features produced by a bag-of-words model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍朴素贝叶斯模型，这是一种与词袋模型产生的文本特征很好配合的概率分类算法。
- en: The code samples for this section are in the notebook `news_text_classification`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的代码示例位于笔记本 `news_text_classification` 中。
- en: The naive Bayes classifier
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: The naive Bayes algorithm is very popular for text classification because its
    low computational cost and memory requirements facilitate training on very large,
    high-dimensional datasets. Its predictive performance can compete with more complex
    models, provides a good baseline, and is best known for successful spam detection.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法在文本分类中非常流行，因为它的低计算成本和内存需求有助于在非常大的高维数据集上进行训练。其预测性能可以与更复杂的模型竞争，提供一个良好的基线，并以成功检测垃圾邮件而闻名。
- en: The model relies on Bayes' theorem and the assumption that the various features
    are independent of each other given the outcome class. In other words, for a given
    outcome, knowing the value of one feature (for example, the presence of a token
    in a document) does not provide any information about the value of another feature.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型依赖于贝叶斯定理和各种特征相互独立的假设。换句话说，对于给定的结果，知道一个特征的值（例如，在文档中存在一个标记）不会提供有关另一个特征值的任何信息。
- en: Bayes' theorem refresher
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝叶斯定理复习
- en: 'Bayes'' theorem expresses the conditional probability of one event (for example,
    that an email is spam as opposed to benign "ham") given another event (for example,
    that the email contains certain words) as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理表达了一个事件（例如，电子邮件是垃圾邮件而不是良性“ham”）在另一个事件（例如，电子邮件包含某些词）给定的条件概率如下：
- en: '![](img/B15439_14_002.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_002.png)'
- en: 'The **posterior** probability that an email is in fact spam given that it contains
    certain words depends on the interplay of three factors:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，电子邮件实际上是垃圾邮件的**后验**概率，而它包含某些词的事实取决于三个因素的相互作用：
- en: The **prior** probability that an email is spam
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件实际上是垃圾邮件的**先验**概率
- en: The **likelihood** of encountering these words in a spam email
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在垃圾邮件中遇到这些词的**似然**
- en: The **evidence**, that is, the probability of seeing these words in an email
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**证据**，即在电子邮件中看到这些词的概率'
- en: To compute the posterior, we can ignore the evidence because it is the same
    for all outcomes (spam versus ham), and the unconditional prior may be easy to
    compute.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算后验，我们可以忽略证据，因为对所有结果（垃圾邮件与ham）来说它是相同的，而无条件先验可能很容易计算。
- en: However, the likelihood poses insurmountable challenges for a reasonably sized
    vocabulary and a real-world corpus of emails. The reason is the combinatorial
    explosion of the words that did or did not appear jointly in different documents
    that prevent the evaluation required to compute a probability table and assign
    a value to the likelihood.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这种可能性给合理大小的词汇表和实际邮件语料库带来了不可逾越的挑战。原因在于在不同文档中联合出现或未出现的单词的组合爆炸，这阻止了计算概率表和为可能性赋值所需的评估。
- en: The conditional independence assumption
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 条件独立假设
- en: 'The key assumption to make the model both tractable and earn it the name *naive*
    is that the features are independent conditional on the outcome. To illustrate,
    let''s classify an email with the three words "Send money now" so that Bayes''
    theorem becomes the following:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 使模型既易于处理又赢得“朴素”名称的关键假设是特征在给定结果的条件下是独立的。为了说明，让我们对包含三个词“发送资金现在”的电子邮件进行分类，这样贝叶斯定理就变成了以下形式：
- en: '![](img/B15439_14_003.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_003.png)'
- en: 'Formally, the assumption that the three words are conditionally independent
    means that the probability of observing "send" is not affected by the presence
    of the other terms given that the mail is spam, that is, *P*(send | money, now,
    spam) = *P*(send | spam). As a result, we can simplify the likelihood function:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，假设这三个单词在条件上是独立的意味着观察到“发送”不受其他术语的影响，前提是邮件是垃圾邮件，即，*P*(send | money, now, spam)
    = *P*(send | spam)。因此，我们可以简化似然函数：
- en: '![](img/B15439_14_004.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_004.png)'
- en: Using the "naive" conditional independence assumption, each term in the numerator
    is straightforward to compute as relative frequencies from the training data.
    The denominator is constant across classes and can be ignored when posterior probabilities
    need to be compared rather than calibrated. The prior probability becomes less
    relevant as the number of factors, that is, features, increases.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 使用“朴素”的条件独立假设，分子中的每个术语都很容易从训练数据的相对频率中计算出来。当需要比较而不是校准后验概率时，分母在类别之间是恒定的，可以忽略。随着因素数量，即特征数量的增加，先验概率变得不太相关。
- en: In sum, the advantages of the naive Bayes model are fast training and prediction
    because the number of parameters is proportional to the number of features, and
    their estimation has a closed-form solution (based on training data frequencies)
    rather than expensive iterative optimization. It is also intuitive and somewhat
    interpretable, does not require hyperparameter tuning and is relatively robust
    to irrelevant features given sufficient signal.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，朴素贝叶斯模型的优点在于训练和预测速度快，因为参数的数量与特征的数量成比例，并且它们的估计具有封闭形式的解决方案（基于训练数据频率），而不是昂贵的迭代优化。它也是直观的并且有些可解释的，不需要超参数调整，并且在存在足够信号的情况下相对不太依赖于不相关的特征。
- en: However, when the independence assumption does not hold and text classification
    depends on combinations of features, or features are correlated, the model will
    perform poorly.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当独立性假设不成立，并且文本分类依赖于特征的组合或特征相关时，模型的性能将较差。
- en: Classifying news articles
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对新闻文章进行分类
- en: 'We start with an illustration of the naive Bayes model for news article classification
    using the BBC articles that we read before to obtain a `DataFrame` with 2,225
    articles from five categories:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从用于新闻文章分类的朴素贝叶斯模型的示例开始，使用之前阅读的BBC文章，以获得包含来自五个类别的2,225篇文章的`DataFrame`：
- en: '[PRE27]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To train and evaluate a multinomial naive Bayes classifier, we split the data
    into the default 75:25 train-test set ratio, ensuring that the test set classes
    closely mirror the train set:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练和评估多项式朴素贝叶斯分类器，我们将数据分成默认的75:25训练测试集比例，确保测试集类别与训练集类别紧密匹配：
- en: '[PRE28]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We proceed to learn the vocabulary from the training set and transform both
    datasets using `CountVectorizer` with default settings to obtain almost 26,000
    features:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续从训练集中学习词汇，并使用默认设置的`CountVectorizer`转换两个数据集，以获得近26,000个特征：
- en: '[PRE29]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Training and prediction follow the standard sklearn fit/predict interface:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和预测遵循标准的sklearn fit/predict接口：
- en: '[PRE30]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We evaluate the multiclass predictions using `accuracy` to find the default
    classifier achieved an accuracy of almost 98 percent:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`accuracy`评估多类预测，以找到默认分类器几乎达到了98％的准确率：
- en: '[PRE31]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Sentiment analysis with Twitter and Yelp data
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用Twitter和Yelp数据进行情感分析
- en: Sentiment analysis is one of the most popular uses of NLP and ML for trading
    because positive or negative perspectives on assets or other price drivers are
    likely to impact returns.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是NLP和ML在交易中最流行的用途之一，因为对资产或其他价格驱动因素的积极或消极看法可能会影响收益。
- en: Generally, modeling approaches to sentiment analysis rely on dictionaries (as
    does the TextBlob library) or models trained on outcomes for a specific domain.
    The latter is often preferable because it permits more targeted labeling, for
    example, by tying text features to subsequent price changes rather than indirect
    sentiment scores.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，情感分析的建模方法依赖于词典（如TextBlob库）或针对特定领域结果进行训练的模型。后者通常更可取，因为它允许更有针对性的标记，例如，通过将文本特征与后续价格变化而不是间接情感分数相关联。
- en: We will illustrate ML for sentiment analysis using a Twitter dataset with binary
    polarity labels and a large Yelp business review dataset with a five-point outcome
    scale.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用具有二进制极性标签的Twitter数据集和具有五点结果量表的大型Yelp商业评论数据集来说明情感分析的ML。
- en: Binary sentiment classification with Twitter data
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用Twitter数据进行二进制情感分类
- en: We use a dataset that contains 1.6 million training and 350 test tweets from
    2009 with algorithmically assigned binary positive and negative sentiment scores
    that are fairly evenly split (see the notebook for more detailed data exploration).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个包含来自2009年的1.6百万训练推文和350个测试推文的数据集，该数据集具有算法分配的二进制积极和消极情感分数，这些分数分布相对均匀（有关更详细的数据探索，请参阅笔记本）。
- en: Multinomial naive Bayes
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯
- en: 'We create a document-term matrix with 934 tokens as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个具有934个标记的文档-术语矩阵如下：
- en: '[PRE32]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We then train the `MultinomialNB` classifier as before and predict the test
    set:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们像以前一样训练`MultinomialNB`分类器并预测测试集：
- en: '[PRE33]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The result has over 77.5 percent accuracy:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的准确率超过了77.5％：
- en: '[PRE34]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Comparison with TextBlob sentiment scores
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与TextBlob情感分数的比较
- en: We also obtain TextBlob sentiment scores for the tweets and note (see the left
    panel in *Figure 14.5*) that positive test tweets receive a significantly higher
    sentiment estimate. We then use the `MultinomialNB` model's `.predict_proba()`
    method to compute predicted probabilities and compare both models using the respective
    area **under the curve**, or **AUC**, that we introduced in *Chapter 6*, *The
    Machine Learning Process* (see the right panel in *Figure 14.5*).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还为推文获取TextBlob情感分数，并注意（参见*图14.5*中的左面板），积极的测试推文收到了显着较高的情感估计。然后，我们使用`MultinomialNB`模型的`.predict_proba()`方法计算预测概率，并使用我们在*第6章*“机器学习过程”中介绍的相应曲线下面积**AUC**来比较两个模型（参见*图14.5*中的右面板）。
- en: '![](img/B15439_14_05.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_05.png)'
- en: 'Figure 14.5: Accuracy of custom versus generic sentiment scores'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5：定制与通用情感分数的准确性
- en: The custom naive Bayes model outperforms TextBlob in this case, achieving a
    test AUC of 0.848 compared to 0.825 for TextBlob.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，定制的朴素贝叶斯模型优于TextBlob，测试AUC为0.848，而TextBlob为0.825。
- en: Multiclass sentiment analysis with Yelp business reviews
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Yelp商业评论的多类情感分析
- en: Finally, we apply sentiment analysis to the significantly larger Yelp business
    review dataset with five outcome classes (see the notebook `sentiment_analysis_yelp`
    for code and additional details).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将情感分析应用于规模大得多的 Yelp 企业评论数据集，其中有五个结果类别（有关代码和其他详细信息，请参见笔记本 `sentiment_analysis_yelp`）。
- en: The data consists of several files with information on the business, the user,
    the review, and other aspects that Yelp provides to encourage data science innovation.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包含了有关企业、用户、评论以及 Yelp 提供的其他方面的信息，以鼓励数据科学创新。
- en: We will use around six million reviews produced over the 2010-2018 period (see
    the notebook for details). The following figure shows the number of reviews and
    the average number of stars per year, as well as the star distribution across
    all reviews.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在2010-2018年期间产生的约六百万条评论（详见笔记本）。以下图表显示了每年的评论数量和平均星级，以及所有评论中星级的分布。
- en: '![](img/B15439_14_06.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_06.png)'
- en: 'Figure 14.6: Basic exploratory analysis of Yelp reviews'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6：Yelp 评论的基本探索性分析
- en: We will train various models on a 10 percent sample of the data through 2017
    and use the 2018 reviews as the test set. In addition to the text features resulting
    from the review texts, we will also use other information submitted with the review
    about the given user.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在截至2017年的数据的10%样本上训练各种模型，并使用2018年的评论作为测试集。除了评论文本生成的文本特征外，我们还将使用有关给定用户的评论提交的其他信息。
- en: Combining text and numerical features
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结合文本和数值特征
- en: The dataset contains various numerical features (see notebook for implementation
    details).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含各种数值特征（有关实现细节，请参见笔记本）。
- en: The vectorizers produce `scipy.sparse` matrices. To combine the vectorized text
    data with other features, we need to first convert these to sparse matrices as
    well; many sklearn objects and other libraries such as LightGBM can handle these
    very memory-efficient data structures. Converting the sparse matrix to a dense
    NumPy array risks memory overflow.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化器产生 `scipy.sparse` 矩阵。要将向量化的文本数据与其他特征结合起来，我们首先需要将其转换为稀疏矩阵；许多 sklearn 对象和其他库（如
    LightGBM）可以处理这些非常节省内存的数据结构。将稀疏矩阵转换为稠密的 NumPy 数组会有内存溢出的风险。
- en: Most variables are categorical, so we use one-hot encoding since we have a fairly
    large dataset to accommodate the increase in features.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数变量都是分类变量，因此由于我们有一个相当大的数据集来容纳特征的增加，我们使用一位有效编码。
- en: 'We convert the encoded numerical features and combine them with the document-term
    matrix:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编码的数值特征转换并与文档-词矩阵相结合：
- en: '[PRE35]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Benchmark accuracy
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准准确率
- en: 'Using the most frequent number of stars (=5) to predict the test set achieves
    an accuracy close to 52 percent:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最频繁的星级数（=5）来预测测试集，准确率接近52%：
- en: '[PRE36]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Multinomial naive Bayes model
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯模型
- en: Next, we train a naive Bayes classifier using a document-term matrix produced
    by `CountVectorizer` with default settings.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用由 `CountVectorizer` 生成的文档-词矩阵来训练一个朴素贝叶斯分类器，其采用默认设置。
- en: '[PRE37]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The prediction produces 64.7 percent accuracy on the test set, a 24.4 percent
    improvement over the benchmark:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 预测在测试集上的准确率达到了64.7%，比基准提高了24.4%：
- en: '[PRE38]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Training with the combination of text and other features improves the test accuracy
    to 0.671.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本和其他特征的组合进行训练将测试准确率提高到0.671。
- en: Logistic regression
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: In *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*, we
    introduced binary logistic regression. sklearn also implements a multiclass model
    with a multinomial and a one-versus-all training option, where the latter trains
    a binary model for each class while considering all other classes as the negative
    class. The multinomial option is much faster and more accurate than the one-versus-all
    implementation.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第7章*，*线性模型-从风险因素到回报预测*中，我们介绍了二元逻辑回归。 sklearn 还实现了一个多类别模型，具有多项式和一对所有训练选项，后者训练一个针对每个类别的二元模型，同时将所有其他类别视为负类。多项式选项比一对所有实现要快得多且更准确。
- en: 'We evaluate a range of values for the regularization parameter `C` to identify
    the best performing model, using the `lbfgs` solver as follows (see the sklearn
    documentation for details):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估正则化参数 `C` 的一系列值，以确定表现最佳的模型，使用 `lbfgs` 求解器如下（有关详细信息，请参见 sklearn 文档）：
- en: '[PRE39]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '*Figure 14.7* shows the plots of the validation results.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14.7* 显示了验证结果的图表。'
- en: Multiclass gradient boosting with LightGBM
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 LightGBM 的多类别梯度提升
- en: 'For comparison, we also train a LightGBM gradient boosting tree ensemble with
    default settings and a `multiclass` objective:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行比较，我们还训练了一个具有默认设置和`multiclass`目标的LightGBM梯度提升树集成：
- en: '[PRE40]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Predictive performance
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预测性能
- en: '*Figure 14.7* displays the accuracy of each model for the combined data. The
    right panel plots the validation performance for the logistic regression models
    for both datasets and different levels of regularization.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.7*显示了每个模型对于组合数据的准确性。右侧面板绘制了逻辑回归模型在两个数据集和不同正则化水平下的验证性能。'
- en: Multinomial logistic regression performs best with a test accuracy slightly
    above 74 percent. Naive Bayes performs significantly worse. The default LightGBM
    settings did not improve over the linear model with an accuracy of 0.736\. However,
    we could tune the hyperparameters of the gradient boosting model and may well
    see performance improvements that put it at least on par with logistic regression.
    Either way, the result serves as a reminder not to discount simple, regularized
    models as they may deliver not only good results, but also do so quickly.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式逻辑回归的测试准确性略高于74％。朴素贝叶斯的表现明显较差。默认的LightGBM设置并未提高线性模型的准确性为0.736\. 但是，我们可以调整梯度提升模型的超参数，并且很可能会看到性能提升，使其至少与逻辑回归持平。无论哪种方式，该结果都提醒我们不要低估简单的、正则化的模型，因为它们不仅可能会产生良好的结果，而且可能会迅速做到这一点。
- en: '![](img/B15439_14_07.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_07.png)'
- en: 'Figure 14.7: Test performance on combined data (all models, left) and for logistic
    regression with varying regularization'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7：组合数据的测试性能（所有模型，左）以及逻辑回归的不同正则化下的性能
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored numerous techniques and options to process unstructured
    data with the goal of extracting semantically meaningful numerical features for
    use in ML models.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了许多处理非结构化数据的技术和选项，目的是提取用于机器学习模型的语义上有意义的数值特征。
- en: We covered the basic tokenization and annotation pipeline and illustrated its
    implementation for multiple languages using spaCy and TextBlob. We built on these
    results to build a document model based on the bag-of-words model to represent
    documents as numerical vectors. We learned how to refine the preprocessing pipeline
    and then used the vectorized text data for classification and sentiment analysis.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们涵盖了基本的分词和注释流水线，并使用spaCy和TextBlob说明了它在多种语言中的实现。我们建立在这些结果上，构建了一个基于词袋模型的文档模型，将文档表示为数值向量。我们学习了如何优化预处理流水线，然后使用向量化的文本数据进行分类和情感分析。
- en: We have two more chapters on alternative text data. In the next chapter, we
    will learn how to summarize texts using unsupervised learning to identify latent
    topics. Then, in *Chapter 16*, *Word Embeddings for Earnings Calls and SEC Filings*,
    we will learn how to represent words as vectors that reflect the context of word
    usage, a technique that has been used very successfully to provide richer text
    features for various classification tasks.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有两章关于替代文本数据。在下一章中，我们将学习如何使用无监督学习总结文本以识别潜在主题。然后，在*第16章*中，*收益电话和SEC备案的词嵌入*，我们将学习如何将单词表示为反映单词用法上下文的向量，这是一种非常成功的技术，为各种分类任务提供了更丰富的文本特征。
