- en: The Machine Learning Process
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will start to illustrate how you can use a broad range
    of supervised and unsupervised **machine learning** (**ML**) models for algorithmic
    trading. We will explain each model''s assumptions and use cases before we demonstrate
    relevant applications using various Python libraries. The categories of models
    will include:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Linear models for the regression and classification of cross-section, time series,
    and panel data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalized additive models, including non-linear tree-based models, such as
    **decision trees**
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble models, including random forest and gradient-boosting machines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised linear and nonlinear methods for dimensionality reduction and clustering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network models, including recurrent and convolutional architectures
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will apply these models to the market, fundamental, and alternative data
    sources introduced in the first part of this book. We will further build on the
    material covered so far by showing you how to embed these models in an algorithmic
    trading strategy to generate or combine alpha factors or to optimize the portfolio-management
    process and evaluate their performance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: There are several aspects that many of these models and their uses have in common.
    This chapter covers these common aspects so that we can focus on model-specific
    usage in the following chapters. They include the overarching goal of learning
    a functional relationship from data by optimizing an objective or loss function.
    They also include the closely related methods of measuring model performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: We distinguish between unsupervised and supervised learning and supervised regression
    and classification problems, and outline use cases for algorithmic trading. We
    contrast the use of supervised learning for statistical inference of relationships
    between input and output data with the use for the prediction of future outputs
    from future inputs. We also illustrate how prediction errors are due to the model's
    bias or variance, or because of a high noise-to-signal ratio in the data. Most
    importantly, we present methods to diagnose sources of errors and improve your
    model's performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: How supervised and unsupervised learning using data works
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to apply the ML workflow
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to formulate loss functions for regression and classification
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train and evaluate supervised learning models
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the bias-variance trade-off impacts prediction errors
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to diagnose and address prediction errors
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train a model using cross-validation to manage the bias-variance trade-off
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement cross-validation using scikit-learn
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why the nature of financial data requires different approaches to out-of-sample
    testing
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are already quite familiar with ML, feel free to skip ahead and dive
    right into learning how to use linear models to produce and combine alpha factors
    for an algorithmic trading strategy.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Learning from data
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There have been many definitions of ML, which all revolve around the automated
    detection of meaningful patterns in data. Two prominent examples include:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ML有许多定义，它们都围绕数据中有意义的模式的自动检测。两个著名的例子包括：
- en: AI pioneer **Arthur Samuelson** defined ML in 1959 as a subfield of computer
    science that gives computers the ability to learn without being explicitly programmed.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI先驱**亚瑟·塞缪尔森**在1959年将ML定义为计算机科学的一个子领域，赋予计算机在没有明确编程的情况下学习的能力。
- en: '**Toni Mitchell**, one of the current leaders in the field, pinned down a well-posed
    learning problem more specifically in 1998: a computer program learns from experience
    with respect to a task and a performance measure whether the performance of the
    task improves with experience.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**托尼·米切尔**，该领域的一位当前领军人物，1998年更加具体地确定了一个明确定义的学习问题：一个计算机程序根据任务和绩效指标从经验中学习，看任务的绩效是否随经验提高而提高。'
- en: Experience is presented to an algorithm in the form of training data. The principal
    difference to previous attempts at building machines that solve problems is that
    the rules that an algorithm uses to make decisions are learned from the data as
    opposed to being programmed or hard-coded—this was the case for expert systems
    prominent in the 1980s.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 经验以训练数据的形式呈现给算法。与以往试图构建解决问题的机器的尝试的主要区别在于，算法用于做出决策的规则是从数据中学习的，而不是被编程或硬编码——这是上世纪80年代突出的专家系统的情况。
- en: The key challenge of automated learning is to identify patterns in the training
    data that are meaningful when generalizing the model's learning to new data. There
    are a large number of potential patterns that a model could identify, while the
    training data only constitute a sample of the larger set of phenomena that the
    algorithm needs to perform the task in the future. The infinite number of functions
    that could generate the given outputs from the given input make the search process
    impossible to solve without restrictions on the eligible set of functions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自动学习的关键挑战在于识别训练数据中的模式，在将模型的学习泛化到新数据时这些模式是有意义的。模型可能识别的潜在模式数量庞大，而训练数据仅构成了算法未来执行任务所需的更大现象集合的一部分样本。可能从给定输入生成给定输出的函数数量无限，这使得搜索过程在没有对可接受函数集合施加限制的情况下无法解决。
- en: The types of patterns that an algorithm is capable of learning are limited by
    the size of its hypothesis spaceon the one hand and the amount of information
    contained in the sample data on the other. The size of the hypothesis space varies
    significantly between algorithms. On the one hand, this limitation enables a successful
    search and on the other hand, it implies an inductive bias as the algorithm generalizes
    from the training sample to new data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 算法能够学习的模式类型受到其假设空间的大小以及样本数据中包含的信息量的限制。假设空间的大小在算法之间有很大的变化。一方面，这种限制使得成功的搜索成为可能，另一方面，它暗示了归纳偏见，因为算法从训练样本推广到新数据。
- en: Hence, the key challenge becomes a matter of how to choose a model with a hypothesis
    space large enough to contain a solution to the learning problem, yet small enough
    to ensure reliable generalization given the size of the training data. With more
    and more informative data, a model with a larger hypothesis space will be successful.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，关键挑战变成了如何选择具有足够大的假设空间以包含学习问题解决方案的模型，同时又足够小以确保在给定训练数据大小的情况下可靠的泛化。随着越来越多的信息化数据，具有更大假设空间的模型将会成功。
- en: The **no-free-lunch theorem** states that there is no universal learning algorithm.
    Instead, a learner's hypothesis space has to be tailored to a specific task using
    prior knowledge about the task domain in order for the search of meaningful patterns
    to succeed. We will pay close attention to the assumptions that a model makes
    about data relationships for a specific task throughout this chapter, and emphasize
    the importance of matching these assumptions with empirical evidence gleaned from
    data exploration. The process required to master the task can be differentiated
    into supervised, unsupervised, and reinforcement learning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**无免费午餐定理**指出没有通用的学习算法。相反，学习者的假设空间必须根据关于任务领域的先验知识来定制，以便搜索有意义的模式成功。在本章中，我们将密切关注模型对特定任务的数据关系所做的假设，并强调通过数据探索获得的经验证据与这些假设匹配的重要性。掌握任务所需的过程可以区分为监督学习、无监督学习和强化学习。'
- en: Supervised learning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: Supervised learning is the most commonly used type of ML. We will dedicate most
    of the chapters in this book to learning about the various applications of models
    in this category. The term *supervised* implies the presence of an outcome variable
    that guides the learning process—that is, it teaches the algorithm the correct
    solution to the task that is being learned. Supervised learning aims at generalizing
    a functional relationship between input and output data that is learned from individual
    samples and applying it to new data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是最常用的机器学习类型。本书的大部分章节将致力于学习该类别中模型的各种应用。术语*监督*意味着有一个结果变量指导学习过程，即，它教会算法任务的正确解决方案正在学习。监督学习旨在将从个别样本学到的输入和输出数据之间的功能关系泛化，并将其应用于新数据。
- en: The output variable is also, depending on the field, interchangeably called
    the label, target, outcome, endogenous, or left-hand-side variable. We will use
    *y[i]* for observations *i = 1, ..., N*, or *y* in vector notation. Some tasks
    are represented by several outcomes, also called **multilabel problems**. The input
    data for a supervised learning problem is also known as features, exogenous, and
    right-hand-side variables, denoted by an *x*[*i* ]for a vector of features for observations
    *i = 1, ..., N*, or *X* in matrix notation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 输出变量也可根据领域不同交替称为标签、目标、结果、内生或左手变量。我们将对观测值*i = 1, ..., N*使用*y[i]*，或者在向量表示中使用*y*。某些任务由多个结果表示，也称为**多标签问题**。监督学习问题的输入数据也称为特征、外生和右手变量，用*x*[*i* ]表示*i
    = 1, ..., N*的一系列特征，或者在矩阵表示中使用*X*。
- en: The solution to a supervised learning problem is a function (![](img/3bd38acb-3634-4212-894b-50e191c3f15c.png)) that
    represents what the model learned about the input-output relationship from the
    sample and approximates the true relationship, represented with ![](img/26f61bd2-e8e2-4a3d-a5ae-196b9ae3e691.png).
    This function can be used to infer statistical associations or potentially even
    causal relationships among variables of interest beyond the sample, or it can
    be used to predict outputs for new input data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 解决监督学习问题的解决方案是一个函数（![](img/3bd38acb-3634-4212-894b-50e191c3f15c.png)），它表示模型从样本中学到的输入-输出关系，并逼近真实关系，用![](img/26f61bd2-e8e2-4a3d-a5ae-196b9ae3e691.png)表示。这个函数可以用来推断变量之间的统计关系，甚至可能是因果关系，超出样本范围，或者用来预测新输入数据的输出。
- en: 'Both goals face an important trade-off: more complex models have more *moving
    parts* that are capable of representing more nuanced relationships, but they may
    also be more difficult to inspect. They are also likely to overfit and learn random
    noise particular to the training sample, as opposed to a systematic signal that
    represents a general pattern of the input-output relationship. Overly simple models,
    on the other hand, will miss signals and deliver biased results. This trade-off
    is known as the **bias-variance trade-off** in supervised learning, but conceptually
    this also applies to the other forms of ML where overly complex models may perform
    poorly beyond the training data.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 两个目标面临着一个重要的权衡：更复杂的模型有更多*移动部件*，能够表示更微妙的关系，但也可能更难以检查。它们也可能过度拟合并学习到训练样本特有的随机噪音，而不是代表输入-输出关系一般模式的系统信号。另一方面，过于简单的模型会忽略信号并产生偏倚的结果。这种权衡在监督学习中被称为**偏差-方差权衡**，但概念上这也适用于其他形式的机器学习，过于复杂的模型可能在训练数据之外表现不佳。
- en: Unsupervised learning
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: When solving an unsupervised learning problem, we only observe the features
    and have no measurements of the outcome. Instead of the prediction of future outcomes
    or the inference of relationships among variables, the task is to find structure
    in the data without any outcome information to guide the search process.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决无监督学习问题时，我们只观察特征，没有测量结果。任务不是预测未来结果或推断变量之间的关系，而是在没有任何结果信息指导搜索过程的情况下，在数据中找到结构。
- en: Often, unsupervised algorithms aim to learn a new representation of the input
    data that is useful for some other tasks. This includes coming up with labels
    that identify commonalities among observations, or a summarized description that
    captures relevant information while requiring data points or features. Unsupervised
    learning algorithms also differ from supervised learning algorithms in the assumptions
    they make about the nature of the structure they are aiming to discover.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，无监督算法旨在学习输入数据的新表示，这对其他任务是有用的。这包括提出标签以识别观察结果之间的共性，或者是一个总结性描述，捕捉到相关信息，同时需要数据点或特征。无监督学习算法在他们试图发现的结构的性质方面也不同于监督学习算法所做出的假设。
- en: Applications
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: 'There are several helpful uses of unsupervised learning that can be applied
    to algorithmic trading, including the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习有几种有用的用途可应用于算法交易，包括以下内容：
- en: Grouping together securities with similar risk and return characteristics (see
    hierarchical risk parity in this chapter (which looks at portfolio optimization))
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将具有相似风险和回报特征的证券分组在一起（请参阅本章的分层风险平价（这是关于投资组合优化的））
- en: Finding a small number of risk factors driving the performance of a much larger
    number of securities
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到驱动大量证券表现的少数风险因素
- en: Identifying trading and price patterns that differ systematically and may pose
    higher risks
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别系统地不同的交易和价格模式，可能具有较高的风险
- en: Identifying latent topics in a body of documents (for example, earnings call
    transcripts) that comprise the most important aspects of those documents
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别文档体系（例如，收益电话转录）中的潜在主题，这些主题包括那些文档中最重要的方面
- en: At a high level, these applications rely on methods to identify clusters and
    methods to reduce the dimensionality of the data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，这些应用依赖于识别聚类的方法和降低数据维度的方法。
- en: Cluster algorithms
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法
- en: Cluster algorithms use a measure of similarity to identify observations or data
    attributes that contain similar information. They summarize a dataset by assigning
    a large number of data points to a smaller number of clusters so that the cluster
    members are more closely related to each other than to members of other clusters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法使用相似性度量来识别包含相似信息的观察或数据属性。它们通过将大量数据点分配给较少数量的聚类来概括数据集，以使聚类成员彼此之间的关系比与其他聚类成员更密切。
- en: 'Cluster algorithms primarily differ with respect to the type of clusters that
    they will produce, which implies different assumptions about the data generation
    process, listed as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法主要在于它们将产生何种类型的聚类，这意味着对数据生成过程有不同的假设，列举如下：
- en: '**K-means clustering**: Data points belong to one of the *k* clusters of equal
    size that take an elliptical form'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K均值聚类**：数据点属于相等大小的*k*个椭圆形聚类之一'
- en: '**Gaussian mixture models**: Data points have been generated by any of the
    various multivariate normal distributions'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯混合模型**：数据点是由任何各种多元正态分布生成的'
- en: '**Density-based clusters**: Clusters are of an arbitrary shape and are defined
    only by the existence of a minimum number of nearby data points'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于密度的聚类**：聚类具有任意形状，并且仅由附近数据点的最小数量的存在来定义'
- en: '**Hierarchical clusters**: Data points belong to various supersets of groups
    that are formed by successively merging smaller clusters'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次聚类**：数据点属于由逐渐合并较小聚类形成的各种超集组'
- en: Dimensionality reduction
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: Dimensionality reduction produces new data that captures the most important
    information contained in the source data. Rather than grouping existing data into
    clusters, these algorithms transform existing data into a new dataset that uses
    significantly fewer features or observations to represent the original information.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 降维生成新的数据，捕捉源数据中包含的最重要信息。这些算法不是将现有数据分组成聚类，而是将现有数据转换为使用显著较少的特征或观察来表示原始信息的新数据集。
- en: 'Algorithms differ with respect to the nature of the new dataset they will produce,
    as shown in the following list:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在生成新数据集的性质上存在差异，如下列表所示：
- en: '**Principal component analysis (PCA)**: Finds the linear transformation that
    captures most of the variance in the existing dataset'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析（PCA）**：找到捕获现有数据集中大部分方差的线性变换'
- en: '**Manifold learning**: Identifies a nonlinear transformation that produces
    a lower-dimensional representation of the data'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流形学习**：确定一个非线性转换，产生数据的低维表示'
- en: '**Autoencoders**: Uses neural networks to compress data non-linearly with minimal
    loss of information'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自编码器**：使用神经网络以最小的信息损失对数据进行非线性压缩'
- en: We will dive deeper into linear, non-linear, and neural-network-based unsupervised
    learning models in several of the following chapters, including important applications of
    **natural language processing** (**NLP**) in the form of topic modeling and Word2vec
    feature extraction.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几章中更深入地探讨线性、非线性和基于神经网络的无监督学习模型，包括**自然语言处理**（**NLP**）在主题建模和Word2vec特征提取形式的重要应用。
- en: Reinforcement learning
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: '**Reinforcement learning** is the third type of ML. It aims to choose the action
    that yields the highest reward, given a set of input data that describes a context
    or environment. It is both dynamic and interactive: the stream of positive and
    negative rewards impacts the algorithm''s learning, and actions taken now may
    influence both the environment and future rewards.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**是第三种ML类型。它旨在选择在给定描述上下文或环境的输入数据集的情况下产生最高奖励的行动。它既动态又交互：正面和负面奖励的流影响算法的学习，现在采取的行动可能影响环境和未来奖励。'
- en: The trade-off between the exploitation of a course of action that has been learned
    to yield a certain reward and the exploration of new actions that may increase
    the reward in the future gives rise to a trial-and-error approach. Reinforcement
    learning optimizes the agent's learning using dynamical systems theory and, in
    particular, the optimal control of Markov decision processes with incomplete information.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在已学习到可以产生某种奖励的行动的开发和可能增加未来奖励的新行动之间的权衡导致了一种*试错*方法。强化学习使用动态系统理论，特别是使用马尔可夫决策过程的最优控制来优化代理的学习，其信息不完全。
- en: Reinforcement learning differs from supervised learning, where the available
    training data lays out both the context and the correct decision for the algorithm.
    It is tailored to interactive settings where the outcomes only become available
    over time and learning has to proceed in an *online* or continuous fashion as
    the agent acquires new experience. However, some of the most notable progress
    in **Artificial Intelligence** (**AI**) involves reinforcement that uses deep
    learning to approximate functional relationships between actions, environments,
    and future rewards. It also differs from unsupervised learning because feedback
    of the consequences will be available, albeit with a delay.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习与监督学习不同，监督学习中训练数据既提供了上下文又提供了算法的正确决策。它专为交互式设置设计，其中结果只在一段时间后才可用，并且学习必须以*在线*或连续的方式进行，因为代理获取新经验。然而，人工智能**AI**中一些最显著的进展涉及使用深度学习来近似动作、环境和未来奖励之间的功能关系的强化学习。它还与无监督学习不同，因为反馈的后果将可用，尽管会有延迟。
- en: Reinforcement learning is particularly suitable for algorithmic trading because
    the concept of a return-maximizing agent in an uncertain, dynamic environment
    has much in common with an investor or a trading strategy that interacts with
    financial markets. This approach has been successfully applied to game-playing
    agents, most prominently to the game of Go, but also to complex video games. It
    is also used in robotics—for example, self-driving cars—or to personalize services
    such as website offerings based on user interaction. We will introduce reinforcement
    learning approaches to building an algorithmic trading strategy in Chapter 21,
    *Reinforcement Learning*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习特别适用于算法交易，因为在不确定、动态环境中追求最大化回报的代理概念与与金融市场互动的投资者或交易策略有很多共同之处。这种方法已成功应用于游戏代理，尤其是围棋，但也用于复杂的视频游戏。它还用于机器人技术——例如，自动驾驶汽车——或根据用户交互个性化服务，例如网站提供的内容。我们将在第21章介绍强化学习方法，用于构建算法交易策略，*强化学习*。
- en: The machine learning workflow
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工作流程
- en: Developing a ML solution for an algorithmic trading strategy requires a systematic
    approach to maximize the chances of success while economizing on resources. It
    is also very important to make the process transparent and replicable in order
    to facilitate collaboration, maintenance, and later refinements.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为算法交易策略开发一个机器学习解决方案需要系统化的方法，以最大化成功的机会，同时节约资源。使过程透明且可复制非常重要，以便促进协作、维护和后续的改进。
- en: 'The following chart outlines the key steps from problem definition to the deployment
    of a predictive solution:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下图概述了从问题定义到部署预测解决方案的关键步骤：
- en: '![](img/7e355379-9a0d-4652-94f1-55e430698c19.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e355379-9a0d-4652-94f1-55e430698c19.png)'
- en: 'The process is iterative throughout the sequence, and the effort required at
    different stages will vary according to the project, but this process should generally
    include the following steps:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程在整个序列中是迭代的，不同阶段需要的工作量会根据项目而变化，但这个过程通常应包括以下步骤：
- en: Frame the problem, identify a target metric, and define success
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定问题框架，确定目标指标，并定义成功。
- en: Source, clean, and validate the data
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 源，清理和验证数据。
- en: Understand your data and generate informative features
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解你的数据并生成信息性特征。
- en: Pick one or more machine learning algorithms suitable for your data
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个或多个适合你的数据的机器学习算法。
- en: Train, test, and tune your models
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练，测试和调整你的模型。
- en: Use your model to solve the original problem
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你的模型解决原始问题。
- en: We will walk through these steps in the following sections using a simple example
    to illustrate some of the key points.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中通过一个简单的例子来演示这些步骤，以阐明一些关键点。
- en: Basic walkthrough – k-nearest neighbors
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本步骤- k最近邻。
- en: The `machine_learning_workflow.ipynb`notebook in this chapter's folder of the
    book's GitHub repository contains several examples that illustrate the machine
    learning workflow using a dataset of house prices.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本章书籍的GitHub存储库中的`machine_learning_workflow.ipynb`笔记本包含了几个示例，演示了使用房价数据集进行机器学习工作流程。
- en: We will use the fairly straightforward **k-nearest neighbors** (**KNN**) algorithm
    that allows us to tackle both regression and classification problems.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相当简单的**k最近邻**（**KNN**）算法，它允许我们解决回归和分类问题。
- en: In its default `sklearn` implementation, it identifies the `k` nearest data
    points (based on the Euclidean distance) to make a prediction. It predicts the
    most frequent class among the neighbors or the average outcome in the classification
    or regression case, respectively.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在其默认的`sklearn`实现中，它识别出与欧几里德距离最近的`k`个数据点来进行预测。在分类或回归情况下，它分别预测邻居中最频繁的类别或平均结果。
- en: Frame the problem – goals and metrics
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定问题框架-目标和指标。
- en: The starting point for any machine learning exercise is the ultimate use case
    it aims to address. Sometimes, this goal will be statistical inference in order
    to identify an association between variables or even a causal relationship. Most
    frequently, however, the goal will be the direct prediction of an outcome to yield
    a trading signal.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习练习的起点都是其旨在解决的最终用例。有时，这个目标将是统计推断，以识别变量之间的关联或甚至因果关系。然而，最常见的目标通常是直接预测结果以产生交易信号。
- en: 'Both inference and prediction use metrics to evaluate how well a model achieves
    its objective. We will focus on common objective functions and the corresponding
    error metrics for predictive models that can be distinguished by the variable
    type of the output: continuous output variables imply a regression problem, categorical
    variables imply classification, and the special case of ordered categorical variables
    implies ranking problems.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 推断和预测都使用指标来评估模型实现其目标的程度。我们将重点放在常见的目标函数和相应的误差度量上，用于可以通过输出变量类型来区分的预测模型：连续输出变量意味着回归问题，分类变量意味着分类问题，而有序分类变量的特殊情况则意味着排名问题。
- en: The problem may be the efficient combination of several alpha factors and could
    be framed as a regression problem that aims to predict returns, a binary classification
    problem that aims to predict the direction of future price movements, or a multiclass
    problem that aims to assign stocks to various performance classes. In the following
    section, we will introduce these objectives and look at how to measure and interpret
    related error metrics.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 问题可能是多个 alpha 因子的高效组合，并且可以构建为一个回归问题，旨在预测回报，一个旨在预测未来价格走势方向的二元分类问题，或者一个旨在将股票分配到不同绩效类别的多类问题。在接下来的部分中，我们将介绍这些目标，并探讨如何衡量和解释相关的误差度量。
- en: Prediction versus inference
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测与推断的对比
- en: The functional relationship produced by a supervised learning algorithm can
    be used for inference—that is, to gain insights into how the outcomes are generated—or
    for prediction—that is, to generate accurate output estimates (represented by ![](img/ff88b978-e72b-4c0c-b9a9-c837a8f06f32.png)) for unknown
    or future inputs (represented by *X*).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由监督学习算法产生的功能关系可用于推断——即获得有关结果生成方式的见解——或用于预测——即为未知或未来输入生成准确的输出估计（由![](img/ff88b978-e72b-4c0c-b9a9-c837a8f06f32.png)表示），用*X*表示未来输入）。
- en: For algorithmic trading, inference can be used to estimate the causal or statistical
    dependence of the returns of an asset on a risk factor, whereas prediction can
    be used to forecast the risk factor. Combining the two can yield a prediction
    of the asset price, which in turn can be translated into a trading signal.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于算法交易，推断可用于估计资产回报对风险因素的因果或统计依赖性，而预测可用于预测风险因素。将两者结合起来可以产生资产价格的预测，进而可以转化为交易信号。
- en: Statistical inference is about drawing conclusions from sample data about the
    parameters of the underlying probability distribution or the population. Potential
    conclusions include hypothesis tests about the characteristics of the distribution
    of an individual variable, or the existence or strength of numerical relationships
    among variables. They also include point or interval estimates of statistical
    metrics.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 统计推断涉及从样本数据中推断出关于潜在概率分布或总体参数的结论。可能的结论包括有关个别变量分布特征的假设检验，或关于变量之间数值关系的存在或强度的假设检验。它们还包括统计指标的点估计或区间估计。
- en: Inference depends on the assumptions about the process that generates the data
    in the first place. We will review these assumptions and the tools that are used
    for inference with linear models where they are well established. More complex
    models make fewer assumptions about the structural relationship between input
    and output, and instead approach the task of function approximation more openly
    while treating the data-generating process as a black box. These models, including
    decision trees, ensemble models, and neural networks, are focused on and often
    outperform when used for prediction tasks. However, random forests have recently
    gained a framework for inference that we will introduce later.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '推断取决于首次生成数据的过程的假设。我们将审查这些假设以及用于线性模型推断的工具，其中它们被很好地确立。更复杂的模型对输入和输出之间的结构关系做出较少的假设，而是更开放地处理函数逼近任务，同时将数据生成过程视为黑盒。这些模型，包括决策树、集成模型和神经网络，在用于预测任务时专注并经常优于其他模型。然而，随机森林最近获得了一种我们将在后面介绍的推断框架。 '
- en: Causal inference
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 因果推断
- en: Causal inference aims to identify relationships so that certain input values
    imply certain outputs—for example, a certain constellation of macro variables
    causing the price of a given asset to move in a certain way, assuming all other
    variables remain constant.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因果推断旨在识别关系，以便某些输入值暗示某些输出值，例如，一定数量的宏观变量组合导致特定资产价格以某种方式变化，假设所有其他变量保持不变。
- en: Statistical inference about relationships among two or more variables produces
    measures of correlation that can only be interpreted as a causal relationship
    when several other conditions are met—for example, when alternative explanations
    or reverse causality has been ruled out. Meeting these conditions requires an
    experimental setting where all relevant variables of interest can be fully controlled
    to isolate causal relationships. Alternatively, quasi-experimental settings expose
    units of observations to changes in inputs in a randomized way to rule out that
    other observable or unobservable features are responsible for the observed effects
    of the change in the environment.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 关于两个或多个变量之间关系的统计推断会产生相关性度量，这些度量只有在满足几个其他条件时才能解释为因果关系——例如，当已经排除了替代解释或逆向因果关系时。满足这些条件需要一个实验设置，其中所有感兴趣的相关变量都可以完全控制，以隔离因果关系。或者，准实验性设置以随机方式使观察单位暴露于输入变化，以排除其他可观察或不可观察的特征对环境变化观察效果的影响。
- en: These conditions are rarely met so inferential conclusions need to be treated
    with care. The same applies to the performance of predictive models that also
    rely on the statistical association between features and outputs, which may change
    with other factors that are not part of the model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些条件很少被满足，因此推断性结论需要谨慎对待。同样适用于预测模型的性能，后者也依赖于特征和输出之间的统计关联，该关联可能会随着不属于模型一部分的其他因素而改变。
- en: The non-parametric nature of the KNN model does not lend itself well to inference,
    so we'll postpone this step in the workflow until we encounter linear models in
    the next chapter.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: KNN模型的非参数性质不利于推断，因此我们将推迟工作流程中的这一步，直到我们在下一章遇到线性模型为止。
- en: Regression problems
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归问题
- en: Regression problems aim to predict a continuous variable. The **root**-**mean-square error**
    (**RMSE**) is the most popular loss function and error metric, not least because
    it is differentiable. The loss is symmetric, but larger errors weigh more in the
    calculation. Using the square root has the advantage of measuring the error in
    the units of the target variable. The same metric in combination with the **RMSE
    log of the error** (**RMSLE**) is appropriate when the target is subject to exponential
    growth because of its asymmetric penalty that weights negative errors less than
    positive errors. You can also log-transform the target first and then use the
    RMSE, as we do in the example later in this section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题旨在预测连续变量。**均方根误差**（**RMSE**）是最流行的损失函数和误差度量，其中之一是因为它是可微的。损失是对称的，但在计算中较大的误差权重更重。使用平方根的优势在于以目标变量的单位度量误差。与**RMSE对数误差**（**RMSLE**）结合使用相同的度量在目标受指数增长影响时是适当的，因为其不对称惩罚使得负误差权重小于正误差。您也可以先对目标进行对数转换，然后使用RMSE，正如我们在本节后面的示例中所做的那样。
- en: The **mean absolute errors** (**MAE**) and **median absolute errors** (**MedAE**)
    are symmetric but do not weigh larger errors more. The MedAE is robust to outliers.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）和**中位数绝对误差**（**MedAE**）是对称的，但不会使较大的误差权重更重。MedAE对异常值具有鲁棒性。'
- en: The explained variance score computes the proportion of the target variance
    that the model accounts for and varies between 0 and 1\. The R² score or coefficient
    of determination yields the same outcome the mean of the residuals is 0, but can
    differ otherwise. In particular, it can be negative when calculated on out-of-sample
    data (or for a linear regression without intercept).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 解释的方差分数计算模型解释的目标方差所占比例，介于0和1之间。R²分数或决定系数产生相同的结果，即残差的平均值为0，但在其他情况下可能不同。特别地，当在样本外数据上计算时（或者对于没有截距的线性回归），它可以为负。
- en: 'The following table defines the formulas used for calculation and the corresponding
    `sklearn` function that can be imported from the `metrics` module. The `scoring `parameter
    is used in combination with automated train-test functions (such as `cross_val_score`
    and `GridSearchCV`) that we will introduce later in this section, and which are
    illustrated in the accompanying notebook:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下表定义了用于计算的公式以及可从`metrics`模块导入的相应`sklearn`函数。`scoring`参数与自动化的训练-测试函数（例如`cross_val_score`和`GridSearchCV`）结合使用，我们稍后将在本节介绍，并在附带的笔记本中进行演示：
- en: '| **Name ** | **Formula** | **sklearn** | **Scoring parameter** |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **公式** | **sklearn** | **评分参数** |'
- en: '| Mean squared error | ![](img/44aab2d2-d348-4d35-84d6-4e123f612378.png) |
    `mean_squared_error` | `neg_mean_squared_error` |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 均方误差 | ![](img/44aab2d2-d348-4d35-84d6-4e123f612378.png) | `mean_squared_error`
    | `neg_mean_squared_error` |'
- en: '| Mean squared log error | ![](img/e16997d7-d66c-4974-bcf0-6162216cbb8c.png)
    | `mean_squared_log_error` | `neg_mean_squared_log_error` |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 平均平方对数误差 | ![](img/e16997d7-d66c-4974-bcf0-6162216cbb8c.png) | `mean_squared_log_error`
    | `neg_mean_squared_log_error` |'
- en: '| Mean absolute error | ![](img/32b35cf0-ffd8-45c5-a8a3-fc90a334258b.png) |
    `mean_absolute_error` | `neg_mean_absolute_error` |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 平均绝对误差 | ![](img/32b35cf0-ffd8-45c5-a8a3-fc90a334258b.png) | `mean_absolute_error`
    | `neg_mean_absolute_error` |'
- en: '| Median absolute error | ![](img/023b4786-ab8a-4929-9bf8-6e8eb0a458dc.png)
    | `median_absolute_error` | `neg_median_absolute_error` |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 中位数绝对误差 | ![](img/023b4786-ab8a-4929-9bf8-6e8eb0a458dc.png) | `median_absolute_error`
    | `neg_median_absolute_error` |'
- en: '| Explained variance | ![](img/f19a2388-1866-4e7c-b552-e92f72083642.png) |
    `explained_variance_score` | `explained_variance` |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 解释的方差 | ![](img/f19a2388-1866-4e7c-b552-e92f72083642.png) | `explained_variance_score`
    | `explained_variance` |'
- en: '| R2 score | ![](img/2561413f-fcfd-4bd9-9b85-8fd55f4c9012.png) | `r2_score`
    | `r2` |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| R2分数 | ![](img/2561413f-fcfd-4bd9-9b85-8fd55f4c9012.png) | `r2_score` | `r2`
    |'
- en: 'The following screenshot shows the various error metrics for the house price
    regression demonstrated in the notebook:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了笔记本中房价回归的各种误差指标：
- en: '![](img/2e196a95-046a-486e-a190-8476e6604cf3.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e196a95-046a-486e-a190-8476e6604cf3.png)'
- en: The `sklearn` function also supports multilabel evaluation—that is, assigning
    multiple outcome values to a single observation; see the documentation referenced
    on GitHub for more details ([https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading/tree/master/Chapter06)).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`函数还支持多标签评估——即将多个结果值分配给单个观测值；有关更多详细信息，请参阅GitHub上引用的文档（[https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading/tree/master/Chapter06)）。'
- en: Classification problems
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类问题
- en: Classification problems have categorical outcome variables. Most predictors
    will output a score to indicate whether an observation belongs to a certain class.
    In the second step, these scores are then translated into actual predictions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题具有分类结果变量。 大多数预测器将输出一个分数，指示观测是否属于某个类。 在第二步中，这些分数然后被转换为实际预测。
- en: In the binary case, where we will label the classes positive and negative, the
    score typically varies between zero or is normalized accordingly. Once the scores are
    converted into 0-1 predictions, there can be four outcomes, because each of the
    two existing classes can be either correctly or incorrectly predicted. With more
    than two classes, there can be more cases if you differentiate between the several potential
    mistakes.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元情况下，我们将标记类为正面和负面，分数通常在零之间变化或相应地进行归一化。 一旦将分数转换为0-1预测，就可能有四种结果，因为两个现有类别中的每一个都可以被正确或错误地预测。
    如果有超过两个类别，则可以有更多的情况，如果您区分了几种潜在的错误。
- en: 'All error metrics are computed from the breakdown of predictions across the
    four fields of the 2 x 2 confusion matrix that associates actual and predicted
    classes. The metrics listed in the table shown in the following diagram, such
    as accuracy, evaluate a model for a given threshold:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的误差指标都是从预测在2 x 2混淆矩阵的四个领域中进行的，该矩阵将实际类与预测类相关联。 表中列出的指标，如准确性，评估了给定阈值的模型：
- en: '![](img/a5196319-bcc0-4ff8-a447-aa59982f60fc.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5196319-bcc0-4ff8-a447-aa59982f60fc.png)'
- en: A classifier does not necessarily need to output calibrated probabilities, but
    should rather produce scores that are relative to each other in distinguishing
    positive from negative cases. Hence, the threshold is a decision variable that
    can and should be optimized, taking into account the costs and benefits of correct
    and incorrect predictions. A lower threshold implies more positive predictions,
    with a potentially rising false positive rate, and for a higher threshold, the
    opposite is likely to be true.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器不一定需要输出校准过的概率，而应该产生相对于彼此的分数，以区分正面和负面案例。 因此，阈值是一个决策变量，可以和应该优化，考虑到正确和错误预测的成本和收益。
    更低的阈值意味着更多的正面预测，可能会出现日益上升的假阳性率，而更高的阈值则可能相反。
- en: Receiver operating characteristics and the area under the curve
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接收操作特性和曲线下面积
- en: The **receiver operating characteristics** (**ROC**) curve allows us to visualize,
    organize, and select classifiers based on their performance. It computes all the
    combinations of **true positive rates** (**TPR**) and **false positive rates**
    (**FPR**) that result from producing predictions using any of the predicted scores
    as a threshold. It then plots these pairs on a square, the side of which has a
    measurement of one in length.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收器操作特性** (**ROC**) 曲线允许我们根据其性能可视化、组织和选择分类器。它计算了使用任何预测分数作为阈值产生的所有**真正例率**
    (**TPR**) 和**假正例率** (**FPR**) 的组合。然后将这些对绘制在一个边长为一的正方形上。'
- en: A classifier that makes random predictions (taking into account class imbalance)
    will on average yield TPR and FPR that are equal so that the combinations will
    lie on the diagonal, which becomes the benchmark case. Since an underperforming
    classifier would benefit from relabeling the predictions, this benchmark also
    becomes the minimum.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 进行随机预测（考虑类别不平衡）的分类器平均产生相等的TPR和FPR，使得组合位于对角线上，这成为基准情况。由于性能不佳的分类器将受益于重新标记预测，因此该基准也成为最低水平。
- en: The **area under the curve** (**AUC**) is defined as the area under the ROC
    plot that varies between 0.5 and the maximum of 1\. It is a summary measure of
    how well the classifier's scores are able to rank data points with respect to
    their class membership. More specifically, the AUC of a classifier has the important
    statistical property of representing the probability that the classifier will
    rank a randomly chosen positive instance higher than a randomly chosen negative
    instance, which is equivalent to the Wilcoxon ranking test. In addition, the AUC
    has the benefit of not being sensitive to class imbalances.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**曲线下面积** (**AUC**) 被定义为ROC图中的曲线下面积，其范围在0.5到最大值1之间变化。它是分类器的得分能够如何排列数据点的类成员身份的摘要度量。更具体地说，分类器的AUC具有重要的统计性质，表示分类器将随机选择的正实例排在随机选择的负实例之前的概率，这等价于Wilcoxon秩和检验。此外，AUC不敏感于类别不平衡，这是它的好处。'
- en: Precision-recall curves
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确度-召回率曲线
- en: 'When predictions for one of the classes are of particular interest, precision
    and recall curves visualize the trade-off between these error metrics for different
    thresholds. Both measures evaluate the quality of predictions for a particular
    class. The following list shows how they are applied to the positive class:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当对其中一个类别的预测特别感兴趣时，精确度和召回率曲线可视化这些误差度量之间的权衡，以及不同阈值的情况。两种度量都评估了对特定类别的预测质量。以下列表显示了它们如何应用于正类：
- en: '**Recall** measures the share of actual positive class members that a classifier
    predicts as positive for a given threshold. It originates in information retrieval
    and measures the share of relevant documents successfully identified by a search
    algorithm.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率** 衡量了对于给定阈值，分类器预测为正的实际正类成员的比例。它源于信息检索，衡量了搜索算法成功识别的相关文档的比例。'
- en: '**Precision**, in contrast, measures the share of positive predictions that
    are correct.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度** 相反，衡量了正确的正预测的比例。'
- en: Recall typically increases with a lower threshold, but precision may decrease.
    Precision-recall curves visualize the attainable combinations and allow for the
    optimization of the threshold given the costs and benefits of missing a lot of
    relevant cases or producing lower-quality predictions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率通常随着阈值降低而增加，但精确度可能会降低。精确度-召回率曲线可视化可达到的组合，并允许根据错过大量相关案例或产生质量较低的预测的成本和收益优化阈值。
- en: The F1 scoreis a harmonic mean of precision and recall for a given threshold
    and can be used to numerically optimize the threshold while taking into account
    the relative weights that these two metrics should assume.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数是给定阈值下精确度和召回率的调和平均数，并可用于在考虑这两个度量应承担的相对权重的情况下进行阈值的数值优化。
- en: 'The following chart illustrates the ROC curve and corresponding AUC alongside
    the precision-recall curve and the F1 score that, using equal weights for precision
    and recall, yields an optimal threshold of 0.37\. The chart is taken from the
    accompanying notebook where you can find the code for the KNN classifier that
    operates on binarized housing prices:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了ROC曲线及其相应的AUC以及精确度-召回率曲线和F1分数，使用精确度和召回率的相等权重，产生阈值为0.37。图表取自附带笔记本，您可以在其中找到作用于二值化房价的KNN分类器的代码：
- en: '![](img/c4da6c85-3930-49a8-a440-a2aa3172c2c1.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4da6c85-3930-49a8-a440-a2aa3172c2c1.png)'
- en: Collecting and preparing the data
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集和准备数据
- en: We already addressed important aspects of the sourcing of market, fundamental,
    and alternative data, and will continue to work with various examples of these
    sources as we illustrate the application of the various models.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了市场、基本和替代数据的采购的重要方面，并将继续使用各种这些来源的示例来说明各种模型的应用。
- en: In addition to market and fundamental data that we will access through the Quantopian
    platform, we will also acquire and transform text data as we explore natural language
    processing and image data when we look at image processing and recognition. Besides
    obtaining, cleaning, and validating the data to relate it to trading data typically
    available in a time-series format, it is important to store it in a format that
    allows for fast access to enable quick exploration and iteration. We have recommended
    the HDF and parquet formats. For larger data volumes, Apache Spark represents
    the best solution.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们将通过Quantopian平台访问的市场和基本数据外，当我们探索自然语言处理时，我们还将获取和转换文本数据，当我们查看图像处理和识别时，我们也会获取和转换图像数据。除了获取、清理和验证数据以将其与通常以时间序列格式可用的交易数据相关联之外，将其存储在能够快速访问以便快速探索和迭代的格式中也很重要。我们推荐使用HDF和parquet格式。对于更大的数据量，Apache
    Spark是最佳解决方案。
- en: Explore, extract, and engineer features
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索、提取和工程化特征
- en: 'Understanding the distribution of individual variables and the relationships
    among outcomes and features is the basis for picking a suitable algorithm. This
    typically starts with visualizations such as scatter plots, as illustrated in
    the companion notebook (and shown in the following image), but also includes numerical
    evaluations ranging from linear metrics, such as the correlation, to nonlinear
    statistics, such as the Spearman rank correlation coefficient that we encountered
    when we introduced the information coefficient. It also includes information-theoretic
    measures, such as mutual information, as illustrated in the next subsection:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 理解个体变量的分布以及结果和特征之间的关系是选择合适算法的基础。这通常从诸如散点图之类的可视化开始，如伴随笔记本中所示（并在以下图片中显示），但也包括从线性指标，如相关性，到非线性统计量，如我们介绍信息系数时遇到的Spearman秩相关系数的数字评估。它还包括信息论量度，如相互信息，如下一小节所示：
- en: '![](img/56162613-8d69-409f-affe-4be75c407875.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56162613-8d69-409f-affe-4be75c407875.png)'
- en: Scatter plots
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图
- en: 'A systematic exploratory analysis is also the basis of what is often the single
    most important ingredient of a successful predictive model: the engineering of features
    that extract information contained in the data, but which are not necessarily
    accessible to the algorithm in their raw form. Feature engineering benefits from
    domain expertise, the application of statistics and information theory, and creativity.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 系统性的探索性分析也是通常成功预测模型的最重要的一个组成部分的基础：工程化的特征，这些特征提取了数据中包含的信息，但这些信息在原始形式下不一定能被算法访问到。特征工程受益于领域专业知识、统计学和信息论的应用，以及创造力。
- en: It relies on an ingenious choice of data transformations that effectively tease
    out the systematic relationship between input and output data. There are many
    choices that include outlier detection and treatment, functional transformations,
    and the combination of several variables, including unsupervised learning. We
    will illustrate examples throughout but will emphasize that this feature is best
    learned through experience. Kaggle is a great place to learn from other data scientists
    who share their experiences with the Kaggle community.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 它依赖于巧妙选择的数据转换，这些转换能有效地挖掘出输入和输出数据之间的系统关系。有许多选择，包括异常值检测和处理、功能转换以及多个变量的组合，包括无监督学习。我们将在整个过程中举例说明，但强调这个特性最好通过经验学习。Kaggle是一个很好的地方，可以从其他数据科学家那里学习，他们与Kaggle社区分享了他们的经验。
- en: Using information theory to evaluate features
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用信息论评估特征
- en: The **mutual information** (**MI**) between a feature and the outcome is a measure
    of the mutual dependence between the two variables. It extends the notion of correlation
    to nonlinear relationships. More specifically, it quantifies the information obtained
    about one random variable through the other random variable.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 特征和结果之间的**相互信息**（**MI**）是两个变量之间相互依赖的一种度量。它将相关性的概念扩展到非线性关系。更具体地说，它量化了通过另一个随机变量获取的关于一个随机变量的信息。
- en: 'The concept of MI is closely related to the fundamental notion of entropy of
    a random variable. Entropy quantifies the amount of information contained in a
    random variable. Formally, the mutual information—*I*(*X*, *Y*)—of two random
    variables, *X* and *Y*, is defined as the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd6521f9-4de8-4c81-b5f0-22a165d97f0d.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: The `sklearn` function implements `feature_selection.mutual_info_regression`
    that computes the mutual information between all features and a continuous outcome
    to select the features that are most likely to contain predictive information.
    There is also a classification version (see the documentation for more details).
    The notebook `mutual_information.ipynb` notebook contains an application to the
    financial data we created in [Chapter 4](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml),
     *Alpha Factor Research*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Selecting an ML algorithm
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The remainder of this book will introduce several model families, ranging from
    linear models, which make fairly strong assumptions about the nature of the functional
    relationship between input and output variables, to deep neural networks, which
    make very few assumptions. As mentioned in the introductory section, fewer assumptions
    will require more data with significant information about the relationship so
    that the learning process can be successful.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: We will outline the key assumptions and how to test them where applicable as
    we introduce these models.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Design and tune the model
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ML process includes steps to diagnose and manage model complexity based
    on estimates of the model's generalization error. An unbiased estimate requires
    a statistically sound and efficient procedure, as well as error metrics that align
    with the output variable type, which also determines whether we are dealing with
    a regression, classification, or ranking problem.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: The bias-variance trade-off
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The errors that an ML model makes when predicting outcomes for new input data
    can be broken down into reducible and irreducible parts. The irreducible part
    is due to random variation (noise) in the data that is not measured, such as relevant
    but missing variables or natural variation. The reducible part of the generalization
    error, in turn, can be broken down into bias and variance. Both are due to differences
    between the true functional relationship and the assumptions made by the machine
    learning algorithm, as detailed in the following list:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '**Error due to bias**: The hypothesis is too simple to capture the complexity
    of the true functional relationship. As a result, whenever the model attempts
    to learn the true function, it makes systematic mistakes and, on average, the
    predictions will be similarly biased. This is also called **underfitting**.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error due to variance**: The algorithm is overly complex in view of the true
    relationship. Instead of capturing the true relationship, it overfits to the data
    and extracts patterns from the noise. As a result, it learns different functional
    relationships from each sample, and out-of-sample predictions will vary widely.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差导致的错误**：算法过于复杂，以至于忽视了真实关系。它不是捕捉真实关系，而是过拟合数据并从噪声中提取模式。结果，它从每个样本中学习到不同的函数关系，并且外样预测将有很大的变化。'
- en: Underfitting versus overfitting
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欠拟合与过拟合
- en: The following diagram illustrates overfitting by approximating a cosine function
    using increasingly complex polynomials and measuring the in-sample error. More
    specifically, we draw 10 random samples with some added noise (*n *= 30) to learn
    a polynomial of varying complexity (see the code in the accompanying notebook).
    Each time, the model predicts new data points and we capture the mean-squared
    error for these predictions, as well as the standard deviation of these errors.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了通过使用越来越复杂的多项式来近似余弦函数并测量样本内误差而产生的过拟合。更具体地说，我们绘制10个带有一些添加噪声的随机样本（*n* =
    30）以学习不同复杂度的多项式（请参阅附带笔记本中的代码）。每次，模型预测新数据点，我们捕获这些预测的均方误差，以及这些误差的标准差。
- en: 'The left-hand panel in the following diagram shows a polynomial of degree 1;
    a straight line clearly underfits the true function. However, the estimated line
    will not differ dramatically from one sample drawn from the true function to the
    next. The middle panel shows that a degree 5 polynomial approximates the true
    relationship reasonably well on the [0, 1] interval. On the other hand, a polynomial
    of degree 15 fits the small sample almost perfectly, but provides a poor estimate
    of the true relationship: it overfits to the random variation in the sample data
    points, and the learned function will vary strongly with each sample drawn:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表中的左侧面板显示了一个一次多项式；一条直线明显地欠拟合真实函数。然而，估计的直线不会从真实函数的一个抽样到下一个抽样有明显的差异。中间面板显示了一个五次多项式在[0,
    1]区间上合理地近似了真实关系。另一方面，一个十五次多项式几乎完美地拟合了小样本，但提供了对真实关系的很差估计：它对样本数据点的随机变化过拟合，而且学习的函数会随着每个抽样而变化。
- en: '![](img/2e20ae8c-f606-40c4-8a14-c223376e5cc7.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e20ae8c-f606-40c4-8a14-c223376e5cc7.png)'
- en: Managing the trade-off
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理权衡
- en: Let's further illustrate the impact of overfitting versus underfitting by trying
    to learn a Taylor series approximation of the cosine function of ninth degree
    with some added noise. In the following diagram, we draw random samples of the
    true function and fit polynomials that underfit, overfit, and provide an approximately
    correct degree of flexibility. We then predict out-of-sample and measure the RMSE.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过尝试学习带有一些添加噪声的九次余弦函数的泰勒级数近似来进一步说明过拟合与欠拟合的影响。在以下图表中，我们绘制真实函数的随机样本，并拟合欠拟合、过拟合和提供近似正确灵活度的多项式。然后我们进行外样预测并测量RMSE。
- en: The high bias but low variance of a polynomial of degree 3 compares to the low
    bias but exceedingly high variance of the various prediction errors visible in
    the first panel. The left-hand panel shows the distribution of the errors that
    result from subtracting the true function values. The underfit case of a straight
    line produces a poor in-sample fit and is significantly off target out of sample.
    The overfit model shows the best fit in-sample with the smallest dispersion of
    errors, but the price is a large variance out-of-sample. The appropriate model
    that matches the functional form of the true model performs the best by far out-of-sample.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 三次多项式的高偏差但低方差与第一个面板中可见的各种预测误差的低偏差但极高方差相比。左侧面板显示了由减去真实函数值产生的误差分布。直线欠拟合情况下产生了糟糕的样本内拟合，并且在样本外明显偏离目标。过拟合模型显示了最佳的样本内拟合以及最小的误差分散，但代价是样本外的大方差。与真实模型的函数形式匹配的适当模型明显在样本外表现最佳。
- en: 'The right-hand panel of the following screenshot shows the actual predictions
    rather than the errors to demonstrate what the different types of fit look like
    in practice:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图的右侧面板显示了实际预测而不是错误，以演示不同类型的拟合在实践中的样子：
- en: '![](img/6d8b7763-5776-41db-81a6-b1f6dfc415ff.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d8b7763-5776-41db-81a6-b1f6dfc415ff.png)'
- en: Learning curves
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习曲线
- en: A learning curve plots the evolution of train and test errors against the size
    of the dataset used to learn the functional relationship. It is a useful tool
    to diagnose the bias-variance trade-off for a given model because the errors will
    behave differently. A model with a high bias will have a high but similar training
    error, both in-sample and out-of-sample, whereas an overfit model will have a
    very low training error.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线绘制了根据用于学习函数关系的数据集大小演变的训练和测试错误。它是诊断给定模型的偏差-方差权衡的有用工具，因为误差会表现出不同的行为。具有高偏差的模型将具有高但相似的训练误差，无论是样本内还是样本外，而过度拟合的模型将具有非常低的训练误差。
- en: 'The declining out-of-sample error illustrates that overfit models may benefit
    from additional data or tools to limit the model''s complexity, such as regularization,
    whereas underfit models need to use either more features or otherwise increase
    the complexity of the model, as shown in the following screenshot:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 外样本错误的下降说明过拟合模型可能会受益于额外的数据或限制模型复杂性的工具，如正则化，而不拟合的模型需要使用更多特征或以其他方式增加模型的复杂性，如下图所示：
- en: '![](img/61a03d74-7a22-4c44-8672-2b303bdc9c4e.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61a03d74-7a22-4c44-8672-2b303bdc9c4e.png)'
- en: How to use cross-validation for model selection
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用交叉验证进行模型选择
- en: When several candidate models (that is, algorithms) are available for your use
    case, the act of choosing one of them is called the **model selection** problem. Model
    selection aims to identify the model that will produce the lowest prediction error
    given new data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的用例中有多个候选模型（即算法）可用时，选择其中一个的行为称为**模型选择**问题。模型选择旨在确定在给定新数据的情况下产生最低预测误差的模型。
- en: 'An unbiased estimate of this generalization error requires a test on data that
    was not part of model training. Hence, we only use part of the available data
    to train the model and set aside another part of the data to test the model. In
    order to obtain an unbiased estimate of the prediction error, absolutely no information
    about the test set may leak into the training set, as shown in the following diagram:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对这种泛化误差的无偏估计需要在未参与模型训练的数据上进行测试。因此，我们只使用部分可用数据来训练模型，并将另一部分数据留出来测试模型。为了获得对预测误差的无偏估计，绝对不能泄漏有关测试集的任何信息到训练集中，如下图所示：
- en: '![](img/7e07b113-5977-4a29-a34a-0fdb59a25de0.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e07b113-5977-4a29-a34a-0fdb59a25de0.png)'
- en: There are several methods that can be used to split the available data, which
    differ in terms of the amount of data used for training, the variance of the error
    estimates, the computational intensity, and whether structural aspects of the
    data are taken into account when splitting the data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种可用于拆分可用数据的方法，这些方法在用于训练的数据量、误差估计的方差、计算强度以及在拆分数据时是否考虑数据的结构方面不同。
- en: How to implement cross-validation in Python
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在 Python 中实现交叉验证
- en: 'We will illustrate various options for splitting data into training and test
    sets by showing how the indices of a mock dataset with ten observations are assigned
    to the train and test set (see `cross_validation.py` for details), as shown in
    following code:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过展示一个具有十个观察值的模拟数据集的索引如何分配到训练集和测试集（详见`cross_validation.py`），来说明将数据拆分为训练集和测试集的各种选项，如下所示：
- en: '[PRE0]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Basic train-test split
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本的训练测试拆分
- en: 'For a single split of your data into a training and a test set, use `sklearn.model_selection.train_test_split`,
    where the `shuffle` parameter, by default ensures the randomized selection of
    observations, which in turn can be replicated by setting `random_state`. There
    is also a `stratify` parameter that, for a classification problem, ensures that
    the train and test sets will contain approximately the same shares of each class,
    as shown in the following code:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于将数据拆分为训练集和测试集的单次拆分，请使用`sklearn.model_selection.train_test_split`，其中`shuffle`参数默认确保观察值的随机选择，而这又可以通过设置`random_state`来复制。还有一个`stratify`参数，对于分类问题，确保训练集和测试集将包含大致相同的每个类的份额，如下图所示：
- en: '[PRE1]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this case, we train a model using all data except row numbers `6` and `9`,
    which will be used to generate predictions and measure the errors given on the
    `know` labels. This method is useful for quick evaluation but is sensitive to
    the split, and the standard error of the test error estimate will be higher.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用除行号`6`和`9`之外的所有数据来训练模型，这些数据将用于生成预测并测量给定`know`标签上的错误。这种方法对于快速评估很有用，但对拆分很敏感，并且测试误差估计的标准误差将更高。
- en: Cross-validation
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证
- en: '**Cross-validation** (**CV**) is a popular strategy for model selection. The
    main idea behind CV is to split the data one or several times so that each split
    is used once as a validation set and the remainder as a training set: part of
    the data (the training sample) is used to train the algorithm, and the remaining
    part (the validation sample) is used for estimating the risk of the algorithm.
    Then, CV selects the algorithm with the smallest estimated risk.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证** (**CV**) 是一种常用的模型选择策略。CV 的主要思想是将数据拆分一次或多次，以便每个拆分都被用作一次验证集，剩余部分用作训练集：数据的一部分（训练样本）用于训练算法，剩余部分（验证样本）用于估计算法的风险。然后，CV
    选择具有最小估计风险的算法。'
- en: While the data-splitting heuristic is very general, a key assumption of CV is
    that the data is **independently and identically distributed** (**IID**). In the
    following sections, we will see that, for time series data, this is often not
    the case and requires a different approach.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据拆分启发式方法非常一般化，但 CV 的一个关键假设是数据是 **独立同分布** (**IID**) 的。在接下来的章节中，我们将看到，对于时间序列数据，通常情况并非如此，需要采用不同的方法。
- en: Using a hold-out test set
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用保留测试集
- en: 'When selecting hyperparameters based on their validation score, be aware that
    this validation score is biased because of multiple tests, and is no longer a
    good estimate of the generalization error. For an unbiased estimate of the error
    rate, we have to estimate the score from a fresh dataset, as shown in the following
    diagram:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于验证分数选择超参数时，请注意，由于多次测试，此验证分数存在偏差，并不再是泛化误差的良好估计。为了对误差率进行无偏估计，我们必须从新数据集中估计分数，如下图所示：
- en: '![](img/06e62cd0-8b6d-485a-9724-f59bfa81c3fc.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06e62cd0-8b6d-485a-9724-f59bfa81c3fc.png)'
- en: 'For this reason, we use a three-way split of the data, as illustrated in the
    preceding diagram: one part is used in cross-validation and is repeatedly split
    into a training and validation set. The remainder is set aside as a hold-out set
    that is only used once cross-validation is complete to generate an unbiased test
    error estimate. We will illustrate this method as we start building ML models
    in the next chapter.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用数据的三向拆分，如前图所示：一部分用于交叉验证，并反复拆分为训练集和验证集。剩余部分作为保留集保留，只有在交叉验证完成后才使用它生成无偏的测试误差估计。我们将在下一章开始构建机器学习模型时说明这种方法。
- en: KFold iterator
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KFold 迭代器
- en: 'The `sklearn.model_selection.KFold` iterator produces several disjunct splits
    and assigns each of these splits once to the validation set, as shown in the following
    code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.model_selection.KFold` 迭代器生成几个不相交的拆分，并将这些拆分中的每一个分配给验证集，如下面的代码所示：'
- en: '[PRE2]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In addition to the number of splits, most CV objects take a `shuffle` argument
    that ensures randomization. To render results reproducible, set the `random_state`,
    as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 除了拆分数量外，大多数 CV 对象还接受一个 `shuffle` 参数，以确保随机性。为了使结果可重现，设置 `random_state` 如下所示：
- en: '[PRE3]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Leave-one-out CV
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留一出 CV
- en: 'The original CV implementation used a leave-one-out method that used each observation
    once as the validation set, as shown in the following code:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 CV 实现使用留一出方法，每次将每个观测值一次用作验证集，如下面的代码所示：
- en: '[PRE4]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This maximizes the number of models that are trained, which increases computational
    costs. While the validation sets do not overlap, the overlap of training sets
    is maximized, driving up the correlation of models and their prediction errors.
    As a result, the variance of the prediction error is higher for a model with a
    larger number of folds.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做可以最大化训练的模型数量，增加计算成本。虽然验证集不重叠，但是训练集的重叠被最大化，推高了模型及其预测误差的相关性。因此，对于具有更多拆分的模型，预测误差的方差更高。
- en: Leave-P-Out CV
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留-P-出 CV
- en: 'A similar version to leave-one-out CV is leave-P-out CV, which generates all
    possible combinations of `p` data rows, as shown in the following code:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 与留一出 CV 类似的版本是留-P-出 CV，它生成所有可能的 `p` 数据行的组合，如下面的代码所示：
- en: '[PRE5]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ShuffleSplit
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ShuffleSplit
- en: 'The `sklearn.model_selection.ShuffleSplit` object creates independent splits
    with potentially overlapping validation sets, as shown in the following code:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.model_selection.ShuffleSplit` 对象创建具有潜在重叠验证集的独立拆分，如下面的代码所示：'
- en: '[PRE6]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameter tuning with scikit-learn
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行参数调整
- en: Model selection typically involves repeated cross-validation of the out-of-sample
    performance of models using different algorithms (such as linear regression and
    random forest) or different configurations. Different configurations may involve
    changes to hyperparameters or the inclusion or exclusion of different variables.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择通常涉及对使用不同算法（如线性回归和随机森林）或不同配置的模型的外样本性能进行重复交叉验证。不同的配置可能涉及对超参数的更改或包含或排除不同变量。
- en: The `yellowbricks` library extends the `sklearn` API to generate diagnostic
    visualization tools to facilitate the model-selection process. These tools can
    be used to investigate relationships among features, analyze classification or
    regression errors, monitor cluster algorithm performance, inspect the characteristics
    of text data, and help with model selection. We will demonstrate validation and
    learning curves that provide valuable information during the parameter-tuning
    phase—see the `machine_learning_workflow.ipynb` notebook for implementation details.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: yellowbricks 库扩展了 sklearn API，生成诊断可视化工具，以便于模型选择过程。这些工具可用于调查特征之间的关系，分析分类或回归错误，监视聚类算法性能，检查文本数据的特性，并帮助进行模型选择。我们将演示提供有关参数调整阶段的有价值信息的验证和学习曲线—有关实现细节，请参阅
    machine_learning_workflow.ipynb 笔记本。
- en: Validation curves with yellowbricks
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 yellowbricks 的验证曲线
- en: Validation curves (see the left-hand panel in the following graph) visualize
    the impact of a single hyperparameter on a model's cross-validation performance.
    This is useful to determine whether the model underfits or overfits the given
    dataset.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 验证曲线（参见下图左侧面板）可视化单个超参数对模型交叉验证性能的影响。这有助于确定模型是否对给定数据集欠拟合或过拟合。
- en: 'In our example of the **KNeighborsRegressor** that only has a single hyperparameter,
    we can clearly see that the model underfits for values of *k* above 20, where
    the validation error drops as we reduce the number of neighbors, thereby making
    our model more complex, because it makes predictions for more distinct groups
    of neighbors or areas in the feature space. For values below 20, the model begins
    to overfit as training and validation errors diverge and average out-of-sample
    performance quickly deteriorates, as shown in the following graph:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，只有一个超参数的**KNeighborsRegressor**，我们可以清楚地看到，当*k*的值高于20时，模型欠拟合，其中验证错误随着减少邻居数量而下降，从而使我们的模型更复杂，因为它为更多不同的邻居组或特征空间中的区域进行预测。对于小于20的值，模型开始过拟合，因为训练和验证错误背离，平均外样本表现迅速恶化，如下图所示：
- en: '![](img/f8e64722-fd9d-4ae9-bb41-e84f239625d1.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8e64722-fd9d-4ae9-bb41-e84f239625d1.png)'
- en: Learning curves
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习曲线
- en: The learning curve (see the right-hand panel of the preceding chart for our
    house price regression example) helps determine whether a model's cross-validation
    performance would benefit from additional data and whether prediction errors are
    more driven by bias or by variance.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线（参见前述图表右侧面板，我们的房价回归示例）有助于确定模型的交叉验证性能是否会受益于更多数据，以及预测错误是更多地受偏差还是方差驱动。
- en: If training and cross-validation performance converges, then more data is unlikely
    to improve the performance. At this point, it is important to evaluate whether
    the model performance meets expectations, determined by a human benchmark. If
    this is not the case, then you should modify the model's hyperparameter settings
    to better capture the relationship between the features and the outcome, or choose
    a different algorithm with a higher capacity to capture complexity.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练和交叉验证性能趋于一致，那么更多的数据不太可能改善性能。在这一点上，重要的是评估模型性能是否符合人类基准的预期。如果不是这种情况，那么您应该修改模型的超参数设置，以更好地捕捉特征和结果之间的关系，或者选择一个具有更高复杂性捕捉能力的不同算法。
- en: In addition, the variation of train and test errors shown by the shaded confidence
    intervals provide clues about the bias and variance sources of the prediction
    error. Variability around the cross-validation error is evidence of variance,
    whereas variability for the training set suggests bias, depending on the size
    of the training error.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，阴影置信区间显示的训练和测试错误的变化提供了有关预测错误的偏差和方差来源的线索。交叉验证错误周围的变异性是方差的证据，而对训练集的变异性则暗示偏差，这取决于训练错误的大小。
- en: In our example, the cross-validation performance has continued to drop, but
    the incremental improvements have shrunk and the errors have plateaued, so there
    are unlikely to be many benefits from a larger training set. On the other hand,
    the data is showing substantial variance given the range of validation errors
    compared to that shown for the training errors.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，交叉验证性能一直在下降，但增量改进已经减少，错误已经趋于稳定，因此从更大的训练集中很少会有很多好处。另一方面，鉴于验证错误的范围与训练错误相比显示出的范围相比，数据显示出相当大的方差。
- en: Parameter tuning using GridSearchCV and pipeline
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GridSearchCV和pipeline进行参数调优
- en: Since hyperparameter tuning is a key ingredient of the machine learning workflow,
    there are tools to automate this process. The `sklearn` library includes a `GridSearchCV`
    interface that cross-validates all combinations of parameters in parallel, captures
    the result, and automatically trains the model using the parameter setting that
    performed best during cross-validation on the full dataset.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 由于超参数调优是机器学习工作流的关键组成部分，有一些工具可以自动化这个过程。`sklearn`库包括一个`GridSearchCV`接口，该接口可以并行交叉验证所有参数组合，捕获结果，并自动使用在完整数据集上交叉验证中表现最佳的参数设置来训练模型。
- en: In practice, the training and validation sets often require some processing
    prior to cross-validation. Scikit-learn offers the `Pipeline` to also automate
    any requisite feature-processing steps in the automated hyperparameter tuning
    facilitated by `GridSearchCV`.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在交叉验证之前，训练集和验证集通常需要进行一些处理。Scikit-learn提供了`Pipeline`，以便在由`GridSearchCV`便利的自动化超参数调优中自动执行任何必要的特征处理步骤。
- en: You can look at the implementation examples in the included `machine_learning_workflow.ipynb`
    notebook to see these tools in action.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看所包含的`machine_learning_workflow.ipynb`笔记本中的实现示例，以查看这些工具的运行情况。
- en: Challenges with cross-validation in finance
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 金融交叉验证中的挑战
- en: A key assumption for the cross-validation methods discussed so far is the **independent
    and identical** (**iid**) distribution of the samples available for training.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止讨论的交叉验证方法的一个关键假设是训练样本的**独立且相同（iid）**分布。
- en: For financial data, this is often not the case. On the contrary, financial data
    is neither independently nor identically distributed because of serial correlation
    and time-varying standard deviation, also known as **heteroskedasticity** (see
    the next two chapters for more details). The `TimeSeriesSplit` in the `sklearn.model_selection`
    module aims to address the linear order of time-series data.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于金融数据，情况通常并非如此。相反，金融数据既不独立也不同分布，因为存在序列相关性和时变标准差，也称为**异方差性**（有关更多详细信息，请参阅接下来的两章）。`sklearn.model_selection`模块中的`TimeSeriesSplit`旨在解决时间序列数据的线性顺序。
- en: Time series cross-validation with sklearn
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用sklearn进行时间序列交叉验证
- en: The time series nature of the data implies that cross-validation produces a
    situation where data from the future will be used to predict data from the past.
    This is unrealistic at best and data snooping at worst, to the extent that future
    data reflects past events.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的时间序列性意味着交叉验证会产生这样一种情况，即未来的数据将用于预测过去的数据。这充其量是不现实的，最坏的情况下是数据窥探，因为未来数据反映了过去事件的程度。
- en: 'To address time dependency, the `sklearn.model_selection.TimeSeriesSplit` object
    implements a walk-forward test with an expanding training set, where subsequent
    training sets are supersets of past training sets, as shown in the following code:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决时间依赖性，`sklearn.model_selection.TimeSeriesSplit`对象实现了一个具有扩展训练集的前向测试，其中后续训练集是过去训练集的超集，如下面的代码所示：
- en: '[PRE7]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can use the `max_train_size` parameter to implement walk-forward cross-validation,
    where the size of the training set remains constant over time, similar to how
    `zipline` tests a trading algorithm. Scikit-learn facilitates the design of custom
    cross-validation methods using subclassing, which we will implement in the following
    chapters.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`max_train_size`参数来实现前向交叉验证，其中训练集的大小随时间保持不变，类似于`zipline`测试交易算法的方式。Scikit-learn通过子类化方便地设计自定义交叉验证方法，我们将在接下来的章节中实现。
- en: Purging, embargoing, and combinatorial CV
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清除、禁止和组合交叉验证
- en: For financial data, labels are often derived from overlapping data points as
    returns are computed from prices in multiple periods. In the context of trading
    strategies, the results of a model's prediction, which may imply taking a position
    in an asset, may only be known later, when this decision is evaluated—for example,
    when a position is closed out.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于金融数据，标签通常是从重叠的数据点派生的，因为收益率是从多个期间的价格计算出来的。在交易策略的背景下，模型的预测结果，可能意味着对某种资产的持仓，只有在稍后，当这个决定被评估时，才能知道，例如，当一个头寸被平仓时。
- en: 'The resulting risks include the leaking of information from the test into the
    training set, likely leading to an artificially inflated performance that needs
    to be addressed by ensuring that all data is point-in-time—that is, truly available
    and known at the time it is used as the input for a model. Several methods have
    been proposed by Marcos Lopez de Prado in *Advances in Financial Machine Learning* to
    address these challenges of financial data for cross-validation, as shown in the
    following list:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 由此产生的风险包括信息从测试集泄漏到训练集，可能导致人为膨胀的性能，需要通过确保所有数据都是时点性的来解决，也就是说，在用作模型输入时真正可用和已知的时间。马科斯·洛佩斯·德·普拉多在《金融机器学习进展》中提出了几种方法来解决金融数据在交叉验证中的这些挑战，如下所示：
- en: '**Purging**: Eliminate training data points where the evaluation occurs after
    the prediction of a point-in-time data point in the validation set to avoid look-ahead
    bias.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清洗**：消除训练数据点，其中评估发生在验证集中一个时间点数据点的预测之后，以避免前瞻性偏差。'
- en: '**Embargoing**: Further eliminate training samples that follow a test period.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**禁运**：进一步消除跟随测试期间的训练样本。'
- en: '**Combinatorial cross-validation**: Walk-forward CV severely limits the historical
    paths that can be tested. Instead, given T observations, compute all possible
    train/test splits for *N*<*T* groups that each maintain their order, and purge
    and embargo potentially overlapping groups. Then, train the model on all combinations
    of *N*-*k* groups while testing the model on the remaining *k* groups. The result
    is a much larger number of possible historical paths.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组合交叉验证**：向前步进 CV 严重限制了可以测试的历史路径。相反，给定 T 个观测值，为 *N*<*T* 个组计算所有可能的训练/测试拆分，每个组都保持其顺序，并清洗和禁运可能重叠的组。然后，在所有
    *N*-*k* 组合上训练模型，同时在剩余的 *k* 组上测试模型。结果是可能历史路径的数量大大增加。'
- en: Prado's *Advances in Financial Machine Learning* contains sample code to implement
    these approaches; the code is also available via the new library, `timeseriescv`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 普拉多的《金融机器学习进展》包含了实现这些方法的示例代码；该代码也可以通过新库`timeseriescv`获得。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced the challenge of learning from data and looked
    at supervised, unsupervised, and reinforcement models as the principal forms of
    learning that we will study in this book to build algorithmic trading strategies.
    We discussed the need for supervised learning algorithms to make assumptions about
    the functional relationships that they attempt to learn in order to limit the
    search space while incurring an inductive bias that may lead to excessive generalization
    errors.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了从数据中学习的挑战，并将监督、无监督和强化模型作为我们将在本书中研究的主要学习形式，以构建算法交易策略。我们讨论了监督学习算法需要对它们试图学习的功能关系进行假设，以限制搜索空间，同时产生归纳偏差，可能导致过度泛化误差。
- en: We presented key aspects of the ML workflow, introduced the most common error
    metrics for regression and classification models, explained the bias-variance
    trade-off, and illustrated the various tools for managing the model selection
    process using cross-validation.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 ML 工作流程的关键方面，介绍了回归和分类模型的最常见错误度量标准，解释了偏差-方差折衷，并说明了使用交叉验证管理模型选择过程的各种工具。
- en: In the following chapter, we will dive into linear models for regression and
    classification to develop our first algorithmic trading strategies that use ML.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入探讨线性模型用于回归和分类，以开发我们的第一个使用 ML 的算法交易策略。
