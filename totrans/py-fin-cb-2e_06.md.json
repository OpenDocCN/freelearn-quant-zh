["```py\n    import pandas as pd\n    import nasdaqdatalink\n    import seaborn as sns\n    from statsmodels.tsa.seasonal import seasonal_decompose\n    nasdaqdatalink.ApiConfig.api_key = \"YOUR_KEY_HERE\" \n    ```", "```py\n    df = (\n        nasdaqdatalink.get(dataset=\"FRED/UNRATENSA\",\n                           start_date=\"2010-01-01\",\n                           end_date=\"2019-12-31\")\n        .rename(columns={\"Value\": \"unemp_rate\"})\n    ) \n    ```", "```py\n    WINDOW_SIZE = 12\n    df[\"rolling_mean\"] = df[\"unemp_rate\"].rolling(window=WINDOW_SIZE).mean()\n    df[\"rolling_std\"] = df[\"unemp_rate\"].rolling(window=WINDOW_SIZE).std()\n    df.plot(title=\"Unemployment rate\") \n    ```", "```py\n    decomposition_results = seasonal_decompose(df[\"unemp_rate\"],\n                                               model=\"additive\")\n    (\n        decomposition_results\n        .plot()\n        .suptitle(\"Additive Decomposition\")\n    ) \n    ```", "```py\nfrom statsmodels.tsa.seasonal import STL\nstl_decomposition = STL(df[[\"unemp_rate\"]]).fit()\nstl_decomposition.plot() \\\n                 .suptitle(\"STL Decomposition\") \n```", "```py\n    import pandas as pd\n    from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n    from statsmodels.tsa.stattools import adfuller, kpss \n    ```", "```py\n    def  adf_test(x):\n        indices = [\"Test Statistic\", \"p-value\",\n                   \"# of Lags Used\", \"# of Observations Used\"]\n\n        adf_test = adfuller(x, autolag=\"AIC\")\n        results = pd.Series(adf_test[0:4], index=indices)\n\n        for key, value in adf_test[4].items():\n            results[f\"Critical Value ({key})\"] = value\n        return results \n    ```", "```py\n    adf_test(df[\"unemp_rate\"]) \n    ```", "```py\n    Test Statistic             -2.053411\n    p-value                     0.263656\n    # of Lags Used             12.000000\n    # of Observations Used    107.000000\n    Critical Value (1%)        -3.492996\n    Critical Value (5%)        -2.888955\n    Critical Value (10%)       -2.581393 \n    ```", "```py\n    def  kpss_test(x, h0_type=\"c\"):    \n        indices = [\"Test Statistic\", \"p-value\", \"# of Lags\"]\n        kpss_test = kpss(x, regression=h0_type)\n        results = pd.Series(kpss_test[0:3], index=indices)\n\n        for key, value in kpss_test[3].items():\n            results[f\"Critical Value ({key})\"] = value\n        return results \n    ```", "```py\n    kpss_test(df[\"unemp_rate\"]) \n    ```", "```py\n    Test Statistic           1.799224\n    p-value                  0.010000\n    # of Lags                6.000000\n    Critical Value (10%)     0.347000\n    Critical Value (5%)      0.463000\n    Critical Value (2.5%)    0.574000\n    Critical Value (1%)      0.739000 \n    ```", "```py\n    N_LAGS = 40\n    SIGNIFICANCE_LEVEL = 0.05\n    fig, ax = plt.subplots(2, 1)\n    plot_acf(df[\"unemp_rate\"], ax=ax[0], lags=N_LAGS,\n             alpha=SIGNIFICANCE_LEVEL)\n    plot_pacf(df[\"unemp_rate\"], ax=ax[1], lags=N_LAGS,\n              alpha=SIGNIFICANCE_LEVEL) \n    ```", "```py\nfrom arch.unitroot import ADF\nadf = ADF(df[\"unemp_rate\"])\nprint(adf.summary().as_text()) \n```", "```py\n Augmented Dickey-Fuller Results   \n=====================================\nTest Statistic                 -2.053\nP-value                         0.264\nLags                               12\n-------------------------------------\nTrend: Constant\nCritical Values: -3.49 (1%), -2.89 (5%), -2.58 (10%)\nNull Hypothesis: The process contains a unit root.\nAlternative Hypothesis: The process is weakly stationary. \n```", "```py\nfrom arch.unitroot import ZivotAndrews\nza = ZivotAndrews(df[\"unemp_rate\"])\nprint(za.summary().as_text()) \n```", "```py\n Zivot-Andrews Results        \n=====================================\nTest Statistic                 -2.551\nP-value                         0.982\nLags                               12\n-------------------------------------\nTrend: Constant\nCritical Values: -5.28 (1%), -4.81 (5%), -4.57 (10%)\nNull Hypothesis: The process contains a unit root with a single structural break.\nAlternative Hypothesis: The process is trend and break stationary. \n```", "```py\n    import pandas as pd\n    import numpy as np\n    import nasdaqdatalink\n    import cpi\n    from datetime import date\n    from chapter_6_utils import test_autocorrelation\n    nasdaqdatalink.ApiConfig.api_key = \"YOUR_KEY_HERE\" \n    ```", "```py\n    df = (\n        nasdaqdatalink.get(dataset=\"WGC/GOLD_MONAVG_USD\",\n                           start_date=\"2000-01-01\",\n                           end_date=\"2010-12-31\")\n        .rename(columns={\"Value\": \"price\"})\n        .resample(\"M\")\n        .last()\n    ) \n    ```", "```py\n    DEFL_DATE = date(2010, 12, 31)\n    df[\"dt_index\"] = pd.to_datetime(df.index)\n    df[\"price_deflated\"] = df.apply(\n        lambda x: cpi.inflate(x[\"price\"], x[\"dt_index\"], DEFL_DATE), \n        axis=1\n    )\n    (\n        df.loc[:, [\"price\", \"price_deflated\"]]\n        .plot(title=\"Gold Price (deflated)\")\n    ) \n    ```", "```py\n    WINDOW = 12\n    selected_columns = [\"price_log\", \"rolling_mean_log\",\n                        \"rolling_std_log\"]\n    df[\"price_log\"] = np.log(df.price_deflated)\n    df[\"rolling_mean_log\"] = df.price_log.rolling(WINDOW) \\\n                               .mean()\n    df[\"rolling_std_log\"] = df.price_log.rolling(WINDOW) \\\n                              .std()\n    (\n        df[selected_columns]\n        .plot(title=\"Gold Price (deflated + logged)\", \n              subplots=True)\n    ) \n    ```", "```py\n    fig = test_autocorrelation(df[\"price_log\"]) \n    ```", "```py\n    ADF test statistic: 1.04 (p-val: 0.99)\n    KPSS test statistic: 1.93 (p-val: 0.01) \n    ```", "```py\n    selected_columns = [\"price_log_diff\", \"roll_mean_log_diff\",\n                        \"roll_std_log_diff\"]\n    df[\"price_log_diff\"] = df.price_log.diff(1)\n    df[\"roll_mean_log_diff\"] = df.price_log_diff.rolling(WINDOW) \\\n                                 .mean()\n    df[\"roll_std_log_diff\"] = df.price_log_diff.rolling(WINDOW) \\\n                                .std()\n    df[selected_columns].plot(title=\"Gold Price (deflated + log + diff)\") \n    ```", "```py\n    fig = test_autocorrelation(df[\"price_log_diff\"].dropna()) \n    ```", "```py\nADF test statistic: -10.87 (p-val: 0.00)\nKPSS test statistic: 0.30 (p-val: 0.10) \n```", "```py\nfrom pmdarima.arima import ndiffs, nsdiffs \n\nprint(f\"Suggested # of differences (ADF): {ndiffs(df['price'], test='adf')}\")\nprint(f\"Suggested # of differences (KPSS): {ndiffs(df['price'], test='kpss')}\")\nprint(f\"Suggested # of differences (PP): {ndiffs(df['price'], test='pp')}\") \n```", "```py\nSuggested # of differences (ADF): 1\nSuggested # of differences (KPSS): 2\nSuggested # of differences (PP): 1 \n```", "```py\nprint(f\"Suggested # of differences (OSCB): {nsdiffs(df['price'], m=12,\ntest='ocsb')}\")\nprint(f\"Suggested # of differences (CH): {nsdiffs(df['price'], m=12, test='ch')}\") \n```", "```py\nSuggested # of differences (OSCB): 0\nSuggested # of differences (CH): 0 \n```", "```py\n    import pandas as pd\n    from datetime import date\n    from statsmodels.tsa.holtwinters import (ExponentialSmoothing,\n                                             SimpleExpSmoothing,\n                                             Holt) \n    ```", "```py\n    TEST_LENGTH = 12\n    df.index.freq = \"MS\"\n    df_train = df.iloc[:-TEST_LENGTH]\n    df_test = df[-TEST_LENGTH:] \n    ```", "```py\n    ses_1 = SimpleExpSmoothing(df_train).fit(smoothing_level=0.5)\n    ses_forecast_1 = ses_1.forecast(TEST_LENGTH)\n    ses_2 = SimpleExpSmoothing(df_train).fit()\n    ses_forecast_2 = ses_2.forecast(TEST_LENGTH)\n    ses_1.params_formatted \n    ```", "```py\n    ses_df = df.copy()\n    ses_df[\"ses_1\"] = ses_1.fittedvalues.append(ses_forecast_1)\n    ses_df[\"ses_2\"] = ses_2.fittedvalues.append(ses_forecast_2)\n    opt_alpha = ses_2.model.params[\"smoothing_level\"]\n    fig, ax = plt.subplots()\n    ses_df[\"2017\":].plot(style=[\"-\",\":\",\"--\"], ax=ax,\n                         title=\"Simple Exponential Smoothing\")\n    labels = [\n        \"unemp_rate\",\n        r\"$\\alpha=0.2$\",\n        r'$\\alpha={0:.2f}$'.format(opt_alpha),\n    ]\n    ax.legend(labels) \n    ```", "```py\n    # Holt's model with linear trend\n    hs_1 = Holt(df_train).fit()\n    hs_forecast_1 = hs_1.forecast(TEST_LENGTH)\n    # Holt's model with exponential trend\n    hs_2 = Holt(df_train, exponential=True).fit()\n    hs_forecast_2 = hs_2.forecast(TEST_LENGTH)\n    # Holt's model with exponential trend and damping\n    hs_3 = Holt(df_train, exponential=False,\n                damped_trend=True).fit()\n    hs_forecast_3 = hs_3.forecast(TEST_LENGTH) \n    ```", "```py\n    hs_df = df.copy()\n    hs_df[\"hs_1\"] = hs_1.fittedvalues.append(hs_forecast_1)\n    hs_df[\"hs_2\"] = hs_2.fittedvalues.append(hs_forecast_2)\n    hs_df[\"hs_3\"] = hs_3.fittedvalues.append(hs_forecast_3)\n    fig, ax = plt.subplots()\n    hs_df[\"2017\":].plot(style=[\"-\",\":\",\"--\", \"-.\"], ax=ax,\n                        title=\"Holt's Double Exponential Smoothing\")\n    labels = [\n        \"unemp_rate\",\n        \"Linear trend\",\n        \"Exponential trend\",\n        \"Exponential trend (damped)\",\n    ]\n    ax.legend(labels) \n    ```", "```py\n    SEASONAL_PERIODS = 12\n    # Holt-Winters' model with exponential trend\n    hw_1 = ExponentialSmoothing(df_train,\n                                trend=\"mul\",\n                                seasonal=\"add\",\n                                seasonal_periods=SEASONAL_PERIODS).fit()\n    hw_forecast_1 = hw_1.forecast(TEST_LENGTH)\n    # Holt-Winters' model with exponential trend and damping\n    hw_2 = ExponentialSmoothing(df_train,\n                                trend=\"mul\",\n                                seasonal=\"add\",\n                                seasonal_periods=SEASONAL_PERIODS,\n                                damped_trend=True).fit()\n    hw_forecast_2 = hw_2.forecast(TEST_LENGTH) \n    ```", "```py\n    hw_df = df.copy()\n    hw_df[\"hw_1\"] = hw_1.fittedvalues.append(hw_forecast_1)\n    hw_df[\"hw_2\"] = hw_2.fittedvalues.append(hw_forecast_2)\n    fig, ax = plt.subplots()\n    hw_df[\"2017\":].plot(\n        style=[\"-\",\":\",\"--\"], ax=ax,\n        title=\"Holt-Winters' Triple Exponential Smoothing\"\n    )\n    phi = hw_2.model.params[\"damping_trend\"]\n    labels = [\n        \"unemp_rate\",\n        \"Seasonal Smoothing\",\n        f\"Seasonal Smoothing (damped with $\\phi={phi:.2f}$)\"\n    ]\n    ax.legend(labels) \n    ```", "```py\n    from sktime.forecasting.ets import AutoETS\n    from sklearn.metrics import mean_absolute_percentage_error \n    ```", "```py\n    auto_ets = AutoETS(auto=True, n_jobs=-1, sp=12)\n    auto_ets.fit(df_train.to_period())\n    auto_ets_fcst = auto_ets.predict(fh=list(range(1, 13))) \n    ```", "```py\n    auto_ets_df = hw_df.to_period().copy()\n    auto_ets_df[\"auto_ets\"] = (\n        auto_ets\n        ._fitted_forecaster\n        .fittedvalues\n        .append(auto_ets_fcst[\"unemp_rate\"])\n    )\n    fig, ax = plt.subplots()\n    auto_ets_df[\"2017\":].plot(\n        style=[\"-\",\":\",\"--\",\"-.\"], ax=ax,\n        title=\"Holt-Winters' models vs. AutoETS\"\n    )\n    labels = [\n        \"unemp_rate\",\n        \"Seasonal Smoothing\",\n        f\"Seasonal Smoothing (damped with $\\phi={phi:.2f}$)\",\n        \"AutoETS\",\n    ]\n    ax.legend(labels) \n    ```", "```py\n    fcst_dict = {\n        \"Seasonal Smoothing\": hw_forecast_1,\n        \"Seasonal Smoothing (damped)\": hw_forecast_2,\n        \"AutoETS\": auto_ets_fcst,\n    }\n\n    print(\"MAPEs ----\")\n    for key, value in fcst_dict.items():\n        mape = mean_absolute_percentage_error(df_test, value)\n        print(f\"{key}: {100 * mape:.2f}%\") \n    ```", "```py\nMAPEs ----\nSeasonal Smoothing: 1.81%\nSeasonal Smoothing (damped): 6.53%\nAutoETS: 1.78% \n```", "```py\n    import pandas as pd\n    import numpy as np\n    from statsmodels.tsa.arima.model import ARIMA\n    from chapter_6_utils import test_autocorrelation\n    from sklearn.metrics import mean_absolute_percentage_error \n    ```", "```py\n    TEST_LENGTH = 12\n    df_train = df.iloc[:-TEST_LENGTH]\n    df_test = df.iloc[-TEST_LENGTH:] \n    ```", "```py\n    df_train[\"unemp_rate_log\"] = np.log(df_train[\"unemp_rate\"])\n    df_train[\"first_diff\"] = df_train[\"unemp_rate_log\"].diff()\n    df_train.plot(subplots=True, \n                  title=\"Original vs transformed series\") \n    ```", "```py\n    fig = test_autocorrelation(df_train[\"first_diff\"].dropna()) \n    ```", "```py\n    ADF test statistic: -2.97 (p-val: 0.04)\n    KPSS test statistic: 0.04 (p-val: 0.10) \n    ```", "```py\n    arima_111 = ARIMA(\n        df_train[\"unemp_rate_log\"], order=(1, 1, 1)\n    ).fit()\n    arima_111.summary() \n    ```", "```py\n    arima_212 = ARIMA(\n        df_train[\"unemp_rate_log\"], order=(2, 1, 2)\n    ).fit()\n    arima_212.summary() \n    ```", "```py\n    df[\"pred_111_log\"] = (\n        arima_111\n        .fittedvalues\n        .append(arima_111.forecast(TEST_LENGTH))\n    )\n    df[\"pred_111\"] = np.exp(df[\"pred_111_log\"])\n\n    df[\"pred_212_log\"] = (\n        arima_212\n        .fittedvalues\n        .append(arima_212.forecast(TEST_LENGTH))\n    )\n    df[\"pred_212\"] = np.exp(df[\"pred_212_log\"])\n    df \n    ```", "```py\n    (\n        df[[\"unemp_rate\", \"pred_111\", \"pred_212\"]]\n        .iloc[1:]\n        .plot(title=\"ARIMA forecast of the US unemployment rate\")\n    ) \n    ```", "```py\n    (\n        df[[\"unemp_rate\", \"pred_111\", \"pred_212\"]]\n        .iloc[-TEST_LENGTH:]\n        .plot(title=\"Zooming in on the out-of-sample forecast\")\n    ) \n    ```", "```py\n    mape_111 = mean_absolute_percentage_error(\n        df[\"unemp_rate\"].iloc[-TEST_LENGTH:],\n        df[\"pred_111\"].iloc[-TEST_LENGTH:]\n    )\n    mape_212 = mean_absolute_percentage_error(\n        df[\"unemp_rate\"].iloc[-TEST_LENGTH:],\n        df[\"pred_212\"].iloc[-TEST_LENGTH:]\n    )\n    print(f\"MAPE of ARIMA(1,1,1): {100 * mape_111:.2f}%\")\n    print(f\"MAPE of ARIMA(2,1,2): {100 * mape_212:.2f}%\") \n    ```", "```py\n    MAPE of ARIMA(1,1,1): 9.14%\n    MAPE of ARIMA(2,1,2): 5.08% \n    ```", "```py\n    preds_df = arima_212.get_forecast(TEST_LENGTH).summary_frame()\n    preds_df.columns = [\"fcst\", \"fcst_se\", \"ci_lower\", \"ci_upper\"]\n    plot_df = df_test[[\"unemp_rate\"]].join(np.exp(preds_df))\n    fig, ax = plt.subplots()\n    (\n        plot_df[[\"unemp_rate\", \"fcst\"]]\n        .plot(ax=ax,\n              title=\"ARIMA(2,1,2) forecast with confidence intervals\")\n    )\n    ax.fill_between(plot_df.index,\n                    plot_df[\"ci_lower\"],\n                    plot_df[\"ci_upper\"],\n                    alpha=0.3,\n                    facecolor=\"g\")\n    ax.legend(loc=\"upper left\") \n    ```", "```py\narima_212.plot_diagnostics(figsize=(18, 14), lags=25) \n```", "```py\nljung_box_results = arima_212.test_serial_correlation(method=\"ljungbox\")\nljung_box_pvals = ljung_box_results[0][1]\nfig, ax = plt.subplots(1, figsize=[16, 5])\nsns.scatterplot(x=range(len(ljung_box_pvals)),\n                y=ljung_box_pvals,\n                ax=ax)\nax.axhline(0.05, ls=\"--\", c=\"r\")\nax.set(title=\"Ljung-Box test's results\",\n       xlabel=\"Lag\",\n       ylabel=\"p-value\") \n```", "```py\n    import pandas as pd\n    import pmdarima as pm\n    from sklearn.metrics import mean_absolute_percentage_error \n    ```", "```py\n    TEST_LENGTH = 12\n    df_train = df.iloc[:-TEST_LENGTH]\n    df_test = df.iloc[-TEST_LENGTH:] \n    ```", "```py\n    auto_arima = pm.auto_arima(df_train,\n                               test=\"adf\",\n                               seasonal=False,\n                               with_intercept=False,\n                               stepwise=True,\n                               suppress_warnings=True,\n                               trace=True)\n\n    auto_arima.summary() \n    ```", "```py\n    Performing stepwise search to minimize aic\n     ARIMA(2,1,2)(0,0,0)[0]             : AIC=7.411, Time=0.24 sec\n     ARIMA(0,1,0)(0,0,0)[0]             : AIC=77.864, Time=0.01 sec\n     ARIMA(1,1,0)(0,0,0)[0]             : AIC=77.461, Time=0.01 sec\n     ARIMA(0,1,1)(0,0,0)[0]             : AIC=75.688, Time=0.01 sec\n     ARIMA(1,1,2)(0,0,0)[0]             : AIC=68.551, Time=0.01 sec\n     ARIMA(2,1,1)(0,0,0)[0]             : AIC=54.321, Time=0.03 sec\n     ARIMA(3,1,2)(0,0,0)[0]             : AIC=7.458, Time=0.07 sec\n     ARIMA(2,1,3)(0,0,0)[0]             : AIC=inf, Time=0.07 sec\n     ARIMA(1,1,1)(0,0,0)[0]             : AIC=78.507, Time=0.02 sec\n     ARIMA(1,1,3)(0,0,0)[0]             : AIC=60.069, Time=0.02 sec\n     ARIMA(3,1,1)(0,0,0)[0]             : AIC=41.703, Time=0.02 sec\n     ARIMA(3,1,3)(0,0,0)[0]             : AIC=10.527, Time=0.10 sec\n     ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=inf, Time=0.08 sec\n    Best model:  ARIMA(2,1,2)(0,0,0)[0]          \n    Total fit time: 0.740 seconds \n    ```", "```py\n    auto_arima.plot_diagnostics(figsize=(18, 14), lags=25) \n    ```", "```py\n    auto_sarima = pm.auto_arima(df_train,\n                                test=\"adf\",\n                                seasonal=True,\n                                m=12,\n                                with_intercept=False,\n                                stepwise=True,\n                                suppress_warnings=True,\n                                trace=True)\n    auto_sarima.summary() \n    ```", "```py\n    auto_sarima.plot_diagnostics(figsize=(18, 14), lags=25) \n    ```", "```py\n    df_test[\"auto_arima\"] = auto_arima.predict(TEST_LENGTH)\n    df_test[\"auto_sarima\"] = auto_sarima.predict(TEST_LENGTH)\n    df_test.plot(title=\"Forecasts of the best ARIMA/SARIMA models\") \n    ```", "```py\nmape_auto_arima = mean_absolute_percentage_error(\n    df_test[\"unemp_rate\"], \n    df_test[\"auto_arima\"]\n)\n\nmape_auto_sarima = mean_absolute_percentage_error(\n    df_test[\"unemp_rate\"], \n    df_test[\"auto_sarima\"]\n)\n\nprint(f\"MAPE of auto-ARIMA: {100*mape_auto_arima:.2f}%\")\nprint(f\"MAPE of auto-SARIMA: {100*mape_auto_sarima:.2f}%\") \n```", "```py\nMAPE of auto-ARIMA: 6.17%\nMAPE of auto-SARIMA: 5.70% \n```", "```py\nfrom pmdarima.pipeline import Pipeline\nfrom pmdarima.preprocessing import FourierFeaturizer\nfrom pmdarima.preprocessing import LogEndogTransformer\nfrom pmdarima import arima \n```", "```py\nmonth_dummies = pd.get_dummies(\n    df.index.month, \n    prefix=\"month_\", \n    drop_first=True\n)\nmonth_dummies.index = df.index\ndf = df.join(month_dummies)\n\ndf_train = df.iloc[:-TEST_LENGTH]\ndf_test = df.iloc[-TEST_LENGTH:] \n```", "```py\nauto_arimax = pm.auto_arima(\n    df_train[[\"unemp_rate\"]],\n    exogenous=df_train.drop(columns=[\"unemp_rate\"]),\n    test=\"adf\",\n    seasonal=False,\n    with_intercept=False,\n    stepwise=True,\n    suppress_warnings=True,\n    trace=True\n)\n\nauto_arimax.summary() \n```", "```py\nauto_arima_pipe = Pipeline([\n    (\"log_transform\", LogEndogTransformer()),\n    (\"fourier\", FourierFeaturizer(m=12)),\n    (\"arima\", arima.AutoARIMA(stepwise=True, trace=1, \n                              error_action=\"warn\",\n                              test=\"adf\", seasonal=False, \n                              with_intercept=False, \n                              suppress_warnings=True))\n])\n\nauto_arima_pipe.fit(df_train[[\"unemp_rate\"]]) \n```", "```py\nBest model:  ARIMA(4,1,0)(0,0,0)[0] intercept \n```", "```py\n    results_df = df_test[[\"unemp_rate\"]].copy()\n    results_df[\"auto_arimax\"] = auto_arimax.predict(\n        TEST_LENGTH,\n        X=df_test.drop(columns=[\"unemp_rate\"])\n    )\n    results_df[\"auto_arima_pipe\"] = auto_arima_pipe.predict(TEST_LENGTH)\n    results_df.plot(title=\"Forecasts of the ARIMAX/pipe models\") \n    ```", "```py\nMAPE of auto-ARIMAX: 6.88%\nMAPE of auto-pipe: 4.61% \n```"]