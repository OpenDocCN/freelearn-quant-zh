["```py\n\nimport pandas as pd\n# Data loading from a CSV file\nfulldata = pd.read_csv('../../data/nayaone_synthetic_data.csv')\n# Transforming data to float\ndata = fulldata.astype(float)\ndata.head(10)\n```", "```py\n\ndata_train = data.groupby([data.index,'train']).filter(lambda x: x['train'] == 1.).reset_index()\ndata_test = data.groupby([data.index,'train']).filter(lambda x: x['train'] == 0.).reset_index()\n```", "```py\n\n# Separate X and y considering dropping not useful columns\nX_train = data_train.drop(['target', 'Unnamed: 0', 'train'] ,axis=\"columns\")\ny_train = data_train['target']\nX_test = data_test.drop(['target', 'Unnamed: 0', 'train'] ,axis=\"columns\")\ny_test = data_test['target']\n```", "```py\n\n# Review the balance of the target variable in train\ny_train.value_counts(normalize=True)*100\n0.0    69.953052\n1.0    30.046948\n# Review the balance of the target variable in test\ny_test.value_counts(normalize=True)*100\n0.0    71.636364\n1.0    28.363636\n```", "```py\n\n# Hard split by half on the dataframe for the LDA dimensionality reduction\n# Train split\nfeatures_a = X_train.iloc[:,:83]\nfeatures_b = X_train.iloc[:,83:]\n# Test split\nfeatures_a_test = X_test.iloc[:,:83]\nfeatures_b_test = X_test.iloc[:,83:]\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n# LDA fit with the separated groups\nlda1 = LDA(n_components=1, solver='svd').fit(features_a, y_train)\nlda2 = LDA(n_components=1, solver='svd').fit(features_b, y_train)\n# LDA train transformation\nfeatures_lda_1 = lda1.transform(features_a)\nfeatures_lda_2 = lda2.transform(features_b)\n# LDA test transformation (using train fit)\nfeatures_lda_1_test = lda1.transform(features_a_test)\nfeatures_lda_2_test = lda2.transform(features_b_test)\n# Arrays to dataframe for join in a single dataframe\nfeatures_lda_1 = pd.DataFrame(features_lda_1)\nfeatures_lda_2 = pd.DataFrame(features_lda_2)\nfeatures_lda_1_test = pd.DataFrame(features_lda_1_test)\nfeatures_lda_2_test = pd.DataFrame(features_lda_2_test)\n# Join of dataframes\nx_train_lda = pd.concat([features_lda_1, features_lda_2], axis=1)\nx_test_lda = pd.concat([features_lda_1_test, features_lda_2_test], axis=1)\n```", "```py\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n## QSVC\nminmax_scaler = MinMaxScaler().fit(x_train_lda)\nX_train_qsvc = minmax_scaler.transform(x_train_lda)\nX_test_qsvc = minmax_scaler.transform(x_test_lda)\n## SVC\nstrd_scaler = StandardScaler().fit(x_train_lda)\nX_train_svc = strd_scaler.transform(x_train_lda)\nX_test_svc = strd_scaler.transform(x_test_lda)\n## VQC\nstrd_scaler = StandardScaler().fit(x_train_lda)\nX_train_vqc = strd_scaler.transform(x_train_lda)\nX_test_vqc = strd_scaler.transform(x_test_lda)\ny_train_vqc = pd.DataFrame(y_train)\ny_test_vqc = pd.DataFrame(y_test)\n## Quantum Neural Network\nminmax_scaler = MinMaxScaler().fit(x_train_lda)\nX_train_nn = minmax_scaler.transform(x_train_lda)\nX_test_nn = minmax_scaler.transform(x_test_lda)\ny_train_nn = y_train.to_numpy()\ny_test_nn = y_test.to_numpy()\n```", "```py\n\nfrom sklearn.svm import SVC\n# Instantiate the SVC\nsvc = SVC()\n# Training\nsvc.fit(X_train_svc,y_train)\n# Testing\nsvc_score = svc.score(X_test_svc, y_test)\nprint(f\"SVC classification test score: {svc_score}\")\nSVC classification test score: 0.7927272727272727\n```", "```py\n\nfrom sklearn import metrics\n# Classification report of SVC\nexpected_y_svc  = y_test\npredicted_y_svc = svc.predict(X_test_svc)\n# Print classification report and confusion matrix\nprint(\"Classification report: \\n\", metrics.classification_report(expected_y_svc, predicted_y_svc))\nClassification report:\n               precision    recall  f1-score   support\n         0.0       0.85      0.85      0.85       194\n         1.0       0.65      0.65      0.65        81\n    accuracy                           0.79       275\n   macro avg       0.75      0.75      0.75       275\nweighted avg       0.79      0.79      0.79       275\n```", "```py\n\nfrom qiskit.providers.aer import AerSimulator\nfrom qiskit.utils import QuantumInstance\nfrom qiskit.circuit.library import ZZFeatureMap\n# Defining backend and feature map to be used\nbackend = QuantumInstance(\n     AerSimulator(method='statevector'),\n    seed_simulator=algorithm_globals.random_seed,\n    seed_transpiler=algorithm_globals.random_seed,\n)\n# ZZ feature map\nfeature_map = ZZFeatureMap(feature_dimension=num_qubits, reps=3)\n# Feature map circuit print\nfeature_map.decompose().draw(output = \"mpl\")\n```", "```py\n\nfrom qiskit_machine_learning.kernels import QuantumKernel\nfrom qiskit_machine_learning.algorithms import QSVC\n# Defining quantum kernel and qsvc\nkernel = QuantumKernel(feature_map=feature_map, quantum_instance=backend)\nqsvc = QSVC(quantum_kernel=kernel, C=C)\n# Training\nqsvc.fit(X_train_qsvc,y_train)\n# Testing\nqsvc_score = qsvc.score(X_test_qsvc, y_test)\nprint(f\"QSVC classification test score: {qsvc_score}\")\nQSVC classification test score: 0.7927272727272727\n```", "```py\n\n# Classification report of QSVC\nexpected_y_qsvc  = y_test\npredicted_y_qsvc = qsvc.predict(X_test_qsvc)\n# Print classification report and confusion matrix\nprint(\"Classification report: \\n\", metrics.classification_report(expected_y_qsvc, predicted_y_qsvc))\nClassification report:\n               precision    recall  f1-score   support\n         0.0       0.88      0.82      0.85       194\n         1.0       0.63      0.73      0.67        81\n    accuracy                           0.79       275\n   macro avg       0.75      0.77      0.76       275\nweighted avg       0.80      0.79      0.80       275\n```", "```py\n\n# Keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n# Fit the model with specific layers and activations\nmodel = Sequential()\nmodel.add(Dense(60,input_dim=X_train_nn.shape[1],activation='relu',kernel_initializer='normal'))\nmodel.add(Dense(40,activation='relu'))\nmodel.add(Dense(20,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\n# Model compiled\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train_nn, y_train, batch_size=batch,\n                 epochs=50, validation_data = (X_test_nn, y_test),verbose=1)\n```", "```py\n\n# Testing\nnn_score = model.evaluate(X_test_nn, y_test)\nprint(\"NN test accuracy score: %.2f%%\" % (nn_score[1]*100))\nNN test accuracy score: 80.36%\n# Classification report of NN\nexpected_y_nn  = y_test\npredicted_y_nn = (model.predict(X_test_nn) > 0.5).astype(\"int32\")\n# Print classification report and confusion matrix\nprint(\"Classification report: \\n\", metrics.classification_report(expected_y_nn, predicted_y_nn))\nClassification report:\n               precision    recall  f1-score   support\n         0.0       0.86      0.87      0.86       194\n         1.0       0.67      0.65      0.66        81\n    accuracy                           0.80       275\n   macro avg       0.76      0.76      0.76       275\nweighted avg       0.80      0.80      0.80       275\n```", "```py\n\nfrom qiskit import QuantumCircuit\nfrom qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n# Declare the feature map\nfeature_map = ZZFeatureMap(num_inputs)\n# Declare the ansatz\nansatz = RealAmplitudes(num_inputs, reps=2)\n# Construct the quantum circuit\nqc = QuantumCircuit(num_inputs)\nqc.append(feature_map, range(num_inputs))\nqc.append(ansatz, range(num_inputs))\nqc.decompose().draw('mpl')\n```", "```py\n\nfrom qiskit.providers.aer import Aer\nfrom qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\nfrom qiskit_machine_learning.neural_networks import CircuitQNN\nfrom qiskit.algorithms.optimizers import L_BFGS_B\n# Maps bitstrings to 0 or 1\ndef parity(x):\n    return \"{:b}\".format(x).count(\"1\") % 2\n# Defining quantum instance\nquantum_instance = QuantumInstance(Aer.get_backend(\"aer_simulator\"), shots=shots)\n# Declare the QNN circuit\ncircuit_qnn = CircuitQNN(\n    circuit=qc,\n    input_params=feature_map.parameters,\n    weight_params=ansatz.parameters,\n    interpret=parity,\n    output_shape=2,\n    quantum_instance=quantum_instance,\n)\n# Declare the classifier\ncircuit_classifier = NeuralNetworkClassifier(\n            neural_network=circuit_qnn, optimizer= L_BFGS_B(maxiter=maxiter), loss='absolute_error'\n)\ncircuit_classifier.fit(X_train_nn,y_train_nn)\ny_pred = circuit_classifier.predict(X_test_nn)\n# print classification report and confusion matrix for the classifier\nprint(\"Classification report: \\n\", metrics.classification_report(y_test_nn, y_pred))\nClassification report:\n               precision    recall  f1-score   support\n         0.0       0.90      0.67      0.77       194\n         1.0       0.51      0.83      0.63        81\n    accuracy                           0.72       275\n   macro avg       0.71      0.75      0.70       275\nweighted avg       0.79      0.72      0.73       275\n```", "```py\n\n# PennyLane\nimport pennylane as qml\nfrom pennylane import numpy as np\nfrom pennylane.templates.embeddings import AngleEmbedding\nfrom pennylane.optimize import AdamOptimizer\n# Device\ndev = qml.device('default.qubit', wires = num_qubits)\n# Our generic candidate circuit\ndef circuit(parameters, X_train_vqc):\n    for i in range(num_qubits):\n        qml.Hadamard(wires = i)\n    # Angle embedding for classical embedding\n    AngleEmbedding(features = X_train_vqc, wires = range(num_qubits), rotation = 'Y')\n    # This will be our PQC of choice\n    qml.StronglyEntanglingLayers(weights = parameters, wires = range(num_qubits))\n    # And measuring on 0 qubit we will get if it corresponds to one or other label\n    return qml.expval(qml.PauliZ(0))\n```", "```py\n\n# QNode: Device + Circuit\nvqc = qml.QNode(circuit, dev, diff_method=\"backprop\")\n```", "```py\n\n# VQC functions\ndef variational_classifier(weights, bias, x):\n    return vqc(weights, x) + bias\n```", "```py\n\ndef square_loss(labels, predictions):\n    loss = 0\n    for l, p in zip(labels, predictions):\n        loss = loss + (l - p) ** 2\n    loss = loss / len(labels)\n    return loss\ndef cost(weights, bias, X, Y):\n    predictions = [variational_classifier(weights, bias, x) for x in X]\n    return square_loss(Y, predictions)\n```", "```py\n\ndef accuracy(labels, predictions):\n    loss = 0\n    for l, p in zip(labels, predictions):\n        if abs(l - p) < 1e-5:\n            loss = loss + 1\n    loss = loss / len(labels)\n    return loss\n# Optimizer declaration and batch parameter\nopt = AdamOptimizer(stepsize=0.1, beta1=0.9, beta2=0.99, eps=1e-08)\nbatch_size = batch\nweights = weights_init\nbias = bias_init\nwbest = 0\nbbest = 0\nabest = 0\nfor it in range(50):\n    # Weights update by each optimizer step\n    batch_index = np.random.randint(0, len(X_train_nn), (batch_size,))\n    X_batch = X_train_nn[batch_index]\n    Y_batch = y_train_nn[batch_index]\n    weights, bias, _, _ = opt.step(cost, weights, bias, X_batch, Y_batch)\n    # Accuracy computation\n    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X_batch]\n    acc = accuracy(Y_batch, predictions)\n    if acc > abest:\n        wbest = weights\n        bbest = bias\n        abest = acc\n        print('New best')\n    print(\n        \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(\n            it + 1, cost(weights, bias, X_batch, Y_batch), acc\n        )\n    )\n# X_test and y_test transformation to be analyzed\nYte = np.array(y_test_vqc.values[:,0] * 2 - np.ones(len(y_test_vqc.values[:,0])), requires_grad = False)\nXte = np.array(normalize(X_test_vqc), requires_grad=False)\n# Testing\npredictions = [np.sign(variational_classifier(wbest, bbest, x)) for x in Xte]\naccuracy_vqc = accuracy(Yte, predictions)\nprint(f'VQC test accuracy score: {np.round(accuracy_vqc, 2) * 100}%')\nVQC test accuracy score: 79.0%\n# Classification report of VQC\nexpected_y_vqc  = Yte\npredicted_y_vqc = predictions\n# Print classification report and confusion matrix\nprint(\"Classification report: \\n\", metrics.classification_report(expected_y_vqc, predicted_y_vqc))\nClassification report:\n               precision    recall  f1-score   support\n        -1.0       0.89      0.79      0.84       194\n         1.0       0.61      0.77      0.68        81\n    accuracy                           0.79       275\n   macro avg       0.75      0.78      0.76       275\nweighted avg       0.81      0.79      0.79       275\n```"]