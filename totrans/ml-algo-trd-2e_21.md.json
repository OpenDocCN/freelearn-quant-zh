["```py\ndef build_generator():\n    return Sequential([Dense(7 * 7 * 256, \n                             use_bias=False,\n                             input_shape=(100,), \n                             name='IN'),\n                       BatchNormalization(name='BN1'),\n                       LeakyReLU(name='RELU1'),\n                       Reshape((7, 7, 256), name='SHAPE1'),\n                       Conv2DTranspose(128, (5, 5), \n                                       strides=(1, 1),\n                                       padding='same', \n                                       use_bias=False,\n                                       name='CONV1'),\n                       BatchNormalization(name='BN2'),\n                       LeakyReLU(name='RELU2'),\n                       Conv2DTranspose(64, (5, 5), \n                                       strides=(2, 2),\n                                       padding='same',\n                                       use_bias=False,\n                                       name='CONV2'),\n                       BatchNormalization(name='BN3'),\n                       LeakyReLU(name='RELU3'),\n                       Conv2DTranspose(1, (5, 5), \n                                       strides=(2, 2),\n                                       padding='same', \n                                       use_bias=False,\n                                       activation='tanh', \n                                       name='CONV3')],\n                      name='Generator') \n```", "```py\ndef build_discriminator():\n    return Sequential([Conv2D(64, (5, 5), \n                              strides=(2, 2), \n                              padding='same',\n                              input_shape=[28, 28, 1], \n                              name='CONV1'),\n                       LeakyReLU(name='RELU1'),\n                       Dropout(0.3, name='DO1'),\n                       Conv2D(128, (5, 5), \n                              strides=(2, 2),\n                              padding='same', \n                              name='CONV2'),\n                       LeakyReLU(name='RELU2'),\n                       Dropout(0.3, name='DO2'),\n                       Flatten(name='FLAT'),\n                       Dense(1, name='OUT')],\n                      name='Discriminator') \n```", "```py\ncross_entropy = BinaryCrossentropy(from_logits=True)\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output) \n```", "```py\ndef discriminator_loss(true_output, fake_output):\n    true_loss = cross_entropy(tf.ones_like(true_output), true_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    return true_loss + fake_loss \n```", "```py\ngen_optimizer = Adam(1e-4)\ndis_optimizer = Adam(1e-4) \n```", "```py\n@tf.function\ndef train_step(images):\n    # generate the random input for the generator\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:     \n        # get the generator output\n        generated_img = generator(noise, training=True)\n        # collect discriminator decisions regarding real and fake input\n        true_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_img, training=True)\n        # compute the loss for each model\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(true_output, fake_output)\n    # compute the gradients for each loss with respect to the model variables\n    grad_generator = gen_tape.gradient(gen_loss,\n                                       generator.trainable_variables)\n    grad_discriminator = disc_tape.gradient(disc_loss,\n                                            discriminator.trainable_variables)\n    # apply the gradient to complete the backpropagation step\n    gen_optimizer.apply_gradients(zip(grad_generator,\n                                      generator.trainable_variables))\n    dis_optimizer.apply_gradients(zip(grad_discriminator,\n                                      discriminator.trainable_variables)) \n```", "```py\ndef train(dataset, epochs, save_every=10):\n    for epoch in tqdm(range(epochs)):\n        for img_batch in dataset:\n            train_step(img_batch)\n        # produce images for the GIF as we go\n        display.clear_output(wait=True)\n        generate_and_save_images(generator, epoch + 1, seed)\n        # Save the model every 10 EPOCHS\n        if (epoch + 1) % save_every == 0:\n            checkpoint.save(file_prefix=checkpoint_prefix)\n        # Generator after final epoch\n    display.clear_output(wait=True)\n    generate_and_save_images(generator, epochs, seed)\ntrain(train_set, EPOCHS) \n```", "```py\ndf = pd.read_hdf(hdf_store, 'data/real')\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(df).astype(np.float32) \n```", "```py\ndata = []\nfor i in range(len(df) - seq_len):\n    data.append(scaled_data[i:i + seq_len])\nn_series = len(data) \n```", "```py\nreal_series = (tf.data.Dataset\n               .from_tensor_slices(data)\n               .shuffle(buffer_size=n_windows)\n               .batch(batch_size))\nreal_series_iter = iter(real_series.repeat()) \n```", "```py\ndef make_random_data():\n    while True:\n        yield np.random.uniform(low=0, high=1, size=(seq_len, n_seq))\nrandom_series = iter(tf.data.Dataset\n                     .from_generator(make_random_data,\n                                     output_types=tf.float32)\n                     .batch(batch_size)\n                     .repeat()) \n```", "```py\ndef make_rnn(n_layers, hidden_units, output_units, name):\n    return Sequential([GRU(units=hidden_units,\n                           return_sequences=True,\n                           name=f'GRU_{i + 1}') for i in range(n_layers)] +\n                      [Dense(units=output_units,\n                             activation='sigmoid',\n                             name='OUT')], name=name) \n```", "```py\nembedder = make_rnn(n_layers=3, \n                    hidden_units=hidden_dim, \n                    output_units=hidden_dim, \n                    name='Embedder')\nrecovery = make_rnn(n_layers=3, \n                    hidden_units=hidden_dim, \n                    output_units=n_seq, \n                    name='Recovery') \n```", "```py\ngenerator = make_rnn(n_layers=3, \n                     hidden_units=hidden_dim, \n                     output_units=hidden_dim, \n                     name='Generator')\ndiscriminator = make_rnn(n_layers=3, \n                         hidden_units=hidden_dim, \n                         output_units=1, \n                         name='Discriminator')\nsupervisor = make_rnn(n_layers=2, \n                      hidden_units=hidden_dim, \n                      output_units=hidden_dim, \n                      name='Supervisor') \n```", "```py\nmse = MeanSquaredError()\nbce = BinaryCrossentropy() \n```", "```py\nH = embedder(X)\nX_tilde = recovery(H)\nautoencoder = Model(inputs=X,\n                    outputs=X_tilde,\n                    name='Autoencoder')\nautoencoder.summary()\nModel: \"Autoencoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nRealData (InputLayer)        [(None, 24, 6)]           0         \n_________________________________________________________________\nEmbedder (Sequential)        (None, 24, 24)            10104     \n_________________________________________________________________\nRecovery (Sequential)        (None, 24, 6)             10950     \n=================================================================\nTrainable params: 21,054 \n```", "```py\nautoencoder_optimizer = Adam()\n@tf.function\ndef train_autoencoder_init(x):\n    with tf.GradientTape() as tape:\n        x_tilde = autoencoder(x)\n        embedding_loss_t0 = mse(x, x_tilde)\n        e_loss_0 = 10 * tf.sqrt(embedding_loss_t0)\n    var_list = embedder.trainable_variables + recovery.trainable_variables\n    gradients = tape.gradient(e_loss_0, var_list)\n    autoencoder_optimizer.apply_gradients(zip(gradients, var_list))\n    return tf.sqrt(embedding_loss_t0) \n```", "```py\nfor step in tqdm(range(train_steps)):\n    X_ = next(real_series_iter)\n    step_e_loss_t0 = train_autoencoder_init(X_)\n    with writer.as_default():\n        tf.summary.scalar('Loss Autoencoder Init', step_e_loss_t0, step=step) \n```", "```py\nsupervisor_optimizer = Adam()\n@tf.function\ndef train_supervisor(x):\n    with tf.GradientTape() as tape:\n        h = embedder(x)\n        h_hat_supervised = supervisor(h)\n        g_loss_s = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n    var_list = supervisor.trainable_variables\n    gradients = tape.gradient(g_loss_s, var_list)\n    supervisor_optimizer.apply_gradients(zip(gradients, var_list))\n    return g_loss_s \n```", "```py\ndef get_generator_moment_loss(y_true, y_pred):\n    y_true_mean, y_true_var = tf.nn.moments(x=y_true, axes=[0])\n    y_pred_mean, y_pred_var = tf.nn.moments(x=y_pred, axes=[0])\n    g_loss_mean = tf.reduce_mean(tf.abs(y_true_mean - y_pred_mean))\n    g_loss_var = tf.reduce_mean(tf.abs(tf.sqrt(y_true_var + 1e-6) - \n                                       tf.sqrt(y_pred_var + 1e-6)))\n    return g_loss_mean + g_loss_var \n```", "```py\nE_hat = generator(Z)\nH_hat = supervisor(E_hat)\nX_hat = recovery(H_hat)\nsynthetic_data = Model(inputs=Z,\n                       outputs=X_hat,\n                       name='SyntheticData')\nModel: \"SyntheticData\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nRandomData (InputLayer)      [(None, 24, 6)]           0         \n_________________________________________________________________\nGenerator (Sequential)       (None, 24, 24)            10104     \n_________________________________________________________________\nSupervisor (Sequential)      (None, 24, 24)            7800      \n_________________________________________________________________\nRecovery (Sequential)        (None, 24, 6)             10950     \n=================================================================\nTrainable params: 28,854 \n```", "```py\ngenerator_optimizer = Adam()\ndiscriminator_optimizer = Adam()\nembedding_optimizer = Adam() \n```", "```py\n@tf.function\ndef train_generator(x, z):\n    with tf.GradientTape() as tape:\n        y_fake = adversarial_supervised(z)\n        generator_loss_unsupervised = bce(y_true=tf.ones_like(y_fake),\n                                          y_pred=y_fake)\n        y_fake_e = adversarial_emb(z)\n        generator_loss_unsupervised_e = bce(y_true=tf.ones_like(y_fake_e),\n                                            y_pred=y_fake_e)\n        h = embedder(x)\n        h_hat_supervised = supervisor(h)\n        generator_loss_supervised = mse(h[:, 1:, :], \n                                        h_hat_supervised[:, 1:, :])\n        x_hat = synthetic_data(z)\n        generator_moment_loss = get_generator_moment_loss(x, x_hat)\n        generator_loss = (generator_loss_unsupervised +\n                          generator_loss_unsupervised_e +\n                          100 * tf.sqrt(generator_loss_supervised) +\n                          100 * generator_moment_loss)\n    var_list = generator.trainable_variables + supervisor.trainable_variables\n    gradients = tape.gradient(generator_loss, var_list)\n    generator_optimizer.apply_gradients(zip(gradients, var_list))\n    return (generator_loss_unsupervised, generator_loss_supervised,\n            generator_moment_loss) \n```", "```py\nfor step in range(train_steps):\n    # Train generator (twice as often as discriminator)\n    for kk in range(2):\n        X_ = next(real_series_iter)\n        Z_ = next(random_series)\n        # Train generator\n        step_g_loss_u, step_g_loss_s, step_g_loss_v = train_generator(X_, Z_)\n        # Train embedder\n        step_e_loss_t0 = train_embedder(X_)\n    X_ = next(real_series_iter)\n    Z_ = next(random_series)\n    step_d_loss = get_discriminator_loss(X_, Z_)\n    if step_d_loss > 0.15:\n        step_d_loss = train_discriminator(X_, Z_)\n    if step % 1000 == 0:\n        print(f'{step:6,.0f} | d_loss: {step_d_loss:6.4f} | '\n              f'g_loss_u: {step_g_loss_u:6.4f} | '\n              f'g_loss_s: {step_g_loss_s:6.4f} | '\n              f'g_loss_v: {step_g_loss_v:6.4f} | '\n              f'e_loss_t0: {step_e_loss_t0:6.4f}')\n    with writer.as_default():\n        tf.summary.scalar('G Loss S', step_g_loss_s, step=step)\n        tf.summary.scalar('G Loss U', step_g_loss_u, step=step)\n        tf.summary.scalar('G Loss V', step_g_loss_v, step=step)\n        tf.summary.scalar('E Loss T0', step_e_loss_t0, step=step)\n        tf.summary.scalar('D Loss', step_d_loss, step=step) \n```", "```py\ngenerated_data = []\nfor i in range(int(n_windows / batch_size)):\n    Z_ = next(random_series)\n    d = synthetic_data(Z_)\n    generated_data.append(d)\nlen(generated_data)\n35 \n```", "```py\ngenerated_data = np.array(np.vstack(generated_data))\ngenerated_data.shape\n(4480, 24, 6) \n```", "```py\ngenerated_data = (scaler.inverse_transform(generated_data\n                                           .reshape(-1, n_seq))\n                  .reshape(-1, seq_len, n_seq)) \n```", "```py\n# same steps to create real sequences for training\nreal_data = get_real_data()\n# reload synthetic data\nsynthetic_data = np.load('generated_data.npy')\nsynthetic_data.shape\n(4480, 24, 6)\n# ensure same number of sequences\nreal_data = real_data[:synthetic_data.shape[0]]\nsample_size = 250\nidx = np.random.permutation(len(real_data))[:sample_size]\nreal_sample = np.asarray(real_data)[idx]\nreal_sample_2d = real_sample.reshape(-1, seq_len)\nreal_sample_2d.shape\n(1500, 24) \n```", "```py\npca = PCA(n_components=2)\npca.fit(real_sample_2d)\npca_real = (pd.DataFrame(pca.transform(real_sample_2d))\n            .assign(Data='Real'))\npca_synthetic = (pd.DataFrame(pca.transform(synthetic_sample_2d))\n                 .assign(Data='Synthetic')) \n```", "```py\ntsne_data = np.concatenate((real_sample_2d,  \n                            synthetic_sample_2d), axis=0)\ntsne = TSNE(n_components=2, perplexity=40)\ntsne_result = tsne.fit_transform(tsne_data) \n```", "```py\nsynthetic_data.shape\n(4480, 24, 6)\nn_series = synthetic_data.shape[0]\nidx = np.arange(n_series)\nn_train = int(.8*n_series)\ntrain_idx, test_idx = idx[:n_train], idx[n_train:]\ntrain_data = np.vstack((real_data[train_idx], \n                        synthetic_data[train_idx]))\ntest_data = np.vstack((real_data[test_idx], \n                       synthetic_data[test_idx]))\nn_train, n_test = len(train_idx), len(test_idx)\ntrain_labels = np.concatenate((np.ones(n_train),\n                               np.zeros(n_train)))\ntest_labels = np.concatenate((np.ones(n_test),\n                              np.zeros(n_test))) \n```", "```py\nts_classifier = Sequential([GRU(6, input_shape=(24, 6), name='GRU'),\n                            Dense(1, activation='sigmoid', name='OUT')])\nts_classifier.compile(loss='binary_crossentropy',\n                      optimizer='adam',\n                      metrics=[AUC(name='AUC'), 'accuracy'])\nModel: \"Time Series Classifier\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nGRU (GRU)                    (None, 6)                 252       \n_________________________________________________________________\nOUT (Dense)                  (None, 1)                 7         \n=================================================================\nTotal params: 259\nTrainable params: 259 \n```", "```py\nresult = ts_classifier.fit(x=train_data,\n                           y=train_labels,\n                           validation_data=(test_data, test_labels),\n                           epochs=250, batch_size=128) \n```", "```py\nts_classifier.evaluate(x=test_data, y=test_labels)\n56/56 [==============================] - 0s 2ms/step - loss: 3.7510 - AUC: 0.1596 - accuracy: 0.4403 \n```", "```py\nreal_data.shape, synthetic_data.shape\n((4480, 24, 6), (4480, 24, 6))\nreal_train_data = real_data[train_idx, :23, :]\nreal_train_label = real_data[train_idx, -1, :]\nreal_test_data = real_data[test_idx, :23, :]\nreal_test_label = real_data[test_idx, -1, :]\nreal_train_data.shape, real_train_label.shape\n((3584, 23, 6), (3584, 6)) \n```", "```py\nsynthetic_train = synthetic_data[:, :23, :]\nsynthetic_label = synthetic_data[:, -1, :]\nsynthetic_train.shape, synthetic_label.shape\n((4480, 23, 6), (4480, 6)) \n```", "```py\ndef get_model():\n    model = Sequential([GRU(12, input_shape=(seq_len-1, n_seq)),\n                        Dense(6)])\n    model.compile(optimizer=Adam(), \n                  loss=MeanAbsoluteError(name='MAE'))\n    return model \n```", "```py\nts_regression = get_model()\nsynthetic_result = ts_regression.fit(x=synthetic_train,\n                                     y=synthetic_label,\n                                     validation_data=(\n                                         real_test_data, \n                                         real_test_label),\n                                     epochs=100,\n                                     batch_size=128) \n```"]