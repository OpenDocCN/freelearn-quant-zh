["```py\nn_days = [0, 1, 3, 5, 10, 25, 50, 100, 500]\noutcomes = sp500_binary.sample(n_days[-1])\np = np.linspace(0, 1, 100)\n# uniform (uninformative) prior\na = b = 1\nfor i, days in enumerate(n_days):\n    up = outcomes.iloc[:days].sum()\n    down = days - up\n    update = stats.beta.pdf(p, a + up , b + down) \n```", "```py\n    with pm.Model() as manual_logistic_model:\n        # coefficients as rvs with uninformative priors\n        intercept = pm.Normal('intercept', 0, sd=100)\n        beta_1 = pm.Normal('beta_1', 0, sd=100)\n        beta_2 = pm.Normal('beta_2', 0, sd=100)\n        # Likelihood transforms rvs into probabilities p(y=1)\n        # according to logistic regression model.\n        likelihood = pm.invlogit(intercept +\n                                 beta_1 * data.yield_curve +\n                                 beta_2 * data.leverage)\n        # Outcome as Bernoulli rv with success probability\n        # given by sigmoid function conditioned on actual data\n        pm.Bernoulli(name='logit',\n                     p=likelihood,\n                     observed=data.recession) \n    ```", "```py\nwith pm.Model() as logistic_model:\n    pm.glm.GLM.from_formula(recession ~ yield_curve + leverage,\n                            data,\n                            family=pm.glm.families.Binomial()) \n```", "```py\nwith logistic_model:\n    map_estimate = pm.find_MAP()\nprint_map(map_estimate)\nIntercept     -4.892884\nyield_curve   -3.032943\nleverage       1.534055 \n```", "```py\nformula = 'recession ~ yield_curve + leverage + financial_conditions + sentiment'\nwith pm.Model() as logistic_model:\n    pm.glm.GLM.from_formula(formula=formula,\n                            data=data,\n                            family=pm.glm.families.Binomial())\n# note that pymc3 uses y for the outcome\nlogistic_model.basic_RVs\n[Intercept, yield_curve, leverage, financial_conditions, sentiment, y] \n```", "```py\nwith logistic_model:\n    trace = pm.sample(draws=100,\n                      tune=1000,\n                      init='adapt_diag',\n                      chains=4,\n                      cores=4,\n                      random_seed=42) \n```", "```py\nplot_traces(trace, burnin=0) \n```", "```py\nwith logistic_model:\n    callback = CheckParametersConvergence(diff='absolute')\n    approx = pm.fit(n=100000,\n                    callbacks=[callback]) \n```", "```py\ntrace_advi = approx.sample(10000) \n```", "```py\nppc = pm.sample_ppc(trace_NUTS, samples=500, model=logistic_model)\nppc['y'].shape\n(500, 445) \n```", "```py\nroc_auc_score(y_score=np.mean(ppc['y'], axis=0),\n              y_true=data.income)\n0.9483627204030226 \n```", "```py\nX = data[['yield_curve']]\nlabels = X.columns\ny = data.recession\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.2,\n                                                    random_state=42,\n                                                    stratify=y) \n```", "```py\nX_shared = theano.shared(X_train.values)\nwith pm.Model() as logistic_model_pred:\n    pm.glm.GLM(x=X_shared, labels=labels,\n               y=y_train, family=pm.glm.families.Binomial()) \n```", "```py\nwith logistic_model_pred:\n    pred_trace = pm.sample(draws=10000,\n                           tune=1000,\n                           chains=2,\n                           cores=2,\n                           init='adapt_diag') \n```", "```py\nX_shared.set_value(X_test)\nppc = pm.sample_ppc(pred_trace,\n                    model=logistic_model_pred,\n                    samples=100)\ny_score = np.mean(ppc['y'], axis=0)\nroc_auc_score(y_score=np.mean(ppc['y'], axis=0),\n              y_true=y_test)\n0.8386 \n```", "```py\nmean_prior = data.stock.mean()\nstd_prior = data.stock.std()\nstd_low = std_prior / 1000\nstd_high = std_prior * 1000\nwith pm.Model() as sharpe_model:\n    mean = pm.Normal('mean', mu=mean_prior, sd=std_prior)\n    std = pm.Uniform('std', lower=std_low, upper=std_high)\n    nu = pm.Exponential('nu_minus_two', 1 / 29, testval=4) + 2\n    returns = pm.StudentT('returns', nu=nu, mu=mean, sd=std,\nobserved=data.stock)\n    sharpe = returns.distribution.mean / returns.distribution.variance ** \n.5 * np.sqrt(252)\n    pm.Deterministic('sharpe', sharpe) \n```", "```py\nplot_posterior(data=trace); \n```", "```py\nmodel_randomwalk = pm.Model()\nwith model_randomwalk:\n    sigma_alpha = pm.Exponential('sigma_alpha', 50.)\n    alpha = pm.GaussianRandomWalk('alpha', \n                                  sd=sigma_alpha,\n                                  shape=len(prices))\n    sigma_beta = pm.Exponential('sigma_beta', 50.)\n    beta = pm.GaussianRandomWalk('beta', \n                                 sd=sigma_beta,\n                                 shape=len(prices)) \n```", "```py\nwith model_randomwalk:\n    # Define regression\n    regression = alpha + beta * prices_normed.GLD\n    # Assume prices are normally distributed\n    # Get mean from regression.\n    sd = pm.HalfNormal('sd', sd=.1)\n    likelihood = pm.Normal('y', \n                           mu=regression, \n                           sd=sd, \n                           observed=prices_normed.GFI) \n```", "```py\nwith model_randomwalk:\n    trace_rw = pm.sample(tune=2000, \n                         cores=4, \n                         draws=200, \n                         nuts_kwargs=dict(target_accept=.9)) \n```", "```py\nprices = pd.read_hdf('../data/assets.h5', key='sp500/prices').loc['2000':,\n                                                                  'Close']\nlog_returns = np.log(prices).diff().dropna()\nwith pm.Model() as model:\n    step_size = pm.Exponential('sigma', 50.)\n    s = GaussianRandomWalk('s', sd=step_size, \n                           shape=len(log_returns))\n    nu = pm.Exponential('nu', .1)\n    r = pm.StudentT('r', nu=nu, \n                    lam=pm.math.exp(-2*s), \n                    observed=log_returns) \n```", "```py\nwith model:\n    trace = pm.sample(tune=2000, \n                      draws=5000,\n                      nuts_kwargs=dict(target_accept=.9))\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [nu, s, sigma]\nSampling 4 chains, 0 divergences: 100%|██████████| 28000/28000 [27:46<00:00, 16.80draws/s]\nThe estimated number of effective samples is smaller than 200 for some parameters. \n```"]