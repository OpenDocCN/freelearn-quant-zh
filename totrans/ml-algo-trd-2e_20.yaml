- en: '20'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoencoders for Conditional Risk Factors and Asset Pricing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter shows how unsupervised learning can leverage deep learning for
    trading. More specifically, we'll discuss **autoencoders** that have been around
    for decades but have recently attracted fresh interest.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised learning** addresses practical ML challenges such as the limited
    availability of labeled data and the curse of dimensionality, which requires exponentially
    more samples for successful learning from complex, real-life data with many features.
    At a conceptual level, unsupervised learning resembles human learning and the
    development of common sense much more closely than supervised and reinforcement
    learning, which we''ll cover in the next chapter. It is also called **predictive
    learning** because it aims to discover structure and regularities from data so
    that it can predict missing inputs, that is, fill in the blanks from the observed
    parts.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'An **autoencode**r is a **neural network** (**NN**) trained to reproduce the
    input while learning a new representation of the data, encoded by the parameters
    of a hidden layer. Autoencoders have long been used for nonlinear dimensionality
    reduction and manifold learning (see *Chapter 13*, *Data-Driven Risk Factors and
    Asset Allocation with Unsupervised Learning*). A variety of designs leverage the
    feedforward, convolutional, and recurrent network architectures we covered in
    the last three chapters. We will see how autoencoders can underpin a **trading
    strategy**: we will build a deep neural network that uses an autoencoder to extract
    risk factors and predict equity returns, conditioned on a range of equity attributes
    (Gu, Kelly, and Xiu 2020).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, in this chapter you will learn about:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Which types of autoencoders are of practical use and how they work
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and training autoencoders using Python
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using autoencoders to extract data-driven risk factors that take into account
    asset characteristics to predict returns
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders for nonlinear feature extraction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 17*, *Deep Learning for Trading*, we saw how neural networks succeed
    at supervised learning by extracting a hierarchical feature representation useful
    for the given task. **Convolutional neural networks** (**CNNs**), for example,
    learn and synthesize increasingly complex patterns from grid-like data, for example,
    to identify or detect objects in an image or to classify time series.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: An autoencoder, in contrast, is a neural network designed exclusively to learn
    a **new representation** that encodes the input in a way that helps solve another
    task. To this end, the training forces the network to reproduce the input. Since
    autoencoders typically use the same data as input and output, they are also considered
    an instance of **self-supervised learning**. In the process, the parameters of
    a hidden layer *h* become the code that represents the input, similar to the word2vec
    model covered in *Chapter 16*, *Word Embeddings for Earnings Calls and SEC Filings*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, the network can be viewed as consisting of an encoder function
    *h=f(x)* that learns the hidden layer''s parameters from input *x*, and a decoder
    function g that learns to reconstruct the input from the encoding *h*. Rather
    than learning the identity function:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_20_001.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: which simply copies the input, autoencoders use **constraints** that force the
    hidden layer to **prioritize which aspects of the data to encod**e. The goal is
    to obtain a representation of practical value.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders can also be viewed as a **special case of a feedforward neural
    network** (see *Chapter 17*, *Deep Learning for Trading*) and can be trained using
    the same techniques. Just as with other models, excess capacity will lead to overfitting,
    preventing the autoencoder from producing an informative encoding that generalizes
    beyond the training samples. See *Chapters 14* and *15* of Goodfellow, Bengio,
    and Courville (2016) for additional background.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Generalizing linear dimensionality reduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A traditional use case includes dimensionality reduction, achieved by limiting
    the size of the hidden layer and thus creating a "bottleneck" so that it performs
    lossy compression. Such an autoencoder is called **undercomplete**, and the purpose
    is to learn the most salient properties of the data by minimizing a loss function
    *L* of the form:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_20_002.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: An example loss function that we will explore in the next section is simply
    the mean squared error evaluated on the pixel values of the input images and their
    reconstruction. We will also use this loss function to extract risk factors from
    time series of financial features when we build a conditional autoencoder for
    trading.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Undercomplete autoencoders differ from linear dimensionality reduction methods
    like **principal component analysis** (**PCA**; see *Chapter 13*, *Data-Driven
    Risk Factors and Asset Allocation with Unsupervised Learning*) when they use **nonlinear
    activation functions**; otherwise, they learn the same subspace as PCA. They can
    thus be viewed as a nonlinear generalization of PCA capable of learning a wider
    range of encodings.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 20.1* illustrates the encoder-decoder logic of an undercomplete feedforward
    autoencoder with three hidden layers: the encoder and decoder have one hidden
    layer each plus a shared encoder output/decoder input layer containing the encoding.
    The three hidden layers use nonlinear activation functions, like **rectified linear
    units** (**ReLU**), *sigmoid*, or *tanh* (see *Chapter 17*, *Deep Learning for
    Trading*) and have fewer units than the input that the network aims to reconstruct.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图20.1*说明了具有三个隐藏层的欠完备前馈自编码器的编码器-解码器逻辑：编码器和解码器各有一个隐藏层，再加上包含编码的共享编码器输出/解码器输入层。这三个隐藏层使用非线性激活函数，如**修正线性单元**（**ReLU**）、*sigmoid*或*tanh*（参见*第17章*，*交易的深度学习*），并且比网络要重建的输入单元更少。'
- en: '![](img/B15439_20_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_20_01.png)'
- en: 'Figure 20.1: Undercomplete encoder-decoder architecture'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.1：欠完备编码器-解码器架构
- en: Depending on the task, a simple autoencoder with a single encoder and decoder
    layer may be adequate. However, **deeper autoencoders** with additional layers
    can have several advantages, just as for other neural networks. These advantages
    include the ability to learn more complex encodings, achieve better compression,
    and do so with less computational effort and fewer training samples, subject to
    the perennial risk of overfitting.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 根据任务的不同，一个具有单个编码器和解码器层的简单自编码器可能是足够的。然而，具有额外层的**更深的自编码器**可以有几个优点，就像对其他神经网络一样。这些优点包括学习更复杂的编码、实现更好的压缩，并且在更少的计算和更少的训练样本的情况下完成，但会受到过拟合的固有风险的影响。
- en: Convolutional autoencoders for image compression
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于图像压缩的卷积自编码器
- en: As discussed in *Chapter 18*, *CNNs for Financial Time Series and Satellite
    Images*, fully connected feedforward architectures are not well suited to capture
    local correlations typical to data with a grid-like structure. Instead, autoencoders
    can also use convolutional layers to learn a hierarchical feature representation.
    Convolutional autoencoders leverage convolutions and parameter sharing to learn
    hierarchical patterns and features irrespective of their location, translation,
    or changes in size.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如*第18章*所讨论的，*用于金融时间序列和卫星图像的CNN*，全连接前馈架构不适合捕获具有网格结构的数据的局部相关性。相反，自编码器也可以使用卷积层来学习分层特征表示。卷积自编码器利用卷积和参数共享来学习层次化模式和特征，而不受其位置、平移或大小变化的影响。
- en: We will illustrate different implementations of convolutional autoencoders for
    image data below. Alternatively, convolutional autoencoders could be applied to
    multivariate time series data arranged in a grid-like format as illustrated in
    *Chapter 18*, *CNNs for Financial Time Series and Satellite Images*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下面为图像数据演示卷积自编码器的不同实现。或者，卷积自编码器也可以应用于网格形式排列的多变量时间序列数据，如*第18章*所示，*用于金融时间序列和卫星图像的CNN*。
- en: Managing overfitting with regularized autoencoders
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过正则化自编码器管理过拟合
- en: The powerful capabilities of neural networks to represent complex functions
    require tight controls of the capacity of encoders and decoders to extract signals
    rather than noise so that the encoding is more useful for a downstream task. In
    other words, when it is too easy for the network to recreate the input, it fails
    to learn only the most interesting aspects of the data and improve the performance
    of a machine learning model that uses the encoding as inputs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络表示复杂函数的强大能力要求对编码器和解码器的容量进行严格控制，以提取信号而不是噪声，从而使编码更适用于下游任务。换句话说，当网络太容易重新创建输入时，它无法仅学习数据的最有趣的方面，并提高使用编码作为输入的机器学习模型的性能。
- en: 'Just as for other models with excessive capacity for the given task, **regularization**
    can help to address the **overfitting** challenge by constraining the autoencoder''s
    learning process and forcing it to produce a useful representation (see, for instance,
    *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*, on regularization
    for linear models, and *Chapter 17*, *Deep Learning for Trading*, for neural networks).
    Ideally, we could precisely match the model''s capacity to the complexity of the
    distribution of the data. In practice, the optimal model often combines (limited)
    excess capacity with appropriate regularization. To this end, we add a sparsity
    penalty ![](img/B15439_20_003.png) that depends on the weights of the encoding
    layer *h* to the training objective:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_20_004.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: A common approach that we explore later in this chapter is the use of **L1 regularization**,
    which adds a penalty to the loss function in the form of the sum of the absolute
    values of the weights. The L1 norm results in sparse encodings because it forces
    the values of parameters to zero if they do not capture independent variation
    in the data (see *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*).
    As a result, even overcomplete autoencoders with hidden layers of a higher dimension
    than the input may be able to learn signal content.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Fixing corrupted data with denoising autoencoders
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The autoencoders we have discussed so far are designed to reproduce the input
    despite capacity constraints. An alternative approach trains autoencoders with
    corrupted inputs ![](img/B15439_20_005.png) to output the desired, original data
    points. In this case, the autoencoder minimizes a loss *L*:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_20_006.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Corrupted inputs are a different way of preventing the network from learning
    the identity function and rather extracting the signal or salient features from
    the data. Denoising autoencoders have been shown to learn the data generating
    process of the original data and have become popular in generative modeling where
    the goal is **to learn the probability distribution** that gives rise to the input
    (Vincent et al., 2008).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Seq2seq autoencoders for time series features
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Recurrent neural networks** (**RNNs**) have been developed for sequential
    data characterized by longitudinal dependencies between data points, potentially
    over long ranges (*Chapter 19*, *RNNs for Multivariate Time Series and Sentiment
    Analysis*). Similarly, sequence-to-sequence (seq2seq) autoencoders aim to learn
    representations attuned to the nature of data generated in sequence (Srivastava,
    Mansimov, and Salakhutdinov, 2016).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Seq2seq autoencoders are based on RNN components like **long short-term memory**
    (**LSTM**) or gated recurrent unit. They learn a representation of sequential
    data and have been successfully applied to video, text, audio, and time series
    data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the last chapter, encoder-decoder architectures allow RNNs
    to process input and output sequences with variable length. These architectures
    underpin many advances in complex sequence prediction tasks, like speech recognition
    and text translation, and are being increasingly applied to (financial) time series.
    At a high level, they work as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如上一章所述，编码器-解码器架构允许RNN处理具有可变长度的输入和输出序列。这些架构支撑了许多复杂序列预测任务的进展，如语音识别和文本翻译，并且越来越多地应用于（金融）时间序列。在高层次上，它们的工作原理如下：
- en: The LSTM encoder processes the input sequence step by step to learn a hidden
    state.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM编码器逐步处理输入序列以学习隐藏状态。
- en: This state becomes a learned representation of the sequence in the form of a fixed-length
    vector.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此状态成为序列的学习表示，以固定长度的向量形式呈现。
- en: The LSTM decoder receives this state as input and uses it to generate the output
    sequence.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM解码器接收此状态作为输入，并使用它来生成输出序列。
- en: See references linked on GitHub for examples on building sequence-to-sequence
    autoencoders to **compress time series data** and **detect anomalies** in time
    series to allow, for example, regulators to uncover potentially illegal trading
    activity.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见GitHub上链接的参考示例，了解构建序列到序列自动编码器以**压缩时间序列数据**和**检测时间序列中的异常**的示例，以便例如监管机构发现潜在的非法交易活动。
- en: Generative modeling with variational autoencoders
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用变分自动编码器进行生成建模
- en: '**Variational autoencoders** (**VAE**) were developed more recently (Kingma
    and Welling, 2014) and focus on generative modeling. In contrast to a discriminative
    model that learns a predictor given data, a generative model aims to solve the
    more general problem of learning a joint probability distribution over all variables.
    If successful, it could simulate how the data is produced in the first place.
    Learning the data-generating process is very valuable: it reveals underlying causal
    relationships and supports semi-supervised learning to effectively generalize
    from a small labeled dataset to a large unlabeled one.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**变分自动编码器**（**VAE**）是最近发展起来的（Kingma和Welling，2014），专注于生成建模。与给定数据学习预测器的判别模型相反，生成模型旨在解决更一般的问题，即学习所有变量的联合概率分布。如果成功，它可以模拟数据首次生成的方式。学习数据生成过程非常有价值：它揭示了潜在的因果关系，并支持半监督学习，以有效地从小型标记数据集推广到大型未标记数据集。'
- en: More specifically, VAEs are designed to learn the latent (meaning *unobserved*)
    variables of the model responsible for the input data. Note that we encountered
    latent variables in *Chapter 15*, *Topic Modeling – Summarizing Financial News*,
    and *Chapter 16*, *Word Embeddings for Earnings Calls and SEC Filings*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，VAEs旨在学习模型负责输入数据的潜在（意思是*未观察到*）变量。请注意，在*第15章*，*主题建模 - 总结财务新闻*和*第16章*，*用于盈利电话和SEC备案的词嵌入*中，我们遇到了潜在变量。
- en: Just like the autoencoders discussed so far, VAEs do not let the network learn
    arbitrary functions as long as it faithfully reproduces the input. Instead, they
    aim to learn the parameters of a probability distribution that generates the input
    data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 就像迄今讨论的自动编码器一样，VAEs不允许网络学习任意函数，只要它忠实地重现输入即可。相反，它们旨在学习生成输入数据的概率分布的参数。
- en: In other words, VAEs are generative models because, if successful, you can generate
    new data points by sampling from the distribution learned by the VAE.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，VAEs是生成模型，因为如果成功，您可以通过从VAE学习的分布中抽样来生成新的数据点。
- en: The operation of a VAE is more complex than the autoencoders discussed so far
    because it involves stochastic backpropagation, that is, taking derivatives of
    stochastic variables, and the details are beyond the scope of this book. They
    are able to learn high-capacity input encodings without regularization that are
    useful because the models aim to maximize the probability of the training data
    rather than to reproduce the input. For a detailed introduction, see Kingma and
    Welling (2019).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: VAE的操作比迄今讨论的自动编码器更复杂，因为它涉及随机反向传播，即对随机变量的导数，并且细节超出了本书的范围。它们能够学习没有正则化的高容量输入编码，这是有用的，因为模型旨在最大化训练数据的概率，而不是复制输入。有关详细介绍，请参见Kingma和Welling（2019）。
- en: The `variational_autoencoder.ipynb` notebook includes a sample VAE implementation
    applied to the Fashion MNIST data, adapted from a Keras tutorial by Francois Chollet
    to work with TensorFlow 2\. The resources linked on GitHub contain a VAE tutorial
    with references to PyTorch and TensorFlow 2 implementations and many additional
    references. See Wang et al. (2019) for an application that combines a VAE with
    an RNN using LSTM and outperforms various benchmark models in futures markets.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`variational_autoencoder.ipynb` 笔记本包含了一个应用于时尚 MNIST 数据集的样本 VAE 实现，该实现改编自 François
    Chollet 的 Keras 教程，以适配 TensorFlow 2。GitHub 上链接的资源包含一个 VAE 教程，其中包含指向 PyTorch 和
    TensorFlow 2 实现的参考资料以及许多其他参考文献。参见 Wang 等人（2019）的应用，该应用将 VAE 与使用 LSTM 的 RNN 结合起来，并在期货市场中表现出优于各种基准模型的效果。'
- en: Implementing autoencoders with TensorFlow 2
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 2 实现自编码器
- en: In this section, we'll illustrate how to implement several of the autoencoder
    models introduced in the previous section using the Keras interface of TensorFlow
    2\. We'll first load and prepare an image dataset that we'll use throughout this
    section. We will use images instead of financial time series because it makes
    it easier to visualize the results of the encoding process. The next section shows
    how to use an autoencoder with financial data as part of a more complex architecture
    that can serve as the basis for a trading strategy.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将说明如何使用 TensorFlow 2 的 Keras 接口来实现上一节介绍的几种自编码器模型。我们首先加载和准备一个图像数据集，我们将在本节中始终使用该数据集。我们将使用图像而不是金融时间序列，因为这样更容易可视化编码过程的结果。下一节将说明如何将自编码器与金融数据结合起来，作为更复杂架构的一部分，该架构可以作为交易策略的基础。
- en: After preparing the data, we'll proceed to build autoencoders using deep feedforward
    nets, sparsity constraints, and convolutions and apply the latter to denoise images.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好数据后，我们将继续构建使用深度前馈网络、稀疏约束和卷积的自编码器，并将后者应用于图像去噪。
- en: How to prepare the data
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何准备数据
- en: For illustration, we'll use the Fashion MNIST dataset, a modern drop-in replacement
    for the classic MNIST handwritten digit dataset popularized by Lecun et al. (1998)
    with LeNet. We also relied on this dataset in *Chapter 13*, *Data-Driven Risk
    Factors and Asset Allocation with Unsupervised Learning*, on unsupervised learning.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，我们将使用时尚 MNIST 数据集，这是由 Lecun 等人（1998）与 LeNet 结合使用的经典 MNIST 手写数字数据集的现代替代品。我们还在
    *第 13 章*，*使用无监督学习进行数据驱动风险因素和资产配置* 中依赖于该数据集。
- en: 'Keras makes it easy to access the 60,000 training and 10,000 test grayscale
    samples with a resolution of 28 × 28 pixels:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 使得很容易访问具有分辨率为 28 × 28 像素的 60,000 个训练和 10,000 个测试灰度样本：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The data contains clothing items from 10 classes. *Figure 20.2* plots a sample
    image for each class:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包含来自 10 类的服装物品。*图 20.2* 绘制了每个类别的一个样本图像：
- en: '![](img/B15439_20_02.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_20_02.png)'
- en: 'Figure 20.2: Fashion MNIST sample images'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.2：时尚 MNIST 样本图像
- en: 'We reshape the data so that each image is represented by a flat one-dimensional
    pixel vector with 28 × 28 = 784 elements normalized to the range [0, 1]:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重新塑造数据，使每个图像由一个扁平的一维像素向量表示，其中有 28 × 28 = 784 个元素，规范化到 [0, 1] 范围内：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: One-layer feedforward autoencoder
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单层前馈自编码器
- en: We start with a vanilla feedforward autoencoder with a single hidden layer to
    illustrate the general design approach using the Functional Keras API and establish
    a performance baseline.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个具有单个隐藏层的普通前馈自编码器开始，以说明使用 Functional Keras API 的一般设计方法，并建立性能基线。
- en: 'The first step is a placeholder for the flattened image vectors with 784 elements:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是使用 784 个元素的扁平图像向量的占位符：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The encoder part of the model consists of a fully connected layer that learns
    the new, compressed representation of the input. We use 32 units for a compression
    ratio of 24.5:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的编码器部分由一个全连接层组成，用于学习输入的新压缩表示。我们使用 32 个单元，压缩比为 24.5：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The decoding part reconstructs the compressed data to its original size in
    a single step:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 解码部分将压缩的数据一次性重构为其原始大小：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We instantiate the `Model` class with the chained input and output elements
    that implicitly define the computational graph as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用链式输入和输出元素实例化 `Model` 类，这些元素隐含地定义了计算图，如下所示：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The encoder-decoder computation thus defined uses almost 51,000 parameters:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，所定义的编码器-解码器计算使用了近 51,000 个参数：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The Functional API allows us to use parts of the model's chain as separate encoder
    and decoder models that use the autoencoder's parameters learned during training.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Functional API 允许我们使用模型链的部分作为单独的编码器和解码器模型，这些模型使用训练期间学到的自编码器的参数。
- en: Defining the encoder
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义编码器
- en: 'The encoder just uses the input and hidden layer with about half the total
    parameters:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We will see shortly that, once we train the autoencoder, we can use the encoder
    to compress the data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Defining the decoder
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The decoder consists of the last autoencoder layer, fed by a placeholder for
    the encoded data:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Training the model
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We compile the model to use the Adam optimizer (see *Chapter 17*, *Deep Learning
    for Trading*) to minimize the mean squared error between the input data and the
    reproduction achieved by the autoencoder. To ensure that the autoencoder learns
    to reproduce the input, we train the model using the same input and output data:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Evaluating the results
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Training stops after some 20 epochs with a test RMSE of 0.1121:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To encode data, we use the encoder we just defined like so:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The decoder takes the compressed data and reproduces the output according to
    the autoencoder training results:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*Figure 20.3* shows 10 original images and their reconstruction by the autoencoder
    and illustrates the loss after compression:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_20_03.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.3: Sample Fashion MNIST images, original and reconstructed'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward autoencoder with sparsity constraints
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The addition of regularization is fairly straightforward. We can apply it to
    the dense encoder layer using Keras'' `activity_regularizer` as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The input and decoding layers remain unchanged. In this example with compression
    of factor 24.5, regularization negatively affects performance with a test RMSE
    of 0.1229\.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Deep feedforward autoencoder
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To illustrate the benefit of adding depth to the autoencoder, we will build
    a three-layer feedforward model that successively compresses the input from 784
    to 128, 64, and 32 units, respectively:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The resulting model has over 222,000 parameters, more than four times the capacity
    of the previous single-layer model:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Training stops after 45 epochs and results in a 14 percent reduction of the
    test RMSE to 0.097\. Due to the low resolution, it is difficult to visually note
    the better reconstruction.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the encoding
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can use the manifold learning technique **t-distributed Stochastic Neighbor
    Embedding** (**t-SNE**; see *Chapter 13*, *Data-Driven Risk Factors and Asset
    Allocation with Unsupervised Learning*) to visualize and assess the quality of
    the encoding learned by the autoencoder's hidden layer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'If the encoding is successful in capturing the salient features of the data,
    then the compressed representation of the data should still reveal a structure
    aligned with the 10 classes that differentiate the observations. We use the output
    of the deep encoder we just trained to obtain the 32-dimensional representation
    of the test set:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*Figure 20.4* shows that the 10 classes are well separated, suggesting that
    the encoding is useful as a lower-dimensional representation that preserves the
    key characteristics of the data (see the `variational_autoencoder.ipynb` notebook
    for a color version):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_20_04.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.4: t-SNE visualization of the Fashion MNIST autoencoder embedding'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.4：Fashion MNIST自编码器嵌入的t-SNE可视化
- en: Convolutional autoencoders
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积自编码器
- en: The insights from *Chapter 18*, *CNNs for Financial Time Series and Satellite
    Images*, on CNNs suggest we incorporate convolutional layers into the autoencoder
    to extract information characteristic of the grid-like structure of image data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 从*第18章*《金融时间序列和卫星图像的CNN》，关于CNN的见解表明我们应该将卷积层合并到自编码器中，以提取具有图像数据网格结构特征的信息。
- en: 'We define a three-layer encoder that uses 2D convolutions with 32, 16, and
    8 filters, respectively, ReLU activations, and `''same''` padding to maintain
    the input size. The resulting encoding size at the third layer is ![](img/B15439_20_007.png),
    higher than for the previous examples:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个三层编码器，它使用32、16和8个滤波器的2D卷积，分别使用ReLU激活和`'same'`填充以保持输入大小。第三层的结果编码大小为![](img/B15439_20_007.png)，比之前的示例要高：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We also define a matching decoder that reverses the number of filters and uses
    2D upsampling instead of max pooling to reverse the reduction of the filter sizes.
    The three-layer autoencoder has 12,785 parameters, a little more than 5 percent
    of the capacity of the deep autoencoder.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个相匹配的解码器，它反转了滤波器数量，并使用2D上采样而不是最大池化来反转滤波器大小的减小。三层自编码器有12,785个参数，略多于深度自编码器容量的5％。
- en: Training stops after 67 epochs and results in a further 9 percent reduction
    in the test RMSE, due to a combination of the ability of convolutional filters
    to learn more efficiently from image data and the larger encoding size.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 训练在67个时代后停止，并导致测试RMSE进一步降低了9％，这是由于卷积滤波器能够更有效地从图像数据中学习以及较大的编码大小的能力的结合。
- en: Denoising autoencoders
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: 'The application of an autoencoder to a denoising task only affects the training
    stage. In this example, we add noise from a standard normal distribution to the
    Fashion MNIST data while maintaining the pixel values in the range [0, 1] as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器应用于去噪任务只影响训练阶段。在本例中，我们向Fashion MNIST数据添加了来自标准正态分布的噪声，同时保持像素值在[0, 1]范围内，如下所示：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We then proceed to train the convolutional autoencoder on noisy inputs, the
    objective being to learn how to generate the uncorrupted originals:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续在嘈杂的输入上训练卷积自编码器，目标是学习如何生成未损坏的原始图像：
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The test RMSE after 60 epochs is 0.0931, unsurprisingly higher than before.
    *Figure 20.5* shows, from top to bottom, the original images as well as the noisy
    and denoised versions. It illustrates that the autoencoder is successful in producing
    compressed encodings from the noisy images that are quite similar to those produced
    from the original images:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 60个时代后的测试RMSE为0.0931，毫不奇怪地比以前高。*图20.5*显示，从上到下，原始图像以及嘈杂和去噪版本。它说明了自编码器成功地从嘈杂的图像中产生了与从原始图像中产生的相似的压缩编码：
- en: '![](img/B15439_20_05.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_20_05.png)'
- en: 'Figure 20.5: Denoising input and output examples'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.5：去噪输入和输出示例
- en: A conditional autoencoder for trading
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于交易的有条件自编码器
- en: Recent research by Gu, Kelly, and Xiu (GKX, 2019) developed an asset pricing
    model based on the exposure of securities to risk factors. It builds on the concept
    of **data-driven risk factors** that we discussed in *Chapter 13*, *Data-Driven
    Risk Factors and Asset Allocation with Unsupervised Learning*, when introducing
    PCA as well as the risk factor models covered in *Chapter 4*, *Financial Feature
    Engineering – How to Research Alpha Factors*. They aim to show that the asset
    characteristics used by factor models to capture the systematic drivers of "anomalies"
    are just proxies for the time-varying exposure to risk factors that cannot be
    directly measured. In this context, anomalies are returns in excess of those explained
    by the exposure to aggregate market risk (see the discussion of the capital asset
    pricing model in *Chapter 5*, *Portfolio Optimization and Performance Evaluation*).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Gu、Kelly和Xiu（GKX，2019）最近的研究开发了一种基于证券对风险因素的暴露的资产定价模型。当我们在*第13章*《数据驱动的风险因素和无监督学习的资产配置》中介绍PCA以及在*第4章*《金融特征工程-如何研究Alpha因子》中介绍的风险因素模型时，它建立在我们讨论的**数据驱动风险因素**概念上。他们的目标是表明因子模型用于捕捉“异常”系统驱动因素的资产特征只是无法直接测量的风险因素的时间变化暴露的代理。在这种情况下，“异常”是超过由暴露于总体市场风险的回报（请参阅*第5章*《投资组合优化和绩效评估》中对资本资产定价模型的讨论）所解释的回报。
- en: 'The **Fama-French factor models** discussed in *Chapter 4* and *Chapter 7*
    explain returns by specifying risk factors like firm size based on empirical observations
    of differences in average stock returns beyond those due to aggregate market risk.
    Given such **specific risk factors**, these models are able to measure the reward
    an investor receives for taking on factor risk using portfolios designed accordingly:
    sort stocks by size, buy the smallest quintile, sell the largest quintile, and
    compute the return. The observed risk factor return then allows linear regression
    to estimate the sensitivity of assets to these factors (called **factor loadings**),
    which in turn helps to predict the returns of (many) assets based on forecasts
    of (far fewer) factor returns.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 4 章*和*第 7 章*讨论的**Fama-French 因子模型**通过指定风险因素如公司规模来解释回报，基于对超过聚合市场风险所致平均股票回报的经验观察。鉴于这些**特定的风险因素**，这些模型能够通过相应设计的组合度量投资者为承担因子风险而获得的回报：按规模分类股票，购买最小的五分位数，卖出最大的五分位数，并计算回报。观察到的风险因素回报然后允许线性回归估计资产对这些因子的敏感性（称为**因子加载**），从而有助于基于（较少的）因子回报的预测来预测（许多）资产的回报。
- en: In contrast, GKX treat **risk factors as latent, or non-observable**, drivers
    of covariance among a number of assets large enough to prevent investors from
    avoiding exposure through diversification. Therefore, investors require a reward
    that adjusts like any price to achieve equilibrium, providing in turn an economic
    rationale for return differences that are no longer anomalous. In this view, risk
    factors are purely statistical in nature while the underlying economic forces
    can be of arbitrary and varying origin.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，GKX 将**风险因素视为潜在的或不可观察的**，在许多资产之间驱动协方差，从而阻止投资者通过分散化来避免暴露。因此，投资者需要一种调整的回报，就像任何价格一样来实现均衡，进而提供不再异常的回报差异的经济合理性。在这种观点中，风险因素纯粹是统计性质的，而潜在的经济力量可以是任意的和多变的来源。
- en: In another recent paper (Kelly, Pruitt, and Su, 2019), Kelly—who teaches finance
    at Yale, works with AQR, and is one of the pioneers in applying ML to trading—and
    his coauthors developed a linear model dubbed **Instrumented Principal Component
    Analysis** (IPCA) to **estimate latent risk factors and the assets' factor loadings
    from data**. IPCA extends PCA to include asset characteristics as covariates and
    produce time-varying factor loadings. (See *Chapter 13*, *Data-Driven Risk Factors
    and Asset Allocation with Unsupervised Learning*, for coverage of PCA.) By conditioning
    asset exposure to factors on observable asset characteristics, IPCA aims to answer
    whether there is a set of common latent risk factors that explain an observed
    anomaly rather than whether there is a specific observable factor that can do
    so.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一篇最近的论文中（Kelly, Pruitt, and Su, 2019），Kelly——在耶鲁大学教授金融学，与 AQR 合作，并是将机器学习应用于交易的先驱之一——及其合著者开发了一个线性模型称为**工具化主成分分析**（IPCA），以**从数据中估计潜在的风险因素和资产的因子加载**。
    IPCA 将 PCA 扩展到包括资产特征作为协变量，并产生时变的因子加载。通过将资产暴露于可观察的资产特征的因素上，IPCA 的目标是回答是否有一组共同的潜在风险因素来解释观察到的异常，而不是是否有一个特定的可观察因素可以这样做。
- en: GKX creates a **conditional autoencoder architecture** to reflect the nonlinear
    nature of return dynamics ignored by the linear Fama-French models and the IPCA
    approach. The result is a deep neural network that simultaneously learns the premia
    on a given number of unobservable factors using an autoencoder, and the factor
    loadings for a large universe of equities based on a broad range of time-varying
    asset characteristics using a feedforward network. The model succeeds in explaining
    and predicting asset returns. It demonstrates a relationship that is both statistically
    and economically significant, yielding an attractive Sharpe ratio when translated
    into a long-short decile spread strategy similar to the examples we have used
    throughout this book.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: GKX 创建了一个**条件自编码器架构**，以反映线性 Fama-French 模型和 IPCA 方法所忽略的回报动态的非线性性质。结果是一个深度神经网络，它同时使用自编码器学习给定数量的不可观测因素的溢价，并使用前馈网络基于广泛的时间变化资产特征学习大量股票的因子加载。该模型成功地解释和预测资产回报。它展示了一个在统计上和经济上都显著的关系，当转化为类似于我们在本书中使用的例子的长短十分位差异策略时，产生了具有吸引力的夏普比率。
- en: In this section, we'll create a simplified version of this model to demonstrate
    how you can **leverage autoencoders to generate tradeable signals**. To this end,
    we'll build a new dataset of close to 4,000 US stocks over the 1990-2019 period
    using yfinance, because it provides some additional information that facilitates
    the computation of the asset characteristics. We'll take a few shortcuts, such
    as using fewer assets and only the most important characteristics. We'll also
    omit some implementation details to simplify the exposition. We'll highlight the
    most important differences so that you can enhance the model accordingly.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: We'll first show how to prepare the data before we explain, build, and train
    the model and evaluate its predictive performance. Please see the above references
    for additional background on the theory and implementation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Sourcing stock prices and metadata information
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GKX reference implementation uses stock price and firm characteristic data
    for over 30,000 US equities from the Center for Research in Security Prices (CRSP)
    from 1957-2016 at a monthly frequency. It computes 94 metrics that include a broad
    range of asset attributes suggested as predictive of returns in previous academic
    research and listed in Green, Hand, and Zhang (2017), who set out to verify these
    claims.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we do not have access to the high-quality but costly CRSP data, we leverage
    yfinance (see *Chapter 2*, *Market and Fundamental Data – Sources and Techniques*)
    to download price and metadata from Yahoo Finance. There are downsides to choosing
    free data, including:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: The lack of quality control regarding adjustments
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Survivorship bias because we cannot get data for stocks that are no longer listed
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A smaller scope in terms of both the number of equities and the length of their
    history
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `build_us_stock_dataset.ipynb` notebook contains the relevant code examples
    for this section.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain the data, we get a list of the 8,882 currently traded symbols from
    NASDAQ using pandas-datareader (see *Chapter 2*, *Market and Fundamental Data
    – Sources and Techniques*):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We remove ETFs and create yfinance `Ticker()` objects for the remainder:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Each ticker''s `.info` attribute contains data points scraped from Yahoo Finance,
    ranging from the outstanding number of shares and other fundamentals to the latest
    market capitalization; coverage varies by security:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: For the tickers with metadata, we download both adjusted and unadjusted prices,
    the latter including corporate actions like stock splits and dividend payments
    that we could use to create a Zipline bundle for strategy backtesting (see *Chapter
    8*, *The ML4T Workflow – From Model to Strategy Backtesting*).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'We get adjusted OHLCV data on 4,314 stocks as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Absent any quality control regarding the underlying price data and the adjustments
    for stock splits, we remove equities with suspicious values such as daily returns
    above 100 percent or below -100 percent:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This removes around 10 percent of the tickers, leaving us with close to 3,900
    assets for the 1990-2019 period.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Computing predictive asset characteristics
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GKX tested 94 asset attributes based on Green et al. (2017) and identified
    the 20 most influential metrics while asserting that feature importance drops
    off quickly thereafter. The top 20 stock characteristics fall into three categories,
    namely:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '**Price trend**, including (industry) momentum, short- and long-term reversal,
    or the recent maximum return'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Liquidity,** such as turnover, dollar volume, or market capitalization'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risk measures**, for instance, total and idiosyncratic return volatility
    or market beta'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of these 20, we limit the analysis to 16 for which we have or can approximate
    the relevant inputs. The `conditional_autoencoder_for_trading_data.ipynb` notebook
    demonstrates how to calculate the relevant metrics. We highlight a few examples
    in this section; see also the *Appendix*, *Alpha Factor Library*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Some metrics require information like sector, market cap, and outstanding shares,
    so we limit our stock price dataset to the securities with relevant metadata:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We run our analysis at a weekly instead of monthly return frequency to compensate
    for the 50 percent shorter time period and around 80 percent lower number of stocks.
    We obtain weekly returns as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Most metrics are fairly straightforward to compute. **Stock momentum**, the
    11-month cumulative stock returns ending 1 month before the current date, can
    be derived as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The **Amihud Illiquidity** measure is the ratio of a stock''s absolute returns
    relative to its dollar volume, measured as a rolling 21-day average:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Idiosyncratic volatility** is measured as the standard deviation of a regression
    of residuals of weekly returns on the returns of equally weighted market index
    returns for the prior three years. We compute this computationally intensive metric
    using `statsmodels`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For the **market beta**, we can use statsmodels'' `RollingOLS` class with the
    weekly asset returns as outcome and the equal-weighted index as input:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We end up with around 3 million observations on 16 metrics for some 3,800 securities
    over the 1990-2019 period. *Figure 20.6* displays a histogram of the number of
    stock returns per week (the left panel) and boxplots outlining the distribution
    of the number of observations for each characteristic:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_20_06.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.6: Number of tickers over time and per - stock characteristic'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'To limit the influence of outliers, we follow GKX and rank-normalize the characteristics
    to the [-1, 1] interval:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Since the neural network cannot handle missing data, we set missing values to
    -2, which lies outside the range for both weekly returns and the characteristics.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: The authors apply additional methods to avoid overweighting microcap stocks
    like market-value-weighted least-squares regression. They also adjust for data-snooping
    biases by factoring in conservative reporting lags for the characteristics.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Creating the conditional autoencoder architecture
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建条件自编码器架构
- en: The conditional autoencoder proposed by GKX allows for time-varying return distributions
    that take into account changing asset characteristics. To this end, the authors
    extend standard autoencoder architectures that we discussed in the first section
    of this chapter to allow for features to shape the encoding.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: GKX提出的条件自编码器允许考虑到变化的资产特征的时变回报分布。为此，作者将我们在本章第一节中讨论的标准自编码器架构扩展，以允许特征来塑造编码。
- en: '*Figure 20.7* illustrates the architecture that models the outcome (asset returns,
    top) as a function of both asset characteristics (left input) and, again, individual
    asset returns (right input). The authors allow for asset returns to be individual
    stock returns or portfolios that are formed from the stocks in the sample based
    on the asset characteristics, similar to the Fama-French factor portfolios we
    discussed in *Chapter 4*, *Financial Feature Engineering – How to Research Alpha
    Factors*, and summarized in the introduction to this section (hence the dotted
    lines from stocks to portfolios in the lower-right box). We will use individual
    stock returns; see GKX for details on how and why to use portfolios instead.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*图20.7*说明了该架构将结果（资产回报，顶部）建模为资产特征（左侧输入）和再次个体资产回报（右侧输入）的函数。作者允许资产回报是个体股票回报或根据资产特征从样本中的股票组成的组合，类似于我们在*第4章*中讨论的法玛-法rench因子组合投资组合，并在本节的介绍中总结（因此从股票到组合的虚线）。我们将使用个体股票回报；有关使用组合而不是个体股票的详细信息，请参阅GKX。'
- en: '![](img/B15439_20_07.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_20_07.png)'
- en: 'Figure 20.7: Conditional autoencoder architecture designed by GKX'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.7：GKX设计的条件自编码器架构
- en: The **feedforward neural network** on the left side of the conditional autoencoder
    models the *K* factor loadings (beta output) of *N* individual stocks as a function
    of their *P* characteristics (input). In our case, *N* is around 3,800 and *P*
    equals 16\. The authors experiment with up to three hidden layers with 32, 16,
    and 8 units, respectively, and find two layers to perform best. Due to the smaller
    number of characteristics, we only use a similar layer and find 8 units most effective.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的**前馈神经网络**模拟了作为其*P*特征（输入）的函数的*N*个个体股票的*K*因子载荷（beta输出）。在我们的案例中，*N*约为3,800，*P*等于16。作者尝试了最多三个隐藏层，分别具有32、16和8个单元，并发现两个层表现最佳。由于特征数量较少，我们仅使用了一个类似的层，并发现8个单元最有效。
- en: The right side of this architecture is a traditional autoencoder when used with
    individual asset returns as inputs because it maps *N* asset returns onto themselves.
    The authors use it in this way to measure how well the derived factors explain
    contemporaneous returns. In addition, they use the autoencoder to predict future
    returns by using input returns from period *t*-1 with output returns from period
    t. We will focus on the use of the architecture for prediction, underlining that
    autoencoders are a special case of a feedforward neural network as mentioned in
    the first section of this chapter.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当以个体资产回报作为输入时，该架构的右侧是传统的自编码器，因为它将*N*个资产回报映射到它们自己。作者以这种方式使用它来衡量导出的因子如何解释同时发生的回报。此外，他们使用自编码器通过使用来自期间*t*-1的输入回报和期间*t*的输出回报来预测未来回报。我们将重点关注该架构用于预测的用途，强调自编码器是本章第一节中提到的前馈神经网络的特殊情况。
- en: The model output is the dot product of the ![](img/B15439_20_008.png) factor
    loadings on the left with the ![](img/B15439_20_009.png) factor premia on the
    right. The authors experiment with values of *K* in the range 2-6, similar to
    established factor models.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出是左侧的![](img/B15439_20_008.png)因子载荷与右侧的![](img/B15439_20_009.png)因子溢价的点积。作者在范围为2-6的*K*值上进行了实验，与已建立的因子模型类似。
- en: 'To create this architecture using TensorFlow 2, we use the Functional Keras
    API and define a `make_model()` function that automates the model compilation
    process as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用TensorFlow 2创建此架构，我们使用Functional Keras API并定义一个`make_model()`函数，该函数自动化了模型编译过程如下：
- en: '[PRE32]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We follow the authors in using batch normalization and compile the model to
    use mean squared error for this regression task and the Adam optimizer. This model
    has 12,418 parameters (see the notebook).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循作者的做法，使用批量归一化并编译模型以在此回归任务中使用均方误差和Adam优化器。该模型有12,418个参数（请参阅笔记本）。
- en: The authors use additional regularization techniques such as L1 penalties on
    network weights and combine the results of various networks with the same architecture
    but using different random seeds. They also use early stopping.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'We cross-validate using 20 years for training and predict the following year
    of weekly returns with five folds corresponding to the years 2015-2019\. We evaluate
    combinations of numbers of factors *K* from 2 to 6 and 8, 16, or 32 hidden layer
    units by computing the **information coefficient** (**IC**) for the validation
    set as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '*Figure 20.8* plots the validation IC averaged over the five annual folds by
    epoch for the five-factor count and three hidden-layer size combinations. The
    upper panel shows the IC across the 52 weeks and the lower panel shows the average
    weekly IC (see the notebook for the color version):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_20_08.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.8: Cross-validation performance for all factor and hidden-layer size
    combinations'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: The results suggest that more factors and fewer hidden layer units work better;
    in particular, four and six factors with eight units perform best with overall
    IC values in the range of 0.02-0.03\.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the economic significance of the model's predictive performance,
    we generate predictions for a four-factor model with eight units trained for 15
    epochs. Then we use Alphalens to compute the spreads between equal-weighted portfolios
    invested by a quintile of the predictions for each point in time, while ignoring
    transaction costs (see the `alphalens_analysis.ipynb` notebook).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 20.9* shows the mean spread for holding periods from 5 to 21 days.
    For the shorter end that also reflects the prediction horizon, the spread between
    the bottom and the top decile is around 10 basis points:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_20_09.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.9: Mean period-wise spread by prediction quintile'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate how the predictive performance might translate into returns over
    time, we lot the cumulative returns of similarly invested portfolios, as well
    as the cumulative return for a long-short portfolio invested in the top and bottom
    half, respectively:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_20_10.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.10: Cumulative returns of quintile-based and long-short portfolios'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The results show significant spreads between quintile portfolios and positive
    cumulative returns for the broader-based long-short portfolio over time. This
    supports the hypothesis that the conditional autoencoder model could contribute
    to a profitable trading strategy.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Lessons learned and next steps
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The conditional autoencoder combines a nonlinear version of the data-driven
    risk factors we explored using PCA in *Chapter 13*, *Data-Driven Risk Factors
    and Asset Allocation with Unsupervised Learning*, with the risk factor approach
    to modeling returns discussed in *Chapter 4* and *Chapter 7*. It illustrates how
    deep neural network architectures can be flexibly adapted to various tasks as
    well as the fluid boundary between autoencoders and feedforward neural networks.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'The numerous simplifications from the data source to the architecture point
    to several avenues for improvements. Besides sourcing more data of better quality
    that also allows the computation of additional characteristics, the following
    modifications are a starting point—there are certainly many more:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Experiment with **data frequencies** other than weekly and forecast horizons
    other than annual, where shorter periods will also increase the amount of training
    data
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify the **model architecture**, especially if using more data, which might
    reverse the finding that an even smaller hidden layer would estimate better factor
    loadings
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced how unsupervised learning leverages deep learning.
    Autoencoders learn sophisticated, nonlinear feature representations that are capable
    of significantly compressing complex data while losing little information. As
    a result, they are very useful to counter the curse of dimensionality associated
    with rich datasets that have many features, especially common datasets with alternative
    data. We also saw how to implement various types of autoencoders using TensorFlow
    2.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, we implemented recent academic research that extracts data-driven
    risk factors from data to predict returns. Different from our linear approach
    to this challenge in *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation
    with Unsupervised Learning*, autoencoders capture nonlinear relationships. Moreover,
    the flexibility of deep learning allowed us to incorporate numerous key asset
    characteristics to model more sensitive factors that helped predict returns.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we focus on generative adversarial networks, which have
    often been called one of the most exciting recent developments in artificial intelligence,
    and see how they are capable of creating synthetic training data.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
