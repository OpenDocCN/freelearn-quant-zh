["```py\nbase_estimator = DecisionTreeClassifier(criterion='gini', \n                                        splitter='best',\n                                        max_depth=1, \n                                        min_samples_split=2, \n                                        min_samples_leaf=20, \n                                        min_weight_fraction_leaf=0.0,\n                                        max_features=None, \n                                        random_state=None, \n                                        max_leaf_nodes=None, \n                                        min_impurity_decrease=0.0, \n                                        min_impurity_split=None) \n```", "```py\nada_clf = AdaBoostClassifier(base_estimator=base_estimator,\n                            n_estimators=100,\n                            learning_rate=1.0,\n                            algorithm='SAMME.R',\n                            random_state=42) \n```", "```py\ncv = OneStepTimeSeriesSplit(n_splits=12, test_period_length=1, shuffle=True)\ndef run_cv(clf, X=X_dummies, y=y, metrics=metrics, cv=cv, fit_params=None):\n    return cross_validate(estimator=clf,\n                          X=X,\n                          y=y,\n                          scoring=list(metrics.keys()),\n                          cv=cv,\n                          return_train_score=True,\n                          n_jobs=-1,                 # use all cores\n                          verbose=1,\n                          fit_params=fit_params) \n```", "```py\n# deviance = logistic reg; exponential: AdaBoost\ngb_clf = GradientBoostingClassifier(loss='deviance',                \n# shrinks the contribution of each tree\n                                   learning_rate=0.1,              \n# number of boosting stages\n                                   n_estimators=100,               \n# fraction of samples used t fit base learners\n                                   subsample=1.0,                  \n# measures the quality of a split\n                                   criterion='friedman_mse',       \n                                   min_samples_split=2,            \n                                   min_samples_leaf=1, \n# min. fraction of sum of weights\n                                   min_weight_fraction_leaf=0.0,   \n# opt value depends on interaction\n                                   max_depth=3,                    \n                                   min_impurity_decrease=0.0, \n                                   min_impurity_split=None, \n                                   max_features=None, \n                                   max_leaf_nodes=None, \n                                   warm_start=False, \n                                   presort='auto',\n                                   validation_fraction=0.1, \n                                   tol=0.0001) \n```", "```py\ngb_cv_result = run_cv(gb_clf, y=y_clean, X=X_dummies_clean)\ngb_result = stack_results(gb_cv_result) \n```", "```py\ncv = OneStepTimeSeriesSplit(n_splits=12)\nparam_grid = dict(\n        n_estimators=[100, 300],\n        learning_rate=[.01, .1, .2],\n        max_depth=list(range(3, 13, 3)),\n        subsample=[.8, 1],\n        min_samples_split=[10, 50],\n        min_impurity_decrease=[0, .01],\n        max_features=['sqrt', .8, 1]) \n```", "```py\ngs = GridSearchCV(gb_clf,\n                  param_grid,\n                  cv=cv,\n                  scoring='roc_auc',\n                  verbose=3,\n                  n_jobs=-1,\n                  return_train_score=True)\ngs.fit(X=X, y=y)\n# persist result using joblib for more efficient storage of large numpy arrays\njoblib.dump(gs, 'gbm_gridsearch.joblib') \n```", "```py\npd.Series(gridsearch_result.best_params_)\nlearning_rate              0.01\nmax_depth                  9.00\nmax_features               1.00\nmin_impurity_decrease      0.01\nmin_samples_split         50.00\nn_estimators             300.00\nsubsample                  1.00\ngridsearch_result.best_score_\n0.5569 \n```", "```py\nidx = pd.IndexSlice\nauc = {}\nfor i, test_date in enumerate(test_dates):\n    test_data = test_feature_data.loc[idx[:, test_date], :]\n    preds = best_model.predict(test_data)\n    auc[i] = roc_auc_score(y_true=test_target.loc[test_data.index], y_score=preds)\nauc = pd.Series(auc) \n```", "```py\ndata = (pd.read_hdf('data.h5', 'model_data')\n            .sort_index()\n            .loc[idx[:, :'2016'], :])\nlabels = sorted(data.filter(like='fwd').columns)\nfeatures = data.columns.difference(labels).tolist()\ncategoricals = ['year', 'weekday', 'month']\nfor feature in categoricals:\n    data[feature] = pd.factorize(data[feature], sort=True)[0] \n```", "```py\nimport lightgbm as lgb\noutcome_data = data.loc[:, features + [label]].dropna()\nlgb_data = lgb.Dataset(data=outcome_data.drop(label, axis=1),\n                           label=outcome_data[label],\n                           categorical_feature=categoricals,\n                           free_raw_data=False) \n```", "```py\ncat_cols_idx = [outcome_data.columns.get_loc(c) for c in categoricals]\ncatboost_data = Pool(label=outcome_data[label],\n                    data=outcome_data.drop(label, axis=1),\n                    cat_features=cat_cols_idx) \n```", "```py\nfor i, (train_idx, test_idx) in enumerate(cv.split(X=outcome_data)):\n   lgb_train = lgb_data.subset(train_idx.tolist()).construct()\n   train_set = catboost_data.slice(train_idx.tolist()) \n```", "```py\nlearning_rate_ops = [.01, .1, .3]\nmax_depths = [2, 3, 5, 7]\nnum_leaves_opts = [2 ** i for i in max_depths]\nfeature_fraction_opts = [.3, .6, .95]\nmin_data_in_leaf_opts = [250, 500, 1000]\ncv_params = list(product(learning_rate_ops,\n                         num_leaves_opts,\n                         feature_fraction_opts,\n                         min_data_in_leaf_opts))\nn_params = len(cv_params)\n# randomly sample 50%\ncvp = np.random.choice(list(range(n_params)),\n                           size=int(n_params / 2), \n                           replace=False)\ncv_params_ = [cv_params[i] for i in cvp] \n```", "```py\nnum_iterations = [10, 25, 50, 75] + list(range(100, 501, 50))\nnum_boost_round = num_iterations[-1]\nfor lookahead, train_length, test_length in test_params:\n   n_splits = int(2 * YEAR / test_length)\n   cv = MultipleTimeSeriesCV(n_splits=n_splits,\n                             lookahead=lookahead,\n                             test_period_length=test_length,\n                             train_period_length=train_length)\n   for p, param_vals in enumerate(cv_params_):\n       for i, (train_idx, test_idx) in enumerate(cv.split(X=outcome_data)):\n           lgb_train = lgb_data.subset(train_idx.tolist()).construct()\n           model = lgb.train(params=params,\n                             train_set=lgb_train,\n                             num_boost_round=num_boost_round,\n                             verbose_eval=False)\n           test_set = outcome_data.iloc[test_idx, :]\n           X_test = test_set.loc[:, model.feature_name()]\n           y_test = test_set.loc[:, label]\n           y_pred = {str(n): model.predict(X_test, num_iteration=n) for n in num_iterations} \n```", "```py\nfig, axes = plot_partial_dependence(estimator=best_model,\n                                    X=X,\n                                    features=['return_12m', 'return_6m', \n                                              'CMA', ('return_12m',\n                                                      'return_6m')],\n                                    percentiles=(0.01, 0.99),\n                                    n_jobs=-1,\n                                    n_cols=2,\n                                    grid_resolution=250) \n```", "```py\ntargets = ['return_12m', 'return_6m']\npdp, axes = partial_dependence(estimator=gb_clf,\n                               features=targets,\n                               X=X_,\n                               grid_resolution=100)\nXX, YY = np.meshgrid(axes[0], axes[1])\nZ = pdp[0].reshape(list(map(np.size, axes))).T\nfig = plt.figure(figsize=(14, 8))\nax = Axes3D(fig)\nsurf = ax.plot_surface(XX, YY, Z,\n                       rstride=1,\n                       cstride=1,\n                       cmap=plt.cm.BuPu,\n                       edgecolor='k')\nax.set_xlabel(' '.join(targets[0].split('_')).capitalize())\nax.set_ylabel(' '.join(targets[1].split('_')).capitalize())\nax.set_zlabel('Partial Dependence')\nax.view_init(elev=22, azim=30) \n```", "```py\n# load JS visualization code to notebook\nshap.initjs()\n# explain the model's predictions using SHAP values\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test, show=False) \n```", "```py\nshap.force_plot(explainer.expected_value, shap_values[:1000,:],\n                X_test.iloc[:1000]) \n```", "```py\nshap.dependence_plot(ind='r01',\n                     shap_values=shap_values,\n                     features=X,\n                     interaction_index='r05',\n                     title='Interaction between 1- and 5-Day Returns') \n```", "```py\ndef load_predictions(bundle):\n    predictions = (pd.read_hdf('predictions.h5', 'train/01')\n                   .append(pd.read_hdf('predictions.h5', 'test/01')\n                   .drop('y_test', axis=1)))\n    predictions = (predictions.loc[~predictions.index.duplicated()]\n                   .iloc[:, :10]\n                   .mean(1)\n                   .sort_index()\n                   .dropna()\n                  .to_frame('prediction')) \n```", "```py\nMultiIndex: 51242505 entries, ('AAL', Timestamp('2014-12-22 09:30:00')) to ('YHOO', Timestamp('2017-06-16 16:00:00'))\nData columns (total 12 columns):\n #   Column  Non-Null Count     Dtype  \n---  ------  --------------     -----  \n 0   first   51242500 non-null  float64\n 1   high    51242500 non-null  float64\n 2   low     51242500 non-null  float64\n 3   last    51242500 non-null  float64\n 4   price   49242369 non-null  float64\n 5   volume  51242505 non-null  int64  \n 6   up      51242505 non-null  int64  \n 7   down    51242505 non-null  int64  \n 8   rup     51242505 non-null  int64  \n 9   rdown   51242505 non-null  int64  \n 10  atask   51242505 non-null  int64  \n 11  atbid   51242505 non-null  int64  \ndtypes: float64(5), int64(7)\nmemory usage: 6.1+ GB \n```", "```py\ndata['MFI'] = (by_ticker\n               .apply(lambda x: talib.MFI(x.high,\n                                          x.low,\n                                          x['last'],\n                                          x.volume,\n                                          timeperiod=14)\n                      .shift())) \n```", "```py\nDAY = 390   # minutes; 6.5 hrs (9:30 - 15:59)\nMONTH = 21  # trading days\nn_splits = 24\ncv = MultipleTimeSeriesCV(n_splits=n_splits,\n                          lookahead=1,\n                          test_period_length=MONTH * DAY,\n                          train_period_length=12 * MONTH * DAY,\n                          date_idx='date_time') \n```", "```py\ndef ic_lgbm(preds, train_data):\n    \"\"\"Custom IC eval metric for lightgbm\"\"\"\n    is_higher_better = True\n    return 'ic', spearmanr(preds, train_data.get_label())[0], is_higher_better\nmodel = lgb.train(params=params,\n                  train_set=lgb_train,\n                  valid_sets=[lgb_train, lgb_test],\n                  feval=ic_lgbm,\n                  num_boost_round=num_boost_round,\n                  early_stopping_rounds=50,\n                  verbose_eval=50) \n```", "```py\nic = spearmanr(cv_preds.y_test, cv_preds.y_pred)[0]\nby_day = cv_preds.groupby(cv_preds.index.get_level_values('date_time').date)\nic_by_day = by_day.apply(lambda x: spearmanr(x.y_test, x.y_pred)[0])\ndaily_ic_mean = ic_by_day.mean()\ndaily_ic_median = ic_by_day.median() \n```", "```py\ndates = cv_preds.index.get_level_values('date_time').date\ncv_preds['decile'] = (cv_preds.groupby(dates, group_keys=False)\nmin_ret_by_decile = cv_preds.groupby(['date_time', 'decile']).y_test.mean()\n                      .apply(lambda x: pd.qcut(x.y_pred, q=10))))\ncumulative_ret_by_decile = (min_ret_by_decile\n                            .unstack('decile')\n                            .add(1)\n                            .cumprod()\n                            .sub(1)) \n```"]