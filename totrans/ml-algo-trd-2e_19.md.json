["```py\nsp500 = web.DataReader('SP500', 'fred', start='2010', end='2020').dropna()\nsp500.info()\nDatetimeIndex: 2463 entries, 2010-03-22 to 2019-12-31\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----  \n 0   SP500   2463 non-null   float64 \n```", "```py\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nsp500_scaled = pd.Series(scaler.fit_transform(sp500).squeeze(), \n                         index=sp500.index) \n```", "```py\ndef create_univariate_rnn_data(data, window_size):\n    y = data[window_size:]\n    data = data.values.reshape(-1, 1) # make 2D\n    n = data.shape[0]\n    X = np.hstack(tuple([data[i: n-j, :] for i, j in enumerate(range(\n                                                     window_size, 0, -1))]))\n    return pd.DataFrame(X, index=y.index), y \n```", "```py\nX, y = create_univariate_rnn_data(sp500_scaled, window_size=63)\nX.shape\n(2356, 63) \n```", "```py\nX_train = X[:'2018'].values.reshape(-1, window_size, 1)\ny_train = y[:'2018']\n# keep the last year for testing\nX_test = X['2019'].values.reshape(-1, window_size, 1)\ny_test = y['2019'] \n```", "```py\nrnn = Sequential([\n    LSTM(units=10,\n         input_shape=(window_size, n_features), name='LSTM'),\n    Dense(1, name='Output')\n]) \n```", "```py\nrnn.summary()\nLayer (type)                 Output Shape              Param #   \nLSTM (LSTM)                  (None, 10)                480       \nOutput (Dense)               (None, 1)                 11        \nTotal params: 491\nTrainable params: 491 \n```", "```py\noptimizer = keras.optimizers.RMSprop(lr=0.001,\n                                     rho=0.9,\n                                     epsilon=1e-08,\n                                     decay=0.0)\nrnn.compile(loss='mean_squared_error', optimizer=optimizer) \n```", "```py\nearly_stopping = EarlyStopping(monitor='val_loss', \n                              patience=50,\n                              restore_best_weights=True)\nlstm_training = rnn.fit(X_train,\n                       y_train,\n                       epochs=500,\n                       batch_size=20,\n                       validation_data=(X_test, y_test),\n                       callbacks=[checkpointer, early_stopping],\n                       verbose=1) \n```", "```py\nloss_history = pd.DataFrame(lstm_training.history).pow(.5)\nloss_history.index += 1\nbest_rmse = loss_history.val_loss.min()\nbest_epoch = loss_history.val_loss.idxmin()\nloss_history.columns=['Training RMSE', 'Validation RMSE']\ntitle = f'Best Validation RMSE: {best_rmse:.4%}'\nloss_history.rolling(5).mean().plot(logy=True, lw=2, title=title, ax=ax) \n```", "```py\ntest_predict_scaled = rnn.predict(X_test)\ntest_predict = (pd.Series(scaler.inverse_transform(test_predict_scaled)\n                          .squeeze(), \n                          index=y_test.index)) \n```", "```py\nprices = (pd.read_hdf('../data/assets.h5', 'quandl/wiki/prices')\n          .adj_close\n          .unstack().loc['2007':])\nprices.info()\nDatetimeIndex: 2896 entries, 2007-01-01 to 2018-03-27\nColumns: 3199 entries, A to ZUMZ \n```", "```py\nreturns = (prices\n           .resample('W')\n           .last()\n           .pct_change()\n           .loc['2008': '2017']\n           .dropna(axis=1)\n           .sort_index(ascending=False))\nreturns.info()\nDatetimeIndex: 2576 entries, 2017-12-29 to 2008-01-01\nColumns: 2489 entries, A to ZUMZ \n```", "```py\nn = len(returns)\nT = 52\ntcols = list(range(T))\ntickers = returns.columns\ndata = pd.DataFrame()\nfor i in range(n-T-1):\n    df = returns.iloc[i:i+T+1]\n    date = df.index.max()    \n    data = pd.concat([data, (df.reset_index(drop=True).T\n                             .assign(date=date, ticker=tickers)\n                             .set_index(['ticker', 'date']))]) \n```", "```py\ndata[tcols] = (data[tcols].apply(lambda x: x.clip(lower=x.quantile(.01),\n                                                  upper=x.quantile(.99))))\ndata['label'] = (data['fwd_returns'] > 0).astype(int) \n```", "```py\ndata.shape\n(1167341, 53) \n```", "```py\ndata['month'] = data.index.get_level_values('date').month\ndata = pd.get_dummies(data, columns=['month'], prefix='month')\ndata['ticker'] = pd.factorize(data.index.get_level_values('ticker'))[0] \n```", "```py\ntrain_data = data[:'2016']\ntest_data = data['2017'] \n```", "```py\nwindow_size=52\nsequence = list(range(1, window_size+1))\nX_train = [\n    train_data.loc[:, sequence].values.reshape(-1, window_size , 1),\n    train_data.ticker,\n    train_data.filter(like='month')\n]\ny_train = train_data.label\n[x.shape for x in X_train], y_train.shape\n[(1035424, 52, 1), (1035424,), (1035424, 12)], (1035424,) \n```", "```py\nn_features = 1\nreturns = Input(shape=(window_size, n_features), name='Returns')\ntickers = Input(shape=(1,), name='Tickers')\nmonths = Input(shape=(12,), name='Months') \n```", "```py\nlstm1 = LSTM(units=lstm1_units,\n             input_shape=(window_size, n_features),\n             name='LSTM1',\n             dropout=.2,\n             return_sequences=True)(returns)\nlstm_model = LSTM(units=lstm2_units,\n             dropout=.2,\n             name='LSTM2')(lstm1) \n```", "```py\nticker_embedding = Embedding(input_dim=n_tickers,\n                             output_dim=5,\n                             input_length=1)(tickers)\nticker_embedding = Reshape(target_shape=(5,))(ticker_embedding) \n```", "```py\nmerged = concatenate([lstm_model, ticker_embedding, months], name='Merged')\nbn = BatchNormalization()(merged) \n```", "```py\nhidden_dense = Dense(10, name='FC1')(bn)\noutput = Dense(1, name='Output', activation='sigmoid')(hidden_dense)\nrnn = Model(inputs=[returns, tickers, months], outputs=output) \n```", "```py\nLayer (type)                    Output Shape         Param #     Connected to\nReturns (InputLayer)            [(None, 52, 1)]      0\nTickers (InputLayer)            [(None, 1)]          0\nLSTM1 (LSTM)                    (None, 52, 25)       2700        Returns[0][0]\nembedding (Embedding)           (None, 1, 5)         12445       Tickers[0][0]\nLSTM2 (LSTM)                    (None, 10)           1440        LSTM1[0][0]\nreshape (Reshape)               (None, 5)           0          embedding[0][0]\nMonths (InputLayer)             [(None, 12)]         0\nMerged (Concatenate)            (None, 27)           0           LSTM2[0][0]\n                                                                 reshape[0][0]\n                                                                 Months[0][0]\nbatch_normalization (BatchNorma (None, 27)           108         Merged[0][0]\nFC1 (Dense)                     (None, 10)           280         \natch_normalization[0][0]\nOutput (Dense)                  (None, 1)            11          FC1[0][0]\nTotal params: 16,984\nTrainable params: 16,930\nNon-trainable params: 54 \n```", "```py\noptimizer = tf.keras.optimizers.RMSprop(lr=0.001,\n                                        rho=0.9,\n                                        epsilon=1e-08,\n                                        decay=0.0)\nrnn.compile(loss='binary_crossentropy',\n            optimizer=optimizer,\n            metrics=['accuracy', \n                     tf.keras.metrics.AUC(name='AUC')]) \n```", "```py\nresult = rnn.fit(X_train,\n                 y_train,\n                 epochs=50,\n                 batch_size=32,\n                 validation_data=(X_test, y_test),\n                 callbacks=[early_stopping]) \n```", "```py\ndf = web.DataReader(['UMCSENT', 'IPGMFN'], 'fred', '1980', '2019-12').dropna()\ndf.columns = ['sentiment', 'ip']\ndf.info()\nDatetimeIndex: 480 entries, 1980-01-01 to 2019-12-01\nData columns (total 2 columns):\nsentiment    480 non-null float64\nip           480 non-null float64 \n```", "```py\ndf_transformed = (pd.DataFrame({'ip': np.log(df.ip).diff(12),\n                               'sentiment': df.sentiment.diff(12)}).dropna())\ndf_transformed = df_transformed.apply(minmax_scale) \n```", "```py\ndef create_multivariate_rnn_data(data, window_size):\n    y = data[window_size:]\n    n = data.shape[0]\n    X = np.stack([data[i: j] for i, j in enumerate(range(window_size, n))],\n                 axis=0)\n    return X, y \n```", "```py\nX, y = create_multivariate_rnn_data(df_transformed, window_size=window_size)\nX.shape, y.shape\n((450, 18, 2), (450, 2)) \n```", "```py\ntest_size = 24\ntrain_size = X.shape[0]-test_size\nX_train, y_train = X[:train_size], y[:train_size]\nX_test, y_test = X[train_size:], y[train_size:]\nX_train.shape, X_test.shape\n((426, 18, 2), (24, 18, 2)) \n```", "```py\nn_features = output_size = 2\nlstm_units = 12\ndense_units = 6\nrnn = Sequential([\n    LSTM(units=lstm_units,\n         dropout=.1,\n         recurrent_dropout=.1,\n         input_shape=(window_size, n_features), name='LSTM',\n         return_sequences=False),\n    Dense(dense_units, name='FC'),\n    Dense(output_size, name='Output')\n])\nrnn.compile(loss='mae', optimizer='RMSProp') \n```", "```py\nLayer (type)                 Output Shape              Param #   \nLSTM (LSTM)                  (None, 12)                720       \nFC (Dense)                   (None, 6)                 78        \nOutput (Dense)               (None, 2)                 14        \nTotal params: 812\nTrainable params: 812 \n```", "```py\nresult = rnn.fit(X_train,\n                y_train,\n                epochs=100,\n                batch_size=20,\n                shuffle=False,\n                validation_data=(X_test, y_test),\n                callbacks=[checkpointer, early_stopping],\n                verbose=1) \n```", "```py\nfrom tensorflow.keras.datasets import imdb\nvocab_size = 20000\n(X_train, y_train), (X_test, y_test) = imdb.load_data(seed=42, \n                                                      skip_top=0,\n                                                      maxlen=None, \n                                                      oov_char=2, \n                                                      index_from=3,\n                                                      num_words=vocab_size) \n```", "```py\nmaxlen = 100\nX_train_padded = pad_sequences(X_train, \n                        truncating='pre', \n                        padding='pre', \n                        maxlen=maxlen) \n```", "```py\nembedding_size = 100\nrnn = Sequential([\n    Embedding(input_dim=vocab_size, \n              output_dim= embedding_size, \n              input_length=maxlen),\n    GRU(units=32,\n        dropout=0.2, # comment out to use optimized GPU implementation\n        recurrent_dropout=0.2),\n    Dense(1, activation='sigmoid')\n]) \n```", "```py\nLayer (type)                 Output Shape              Param #   \nembedding (Embedding)        (None, 100, 100)          2000000   \ngru (GRU)                    (None, 32)                12864     \ndense (Dense)                (None, 1)                 33        \nTotal params: 2,012,897\nTrainable params: 2,012,897 \n```", "```py\nrnn.fit(X_train_padded, \n       y_train, \n       batch_size=32, \n       epochs=25, \n       validation_data=(X_test_padded, y_test),\n       callbacks=[early_stopping],\n       verbose=1) \n```", "```py\ny_score = rnn.predict(X_test_padded)\nroc_auc_score(y_score=y_score.squeeze(), y_true=y_test)\n0.9393289376 \n```", "```py\nnum_words = 10000\nt = Tokenizer(num_words=num_words,\n              lower=True, \n              oov_token=2)\nt.fit_on_texts(train_data.review)\nvocab_size = len(t.word_index) + 1\ntrain_data_encoded = t.texts_to_sequences(train_data.review)\ntest_data_encoded = t.texts_to_sequences(test_data.review) \n```", "```py\nmax_length = 100\nX_train_padded = pad_sequences(train_data_encoded, \n                               maxlen=max_length, \n                               padding='post',\n                               truncating='post')\ny_train = train_data['label']\nX_train_padded.shape\n(25000, 100) \n```", "```py\nglove_path = Path('data/glove/glove.6B.100d.txt')\nembeddings_index = dict()\nfor line in glove_path.open(encoding='latin1'):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs \n```", "```py\nembedding_matrix = np.zeros((vocab_size, 100))\nfor word, i in t.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector \n```", "```py\nrnn = Sequential([\n    Embedding(input_dim=vocab_size,\n              output_dim=embedding_size,\n              input_length=max_length,\n              weights=[embedding_matrix],\n              trainable=False),\n    GRU(units=32,  dropout=0.2, recurrent_dropout=0.2),\n    Dense(1, activation='sigmoid')]) \n```", "```py\nyf_data, missing = [], []\nfor i, (symbol, dates) in enumerate(filing_index.groupby('ticker').date_filed, \n                                    1):\n    ticker = yf.Ticker(symbol)\n    for idx, date in dates.to_dict().items():\n        start = date - timedelta(days=93)\n        end = date + timedelta(days=31)\n        df = ticker.history(start=start, end=end)\n        if df.empty:\n            missing.append(symbol)\n        else:\n            yf_data.append(df.assign(ticker=symbol, filing=idx)) \n```", "```py\nfwd_return = {}\nfor filing in filings:\n    date_filed = filing_index.at[filing, 'date_filed']\n    price_data = prices[prices.filing==filing].close.sort_index()\n\n    try:\n        r = (price_data\n             .pct_change(periods=5)\n             .shift(-5)\n             .loc[:date_filed]\n             .iloc[-1])\n    except:\n        continue\n    if not np.isnan(r) and -.5 < r < 1:\n        fwd_return[filing] = r \n```", "```py\ny, X = [], []\nfor filing_id, fwd_ret in fwd_return.items():\n    X.append(np.load(vector_path / f'{filing_id}.npy') + 2)\n    y.append(fwd_ret)\ny = np.array(y) \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1)\nX_train = pad_sequences(X_train, \n                        truncating='pre', \n                        padding='pre', \n                        maxlen=maxlen)\nX_test = pad_sequences(X_test, \n                       truncating='pre', \n                       padding='pre', \n                       maxlen=maxlen)\nX_train.shape, X_test.shape\n((14719, 20000), (1636, 20000)) \n```", "```py\nembedding_size = 100\ninput_dim = X_train.max() + 1\nrnn = Sequential([\n    Embedding(input_dim=input_dim, \n              output_dim=embedding_size, \n              input_length=maxlen,\n             name='EMB'),\n    BatchNormalization(name='BN1'),\n    Bidirectional(GRU(32), name='BD1'),\n    BatchNormalization(name='BN2'),\n    Dropout(.1, name='DO1'),\n    Dense(5, name='D'),\n    Dense(1, activation='linear', name='OUT')]) \n```", "```py\nrnn.summary()\nLayer (type)                 Output Shape              Param #   \nEMB (Embedding)              (None, 20000, 100)        2500000   \nBN1 (BatchNormalization)     (None, 20000, 100)        400       \nBD1 (Bidirectional)          (None, 64)                25728     \nBN2 (BatchNormalization)     (None, 64)                256       \nDO1 (Dropout)                (None, 64)                0         \nD (Dense)                    (None, 5)                 325       \nOUT (Dense)                  (None, 1)                 6         \nTotal params: 2,526,715\nTrainable params: 2,526,387\nNon-trainable params: 328 \n```", "```py\nrnn.compile(loss='mse', \n            optimizer='Adam',\n            metrics=[RootMeanSquaredError(name='RMSE'),\n                     MeanAbsoluteError(name='MAE')]) \n```", "```py\nearly_stopping = EarlyStopping(monitor='val_MAE', \n                               patience=5,\n                               restore_best_weights=True)\ntraining = rnn.fit(X_train,\n                   y_train,\n                   batch_size=32,\n                   epochs=100,\n                   validation_data=(X_test, y_test),\n                   callbacks=[early_stopping],\n                   verbose=1) \n```", "```py\ny_score = rnn.predict(X_test)\nrho, p = spearmanr(y_score.squeeze(), y_test)\nprint(f'{rho*100:.2f} ({p:.2%})')\n6.02 (1.48%) \n```"]