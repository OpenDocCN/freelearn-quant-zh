["```py\n# compute covariance matrix: \ncov = np.cov(data.T) # expects variables in rows by default\ncov.shape\n(3, 3) \n```", "```py\neigen_values, eigen_vectors = eig(cov)\neigen_vectors\narray([[ 0.71409739, -0.66929454, -0.20520656],\n       [-0.70000234, -0.68597301, -0.1985894 ],\n       [ 0.00785136, -0.28545725,  0.95835928]]) \n```", "```py\npca = PCA()\npca.fit(data)\nC = pca.components_.T # columns = principal components\nC\narray([[ 0.71409739,  0.66929454,  0.20520656],\n       [-0.70000234,  0.68597301,  0.1985894 ],\n       [ 0.00785136,  0.28545725, -0.95835928]])\nnp.allclose(np.abs(C), np.abs(eigen_vectors))\nTrue \n```", "```py\n# eigenvalue matrix\nev = np.zeros((3, 3))\nnp.fill_diagonal(ev, eigen_values)\nev # diagonal matrix\narray([[1.92923132, 0\\.        , 0\\.        ],\n       [0\\.        , 0.55811089, 0\\.        ],\n       [0\\.        , 0\\.        , 0.00581353]]) \n```", "```py\ndecomposition = eigen_vectors.dot(ev).dot(inv(eigen_vectors))\nnp.allclose(cov, decomposition) \n```", "```py\nn_features = data.shape[1]\ndata_ = data - data.mean(axis=0) \n```", "```py\nU, s, Vt = svd(data_)\nU.shape, s.shape, Vt.shape\n((100, 100), (3,), (3, 3)) \n```", "```py\nS = np.zeros_like(data_)\nS[:n_features, :n_features] = np.diag(s)\nS.shape\n(100, 3) \n```", "```py\nnp.allclose(data_, U.dot(S).dot(Vt))\nTrue \n```", "```py\nnp.allclose(np.abs(C), np.abs(Vt.T)) \n```", "```py\npca2 = PCA(n_components=2)\nprojected_data  = pca2.fit_transform(data)\nprojected_data.shape\n(100, 2) \n```", "```py\npca2.explained_variance_ratio_\narray([0.77381099, 0.22385721]) \n```", "```py\nidx = pd.IndexSlice\nwith pd.HDFStore('../../data/assets.h5') as store:\n    stocks = store['us_equities/stocks'].marketcap.nlargest(500)\n    returns = (store['quandl/wiki/prices']\n               .loc[idx['2010': '2018', stocks.index], 'adj_close']\n               .unstack('ticker')\n               .pct_change()) \n```", "```py\nreturns.info()\nDatetimeIndex: 2072 entries, 2010-01-04 to 2018-03-27\nColumns: 351 entries, A to ZTS \n```", "```py\nreturns = returns.dropna(thresh=int(returns.shape[0] * .95), axis=1)\nreturns = returns.dropna(thresh=int(returns.shape[1] * .95)) \n```", "```py\nreturns.info()\nDatetimeIndex: 2071 entries, 2010-01-05 to 2018-03-27\nColumns: 315 entries, A to LYB \n```", "```py\ndaily_avg = returns.mean(1)\nreturns = returns.apply(lambda x: x.fillna(daily_avg)) \n```", "```py\npca = PCA(n_components='mle')\npca.fit(returns) \n```", "```py\nrisk_factors = pd.DataFrame(pca.transform(returns)[:, :2], \n                            columns=['Principal Component 1', \n                                     'Principal Component 2'], \n                            index=returns.index)\n(risk_factors['Principal Component 1']\n.corr(risk_factors['Principal Component 2']))\n7.773256996252084e-15 \n```", "```py\nidx = pd.IndexSlice\nwith pd.HDFStore('../../data/assets.h5') as store:\n    stocks = store['us_equities/stocks'].marketcap.nlargest(30)\n    returns = (store['quandl/wiki/prices']\n               .loc[idx['2010': '2018', stocks.index], 'adj_close']\n               .unstack('ticker')\n               .pct_change()) \n```", "```py\nnormed_returns = scale(returns\n                       .clip(lower=returns.quantile(q=.025), \n                             upper=returns.quantile(q=.975), \n                             axis=1)\n                       .apply(lambda x: x.sub(x.mean()).div(x.std()))) \n```", "```py\ncov = returns.cov()\npca = PCA()\npca.fit(cov)\npd.Series(pca.explained_variance_ratio_).head()\n0\t55.91%\n1\t15.52%\n2\t5.36%\n3\t4.85%\n4\t3.32% \n```", "```py\ntop4 = pd.DataFrame(pca.components_[:4], columns=cov.columns)\neigen_portfolios = top4.div(top4.sum(1), axis=0)\neigen_portfolios.index = [f'Portfolio {i}' for i in range(1, 5)] \n```", "```py\ndef get_distance_matrix(corr):\n    \"\"\"Compute distance matrix from correlation;\n        0 <= d[i,j] <= 1\"\"\"\n    return np.sqrt((1 - corr) / 2)\ndistance_matrix = get_distance_matrix(corr)\nlinkage_matrix = linkage(squareform(distance_matrix), 'single') \n```", "```py\nclustergrid = sns.clustermap(distance_matrix,\n                             method='single',\n                             row_linkage=linkage_matrix,\n                             col_linkage=linkage_matrix,\n                             cmap=cmap, center=0)\nsorted_idx = clustergrid.dendrogram_row.reordered_ind\nsorted_tickers = corr.index[sorted_idx].tolist() \n```", "```py\ndef get_inverse_var_pf(cov):\n    \"\"\"Compute the inverse-variance portfolio\"\"\"\n    ivp = 1 / np.diag(cov)\n    return ivp / ivp.sum()\ndef get_cluster_var(cov, cluster_items):\n    \"\"\"Compute variance per cluster\"\"\"\n    cov_ = cov.loc[cluster_items, cluster_items]  # matrix slice\n    w_ = get_inverse_var_pf(cov_)\n    return (w_ @ cov_ @ w_).item() \n```", "```py\ndef get_hrp_allocation(cov, tickers):\n    \"\"\"Compute top-down HRP weights\"\"\"\n    weights = pd.Series(1, index=tickers)\n    clusters = [tickers]  # initialize one cluster with all assets\n    while len(clusters) > 0:\n        # run bisectional search:\n        clusters = [c[start:stop] for c in clusters\n                    for start, stop in ((0, int(len(c) / 2)),\n                                        (int(len(c) / 2), len(c)))\n                    if len(c) > 1]\n        for i in range(0, len(clusters), 2):  # parse in pairs\n            cluster0 = clusters[i]\n            cluster1 = clusters[i + 1]\n            cluster0_var = get_cluster_var(cov, cluster0)\n            cluster1_var = get_cluster_var(cov, cluster1)\n            weight_scaler = 1 - cluster0_var / (cluster0_var + cluster1_var)\n            weights[cluster0] *= weight_scaler\n            weights[cluster1] *= 1 - weight_scaler\n    return weights \n```", "```py\ndef load_predictions(bundle):\n    path = Path('../../12_gradient_boosting_machines/data')\n    predictions = (pd.read_hdf(path / 'predictions.h5', 'lgb/train/01')\n                   .append(pd.read_hdf(path / 'predictions.h5', 'lgb/test/01').drop('y_test', axis=1)))\n    predictions = (predictions.loc[~predictions.index.duplicated()]\n                   .iloc[:, :10]\n                   .mean(1)\n                   .sort_index()\n                   .dropna()\n                  .to_frame('prediction')) \n```", "```py\ndef before_trading_start(context, data):\n    \"\"\"\n    Called every day before market open.\n    \"\"\"\n    output = pipeline_output('signals')['longs'].astype(int)\n    context.longs = output[output!=0].index\n    if len(context.longs) < MIN_POSITIONS:\n        context.divest = set(context.portfolio.positions.keys())\n    else:\n        context.divest = context.portfolio.positions.keys() - context.longs \n```", "```py\ndef rebalance_hierarchical_risk_parity(context, data):\n    \"\"\"Execute orders according to schedule_function()\"\"\"\n    for symbol, open_orders in get_open_orders().items():\n        for open_order in open_orders:\n            cancel_order(open_order)\n    for asset in context.divest:\n        order_target(asset, target=0)\n\n    if len(context.longs) > context.min_positions:\n        returns = (data.history(context.longs, fields='price',\n                          bar_count=252+1, # for 1 year of returns \n                          frequency='1d')\n                   .pct_change()\n                   .dropna(how='all'))\n        hrp_weights = HRPOpt(returns=returns).hrp_portfolio()\n        for asset, target in hrp_weights.items():\n            order_target_percent(asset=asset, target=target) \n```"]