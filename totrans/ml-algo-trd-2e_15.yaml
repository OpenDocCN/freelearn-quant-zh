- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topic Modeling – Summarizing Financial News
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we used the **bag-of-words** (**BOW**) model to convert
    unstructured text data into a numerical format. This model abstracts from word
    order and represents documents as word vectors, where each entry represents the
    relevance of a token to the document. The resulting **document-term matrix** (**DTM**)—or
    transposed as the term-document matrix—is useful for comparing documents to each
    other or a query vector for similarity based on their token content and, therefore,
    finding the proverbial needle in a haystack. It provides informative features
    to classify documents, such as in our sentiment analysis examples.
  prefs: []
  type: TYPE_NORMAL
- en: However, this document model produces both high-dimensional data and very sparse
    data, yet it does little to summarize the content or get closer to understanding
    what it is about. In this chapter, we will use **unsupervised machine learning**
    to extract hidden themes from documents using **topic modeling**. These themes
    can produce detailed insights into a large body of documents in an automated way.
    They are very useful in order to understand the haystack itself and allow us to
    tag documents based on their affinity with the various topics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Topic models** generate sophisticated, interpretable text features that can
    be a first step toward extracting trading signals from large collections of documents.
    They speed up the review of documents, help identify and cluster similar documents,
    and support predictive modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Applications** include the unsupervised discovery of potentially insightful
    themes in company disclosures or earnings call transcripts, customer reviews,
    or contracts. Furthermore, the document-topic associations facilitate the labeling
    by assigning, for example, sentiment metrics or, more directly, subsequent relevant
    asset returns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, after reading this chapter, you''ll understand:'
  prefs: []
  type: TYPE_NORMAL
- en: How topic modeling has evolved, what it achieves, and why it matters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the dimensionality of the DTM using **latent semantic indexing** (**LSI**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting topics with **probabilistic latent semantic analysis** (**pLSA**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How **latent Dirichlet allocation** (**LDA**) improves pLSA to become the most
    popular topic model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing and evaluating topic modeling results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running LDA using sklearn and Gensim
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to apply topic modeling to collections of earnings calls and financial news articles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  prefs: []
  type: TYPE_NORMAL
- en: Learning latent topics – Goals and approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topic modeling discovers hidden themes that capture semantic information beyond
    individual words in a body of documents. It aims to address a key challenge for
    a machine learning algorithm that learns from text data by transcending the lexical
    level of "what actually has been written" to the semantic level of "what was intended."
    The resulting topics can be used to annotate documents based on their association
    with various topics.
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, topic models automatically **summarize large collections
    of documents** to facilitate organization and management as well as search and
    recommendations. At the same time, it enables the understanding of documents to
    the extent that humans can interpret the descriptions of topics.
  prefs: []
  type: TYPE_NORMAL
- en: Topic models also mitigate the **curse of dimensionality** that often plagues
    the BOW model; representing documents with high-dimensional, sparse vectors can
    make similarity measures noisy, lead to inaccurate distance measurements, and
    result in the overfitting of text classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the BOW model loses context as well as semantic information since
    it ignores word order. It is also unable to capture synonymy (where several words
    have the same meaning) or polysemy (where one word has several meanings). As a
    result of the latter, document retrieval or similarity search may miss the point
    when the documents are not indexed by the terms used to search or compare.
  prefs: []
  type: TYPE_NORMAL
- en: 'These shortcomings of the BOW model prompt the question: how can we learn meaningful
    topics from data that facilitate a more productive interaction with documentary
    data?'
  prefs: []
  type: TYPE_NORMAL
- en: Initial attempts by topic models to improve on the vector space model (developed
    in the mid-1970s) applied linear algebra to reduce the dimensionality of the DTM.
    This approach is similar to the algorithm that we discussed as principal component
    analysis in *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation with
    Unsupervised Learning*. While effective, it is difficult to evaluate the results
    of these models without a benchmark model. In response, probabilistic models have
    emerged that assume an explicit document generation process and provide algorithms
    to reverse engineer this process and recover the underlying topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table highlights key milestones in the model evolution, which
    we will address in more detail in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Year | Description |'
  prefs: []
  type: TYPE_TB
- en: '| **Latent semantic indexing (LSI)** | 1988 | Captures the semantic document-term
    relationship by reducing the dimensionality of the word space |'
  prefs: []
  type: TYPE_TB
- en: '| **Probabilistic latent semantic analysis (pLSA)** | 1999 | Reverse engineers
    a generative process that assumes words generate a topic and documents as a mix
    of topics |'
  prefs: []
  type: TYPE_TB
- en: '| **Latent Dirichlet allocation (LDA)** | 2003 | Adds a generative process
    for documents: a three-level hierarchical Bayesian model |'
  prefs: []
  type: TYPE_TB
- en: Latent semantic indexing
  prefs: []
  type: TYPE_NORMAL
- en: '**Latent semantic indexing** (**LSI**)—also called **latent semantic analysis**
    (**LSA**)—set out to improve the results of queries that omitted relevant documents
    containing synonyms of query terms (Dumais et al. 1988). Its goal was to model
    the relationships between documents and terms so that it could predict that a
    term should be associated with a document, even though, because of the variability
    in word use, no such association was observed.'
  prefs: []
  type: TYPE_NORMAL
- en: LSI uses linear algebra to find a given number *k* of latent topics by decomposing
    the DTM. More specifically, it uses the **singular value decomposition** (**SVD**)
    to find the best lower-rank DTM approximation using *k* singular values and vectors.
    In other words, LSI builds on some of the dimensionality reduction techniques
    we encountered in *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation
    with Unsupervised Learning*. The authors also experimented with hierarchical clustering
    but found it too restrictive for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this context, SVD identifies a set of uncorrelated indexing variables or
    factors that represent each term and document by its vector of factor values.
    *Figure 15.1* illustrates how SVD decomposes the DTM into three matrices: two
    matrices that contain orthogonal singular vectors and a diagonal matrix with singular
    values that serve as scaling factors.'
  prefs: []
  type: TYPE_NORMAL
- en: Assuming some correlation in the input DTM, singular values decay in value.
    Therefore, selecting the *T*-largest singular values yields a lower-dimensional
    approximation of the original DTM that loses relatively little information. In
    the compressed version, the rows or columns that had *N* items only have *T* <
    *N* entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LSI decomposition of the DTM can be interpreted as shown in *Figure 15.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: The first ![](img/B15439_15_001.png) matrix represents the relationships between
    documents and topics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The diagonal matrix scales the topics by their corpus strength.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third matrix models the term-topic relationship.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15439_15_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.1: LSI and the SVD'
  prefs: []
  type: TYPE_NORMAL
- en: The rows of the matrix produced by multiplying the first two matrices ![](img/B15439_15_002.png)
    correspond to the locations of the original documents projected into the latent
    topic space.
  prefs: []
  type: TYPE_NORMAL
- en: How to implement LSI using sklearn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will illustrate LSI using the BBC articles data that we introduced in the
    last chapter because they are small enough for quick training and allow us to
    compare topic assignments with category labels. Refer to the notebook `latent_semantic_indexing`
    for additional implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by loading the documents and creating a train and (stratified) test
    set with 50 articles. Then, we vectorize the data using `TfidfVectorizer` to obtain
    weighted DTM counts and filter out words that appear in less than 1 percent or
    more than 25 percent of the documents, as well as generic stopwords, to obtain
    a vocabulary of around 2,900 words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We use scikit-learn's `TruncatedSVD` class, which only computes the *k*-largest
    singular values, to reduce the dimensionality of the DTM. The deterministic `arpack`
    algorithm delivers an exact solution, but the default "randomized" implementation
    is more efficient for large matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'We compute five topics to match the five categories, which explain only 5.4
    percent of the total DTM variance, so a larger number of topics would be reasonable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'LSI identifies a new orthogonal basis for the DTM that reduces the rank to
    the number of desired topics. The `.transform()` method of the trained `svd` object
    projects the documents into the new topic space. This space results from reducing
    the dimensionality of the document vectors and corresponds to the ![](img/B15439_15_002.png)
    transformation illustrated earlier in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can sample an article to view its location in the topic space. We draw a
    "Politics" article that is most (positively) associated with topics 1 and 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The topic assignments for this sample align with the average topic weights for
    each category illustrated in *Figure 15.2* ("Politics" is the rightmost bar).
    They illustrate how LSI expresses the *k* topics as directions in a *k*-dimensional
    space (the notebook includes a projection of the average topic assignments per
    category into two-dimensional space).
  prefs: []
  type: TYPE_NORMAL
- en: Each category is clearly defined, and the test assignments match with train
    assignments. However, the weights are both positive and negative, making it more
    difficult to interpret the topics.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a video game  Description automatically generated](img/B15439_15_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.2: LSI topic weights for train and test data'
  prefs: []
  type: TYPE_NORMAL
- en: We can also display the words that are most closely associated with each topic
    (in absolute terms). The topics appear to capture some semantic information but
    are not clearly differentiated (refer to *Figure 15.3*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.3: Top 10 words per LSI topic'
  prefs: []
  type: TYPE_NORMAL
- en: Strengths and limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The strengths of LSI include the removal of noise and the mitigation of the
    curse of dimensionality. It also captures some semantic aspects, like synonymy,
    and clusters both documents and terms via their topic associations. Furthermore,
    it does not require knowledge of the document language, and both information retrieval
    queries and document comparisons are easy to do.
  prefs: []
  type: TYPE_NORMAL
- en: However, the results of LSI are difficult to interpret because topics are word
    vectors with both positive and negative entries. In addition, there is no underlying
    model that would permit the evaluation of fit or provide guidance when selecting
    the number of dimensions or topics to use.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic latent semantic analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Probabilistic latent semantic analysis** (**pLSA**) takes a **statistical
    perspective** on LSI/LSA and creates a generative model to address the lack of
    theoretical underpinnings of LSA (Hofmann 2001).'
  prefs: []
  type: TYPE_NORMAL
- en: pLSA explicitly models the probability word *w* appearing in document *d*, as
    described by the DTM as a mixture of conditionally independent multinomial distributions
    that involve topics *t*.
  prefs: []
  type: TYPE_NORMAL
- en: There are both **symmetric and asymmetric formulations** of how word-document
    co-occurrences come about. The former assumes that both words and documents are
    generated by the latent topic class. In contrast, the asymmetric model assumes
    that topics are selected given the document, and words result in a second step
    given the topic.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_004.png)'
  prefs: []
  type: TYPE_IMG
- en: The number of topics is a **hyperparameter** chosen prior to training and is
    not learned from the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **plate notation** in *Figure 15.4* describes the statistical dependencies
    in a probabilistic model. More specifically, it encodes the relationship just
    described for the asymmetric model. Each rectangle represents multiple items:
    the outer block stands for *M* documents, while the inner shaded rectangle symbolizes
    *N* words for each document. We only observe the documents and their content;
    the model infers the hidden or latent topic distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.4: The statistical dependencies modeled by pLSA in plate notation'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now take a look at how we can implement this model in practice.
  prefs: []
  type: TYPE_NORMAL
- en: How to implement pLSA using sklearn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: pLSA is equivalent to **non-negative matrix factorization** (**NMF**) using
    a Kullback-Leibler divergence objective (view the references on GitHub). Therefore,
    we can use the `sklearn.decomposition.NMF` class to implement this model following
    the LSI example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same train-test split of the DTM produced by `TfidfVectorizer`, we
    fit pLSA like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We get a measure of the reconstruction error that is a substitute for the explained
    variance measure from earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Due to its probabilistic nature, pLSA produces only positive topic weights
    that result in more straightforward topic-category relationships for the test
    and training sets, as shown in *Figure 15.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.5: pLSA weights by topic for train and test data'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also note that the word lists that describe each topic begin to make more
    sense; for example, the "Entertainment" category is most directly associated with
    Topic 4, which includes the words "film," "star," and so forth, as you can see
    in *Figure 15.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.6: Top words per topic for pLSA'
  prefs: []
  type: TYPE_NORMAL
- en: Strengths and limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The benefit of using a probability model is that we can now compare the performance
    of different models by evaluating the probability they assign to new documents
    given the parameters learned during training. It also means that the results have
    a clear probabilistic interpretation. In addition, pLSA captures more semantic
    information, including polysemy.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, pLSA increases the computational complexity compared to LSI,
    and the algorithm may only yield a local as opposed to a global maximum. Finally,
    it does not yield a generative model for new documents because it takes them as
    given.
  prefs: []
  type: TYPE_NORMAL
- en: Latent Dirichlet allocation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Latent Dirichlet allocation** (**LDA**) extends pLSA by adding a generative
    process for topics (Blei, Ng, and Jordan 2003). It is the most popular topic model
    because it tends to produce meaningful topics that humans can relate to, can assign
    topics to new documents, and is extensible. Variants of LDA models can include
    metadata, like authors or image data, or learn hierarchical topics.'
  prefs: []
  type: TYPE_NORMAL
- en: How LDA works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LDA is a **hierarchical Bayesian model** that assumes topics are probability
    distributions over words, and documents are distributions over topics. More specifically,
    the model assumes that topics follow a sparse Dirichlet distribution, which implies
    that documents reflect only a small set of topics, and topics use only a limited
    number of terms frequently.
  prefs: []
  type: TYPE_NORMAL
- en: The Dirichlet distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Dirichlet distribution produces probability vectors that can be used as
    a discrete probability distribution. That is, it randomly generates a given number
    of values that are positive and sum to one. It has a parameter ![](img/B15439_15_005.png)
    of positive real value that controls the concentration of the probabilities. Values
    closer to zero mean that only a few values will be positive and receive most of
    the probability mass. *Figure 15.7* illustrates three draws of size 10 for ![](img/B15439_15_006.png)
    = 0.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.7: Three draws from the Dirichlet distribution'
  prefs: []
  type: TYPE_NORMAL
- en: The notebook `dirichlet_distribution` contains a simulation that lets you experiment
    with different parameter values.
  prefs: []
  type: TYPE_NORMAL
- en: The generative model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The LDA topic model assumes the following generative process when an author
    adds an article to a body of documents:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly mix a small subset of topics with proportions defined by the Dirichlet
    probabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each word in the text, select one of the topics according to the document-topic
    probabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a word from the topic's word list according to the topic-word probabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a result, the article content depends on the weight of each topic and the
    terms that make up each topic. The Dirichlet distribution governs the selection
    of topics for documents and words for topics. It encodes the idea that a document
    only covers a few topics, while each topic uses only a small number of words frequently.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **plate notation** for the LDA model in *Figure 15.8* summarizes these
    relationships and highlights the key model parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.8: The statistical dependencies of the LDA model in plate notation'
  prefs: []
  type: TYPE_NORMAL
- en: Reverse engineering the process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The generative process is clearly fictional but turns out to be useful because
    it permits the recovery of the various distributions. The LDA algorithm reverse
    engineers the work of the imaginary author and arrives at a summary of the document-topic-word
    relationships that concisely describes:'
  prefs: []
  type: TYPE_NORMAL
- en: The percentage contribution of each topic to a document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probabilistic association of each word with a topic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDA solves the **Bayesian inference** problem of recovering the distributions
    from the body of documents and the words they contain by reverse engineering the
    assumed content generation process. The original paper by Blei et al. (2003) uses
    **variational Bayes** (**VB**) to approximate the posterior distribution. Alternatives
    include Gibbs sampling and expectation propagation. We will illustrate, shortly,
    the implementations by the sklearn and Gensim libraries.
  prefs: []
  type: TYPE_NORMAL
- en: How to evaluate LDA topics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised topic models do not guarantee that the result will be meaningful
    or interpretable, and there is no objective metric to assess the quality of the
    result as in supervised learning. Human topic evaluation is considered the gold
    standard, but it is potentially expensive and not readily available at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Two options to evaluate results more objectively include **perplexity**, which
    evaluates the model on unseen documents, and **topic coherence** metrics, which
    aim to evaluate the semantic quality of the uncovered patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Perplexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perplexity, when applied to LDA, measures how well the topic-word probability
    distribution recovered by the model predicts a sample of unseen text documents.
    It is based on the entropy *H*(*p*) of this distribution *p* and is computed with
    respect to the set of tokens *w*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_007.png)'
  prefs: []
  type: TYPE_IMG
- en: Measures closer to zero imply the distribution is better at predicting the sample.
  prefs: []
  type: TYPE_NORMAL
- en: Topic coherence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Topic coherence measures the semantic consistency of the topic model results,
    that is, whether humans would perceive the words and their probabilities associated
    with topics as meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, it scores each topic by measuring the degree of semantic similarity
    between the words most relevant to the topic. More specifically, coherence measures
    are based on the probability of observing the set of words *W* that defines a
    topic together.
  prefs: []
  type: TYPE_NORMAL
- en: There are two measures of coherence that have been designed for LDA and are
    shown to align with human judgments of topic quality, namely the UMass and the
    UCI metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The UCI metric (Stevens et al. 2012) defines a word pair''s score to be the
    sum of the **pointwise mutual information** (**PMI**) between two distinct pairs
    of (top) topic words *w*[i], *w*[j] ![](img/B15439_15_008.png) *w* and a smoothing
    factor ![](img/B15439_15_009.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_010.png)'
  prefs: []
  type: TYPE_IMG
- en: The probabilities are computed from word co-occurrence frequencies in a sliding
    window over an external corpus like Wikipedia so that this metric can be thought
    of as an external comparison to semantic ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, the UMass metric (Mimno et al. 2011) uses the co-occurrences in
    a number of documents *D* from the training corpus to compute a coherence score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_011.png)'
  prefs: []
  type: TYPE_IMG
- en: Rather than comparing the model result to extrinsic ground truth, this measure
    reflects intrinsic coherence. Both measures have been evaluated to align well
    with human judgment (Röder, Both, and Hinneburg 2015). In both cases, values closer
    to zero imply that a topic is more coherent.
  prefs: []
  type: TYPE_NORMAL
- en: How to implement LDA using sklearn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the BBC data as before and train an LDA model using sklearn''s
    `decomposition.LatentDirichletAllocation` class with five topics (refer to the
    sklearn documentation for details on the parameters and the notebook `lda_with_sklearn`
    for implementation details):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The model tracks the in-sample perplexity during training and stops iterating
    once this measure stops improving. We can persist and load the result as usual
    with sklearn objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How to visualize LDA results using pyLDAvis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Topic visualization facilitates the evaluation of topic quality using human
    judgment. pyLDAvis is a Python port of LDAvis, developed in R and `D3.js` (Sievert
    and Shirley 2014). We will introduce the key concepts; each LDA application notebook
    contains examples.
  prefs: []
  type: TYPE_NORMAL
- en: pyLDAvis displays the global relationships among topics while also facilitating
    their semantic evaluation by inspecting the terms most closely associated with
    each individual topic and, inversely, the topics associated with each term. It
    also addresses the challenge that terms that are frequent in a corpus tend to
    dominate the distribution over words that define a topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, LDAVis introduces the **relevance** *r* of term *w* to topic *t*.
    The relevance produces a flexible ranking of terms by topic, by computing a weighted
    average of two metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: The degree of association of topic *t* with term *w*, expressed as the conditional
    probability *p*(*w* | *t*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The saliency, or lift, which measures how the frequency of term *w* for the
    topic t, *p*(*w* | *t*), compares to its overall frequency across all documents,
    *p*(*w*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More specifically, we can compute the relevance *r* for a term *w* and a topic
    *t* given a user-defined weight ![](img/B15439_15_012.png), like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_013.png)'
  prefs: []
  type: TYPE_IMG
- en: The tool allows the user to interactively change ![](img/B15439_15_014.png)
    to adjust the relevance, which updates the ranking of terms. User studies have
    found ![](img/B15439_15_015.png) to produce the most plausible results.
  prefs: []
  type: TYPE_NORMAL
- en: How to implement LDA using Gensim
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gensim is a specialized **natural language processing** (**NLP**) library with
    a fast LDA implementation and many additional features. We will also use it in
    the next chapter on word vectors (refer to the notebook `lda_with_gensim` for
    details and the installation directory for related instructions).
  prefs: []
  type: TYPE_NORMAL
- en: 'We convert the DTM produced by sklearn''s `CountVectorizer` or `TfIdfVectorizer`
    into Gensim data structures as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Gensim''s LDA algorithm includes numerous settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Gensim also provides an `LdaMulticore` model for parallel training that may
    speed up training using Python's multiprocessing features for parallel computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model training just requires instantiating `LdaModel`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Gensim evaluates topic coherence, as introduced in the previous section, and
    shows the most important words per topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can display the results as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows the following top words for each topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Topic 1 | Topic 2 | Topic 3 | Topic 4 | Topic 5 |'
  prefs: []
  type: TYPE_TB
- en: '| Probability | Term | Probability | Term | Probability | Term | Probability
    | Term | Probability | Term |'
  prefs: []
  type: TYPE_TB
- en: '| 0.55% | online | 0.90% | best | 1.04% | mobile | 0.64% | market | 0.94% |
    labour |'
  prefs: []
  type: TYPE_TB
- en: '| 0.51% | site | 0.87% | game | 0.98% | phone | 0.53% | growth | 0.72% | blair
    |'
  prefs: []
  type: TYPE_TB
- en: '| 0.46% | game | 0.62% | play | 0.51% | music | 0.52% | sales | 0.72% | brown
    |'
  prefs: []
  type: TYPE_TB
- en: '| 0.45% | net | 0.61% | won | 0.48% | film | 0.49% | economy | 0.65% | election
    |'
  prefs: []
  type: TYPE_TB
- en: '| 0.44% | used | 0.56% | win | 0.48% | use | 0.45% | prices | 0.57% | united
    |'
  prefs: []
  type: TYPE_TB
- en: 'The left panel of *Figure 15.9* displays the topic coherence scores, which
    highlight the decay of topic quality (at least, in part, due to the relatively
    small dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.9: Topic coherence and test set assignments'
  prefs: []
  type: TYPE_NORMAL
- en: The right panel displays the evaluation of our test set of 50 articles with
    our trained model. The model makes four mistakes for an accuracy of 92 percent.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling topics discussed in earnings calls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 3*, *Alternative Data for Finance – Categories and Use Cases*, we
    learned how to scrape earnings call data from the SeekingAlpha site. In this section,
    we will illustrate topic modeling using this source. I'm using a sample of some
    700 earnings call transcripts between 2018 and 2019\. This is a fairly small dataset;
    for a practical application, we would need a larger dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The directory `earnings_calls` contains several files with the code examples
    used in this section. Refer to the notebook `lda_earnings_calls` for details on
    loading, exploring, and preprocessing the data, as well as training and evaluating
    individual models, and the `run_experiments.py` file for the experiments described
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The transcripts consist of individual statements by company representatives,
    an operator, and a Q&A session with analysts. We will treat each of these statements
    as separate documents, ignoring operator statements, to obtain 32,047 items with
    mean and median word counts of 137 and 62, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We use spaCy to preprocess these documents, as illustrated in *Chapter 13*,
    *Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning*, (refer
    to the notebook), and store the cleaned and lemmatized text as a new text file.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration of the most common tokens, as shown in *Figure 15.10*, reveals domain-specific
    stopwords like "year" and "quarter" that we remove in a second step, where we
    also filter out statements with fewer than 10 words so that some 22,582 remain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.10: Most common earnings call tokens'
  prefs: []
  type: TYPE_NORMAL
- en: Model training and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For illustration, we create a DTM containing terms appearing in between 0.5
    and 25 percent of documents that results in 1,529 features. Now we proceed to
    train a 15-topic model using 25 passes over the corpus. This takes a bit over
    two minutes on a 4-core i7.
  prefs: []
  type: TYPE_NORMAL
- en: The top 10 words per topic, as shown in *Figure 15.11*, identify several distinct
    themes that range from obvious financial information to clinical trials (Topic
    5), China and tariff issues (Topic 9), and technology issues (Topic 11).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.11: Most important words for earnings call topics'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using pyLDAvis'' relevance metric with a 0.6 weighting of unconditional frequency
    relative to lift, topic definitions become more intuitive, as illustrated in *Figure
    15.12* for Topic 7 about China and the trade wars:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.12: pyLDAVis'' interactive topic explorer'
  prefs: []
  type: TYPE_NORMAL
- en: The notebook also illustrates how you can look up documents by their topic association.
    In this case, an analyst can review relevant statements for nuances, use sentiment
    analysis to further process the topic-specific text data, or assign labels derived
    from market prices.
  prefs: []
  type: TYPE_NORMAL
- en: Running experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To illustrate the impact of different parameter settings, we run a few hundred
    experiments for different DTM constraints and model parameters. More specifically,
    we let the `min_df` and `max_df` parameters range from 50-500 words and 10 to
    100 percent of documents, respectively, using alternatively binary and absolute
    counts. We then train LDA models with 3 to 50 topics, using 1 and 25 passes over
    the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: The chart in *Figure 15.13* illustrates the results in terms of topic coherence
    (higher is better) and perplexity (lower is better). Coherence drops after 25-30
    topics, and perplexity similarly increases.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.13: Impact of LDA hyperparameter settings on topic quality'
  prefs: []
  type: TYPE_NORMAL
- en: The notebook includes regression results that quantify the relationships between
    parameters and outcomes. We generally get better results using absolute counts
    and a smaller vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling for with financial news
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The notebook `lda_financial_news` contains an example of LDA applied to a subset
    of over 306,000 financial news articles from the first five months of 2018\. The
    datasets have been posted on Kaggle, and the articles have been sourced from CNBC,
    Reuters, the Wall Street Journal, and more. The notebook contains download instructions.
  prefs: []
  type: TYPE_NORMAL
- en: We select the most relevant 120,000 articles based on their section titles with
    a total of 54 million tokens for an average word count of 429 words per article.
    To prepare the data for the LDA model, we rely on spaCy to remove numbers and
    punctuation and lemmatize the results.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 15.14* highlights the remaining most frequent tokens and the article
    length distribution with a median length of 231 tokens; the 90th percentile is
    642 words.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.14: Corpus statistics for financial news data'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 15.15*, we show results for one model using a vocabulary of 3,570
    tokens based on `min_df`=0.005 and `max_df`=0.1, with a single pass to avoid the
    length training time for 15 topics. We can use the `top_topics` attribute of the
    trained `LdaModel` to obtain the most likely words for each topic (refer to the
    notebook for more details).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_15_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.15: Top 15 words for financial news topics'
  prefs: []
  type: TYPE_NORMAL
- en: The topics outline several issues relevant to the time period, including Brexit
    (Topic 8), North Korea (Topic 4), and Tesla (Topic 14).
  prefs: []
  type: TYPE_NORMAL
- en: Gensim provides a `LdaMultiCore` implementation that allows for parallel training
    using Python's multiprocessing module and improves performance by 50 percent when
    using four workers. More workers do not further reduce training time, though,
    due to I/O bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the use of topic modeling to gain insights into
    the content of a large collection of documents. We covered latent semantic indexing
    that uses dimensionality reduction of the DTM to project documents into a latent
    topic space. While effective in addressing the curse of dimensionality caused
    by high-dimensional word vectors, it does not capture much semantic information.
    Probabilistic models make explicit assumptions about the interplay of documents,
    topics, and words that allow algorithms to reverse engineer the document generation
    process and evaluate the model fit on new documents. We learned that LDA is capable
    of extracting plausible topics that allow us to gain a high-level understanding
    of large amounts of text in an automated way, while also identifying relevant
    documents in a targeted way.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to train neural networks that embed individual
    words in a high-dimensional vector space that captures important semantic information
    and allows us to use the resulting word vectors as high-quality text features.
  prefs: []
  type: TYPE_NORMAL
