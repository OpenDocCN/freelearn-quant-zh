- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Topic Modeling – Summarizing Financial News
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模 – 总结财经新闻
- en: In the last chapter, we used the **bag-of-words** (**BOW**) model to convert
    unstructured text data into a numerical format. This model abstracts from word
    order and represents documents as word vectors, where each entry represents the
    relevance of a token to the document. The resulting **document-term matrix** (**DTM**)—or
    transposed as the term-document matrix—is useful for comparing documents to each
    other or a query vector for similarity based on their token content and, therefore,
    finding the proverbial needle in a haystack. It provides informative features
    to classify documents, such as in our sentiment analysis examples.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用了**词袋**（**BOW**）模型将非结构化文本数据转换为数字格式。该模型抽象了词序，并将文档表示为词向量，其中每个条目表示令牌对文档的相关性。由此产生的**文档-术语矩阵**（**DTM**）—或作为术语-文档矩阵的转置—用于比较文档之间或基于其令牌内容的查询向量的相似性，因此，找到干草堆中的大头针。它提供了有用的功能来对文档进行分类，例如在我们的情感分析示例中。
- en: However, this document model produces both high-dimensional data and very sparse
    data, yet it does little to summarize the content or get closer to understanding
    what it is about. In this chapter, we will use **unsupervised machine learning**
    to extract hidden themes from documents using **topic modeling**. These themes
    can produce detailed insights into a large body of documents in an automated way.
    They are very useful in order to understand the haystack itself and allow us to
    tag documents based on their affinity with the various topics.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种文档模型产生了高维度数据和非常稀疏的数据，但它很少总结内容或接近理解内容是什么。在本章中，我们将使用**无监督机器学习**从文档中提取隐藏的主题，使用**主题建模**。这些主题可以以自动化方式为大量文档提供详细的见解。它们非常有用，可以理解干草堆本身，并允许我们基于文档与各种主题的关联度对文档进行标记。
- en: '**Topic models** generate sophisticated, interpretable text features that can
    be a first step toward extracting trading signals from large collections of documents.
    They speed up the review of documents, help identify and cluster similar documents,
    and support predictive modeling.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题模型**生成复杂且可解释的文本特征，可成为从大量文档中提取交易信号的第一步。它们加快了文档的审阅，帮助识别和聚类类似的文档，并支持预测建模。'
- en: '**Applications** include the unsupervised discovery of potentially insightful
    themes in company disclosures or earnings call transcripts, customer reviews,
    or contracts. Furthermore, the document-topic associations facilitate the labeling
    by assigning, for example, sentiment metrics or, more directly, subsequent relevant
    asset returns.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用程序**包括无监督地发现公司披露或收入电话抄本、客户评论或合同中潜在有见地的主题。此外，文档-主题关联有助于通过分配例如情感度量或更直接的后续相关资产收益来进行标记。'
- en: 'More specifically, after reading this chapter, you''ll understand:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，阅读完本章后，您将了解：
- en: How topic modeling has evolved, what it achieves, and why it matters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模的演变，它的成就以及为什么它很重要
- en: Reducing the dimensionality of the DTM using **latent semantic indexing** (**LSI**)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**潜在语义索引**（**LSI**）降低DTM的维度
- en: Extracting topics with **probabilistic latent semantic analysis** (**pLSA**)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**概率隐含语义分析**（**pLSA**）提取主题
- en: How **latent Dirichlet allocation** (**LDA**) improves pLSA to become the most
    popular topic model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）如何改进pLSA成为最流行的主题模型'
- en: Visualizing and evaluating topic modeling results
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化和评估主题建模结果
- en: Running LDA using sklearn and Gensim
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用sklearn和Gensim运行LDA
- en: How to apply topic modeling to collections of earnings calls and financial news articles
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将主题建模应用于收入电话和财经新闻文章的集合
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库的相应目录中找到此章节的代码示例和其他资源链接。笔记本包括图像的彩色版本。
- en: Learning latent topics – Goals and approaches
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习潜在主题 – 目标和方法
- en: Topic modeling discovers hidden themes that capture semantic information beyond
    individual words in a body of documents. It aims to address a key challenge for
    a machine learning algorithm that learns from text data by transcending the lexical
    level of "what actually has been written" to the semantic level of "what was intended."
    The resulting topics can be used to annotate documents based on their association
    with various topics.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, topic models automatically **summarize large collections
    of documents** to facilitate organization and management as well as search and
    recommendations. At the same time, it enables the understanding of documents to
    the extent that humans can interpret the descriptions of topics.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Topic models also mitigate the **curse of dimensionality** that often plagues
    the BOW model; representing documents with high-dimensional, sparse vectors can
    make similarity measures noisy, lead to inaccurate distance measurements, and
    result in the overfitting of text classification models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the BOW model loses context as well as semantic information since
    it ignores word order. It is also unable to capture synonymy (where several words
    have the same meaning) or polysemy (where one word has several meanings). As a
    result of the latter, document retrieval or similarity search may miss the point
    when the documents are not indexed by the terms used to search or compare.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'These shortcomings of the BOW model prompt the question: how can we learn meaningful
    topics from data that facilitate a more productive interaction with documentary
    data?'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Initial attempts by topic models to improve on the vector space model (developed
    in the mid-1970s) applied linear algebra to reduce the dimensionality of the DTM.
    This approach is similar to the algorithm that we discussed as principal component
    analysis in *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation with
    Unsupervised Learning*. While effective, it is difficult to evaluate the results
    of these models without a benchmark model. In response, probabilistic models have
    emerged that assume an explicit document generation process and provide algorithms
    to reverse engineer this process and recover the underlying topics.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table highlights key milestones in the model evolution, which
    we will address in more detail in the following sections:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Year | Description |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
- en: '| **Latent semantic indexing (LSI)** | 1988 | Captures the semantic document-term
    relationship by reducing the dimensionality of the word space |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '| **Probabilistic latent semantic analysis (pLSA)** | 1999 | Reverse engineers
    a generative process that assumes words generate a topic and documents as a mix
    of topics |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| **Latent Dirichlet allocation (LDA)** | 2003 | Adds a generative process
    for documents: a three-level hierarchical Bayesian model |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: Latent semantic indexing
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '**Latent semantic indexing** (**LSI**)—also called **latent semantic analysis**
    (**LSA**)—set out to improve the results of queries that omitted relevant documents
    containing synonyms of query terms (Dumais et al. 1988). Its goal was to model
    the relationships between documents and terms so that it could predict that a
    term should be associated with a document, even though, because of the variability
    in word use, no such association was observed.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在语义索引**（**LSI**）—也称为**潜在语义分析**（**LSA**）—旨在改进省略了包含查询词同义词的相关文档的查询结果（Dumais等人，1988年）。其目标是建模文档与术语之间的关系，以便可以预测术语应与文档关联，即使由于单词使用的变异性，没有观察到这种关联。'
- en: LSI uses linear algebra to find a given number *k* of latent topics by decomposing
    the DTM. More specifically, it uses the **singular value decomposition** (**SVD**)
    to find the best lower-rank DTM approximation using *k* singular values and vectors.
    In other words, LSI builds on some of the dimensionality reduction techniques
    we encountered in *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation
    with Unsupervised Learning*. The authors also experimented with hierarchical clustering
    but found it too restrictive for this purpose.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LSI使用线性代数来找到给定数量*k*的潜在主题，通过分解DTM。更具体地说，它使用**奇异值分解**（**SVD**）来找到使用*k*个奇异值和向量的最佳低秩DTM近似。换句话说，LSI建立在我们在*第13章*，*使用无监督学习的数据驱动风险因子和资产配置*中遇到的一些降维技术上。作者还尝试过分层聚类，但发现这对于此目的来说太受限制了。
- en: 'In this context, SVD identifies a set of uncorrelated indexing variables or
    factors that represent each term and document by its vector of factor values.
    *Figure 15.1* illustrates how SVD decomposes the DTM into three matrices: two
    matrices that contain orthogonal singular vectors and a diagonal matrix with singular
    values that serve as scaling factors.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，SVD识别一组未相关的索引变量或因子，通过其因子值的向量表示每个术语和文档。 *图15.1*说明了SVD如何将DTM分解为三个矩阵：包含正交奇异向量的两个矩阵和具有奇异值的对角矩阵，该奇异值用作缩放因子。
- en: Assuming some correlation in the input DTM, singular values decay in value.
    Therefore, selecting the *T*-largest singular values yields a lower-dimensional
    approximation of the original DTM that loses relatively little information. In
    the compressed version, the rows or columns that had *N* items only have *T* <
    *N* entries.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设输入DTM中存在一些相关性，则奇异值会衰减。因此，选择*T*个最大奇异值会产生原始DTM的低维近似，且丢失的信息相对较少。在压缩版本中，原本有*N*个条目的行或列只有*T*
    < *N*个条目。
- en: 'The LSI decomposition of the DTM can be interpreted as shown in *Figure 15.1*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: DTM的LSI分解可以解释如*图15.1*所示：
- en: The first ![](img/B15439_15_001.png) matrix represents the relationships between
    documents and topics.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个![](img/B15439_15_001.png)矩阵表示文档与主题之间的关系。
- en: The diagonal matrix scales the topics by their corpus strength.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对角矩阵通过其语料库强度对主题进行缩放。
- en: The third matrix models the term-topic relationship.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个矩阵建模了术语-主题关系。
- en: '![](img/B15439_15_01.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_01.png)'
- en: 'Figure 15.1: LSI and the SVD'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1：LSI和SVD
- en: The rows of the matrix produced by multiplying the first two matrices ![](img/B15439_15_002.png)
    correspond to the locations of the original documents projected into the latent
    topic space.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 将第一个两个矩阵相乘产生的矩阵的行对应于原始文档投影到潜在主题空间中的位置。
- en: How to implement LSI using sklearn
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用sklearn实现LSI
- en: We will illustrate LSI using the BBC articles data that we introduced in the
    last chapter because they are small enough for quick training and allow us to
    compare topic assignments with category labels. Refer to the notebook `latent_semantic_indexing`
    for additional implementation details.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上一章介绍的BBC文章数据来说明LSI，因为它们足够小，可以快速训练，并且允许我们将主题分配与类别标签进行比较。有关其他实施细节，请参阅笔记本`latent_semantic_indexing`。
- en: 'We begin by loading the documents and creating a train and (stratified) test
    set with 50 articles. Then, we vectorize the data using `TfidfVectorizer` to obtain
    weighted DTM counts and filter out words that appear in less than 1 percent or
    more than 25 percent of the documents, as well as generic stopwords, to obtain
    a vocabulary of around 2,900 words:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载文档，并创建一个包含50篇文章的训练和（分层）测试集。然后，我们使用`TfidfVectorizer`对数据进行向量化，以获得加权DTM计数，并过滤出出现在不到1％或超过25％的文档中的词语，以及常见的停用词，以获得大约2,900个词汇：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We use scikit-learn's `TruncatedSVD` class, which only computes the *k*-largest
    singular values, to reduce the dimensionality of the DTM. The deterministic `arpack`
    algorithm delivers an exact solution, but the default "randomized" implementation
    is more efficient for large matrices.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用scikit-learn的`TruncatedSVD`类，它只计算*k*个最大的奇异值，以减少DTM的维度。确定性的`arpack`算法提供了一个精确的解，但默认的“随机化”实现对于大矩阵更有效。
- en: 'We compute five topics to match the five categories, which explain only 5.4
    percent of the total DTM variance, so a larger number of topics would be reasonable:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了五个主题以匹配五个类别，这解释了总DTM方差的仅5.4％，因此使用更多主题是合理的：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'LSI identifies a new orthogonal basis for the DTM that reduces the rank to
    the number of desired topics. The `.transform()` method of the trained `svd` object
    projects the documents into the new topic space. This space results from reducing
    the dimensionality of the document vectors and corresponds to the ![](img/B15439_15_002.png)
    transformation illustrated earlier in this section:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LSI为DTM确定了一个新的正交基，将排名降低到所需主题的数量。训练过的`svd`对象的`.transform()`方法将文档投影到新的主题空间中。这个空间由减少文档向量维度而产生，并且对应于本节前面所示的![](img/B15439_15_002.png)转换：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can sample an article to view its location in the topic space. We draw a
    "Politics" article that is most (positively) associated with topics 1 and 2:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对文章进行采样，以查看其在主题空间中的位置。我们选择了一个与主题1和2最（积极）相关的“政治”文章：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The topic assignments for this sample align with the average topic weights for
    each category illustrated in *Figure 15.2* ("Politics" is the rightmost bar).
    They illustrate how LSI expresses the *k* topics as directions in a *k*-dimensional
    space (the notebook includes a projection of the average topic assignments per
    category into two-dimensional space).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的主题分配与每个类别的平均主题权重一致，如*图15.2*所示（“政治”是最右边的条）。它们说明了LSI如何将*k*个主题表达为*k*维空间中的方向（笔记本包括每个类别的平均主题分配在二维空间中的投影）。
- en: Each category is clearly defined, and the test assignments match with train
    assignments. However, the weights are both positive and negative, making it more
    difficult to interpret the topics.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每个类别都有明确的定义，测试分配与训练分配匹配。但是，权重既有正值又有负值，这使得解释主题更加困难。
- en: '![A screenshot of a video game  Description automatically generated](img/B15439_15_02.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![一个视频游戏的屏幕截图 自动生成的描述](img/B15439_15_02.png)'
- en: 'Figure 15.2: LSI topic weights for train and test data'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2：训练和测试数据的LSI主题权重
- en: We can also display the words that are most closely associated with each topic
    (in absolute terms). The topics appear to capture some semantic information but
    are not clearly differentiated (refer to *Figure 15.3*).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以显示与每个主题最相关的单词（绝对值）。主题似乎捕捉到了一些语义信息，但并没有明显区分（参见*图15.3*）。
- en: '![](img/B15439_15_03.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_03.png)'
- en: 'Figure 15.3: Top 10 words per LSI topic'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3：LSI主题的前10个单词
- en: Strengths and limitations
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优缺点
- en: The strengths of LSI include the removal of noise and the mitigation of the
    curse of dimensionality. It also captures some semantic aspects, like synonymy,
    and clusters both documents and terms via their topic associations. Furthermore,
    it does not require knowledge of the document language, and both information retrieval
    queries and document comparisons are easy to do.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: LSI的优点包括消除噪音和缓解维度诅咒。它还捕获了一些语义方面，如同义词，并通过它们的主题关联来聚类文档和术语。此外，它不需要对文档语言有所了解，并且信息检索查询和文档比较都很容易。
- en: However, the results of LSI are difficult to interpret because topics are word
    vectors with both positive and negative entries. In addition, there is no underlying
    model that would permit the evaluation of fit or provide guidance when selecting
    the number of dimensions or topics to use.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LSI的结果很难解释，因为主题是具有正值和负值的词向量。此外，没有底层模型可以允许拟合的评估，也没有在选择要使用的维度或主题数量时提供指导。
- en: Probabilistic latent semantic analysis
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率隐含语义分析
- en: '**Probabilistic latent semantic analysis** (**pLSA**) takes a **statistical
    perspective** on LSI/LSA and creates a generative model to address the lack of
    theoretical underpinnings of LSA (Hofmann 2001).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率隐含语义分析**（**pLSA**）以**统计视角**看待LSI/LSA，并创建一个生成模型来解决LSA缺乏理论基础的问题（Hofmann 2001）。'
- en: pLSA explicitly models the probability word *w* appearing in document *d*, as
    described by the DTM as a mixture of conditionally independent multinomial distributions
    that involve topics *t*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: pLSA 明确地将词 *w* 出现在文档 *d* 中的概率建模为条件独立的多项式分布的混合，其中涉及主题 *t*。
- en: There are both **symmetric and asymmetric formulations** of how word-document
    co-occurrences come about. The former assumes that both words and documents are
    generated by the latent topic class. In contrast, the asymmetric model assumes
    that topics are selected given the document, and words result in a second step
    given the topic.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于词-文档共现的形成，有对称和不对称的两种表述**。前者假设单词和文档都是由潜在主题类生成的。相反，不对称模型假设在给定文档的情况下选择主题，并且在给定主题的情况下产生单词。'
- en: '![](img/B15439_15_004.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_004.png)'
- en: The number of topics is a **hyperparameter** chosen prior to training and is
    not learned from the data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 主题数量是在训练之前选择的**超参数**，并不是从数据中学习得到的。
- en: 'The **plate notation** in *Figure 15.4* describes the statistical dependencies
    in a probabilistic model. More specifically, it encodes the relationship just
    described for the asymmetric model. Each rectangle represents multiple items:
    the outer block stands for *M* documents, while the inner shaded rectangle symbolizes
    *N* words for each document. We only observe the documents and their content;
    the model infers the hidden or latent topic distribution:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15.4* 中的**板块表示法**描述了概率模型中的统计依赖关系。更具体地说，它对称编码了刚才描述的不对称模型的关系。每个矩形代表多个项目：外部块代表
    *M* 个文档，而内部阴影矩形象征着每个文档的 *N* 个单词。我们只观察文档及其内容；模型推断出隐藏或潜在的主题分布：'
- en: '![](img/B15439_15_04.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_04.png)'
- en: 'Figure 15.4: The statistical dependencies modeled by pLSA in plate notation'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.4：pLSA 模型的统计依赖关系的板块表示法
- en: Let's now take a look at how we can implement this model in practice.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在实践中实现这个模型。
- en: How to implement pLSA using sklearn
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 sklearn 实现 pLSA
- en: pLSA is equivalent to **non-negative matrix factorization** (**NMF**) using
    a Kullback-Leibler divergence objective (view the references on GitHub). Therefore,
    we can use the `sklearn.decomposition.NMF` class to implement this model following
    the LSI example.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: pLSA 相当于使用 Kullback-Leibler 散度目标的**非负矩阵分解**（**NMF**）。因此，我们可以使用 `sklearn.decomposition.NMF`
    类来实现这个模型，按照 LSI 示例。
- en: 'Using the same train-test split of the DTM produced by `TfidfVectorizer`, we
    fit pLSA like so:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用由 `TfidfVectorizer` 产生的 DTM 的相同训练-测试拆分，我们这样适配 pLSA：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We get a measure of the reconstruction error that is a substitute for the explained
    variance measure from earlier:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个重建误差的度量，它是对之前解释的方差度量的替代：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Due to its probabilistic nature, pLSA produces only positive topic weights
    that result in more straightforward topic-category relationships for the test
    and training sets, as shown in *Figure 15.5*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其概率性质，pLSA 仅产生正主题权重，这导致了更直接的主题-类别关系，如 *图 15.5* 所示，适用于测试和训练集：
- en: '![](img/B15439_15_05.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_05.png)'
- en: 'Figure 15.5: pLSA weights by topic for train and test data'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.5：pLSA 对训练和测试数据的主题权重
- en: 'We also note that the word lists that describe each topic begin to make more
    sense; for example, the "Entertainment" category is most directly associated with
    Topic 4, which includes the words "film," "star," and so forth, as you can see
    in *Figure 15.6*:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还注意到，描述每个主题的词列表开始变得更有意义；例如，“娱乐”类别与主题 4 最直接关联，其中包括“电影”，“明星”等词，正如您在*图 15.6*
    中所看到的：
- en: '![](img/B15439_15_06.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_06.png)'
- en: 'Figure 15.6: Top words per topic for pLSA'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.6：pLSA 的每个主题的前几个词
- en: Strengths and limitations
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: The benefit of using a probability model is that we can now compare the performance
    of different models by evaluating the probability they assign to new documents
    given the parameters learned during training. It also means that the results have
    a clear probabilistic interpretation. In addition, pLSA captures more semantic
    information, including polysemy.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用概率模型的好处是，我们现在可以通过评估它们在训练期间学习的参数给出的新文档的概率来比较不同模型的性能。这也意味着结果具有清晰的概率解释。此外，pLSA
    捕捉到了更多的语义信息，包括一词多义。
- en: On the other hand, pLSA increases the computational complexity compared to LSI,
    and the algorithm may only yield a local as opposed to a global maximum. Finally,
    it does not yield a generative model for new documents because it takes them as
    given.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，与LSI相比，pLSA增加了计算复杂性，并且该算法可能仅产生局部而不是全局最大值。最后，它不会为新文档产生生成模型，因为它将它们视为给定的。
- en: Latent Dirichlet allocation
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配
- en: '**Latent Dirichlet allocation** (**LDA**) extends pLSA by adding a generative
    process for topics (Blei, Ng, and Jordan 2003). It is the most popular topic model
    because it tends to produce meaningful topics that humans can relate to, can assign
    topics to new documents, and is extensible. Variants of LDA models can include
    metadata, like authors or image data, or learn hierarchical topics.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）通过为主题添加一个生成过程（Blei、Ng 和 Jordan，2003）扩展了 pLSA。它是最流行的主题模型，因为它倾向于生成人类可以关联的有意义的主题，可以将主题分配给新文档，并且是可扩展的。LDA
    模型的变体可以包括元数据，如作者或图像数据，或者学习分层主题。'
- en: How LDA works
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LDA 的工作原理
- en: LDA is a **hierarchical Bayesian model** that assumes topics are probability
    distributions over words, and documents are distributions over topics. More specifically,
    the model assumes that topics follow a sparse Dirichlet distribution, which implies
    that documents reflect only a small set of topics, and topics use only a limited
    number of terms frequently.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 是一个假设主题是单词概率分布、文档是主题分布的**分层贝叶斯模型**。更具体地说，该模型假设主题遵循稀疏狄利克雷分布，这意味着文档仅反映了一小部分主题，而主题仅频繁使用了有限数量的术语。
- en: The Dirichlet distribution
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 狄利克雷分布
- en: 'The Dirichlet distribution produces probability vectors that can be used as
    a discrete probability distribution. That is, it randomly generates a given number
    of values that are positive and sum to one. It has a parameter ![](img/B15439_15_005.png)
    of positive real value that controls the concentration of the probabilities. Values
    closer to zero mean that only a few values will be positive and receive most of
    the probability mass. *Figure 15.7* illustrates three draws of size 10 for ![](img/B15439_15_006.png)
    = 0.1:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 狄利克雷分布产生可以用作离散概率分布的概率向量。也就是说，它随机生成一定数量的值，这些值为正并总和为一。它有一个正实值参数 ![](img/B15439_15_005.png)，它控制概率的集中度。值越接近零，意味着只有少数值将为正，并且接收大部分概率质量。*图
    15.7* 说明了 ![](img/B15439_15_006.png) = 0.1 时大小为 10 的三次绘制：
- en: '![](img/B15439_15_07.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_07.png)'
- en: 'Figure 15.7: Three draws from the Dirichlet distribution'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.7：来自狄利克雷分布的三次绘制
- en: The notebook `dirichlet_distribution` contains a simulation that lets you experiment
    with different parameter values.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本 `dirichlet_distribution` 包含一个模拟，让您可以尝试不同的参数值。
- en: The generative model
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成模型
- en: 'The LDA topic model assumes the following generative process when an author
    adds an article to a body of documents:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当作者将文章添加到文档集时，LDA 主题模型假定以下生成过程：
- en: Randomly mix a small subset of topics with proportions defined by the Dirichlet
    probabilities.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用由狄利克雷概率定义的比例随机混合一小部分主题。
- en: For each word in the text, select one of the topics according to the document-topic
    probabilities.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对文本中的每个单词，根据文档-主题概率选择其中一个主题。
- en: Select a word from the topic's word list according to the topic-word probabilities.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据主题的单词列表中的主题-单词概率选择一个词。
- en: As a result, the article content depends on the weight of each topic and the
    terms that make up each topic. The Dirichlet distribution governs the selection
    of topics for documents and words for topics. It encodes the idea that a document
    only covers a few topics, while each topic uses only a small number of words frequently.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，文章内容取决于每个主题的权重以及构成每个主题的术语。狄利克雷分布控制文档的主题和主题的词的选择。它编码了一个文档仅涵盖少数主题的想法，而每个主题仅使用少量频繁的单词。
- en: 'The **plate notation** for the LDA model in *Figure 15.8* summarizes these
    relationships and highlights the key model parameters:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15.8* 中 LDA 模型的**板符号**总结了这些关系，并突出显示了关键的模型参数：'
- en: '![](img/B15439_15_08.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_08.png)'
- en: 'Figure 15.8: The statistical dependencies of the LDA model in plate notation'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.8：LDA 模型的统计依赖关系，以板块符号表示
- en: Reverse engineering the process
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向工程过程
- en: 'The generative process is clearly fictional but turns out to be useful because
    it permits the recovery of the various distributions. The LDA algorithm reverse
    engineers the work of the imaginary author and arrives at a summary of the document-topic-word
    relationships that concisely describes:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 生成过程显然是虚构的，但事实证明是有用的，因为它允许恢复各种分布。LDA算法逆向工程了想象作者的工作，并得出了对文档-主题-词关系进行简洁描述的总结：
- en: The percentage contribution of each topic to a document
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个主题对文档的百分比贡献
- en: The probabilistic association of each word with a topic
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个单词与主题的概率关联
- en: LDA solves the **Bayesian inference** problem of recovering the distributions
    from the body of documents and the words they contain by reverse engineering the
    assumed content generation process. The original paper by Blei et al. (2003) uses
    **variational Bayes** (**VB**) to approximate the posterior distribution. Alternatives
    include Gibbs sampling and expectation propagation. We will illustrate, shortly,
    the implementations by the sklearn and Gensim libraries.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: LDA解决了从文档体和它们包含的单词中恢复分布的**贝叶斯推理**问题，通过逆向工程所假定的内容生成过程。 Blei等人（2003年）的原始论文使用**变分贝叶斯**（**VB**）来近似后验分布。替代方案包括吉布斯采样和期望传播。我们将简要介绍sklearn和Gensim库的实现。
- en: How to evaluate LDA topics
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何评估LDA主题
- en: Unsupervised topic models do not guarantee that the result will be meaningful
    or interpretable, and there is no objective metric to assess the quality of the
    result as in supervised learning. Human topic evaluation is considered the gold
    standard, but it is potentially expensive and not readily available at scale.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督主题模型不能保证结果是有意义的或可解释的，并且没有客观的度量来评估结果的质量，就像在监督学习中一样。人类主题评估被认为是黄金标准，但可能昂贵，并且不易大规模获得。
- en: Two options to evaluate results more objectively include **perplexity**, which
    evaluates the model on unseen documents, and **topic coherence** metrics, which
    aim to evaluate the semantic quality of the uncovered patterns.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 更客观地评估结果的两个选项包括**困惑度**，它在未见文档上评估模型，以及**主题连贯性**度量，旨在评估所发现模式的语义质量。
- en: Perplexity
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 困惑度
- en: 'Perplexity, when applied to LDA, measures how well the topic-word probability
    distribution recovered by the model predicts a sample of unseen text documents.
    It is based on the entropy *H*(*p*) of this distribution *p* and is computed with
    respect to the set of tokens *w*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度，当应用于LDA时，衡量模型恢复的主题-词概率分布对未见文本文档样本的预测能力。它基于这个分布*p*的熵*H*(*p*)，并针对标记集*w*计算：
- en: '![](img/B15439_15_007.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_007.png)'
- en: Measures closer to zero imply the distribution is better at predicting the sample.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接近零的度量意味着分布在预测样本方面更好。
- en: Topic coherence
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主题连贯性
- en: Topic coherence measures the semantic consistency of the topic model results,
    that is, whether humans would perceive the words and their probabilities associated
    with topics as meaningful.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 主题连贯性衡量主题模型结果的语义一致性，即人类是否会将与主题相关的单词及其概率视为有意义。
- en: To this end, it scores each topic by measuring the degree of semantic similarity
    between the words most relevant to the topic. More specifically, coherence measures
    are based on the probability of observing the set of words *W* that defines a
    topic together.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '为此，它通过测量与主题最相关的单词之间的语义相似度来对每个主题进行评分。更具体地说，连贯性度量基于观察到的定义一个主题的单词集合*W*的概率。  '
- en: There are two measures of coherence that have been designed for LDA and are
    shown to align with human judgments of topic quality, namely the UMass and the
    UCI metrics.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个连贯性度量被设计用于LDA，并且已经显示与主题质量的人类判断相一致，即UMass和UCI度量。
- en: 'The UCI metric (Stevens et al. 2012) defines a word pair''s score to be the
    sum of the **pointwise mutual information** (**PMI**) between two distinct pairs
    of (top) topic words *w*[i], *w*[j] ![](img/B15439_15_008.png) *w* and a smoothing
    factor ![](img/B15439_15_009.png):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: UCI度量（Stevens等，2012年）将词对的分数定义为两个不同的（顶部）主题词*w*[i]，*w*[j]之间的**点间互信息**（**PMI**）的和*w*以及平滑因子的乘积：
- en: '![](img/B15439_15_010.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_010.png)'
- en: The probabilities are computed from word co-occurrence frequencies in a sliding
    window over an external corpus like Wikipedia so that this metric can be thought
    of as an external comparison to semantic ground truth.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 概率是根据滑动窗口在外部语料库（如维基百科）上的词共现频率计算的，因此可以将这个度量视为与语义基准的外部比较。
- en: 'In contrast, the UMass metric (Mimno et al. 2011) uses the co-occurrences in
    a number of documents *D* from the training corpus to compute a coherence score:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相反，UMass指标（Mimno等人，2011年）使用训练语料库中来自多个文档*D*的共现性来计算一致性得分：
- en: '![](img/B15439_15_011.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_011.png)'
- en: Rather than comparing the model result to extrinsic ground truth, this measure
    reflects intrinsic coherence. Both measures have been evaluated to align well
    with human judgment (Röder, Both, and Hinneburg 2015). In both cases, values closer
    to zero imply that a topic is more coherent.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与将模型结果与外部真实值进行比较不同，此度量反映了内在一致性。两种度量方法都经过评估，与人类判断很好地一致（Röder、Both和Hinneburg，2015年）。在这两种情况下，接近零的值意味着主题更一致。
- en: How to implement LDA using sklearn
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用sklearn实现LDA
- en: 'We will use the BBC data as before and train an LDA model using sklearn''s
    `decomposition.LatentDirichletAllocation` class with five topics (refer to the
    sklearn documentation for details on the parameters and the notebook `lda_with_sklearn`
    for implementation details):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像以前一样使用BBC数据，并使用sklearn的`decomposition.LatentDirichletAllocation`类训练一个具有五个主题的LDA模型（有关参数的详细信息，请参阅sklearn文档和笔记本`lda_with_sklearn`中的实现细节）：
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The model tracks the in-sample perplexity during training and stops iterating
    once this measure stops improving. We can persist and load the result as usual
    with sklearn objects:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在训练期间跟踪样本内困惑度，并在此度量停止改善时停止迭代。我们可以像往常一样使用sklearn对象进行持久化和加载结果：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How to visualize LDA results using pyLDAvis
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用pyLDAvis可视化LDA结果
- en: Topic visualization facilitates the evaluation of topic quality using human
    judgment. pyLDAvis is a Python port of LDAvis, developed in R and `D3.js` (Sievert
    and Shirley 2014). We will introduce the key concepts; each LDA application notebook
    contains examples.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 话题可视化有助于使用人类判断评估话题质量。 pyLDAvis是LDAvis的Python版本，由R和`D3.js`（Sievert和Shirley，2014年）开发。我们将介绍关键概念；每个LDA应用笔记本都包含示例。
- en: pyLDAvis displays the global relationships among topics while also facilitating
    their semantic evaluation by inspecting the terms most closely associated with
    each individual topic and, inversely, the topics associated with each term. It
    also addresses the challenge that terms that are frequent in a corpus tend to
    dominate the distribution over words that define a topic.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: pyLDAvis显示了主题之间的全局关系，同时通过检查与每个单独主题最密切关联的术语以及与每个术语相关联的主题，促进了其语义评估。它还解决了语料库中频繁出现的术语往往支配了定义主题的单词分布的挑战。
- en: 'To this end, LDAVis introduces the **relevance** *r* of term *w* to topic *t*.
    The relevance produces a flexible ranking of terms by topic, by computing a weighted
    average of two metrics:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，LDAVis引入了术语*w*对主题*t*的**相关性** *r*。相关性通过计算两个指标的加权平均值产生了对话题的术语的灵活排名：
- en: The degree of association of topic *t* with term *w*, expressed as the conditional
    probability *p*(*w* | *t*)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 话题*t*与术语*w*的关联程度，表示为条件概率*p*（*w* | *t*）
- en: The saliency, or lift, which measures how the frequency of term *w* for the
    topic t, *p*(*w* | *t*), compares to its overall frequency across all documents,
    *p*(*w*)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显著性或提升，它衡量了术语*w*对主题t的频率*p*（*w* | *t*）与其在所有文档中的总体频率*p*（*w*）的比较
- en: 'More specifically, we can compute the relevance *r* for a term *w* and a topic
    *t* given a user-defined weight ![](img/B15439_15_012.png), like the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们可以计算术语*w*和主题*t*的相关性*r*，给定用户定义的权重![](img/B15439_15_012.png)，如下所示：
- en: '![](img/B15439_15_013.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_013.png)'
- en: The tool allows the user to interactively change ![](img/B15439_15_014.png)
    to adjust the relevance, which updates the ranking of terms. User studies have
    found ![](img/B15439_15_015.png) to produce the most plausible results.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 该工具允许用户交互地更改![](img/B15439_15_014.png)以调整相关性，这会更新术语的排名。用户研究发现![](img/B15439_15_015.png)产生最合理的结果。
- en: How to implement LDA using Gensim
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用Gensim实现LDA
- en: Gensim is a specialized **natural language processing** (**NLP**) library with
    a fast LDA implementation and many additional features. We will also use it in
    the next chapter on word vectors (refer to the notebook `lda_with_gensim` for
    details and the installation directory for related instructions).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim是一个专门的**自然语言处理**（**NLP**）库，具有快速的LDA实现和许多附加功能。我们还将在下一章关于词向量的笔记本`lda_with_gensim`中使用它（有关详细信息，请参阅安装目录中的相关说明）。
- en: 'We convert the DTM produced by sklearn''s `CountVectorizer` or `TfIdfVectorizer`
    into Gensim data structures as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将由sklearn的`CountVectorizer`或`TfIdfVectorizer`生成的DTM转换为Gensim数据结构，如下所示：
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Gensim''s LDA algorithm includes numerous settings:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim的LDA算法包括许多设置：
- en: '[PRE9]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Gensim also provides an `LdaMulticore` model for parallel training that may
    speed up training using Python's multiprocessing features for parallel computation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim还提供了一个`LdaMulticore`模型进行并行训练，可以利用Python的多进程功能加快训练速度。
- en: 'Model training just requires instantiating `LdaModel`, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练只需要实例化`LdaModel`，如下所示：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Gensim evaluates topic coherence, as introduced in the previous section, and
    shows the most important words per topic:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim评估主题一致性，如前一节所介绍的，并显示每个主题的最重要单词：
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can display the results as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下显示结果：
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This shows the following top words for each topic:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了每个主题的顶级单词：
- en: '| Topic 1 | Topic 2 | Topic 3 | Topic 4 | Topic 5 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 主题 1 | 主题 2 | 主题 3 | 主题 4 | 主题 5 |'
- en: '| Probability | Term | Probability | Term | Probability | Term | Probability
    | Term | Probability | Term |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 概率 | 术语 | 概率 | 术语 | 概率 | 术语 | 概率 | 术语 | 概率 | 术语 |'
- en: '| 0.55% | online | 0.90% | best | 1.04% | mobile | 0.64% | market | 0.94% |
    labour |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 0.55% | 在线 | 0.90% | 最佳 | 1.04% | 移动 | 0.64% | 市场 | 0.94% | 劳工 |'
- en: '| 0.51% | site | 0.87% | game | 0.98% | phone | 0.53% | growth | 0.72% | blair
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 0.51% | 网站 | 0.87% | 游戏 | 0.98% | 手机 | 0.53% | 增长 | 0.72% | 布莱尔 |'
- en: '| 0.46% | game | 0.62% | play | 0.51% | music | 0.52% | sales | 0.72% | brown
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 0.46% | 游戏 | 0.62% | 玩 | 0.51% | 音乐 | 0.52% | 销售 | 0.72% | 布朗 |'
- en: '| 0.45% | net | 0.61% | won | 0.48% | film | 0.49% | economy | 0.65% | election
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 0.45% | 净 | 0.61% | 赢 | 0.48% | 电影 | 0.49% | 经济 | 0.65% | 选举 |'
- en: '| 0.44% | used | 0.56% | win | 0.48% | use | 0.45% | prices | 0.57% | united
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 0.44% | 使用 | 0.56% | 赢 | 0.48% | 使用 | 0.45% | 价格 | 0.57% | 联合 |'
- en: 'The left panel of *Figure 15.9* displays the topic coherence scores, which
    highlight the decay of topic quality (at least, in part, due to the relatively
    small dataset):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15.9*的左侧面板显示了主题一致性分数，突显了主题质量的衰减（至少部分是由于相对较小的数据集）：'
- en: '![](img/B15439_15_09.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_09.png)'
- en: 'Figure 15.9: Topic coherence and test set assignments'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9：主题一致性和测试集分配
- en: The right panel displays the evaluation of our test set of 50 articles with
    our trained model. The model makes four mistakes for an accuracy of 92 percent.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧面板显示了我们训练模型的50篇文章的测试集的评估。模型对四个错误，准确率为92%。
- en: Modeling topics discussed in earnings calls
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对盈利电话中讨论的主题进行建模
- en: In *Chapter 3*, *Alternative Data for Finance – Categories and Use Cases*, we
    learned how to scrape earnings call data from the SeekingAlpha site. In this section,
    we will illustrate topic modeling using this source. I'm using a sample of some
    700 earnings call transcripts between 2018 and 2019\. This is a fairly small dataset;
    for a practical application, we would need a larger dataset.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3章*，*金融的替代数据 - 类别和用例*中，我们学习了如何从SeekingAlpha网站抓取盈利电话数据。在本节中，我们将使用这个数据源进行主题建模。我使用了2018年至2019年之间的约700份盈利电话转录样本。这是一个相当小的数据集；对于实际应用，我们需要一个更大的数据集。
- en: The directory `earnings_calls` contains several files with the code examples
    used in this section. Refer to the notebook `lda_earnings_calls` for details on
    loading, exploring, and preprocessing the data, as well as training and evaluating
    individual models, and the `run_experiments.py` file for the experiments described
    next.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 目录`earnings_calls`中包含了本节中使用的代码示例的多个文件。有关加载、探索和预处理数据的详细信息，请参阅笔记本`lda_earnings_calls`，以及用于描述下一步实验的`run_experiments.py`文件。
- en: Data preprocessing
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'The transcripts consist of individual statements by company representatives,
    an operator, and a Q&A session with analysts. We will treat each of these statements
    as separate documents, ignoring operator statements, to obtain 32,047 items with
    mean and median word counts of 137 and 62, respectively:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 转录包括公司代表的个别声明，操作员和分析师的问答环节。我们将这些声明中的每一条都视为单独的文档，忽略操作员的声明，以获取32,047个项目，平均字数和中位数分别为137和62：
- en: '[PRE13]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We use spaCy to preprocess these documents, as illustrated in *Chapter 13*,
    *Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning*, (refer
    to the notebook), and store the cleaned and lemmatized text as a new text file.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用spaCy对这些文档进行预处理，如*第13章*，*使用无监督学习进行数据驱动的风险因素和资产配置*中所示（参考笔记本），并将清理和词形还原后的文本存储为一个新的文本文件。
- en: Exploration of the most common tokens, as shown in *Figure 15.10*, reveals domain-specific
    stopwords like "year" and "quarter" that we remove in a second step, where we
    also filter out statements with fewer than 10 words so that some 22,582 remain.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 15.10* 所示，探索最常见的标记揭示出领域特定的停用词，如 "year" 和 "quarter"，我们在第二步中去除，同时过滤掉少于 10
    个词的语句，剩余约 22,582 个。
- en: '![](img/B15439_15_10.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_10.png)'
- en: 'Figure 15.10: Most common earnings call tokens'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.10：最常见的收益电话标记
- en: Model training and evaluation
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练和评估
- en: For illustration, we create a DTM containing terms appearing in between 0.5
    and 25 percent of documents that results in 1,529 features. Now we proceed to
    train a 15-topic model using 25 passes over the corpus. This takes a bit over
    two minutes on a 4-core i7.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，我们创建了一个包含出现在 0.5 到 25% 文档中的术语的 DTM，结果为 1,529 个特征。现在我们继续使用 25 个语料库训练 15
    个主题模型。在 4 核 i7 上，这需要两分钟多一点。
- en: The top 10 words per topic, as shown in *Figure 15.11*, identify several distinct
    themes that range from obvious financial information to clinical trials (Topic
    5), China and tariff issues (Topic 9), and technology issues (Topic 11).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 15.11* 所示，每个主题的前 10 个词识别出几个明显的主题，从明显的财务信息到临床试验（主题 5）、中国和关税问题（主题 9）以及技术问题（主题
    11）。
- en: '![](img/B15439_15_11.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_11.png)'
- en: 'Figure 15.11: Most important words for earnings call topics'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.11：收益电话主题中最重要的词语
- en: 'Using pyLDAvis'' relevance metric with a 0.6 weighting of unconditional frequency
    relative to lift, topic definitions become more intuitive, as illustrated in *Figure
    15.12* for Topic 7 about China and the trade wars:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pyLDAvis 的相关度指标，将无条件频率相对于提升的权重设置为 0.6，主题定义变得更加直观，如 *图 15.12* 所示，关于中国和贸易战的第
    7 个主题：
- en: '![](img/B15439_15_12.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_12.png)'
- en: 'Figure 15.12: pyLDAVis'' interactive topic explorer'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.12：pyLDAVis 的交互式主题探索器
- en: The notebook also illustrates how you can look up documents by their topic association.
    In this case, an analyst can review relevant statements for nuances, use sentiment
    analysis to further process the topic-specific text data, or assign labels derived
    from market prices.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '该笔记本还说明了如何根据主题关联查找文档。在这种情况下，分析师可以审查相关陈述以了解细微差别，使用情感分析进一步处理特定主题的文本数据，或者根据市场价格派生标签。  '
- en: Running experiments
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行实验
- en: To illustrate the impact of different parameter settings, we run a few hundred
    experiments for different DTM constraints and model parameters. More specifically,
    we let the `min_df` and `max_df` parameters range from 50-500 words and 10 to
    100 percent of documents, respectively, using alternatively binary and absolute
    counts. We then train LDA models with 3 to 50 topics, using 1 and 25 passes over
    the corpus.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明不同参数设置的影响，我们运行了几百个实验，针对不同的 DTM 约束和模型参数。更具体地，我们让 `min_df` 和 `max_df` 参数分别从
    50-500 个词和 10 到 100% 的文档变化，交替使用二进制和绝对计数。然后，我们使用 1 和 25 个语料库训练 LDA 模型，主题从 3 到 50
    个。
- en: The chart in *Figure 15.13* illustrates the results in terms of topic coherence
    (higher is better) and perplexity (lower is better). Coherence drops after 25-30
    topics, and perplexity similarly increases.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15.13* 中的图表以主题连贯性（较高为较好）和困惑度（较低为较好）的形式呈现了结果。连贯性在 25-30 个主题后下降，困惑度同样增加。'
- en: '![](img/B15439_15_13.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_13.png)'
- en: 'Figure 15.13: Impact of LDA hyperparameter settings on topic quality'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.13：LDA 超参数设置对主题质量的影响
- en: The notebook includes regression results that quantify the relationships between
    parameters and outcomes. We generally get better results using absolute counts
    and a smaller vocabulary.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中包含量化参数与结果之间关系的回归结果。我们通常使用绝对计数和较小的词汇量能获得更好的结果。
- en: Topic modeling for with financial news
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与财经新闻相关的主题建模
- en: The notebook `lda_financial_news` contains an example of LDA applied to a subset
    of over 306,000 financial news articles from the first five months of 2018\. The
    datasets have been posted on Kaggle, and the articles have been sourced from CNBC,
    Reuters, the Wall Street Journal, and more. The notebook contains download instructions.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本 `lda_financial_news` 包含了应用于 2018 年前五个月超过 306,000 篇财经新闻文章的 LDA 示例。这些数据集已经发布在
    Kaggle 上，文章来源于 CNBC、路透社、华尔街日报等。笔记本包含下载说明。
- en: We select the most relevant 120,000 articles based on their section titles with
    a total of 54 million tokens for an average word count of 429 words per article.
    To prepare the data for the LDA model, we rely on spaCy to remove numbers and
    punctuation and lemmatize the results.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据文章标题选择了最相关的 120,000 篇文章，共计 5400 万个标记，平均每篇文章 429 个单词。为了为 LDA 模型准备数据，我们依赖
    spaCy 来删除数字和标点，并对结果进行词形还原。
- en: '*Figure 15.14* highlights the remaining most frequent tokens and the article
    length distribution with a median length of 231 tokens; the 90th percentile is
    642 words.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15.14* 突出显示了剩余的最常见标记和文章长度分布，其中中位数长度为 231 个标记；第 90 百分位是 642 个单词。'
- en: '![](img/B15439_15_14.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_14.png)'
- en: 'Figure 15.14: Corpus statistics for financial news data'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.14：金融新闻数据的语料库统计
- en: In *Figure 15.15*, we show results for one model using a vocabulary of 3,570
    tokens based on `min_df`=0.005 and `max_df`=0.1, with a single pass to avoid the
    length training time for 15 topics. We can use the `top_topics` attribute of the
    trained `LdaModel` to obtain the most likely words for each topic (refer to the
    notebook for more details).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 15.15* 中，我们展示了一个使用 3,570 个标记的词汇表的模型的结果，基于 `min_df`=0.005 和 `max_df`=0.1，采用单次遍历以避免长时间训练
    15 个主题。我们可以使用训练后的 `LdaModel` 的 `top_topics` 属性来获取每个主题最可能的词（详细信息请参阅笔记本）。
- en: '![](img/B15439_15_15.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_15.png)'
- en: 'Figure 15.15: Top 15 words for financial news topics'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.15：金融新闻主题的前 15 个词
- en: The topics outline several issues relevant to the time period, including Brexit
    (Topic 8), North Korea (Topic 4), and Tesla (Topic 14).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这些主题概述了与时期相关的几个问题，包括 Brexit（主题 8）、朝鲜（主题 4）和特斯拉（主题 14）。
- en: Gensim provides a `LdaMultiCore` implementation that allows for parallel training
    using Python's multiprocessing module and improves performance by 50 percent when
    using four workers. More workers do not further reduce training time, though,
    due to I/O bottlenecks.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim 提供了 `LdaMultiCore` 实现，允许使用 Python 的多进程模块进行并行训练，并且在使用四个工作线程时性能提高了 50%。但是，由于
    I/O 瓶颈，使用更多工作线程并不会进一步减少训练时间。
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the use of topic modeling to gain insights into
    the content of a large collection of documents. We covered latent semantic indexing
    that uses dimensionality reduction of the DTM to project documents into a latent
    topic space. While effective in addressing the curse of dimensionality caused
    by high-dimensional word vectors, it does not capture much semantic information.
    Probabilistic models make explicit assumptions about the interplay of documents,
    topics, and words that allow algorithms to reverse engineer the document generation
    process and evaluate the model fit on new documents. We learned that LDA is capable
    of extracting plausible topics that allow us to gain a high-level understanding
    of large amounts of text in an automated way, while also identifying relevant
    documents in a targeted way.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用主题建模来深入了解大量文档内容的用途。我们涵盖了使用 DTM 的降维技术将文档投射到潜在主题空间中的潜在语义索引。虽然在解决由高维单词向量引起的维度灾难方面很有效，但它并不捕捉太多语义信息。概率模型对文档、主题和单词之间的相互作用做出了明确的假设，允许算法逆向工程文档生成过程，并在新文档上评估模型拟合度。我们了解到
    LDA 能够提取出合理的主题，让我们以自动化的方式对大量文本获得高层次的理解，同时以有针对性的方式识别相关文档。
- en: In the next chapter, we will learn how to train neural networks that embed individual
    words in a high-dimensional vector space that captures important semantic information
    and allows us to use the resulting word vectors as high-quality text features.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何训练神经网络，将单词嵌入到一个捕捉重要语义信息的高维向量空间中，并且可以使用生成的单词向量作为高质量的文本特征。
