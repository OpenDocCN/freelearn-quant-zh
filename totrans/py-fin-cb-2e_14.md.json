["```py\n    from sklearn.ensemble import (RandomForestClassifier,\n                                  GradientBoostingClassifier)\n    from xgboost.sklearn import XGBClassifier\n    from lightgbm import LGBMClassifier\n    from chapter_14_utils import performance_evaluation_report \n    ```", "```py\n    rf = RandomForestClassifier(random_state=42)\n    rf_pipeline = Pipeline(\n        steps=[(\"preprocessor\", preprocessor),\n               (\"classifier\", rf)]\n    )\n    rf_pipeline.fit(X_train, y_train)\n    rf_perf = performance_evaluation_report(rf_pipeline, X_test,\n                                            y_test, labels=LABELS,\n                                            show_plot=True,\n                                            show_pr_curve=True) \n    ```", "```py\n    gbt = GradientBoostingClassifier(random_state=42)\n    gbt_pipeline = Pipeline(\n        steps=[(\"preprocessor\", preprocessor),\n               (\"classifier\", gbt)]\n    )\n    gbt_pipeline.fit(X_train, y_train)\n    gbt_perf = performance_evaluation_report(gbt_pipeline, X_test,\n                                             y_test, labels=LABELS,\n                                             show_plot=True,\n                                             show_pr_curve=True) \n    ```", "```py\n    xgb = XGBClassifier(random_state=42)\n    xgb_pipeline = Pipeline(\n        steps=[(\"preprocessor\", preprocessor),\n               (\"classifier\", xgb)]\n    )\n    xgb_pipeline.fit(X_train, y_train)\n    xgb_perf = performance_evaluation_report(xgb_pipeline, X_test,\n                                             y_test, labels=LABELS,\n                                             show_plot=True,\n                                             show_pr_curve=True) \n    ```", "```py\n    lgbm = LGBMClassifier(random_state=42)\n    lgbm_pipeline = Pipeline(\n        steps=[(\"preprocessor\", preprocessor),\n               (\"classifier\", lgbm)]\n    )\n    lgbm_pipeline.fit(X_train, y_train)\n    lgbm_perf = performance_evaluation_report(lgbm_pipeline, X_test,\n                                              y_test, labels=LABELS,\n                                              show_plot=True,\n                                              show_pr_curve=True) \n    ```", "```py\n    import category_encoders as ce\n    from sklearn.base import clone \n    ```", "```py\n    pipeline_target_enc = clone(rf_pipeline)\n    pipeline_target_enc.set_params(\n        preprocessor__categorical__cat_encoding=ce.TargetEncoder()\n    )\n    pipeline_target_enc.fit(X_train, y_train)\n    target_enc_perf = performance_evaluation_report(\n        pipeline_target_enc, X_test,\n        y_test, labels=LABELS,\n        show_plot=True,\n        show_pr_curve=True\n    )\n    print(f\"Recall: {target_enc_perf['recall']:.4f}\") \n    ```", "```py\n    pipeline_loo_enc = clone(rf_pipeline)\n    pipeline_loo_enc.set_params(\n       preprocessor__categorical__cat_encoding=ce.LeaveOneOutEncoder()\n    )\n    pipeline_loo_enc.fit(X_train, y_train)\n    loo_enc_perf = performance_evaluation_report(\n        pipeline_loo_enc, X_test,\n        y_test, labels=LABELS,\n        show_plot=True,\n        show_pr_curve=True\n    )\n    print(f\"Recall: {loo_enc_perf['recall']:.4f}\") \n    ```", "```py\n    pipeline_woe_enc = clone(rf_pipeline)\n    pipeline_woe_enc.set_params(\n        preprocessor__categorical__cat_encoding=ce.WOEEncoder()\n    )\n    pipeline_woe_enc.fit(X_train, y_train)\n    woe_enc_perf = performance_evaluation_report(\n        pipeline_woe_enc, X_test,\n        y_test, labels=LABELS,\n        show_plot=True,\n        show_pr_curve=True\n    )\n    print(f\"Recall: {woe_enc_perf['recall']:.4f}\") \n    ```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.preprocessing import RobustScaler\n    from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n    from imblearn.under_sampling import RandomUnderSampler\n    from imblearn.ensemble import BalancedRandomForestClassifier\n    from chapter_14_utils import performance_evaluation_report \n    ```", "```py\n    RANDOM_STATE = 42\n    df = pd.read_csv(\"../Datasets/credit_card_fraud.csv\")\n    X = df.copy().drop(columns=[\"Time\"])\n    y = X.pop(\"Class\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y,\n        test_size=0.2,\n        stratify=y,\n        random_state=RANDOM_STATE\n    ) \n    ```", "```py\n    robust_scaler = RobustScaler()\n    X_train = robust_scaler.fit_transform(X_train)\n    X_test = robust_scaler.transform(X_test) \n    ```", "```py\n    rf = RandomForestClassifier(\n        random_state=RANDOM_STATE, n_jobs=-1\n    )\n    rf.fit(X_train, y_train) \n    ```", "```py\n    rus = RandomUnderSampler(random_state=RANDOM_STATE)\n    X_rus, y_rus = rus.fit_resample(X_train, y_train)\n    rf.fit(X_rus, y_rus)\n    rf_rus_perf = performance_evaluation_report(rf, X_test, y_test) \n    ```", "```py\n    ros = RandomOverSampler(random_state=RANDOM_STATE)\n    X_ros, y_ros = ros.fit_resample(X_train, y_train)\n    rf.fit(X_ros, y_ros)\n    rf_ros_perf = performance_evaluation_report(rf, X_test, y_test) \n    ```", "```py\n    smote = SMOTE(random_state=RANDOM_STATE)\n    X_smote, y_smote = smote.fit_resample(X_train, y_train)\n    rf.fit(X_smote, y_smote)\n    rf_smote_perf = peformance_evaluation_report(\n        rf, X_test, y_test, \n    ) \n    ```", "```py\n    adasyn = ADASYN(random_state=RANDOM_STATE)\n    X_adasyn, y_adasyn = adasyn.fit_resample(X_train, y_train)\n    rf.fit(X_adasyn, y_adasyn)\n    rf_adasyn_perf = performance_evaluation_report(\n        rf, X_test, y_test, \n    ) \n    ```", "```py\n    rf_cw = RandomForestClassifier(random_state=RANDOM_STATE,\n                                   class_weight=\"balanced\",\n                                   n_jobs=-1)\n    rf_cw.fit(X_train, y_train)\n    rf_cw_perf = performance_evaluation_report(\n        rf_cw, X_test, y_test, \n    ) \n    ```", "```py\n    balanced_rf = BalancedRandomForestClassifier(\n        random_state=RANDOM_STATE\n    )\n    balanced_rf.fit(X_train, y_train)\n    balanced_rf_perf = performance_evaluation_report(\n        balanced_rf, X_test, y_test,\n    ) \n    ```", "```py\n    balanced_rf_cw = BalancedRandomForestClassifier(\n        random_state=RANDOM_STATE,\n        class_weight=\"balanced\",\n        n_jobs=-1\n    )\n    balanced_rf_cw.fit(X_train, y_train)\n    balanced_rf_cw_perf = performance_evaluation_report(\n        balanced_rf_cw, X_test, y_test,\n    ) \n    ```", "```py\n    performance_results = {\n        \"random_forest\": rf_perf,\n        \"undersampled rf\": rf_rus_perf,\n        \"oversampled_rf\": rf_ros_perf,\n        \"smote\": rf_smote_perf,\n        \"adasyn\": rf_adasyn_perf,\n        \"random_forest_cw\": rf_cw_perf,\n        \"balanced_random_forest\": balanced_rf_perf,\n        \"balanced_random_forest_cw\": balanced_rf_cw_perf,\n    }                       \n    pd.DataFrame(performance_results).round(4).T \n    ```", "```py\n    import pandas as pd\n    from sklearn.model_selection import (train_test_split,\n                                         StratifiedKFold)\n    from sklearn.metrics import recall_score\n    from sklearn.preprocessing import RobustScaler\n    from sklearn.svm import SVC\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import StackingClassifier \n    ```", "```py\n    RANDOM_STATE = 42\n    df = pd.read_csv(\"../Datasets/credit_card_fraud.csv\")\n    X = df.copy().drop(columns=[\"Time\"])\n    y = X.pop(\"Class\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y,\n        test_size=0.2,\n        stratify=y,\n        random_state=RANDOM_STATE\n    )\n    robust_scaler = RobustScaler()\n    X_train = robust_scaler.fit_transform(X_train)\n    X_test = robust_scaler.transform(X_test) \n    ```", "```py\n    base_models = [\n        (\"dec_tree\", DecisionTreeClassifier()),\n        (\"log_reg\", LogisticRegression()),\n        (\"svc\", SVC()),   \n        (\"naive_bayes\", GaussianNB())\n    ] \n    ```", "```py\n    for model_tuple in base_models:\n        clf = model_tuple[1]\n        if \"n_jobs\" in clf.get_params().keys():\n            clf.set_params(n_jobs=-1)\n        clf.fit(X_train, y_train)\n        recall = recall_score(y_test, clf.predict(X_test))\n        print(f\"{model_tuple[0]}'s recall score: {recall:.4f}\") \n    ```", "```py\n    dec_tree's recall score: 0.7551\n    log_reg's recall score: 0.6531\n    svc's recall score: 0.7041\n    naive_bayes's recall score: 0.8469 \n    ```", "```py\n    cv_scheme = StratifiedKFold(n_splits=5,\n                                shuffle=True,\n                                random_state=RANDOM_STATE)\n    meta_model = LogisticRegression(random_state=RANDOM_STATE)\n    stack_clf = StackingClassifier(\n        base_models,\n        final_estimator=meta_model,\n        cv=cv_scheme,\n        n_jobs=-1\n    )\n    stack_clf.fit(X_train, y_train)\n    recall = recall_score(y_test, stack_clf.predict(X_test))\n    print(f\"The stacked ensemble's recall score: {recall:.4f}\") \n    ```", "```py\n    The stacked ensemble's recall score: 0.7449 \n    ```", "```py\n    meta_model = RandomForestClassifier(random_state=RANDOM_STATE)\n    stack_clf = StackingClassifier(\n        base_models,\n        final_estimator=meta_model,\n        cv=cv_scheme,\n        passthrough=True,\n        n_jobs=-1\n    )\n    stack_clf.fit(X_train, y_train) \n    ```", "```py\nlevel_0_names = [f\"{model[0]}_pred\" for model in base_models]\nlevel_0_df = pd.DataFrame(\n    stack_clf.transform(X_train),\n    columns=level_0_names + list(X.columns)\n)\nlevel_0_df.head() \n```", "```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.model_selection import (cross_val_score,\n                                         StratifiedKFold)\n    from lightgbm import LGBMClassifier\n    from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, space_eval\n    from hyperopt.pyll import scope\n    from hyperopt.pyll.stochastic import sample\n    from chapter_14_utils import performance_evaluation_report \n    ```", "```py\n    N_FOLDS = 5\n    MAX_EVALS = 200\n    RANDOM_STATE = 42\n    EVAL_METRIC = \"recall\" \n    ```", "```py\n    df = pd.read_csv(\"../Datasets/credit_card_fraud.csv\")\n    X = df.copy().drop(columns=[\"Time\"])\n    y = X.pop(\"Class\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y,\n        test_size=0.2,\n        stratify=y,\n        random_state=RANDOM_STATE\n    ) \n    ```", "```py\n    clf = LGBMClassifier(random_state=RANDOM_STATE)\n    clf.fit(X_train, y_train)\n    benchmark_perf = performance_evaluation_report(\n        clf, X_test, y_test, \n        show_plot=True, \n        show_pr_curve=True\n    )\n    print(f'Recall: {benchmark_perf[\"recall\"]:.4f}') \n    ```", "```py\n    def objective(params, n_folds=N_FOLDS, \n     random_state=RANDOM_STATE, \n     metric=EVAL_METRIC):\n\n        model = LGBMClassifier(**params, random_state=random_state)\n        k_fold = StratifiedKFold(n_folds, shuffle=True,\n                                 random_state=random_state)\n        scores = cross_val_score(model, X_train, y_train,\n                                 cv=k_fold, scoring=metric)\n        loss = -1 * scores.mean()\n\n        return {\"loss\": loss, \"params\": params, \"status\": STATUS_OK} \n    ```", "```py\n    search_space = {\n        \"n_estimators\": hp.choice(\"n_estimators\", [50, 100, 250, 500]),\n        \"boosting_type\": hp.choice(\n            \"boosting_type\", [\"gbdt\", \"dart\", \"goss\"]\n        ),\n        \"is_unbalance\": hp.choice(\"is_unbalance\", [True, False]),\n        \"max_depth\": scope.int(hp.uniform(\"max_depth\", 3, 20)),\n        \"num_leaves\": scope.int(hp.quniform(\"num_leaves\", 5, 100, 1)),\n        \"min_child_samples\": scope.int(\n            hp.quniform(\"min_child_samples\", 20, 500, 5)\n        ),\n        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.3, 1.0),\n        \"learning_rate\": hp.loguniform(\n            \"learning_rate\", np.log(0.01), np.log(0.5)\n        ),\n        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 1.0),\n        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n    } \n    ```", "```py\n    sample(search_space) \n    ```", "```py\n    {'boosting_type': 'gbdt',\n     'colsample_bytree': 0.5718346953027432,\n     'is_unbalance': False,\n     'learning_rate': 0.44862566076557925,\n     'max_depth': 3,\n     'min_child_samples': 75,\n     'n_estimators': 250,\n     'num_leaves': 96,\n     'reg_alpha': 0.31830737977056545,\n     'reg_lambda': 0.637449220342909} \n    ```", "```py\n    trials = Trials()\n    best_set = fmin(fn=objective,\n                    space=search_space,\n                    algo=tpe.suggest,\n                    max_evals=MAX_EVALS,\n                    trials=trials,\n                    rstate=np.random.default_rng(RANDOM_STATE)) \n    ```", "```py\n    space_eval(search_space , best_set) \n    ```", "```py\n    {'boosting_type': 'dart',\n     'colsample_bytree': 0.8764301395665521,\n     'is_unbalance': True,\n     'learning_rate': 0.019245717855584647,\n     'max_depth': 19,\n     'min_child_samples': 160,\n     'n_estimators': 50,\n     'num_leaves': 16,\n     'reg_alpha': 0.3902317904740905,\n     'reg_lambda': 0.48349252432635764} \n    ```", "```py\n    tuned_lgbm = LGBMClassifier(\n        **space_eval(search_space, best_set),\n        random_state=RANDOM_STATE\n    )\n    tuned_lgbm.fit(X_train, y_train) \n    ```", "```py\n    tuned_perf = performance_evaluation_report(\n        tuned_lgbm, X_test, y_test, \n        show_plot=True, \n        show_pr_curve=True\n    )\n    print(f'Recall: {tuned_perf[\"recall\"]:.4f}') \n    ```", "```py\nconditional_search_space = {\n    \"boosting_type\": hp.choice(\"boosting_type\", [\n        {\"boosting_type\": \"gbdt\",\n         \"subsample\": hp.uniform(\"gdbt_subsample\", 0.5, 1),\n         \"subsample_freq\": scope.int(\n            hp.uniform(\"gdbt_subsample_freq\", 1, 20)\n         )},\n        {\"boosting_type\": \"dart\",\n         \"subsample\": hp.uniform(\"dart_subsample\", 0.5, 1),\n         \"subsample_freq\": scope.int(\n            hp.uniform(\"dart_subsample_freq\", 1, 20)\n         )},\n        {\"boosting_type\": \"goss\",\n         \"subsample\": 1.0,\n         \"subsample_freq\": 0},\n    ]),\n    \"n_estimators\": hp.choice(\"n_estimators\", [50, 100, 250, 500]),\n} \n```", "```py\n{'boosting_type': {'boosting_type': 'dart',\n  'subsample': 0.9301284507624732,\n  'subsample_freq': 17},\n 'n_estimators': 250} \n```", "```py\n# draw from the search space\nparams = sample(conditional_search_space)\n# retrieve the conditional parameters, set to default if missing\nsubsample = params[\"boosting_type\"].get(\"subsample\", 1.0)\nsubsample_freq = params[\"boosting_type\"].get(\"subsample_freq\", 0)\n# fill in the params dict with the conditional values\nparams[\"boosting_type\"] = params[\"boosting_type\"][\"boosting_type\"]\nparams[\"subsample\"] = subsample\nparams[\"subsample_freq\"] = subsample_freq\nparams \n```", "```py\n{'boosting_type': 'dart',\n 'n_estimators': 250\n 'subsample': 0.9301284507624732,\n 'subsample_freq': 17} \n```", "```py\nhp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.5)) \n```", "```py\nscope.int(hp.quniform(\"min_child_samples\", 20, 500, 5)) \n```", "```py\nfrom pandas.io.json import json_normalize\nresults_df = pd.DataFrame(trials.results)\nparams_df = json_normalize(results_df[\"params\"])\nresults_df = pd.concat([results_df.drop(\"params\", axis=1), params_df],\n                       axis=1)\nresults_df[\"iteration\"] = np.arange(len(results_df)) + 1\nresults_df.sort_values(\"loss\") \n```", "```py\n    import numpy as np\n    import pandas as pd\n    from sklearn.inspection import permutation_importance\n    from sklearn.metrics import recall_score\n    from sklearn.base import clone \n    ```", "```py\n    rf_classifier = rf_pipeline.named_steps[\"classifier\"]\n    preprocessor = rf_pipeline.named_steps[\"preprocessor\"] \n    ```", "```py\n    feat_names = list(preprocessor.get_feature_names_out())\n    X_train_preprocessed = pd.DataFrame(\n        preprocessor.transform(X_train),\n        columns=feat_names\n    )\n    X_test_preprocessed = pd.DataFrame(\n        preprocessor.transform(X_test),\n        columns=feat_names\n    ) \n    ```", "```py\n    rf_feat_imp = pd.DataFrame(rf_classifier.feature_importances_,\n                               index=feat_names,\n                               columns=[\"mdi\"])\n    rf_feat_imp[\"mdi_cumul\"] = np.cumsum(\n        rf_feat_imp\n        .sort_values(\"mdi\", ascending=False)\n        .loc[:, \"mdi\"]\n    ).loc[feat_names] \n    ```", "```py\n    def plot_most_important_features(feat_imp, title, \n     n_features=10, \n     bottom=False):   \n        if bottom:\n            indicator = \"Bottom\"\n            feat_imp = feat_imp.sort_values(ascending=True)\n        else:\n            indicator = \"Top\"\n            feat_imp = feat_imp.sort_values(ascending=False)\n\n        ax = feat_imp.head(n_features).plot.barh()\n        ax.invert_yaxis()\n        ax.set(title=f\"{title} ({indicator}  {n_features})\",\n               xlabel=\"Importance\",\n               ylabel=\"Feature\")\n\n        return ax \n    ```", "```py\n    plot_most_important_features(rf_feat_imp[\"mdi\"],\n                                 title=\"MDI Importance\") \n    ```", "```py\n    x_values = range(len(feat_names))\n    fig, ax = plt.subplots()\n    ax.plot(x_values, rf_feat_imp[\"mdi_cumul\"].sort_values(), \"b-\")\n    ax.hlines(y=0.95, xmin=0, xmax=len(x_values),\n              color=\"g\", linestyles=\"dashed\")\n    ax.set(title=\"Cumulative MDI Importance\",\n           xlabel=\"# Features\",\n           ylabel=\"Importance\") \n    ```", "```py\n    perm_result_train = permutation_importance(\n        rf_classifier, X_train_preprocessed, y_train,\n        n_repeats=25, scoring=\"recall\",\n        random_state=42, n_jobs=-1\n    )\n    rf_feat_imp[\"perm_imp_train\"] = (\n        perm_result_train[\"importances_mean\"]\n    )\n    plot_most_important_features(\n        rf_feat_imp[\"perm_imp_train\"],\n        title=\"Permutation importance - training set\"\n    ) \n    ```", "```py\n    perm_result_test = permutation_importance(\n        rf_classifier, X_test_preprocessed, y_test,\n        n_repeats=25, scoring=\"recall\",\n        random_state=42, n_jobs=-1\n    )\n    rf_feat_imp[\"perm_imp_test\"] = (\n        perm_result_test[\"importances_mean\"]\n    )\n    plot_most_important_features(\n        rf_feat_imp[\"perm_imp_test\"],\n        title=\"Permutation importance - test set\"\n    ) \n    ```", "```py\n    def drop_col_feat_imp(model, X, y, metric, random_state=42):\n        model_clone = clone(model)\n        model_clone.random_state = random_state\n        model_clone.fit(X, y)\n        benchmark_score = metric(y, model_clone.predict(X))\n\n        importances = []\n\n        for ind, col in enumerate(X.columns):\n            print(f\"Dropping {col} ({ind+1}/{len(X.columns)})\")\n            model_clone = clone(model)\n            model_clone.random_state = random_state\n            model_clone.fit(X.drop(col, axis=1), y)\n            drop_col_score = metric(\n                y, model_clone.predict(X.drop(col, axis=1))\n            )\n            importances.append(benchmark_score - drop_col_score)\n\n        return importances \n    ```", "```py\n    rf_feat_imp[\"drop_column_imp\"] = drop_col_feat_imp(\n        rf_classifier.set_params(**{\"n_jobs\": -1}),\n        X_train_preprocessed,\n        y_train,\n        metric=recall_score,\n        random_state=42\n    ) \n    ```", "```py\n    plot_most_important_features(\n        rf_feat_imp[\"drop_column_imp\"], \n        title=\"Drop column importance\"\n    ) \n    ```", "```py\n    plot_most_important_features(\n        rf_feat_imp[\"drop_column_imp\"], \n        title=\"Drop column importance\", \n        bottom=True\n    ) \n    ```", "```py\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import recall_score\n    from sklearn.feature_selection import (RFE, RFECV, \n                                           SelectKBest, \n                                           SelectFromModel, \n                                           mutual_info_classif)\n    from sklearn.model_selection import StratifiedKFold \n    ```", "```py\n    rf = RandomForestClassifier(random_state=RANDOM_STATE,\n                                n_jobs=-1)\n    rf.fit(X_train, y_train)\n    recall_train = recall_score(y_train, rf.predict(X_train))\n    recall_test = recall_score(y_test, rf.predict(X_test))\n    print(f\"Recall score training: {recall_train:.4f}\")\n    print(f\"Recall score test: {recall_test:.4f}\") \n    ```", "```py\n    Recall score training: 1.0000\n    Recall score test: 0.8265 \n    ```", "```py\n    scores = []\n    n_features_list = list(range(2, len(X_train.columns)+1))\n    for n_feat in n_features_list:\n        print(f\"Keeping {n_feat} most important features\")\n        mi_selector = SelectKBest(mutual_info_classif, k=n_feat)\n        X_train_new = mi_selector.fit_transform(X_train, y_train)\n        X_test_new = mi_selector.transform(X_test)\n        rf.fit(X_train_new, y_train)\n        recall_scores = [\n            recall_score(y_train, rf.predict(X_train_new)),\n            recall_score(y_test, rf.predict(X_test_new))\n        ]\n        scores.append(recall_scores)\n    mi_scores_df = pd.DataFrame(\n        scores,\n        columns=[\"train_score\", \"test_score\"],\n        index=n_features_list\n    ) \n    ```", "```py\n    (\n        mi_scores_df[\"test_score\"]\n        .plot(kind=\"bar\",\n              title=\"Feature selection using Mutual Information\",\n              xlabel=\"# of features\",\n              ylabel=\"Recall (test set)\")\n    ) \n    ```", "```py\n    mi_selector = SelectKBest(mutual_info_classif, k=8)\n    mi_selector.fit(X_train, y_train)\n    print(f\"Most importance features according to MI: {mi_selector.get_feature_names_out()}\") \n    ```", "```py\n    Most importance features according to MI: ['V3' 'V4' 'V10' 'V11' 'V12' 'V14' 'V16' 'V17'] \n    ```", "```py\n    rf_selector = SelectFromModel(rf)\n    rf_selector.fit(X_train, y_train)\n    mdi_features = X_train.columns[rf_selector.get_support()]\n    rf.fit(X_train[mdi_features], y_train)\n    recall_train = recall_score(\n        y_train, rf.predict(X_train[mdi_features])\n    )\n    recall_test = recall_score(y_test, rf.predict(X_test[mdi_features]))\n    print(f\"Recall score training: {recall_train:.4f}\")\n    print(f\"Recall score test: {recall_test:.4f}\") \n    ```", "```py\n    Recall score training: 1.0000\n    Recall score test: 0.8367 \n    ```", "```py\n    print(f\"MDI importance threshold: {rf_selector.threshold_:.4f}\")\n    print(f\"Most importance features according to MI: {rf_selector.get_feature_names_out()}\") \n    ```", "```py\n    MDI importance threshold: 0.0345\n    Most importance features according to MDI: ['V10' 'V11' 'V12' 'V14' 'V16' 'V17'] \n    ```", "```py\n    rfe = RFE(estimator=rf, n_features_to_select=10, verbose=1)\n    rfe.fit(X_train, y_train) \n    ```", "```py\n    Most importance features according to RFE: ['V4' 'V7' 'V9' 'V10' 'V11' 'V12' 'V14' 'V16' 'V17' 'V26']\n    Recall score training: 1.0000\n    Recall score test: 0.8367 \n    ```", "```py\n    k_fold = StratifiedKFold(5, shuffle=True, random_state=42)\n    rfe_cv = RFECV(estimator=rf, step=1,\n                   cv=k_fold,\n                   min_features_to_select=5,\n                   scoring=\"recall\",\n                   verbose=1, n_jobs=-1)\n    rfe_cv.fit(X_train, y_train) \n    ```", "```py\n    Most importance features according to RFECV: ['V1' 'V4' 'V6' 'V7' 'V9' 'V10' 'V11' 'V12' 'V14' 'V15' 'V16' 'V17' 'V18'\n     'V20' 'V21' 'V26']\n    Recall score training: 1.0000\n    Recall score test: 0.8265 \n    ```", "```py\n    cv_results_df = pd.DataFrame(rfe_cv.cv_results_)\n    cv_results_df.index += 5\n    (\n        cv_results_df[\"mean_test_score\"]\n        .plot(title=\"Average CV score over iterations\",\n              xlabel=\"# of features retained\",\n              ylabel=\"Avg. recall\")\n    ) \n    ```", "```py\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\npipeline = Pipeline(\n   [\n    (\"selector\", SelectKBest(mutual_info_classif)),\n    (\"model\", rf)\n   ]\n)\nparam_grid = {\n   \"selector__k\": [5, 10, 20, 29],\n   \"model__n_estimators\": [10, 50, 100, 200]  \n}\ngs = GridSearchCV(\n   estimator=pipeline,\n   param_grid=param_grid,\n   n_jobs=-1,\n   scoring=\"recall\",\n   cv=k_fold,\n   verbose=1\n)\ngs.fit(X_train, y_train)\nprint(f\"Best hyperparameters: {gs.best_params_}\") \n```", "```py\nBest hyperparameters: {'model__n_estimators': 50, 'selector__k': 20} \n```", "```py\n    from xgboost import XGBClassifier\n    from sklearn.metrics import recall_score\n    from sklearn.inspection import (partial_dependence,\n                                    PartialDependenceDisplay)\n    import shap \n    ```", "```py\n    xgb = XGBClassifier(random_state=RANDOM_STATE,\n                        n_jobs=-1)\n    xgb.fit(X_train, y_train)\n    recall_train = recall_score(y_train, xgb.predict(X_train))\n    recall_test = recall_score(y_test, xgb.predict(X_test))\n    print(f\"Recall score training: {recall_train:.4f}\")\n    print(f\"Recall score test: {recall_test:.4f}\") \n    ```", "```py\n    Recall score training: 1.0000\n    Recall score test: 0.8163 \n    ```", "```py\n    PartialDependenceDisplay.from_estimator(\n        xgb, X_train, features=[\"V4\"], \n        kind=\"individual\", \n        subsample=5000, \n        line_kw={\"linewidth\": 2},\n        random_state=RANDOM_STATE\n    ) \n    plt.title(\"ICE curves of V4\") \n    ```", "```py\n    PartialDependenceDisplay.from_estimator(\n        xgb, X_train, features=[\"V4\"], \n        kind=\"individual\", \n        subsample=5000,\n        centered=True,\n        line_kw={\"linewidth\": 2},\n        random_state=RANDOM_STATE\n    )\n    plt.title(\"Centered ICE curves of V4\") \n    ```", "```py\n    PartialDependenceDisplay.from_estimator(\n        xgb, X_train, \n        features=[\"V4\"], \n        random_state=RANDOM_STATE\n    )\n    plt.title(\"Partial Dependence Plot of V4\") \n    ```", "```py\n    PartialDependenceDisplay.from_estimator(\n        xgb, X_train, features=[\"V4\"], \n        kind=\"both\", \n        subsample=5000, \n        ice_lines_kw={\"linewidth\": 2},\n        pd_line_kw={\"color\": \"red\"},\n        random_state=RANDOM_STATE\n    ) \n    plt.title(\"Partial Dependence Plot of V4, together with ICE curves\") \n    ```", "```py\n    fig, ax = plt.subplots(figsize=(20, 8))\n    PartialDependenceDisplay.from_estimator(\n        xgb,\n        X_train.sample(20000, random_state=RANDOM_STATE),\n        features=[\"V4\", \"V8\", (\"V4\", \"V8\")],\n        centered=True,\n        ax=ax\n    )\n    ax.set_title(\"Centered Partial Dependence Plots of V4 and V8\") \n    ```", "```py\n    explainer = shap.TreeExplainer(xgb)\n    shap_values = explainer.shap_values(X)\n    explainer_x = explainer(X) \n    ```", "```py\n    shap.summary_plot(shap_values, X) \n    ```", "```py\n    shap.summary_plot(shap_values, X, plot_type=\"bar\") \n    ```", "```py\n    negative_ind = y[y == 0].index[0]\n    positive_ind = y[y == 1].index[0] \n    ```", "```py\n    shap.force_plot(\n        explainer.expected_value,\n        shap_values[negative_ind, :],\n        X.iloc[negative_ind, :]\n    ) \n    ```", "```py\n    shap.force_plot(\n        explainer.expected_value,\n        shap_values[positive_ind, :],\n        X.iloc[positive_ind, :]\n    ) \n    ```", "```py\n    shap.plots.waterfall(explainer(X)[positive_ind]) \n    ```", "```py\n    shap.dependence_plot(\"V4\", shap_values, X) \n    ```"]