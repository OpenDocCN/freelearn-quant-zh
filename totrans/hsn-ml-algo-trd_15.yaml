- en: Word Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the two previous chapters, we applied the bag-of-words model to convert text
    data into a numerical format. The results were sparse, fixed-length vectors that
    represent documents in a high-dimensional word space. This allows evaluating the
    similarity of documents and creates features to train a machine learning algorithm
    and classify a document's content or rate the sentiment expressed in it. However,
    these vectors ignore the context in which a term is used so that, for example,
    a different sentence containing the same words would be encoded by the same vector.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce an alternative class of algorithms that use
    neural networks to learn a vector representation of individual semantic units such
    as a word or a paragraph. These vectors are dense rather than sparse, and have
    a few hundred real-valued rather than tens of thousands of binary or discrete
    entries. They are called **embeddings** because they assign each semantic unit
    a location in a continuous vector space.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings result from training a model to relate tokens to their context with
    the benefit that similar usage implies a similar vector. Moreover, we will see
    how the embeddings encode semantic aspects, such as relationships among words
    by means of their relative location. As a result, they are powerful features for
    use in the deep learning models that we will introduce in the following chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, in this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: What word embeddings are and how they work and capture semantic information
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use trained word vectors
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which network architectures are useful to train Word2vec models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train a Word2vec model using Keras, gensim, and TensorFlow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to visualize and evaluate the quality of word vectors
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train a Word2vec model using SEC filings
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How `Doc2vec` extends Word2vec
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How word embeddings encode semantics
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bag-of-words model represents documents as vectors that reflect the tokens
    they contain. Word embeddings represent tokens as lower dimensional vectors so
    that their relative location reflects their relationship in terms of how they
    are used in context. They embody the distributional hypothesis from linguistics
    that claims words are best defined by the company they keep.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Word vectors are capable of capturing numerous semantic aspects; not only are
    synonyms close to each other, but words can have multiple degrees of similarity,
    for example, the word driver could be similar to motorist or to cause. Furthermore,
    embeddings reflect relationships among pairs of words such as analogies (Tokyo
    is to Japan what Paris is to France, or went is to go what saw is to see) as we
    will illustrate later in this section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings result from training a machine learning model to predict words from
    their context or vice versa. In the following section, we will introduce how these
    neural language models work and present successful approaches including Word2vec,
    `Doc2vec`, and fastText.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: How neural language models learn usage in context
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经语言模型如何学习上下文中的用法
- en: Word embeddings result from training a shallow neural network to predict a word
    given its context. Whereas traditional language models define context as the words
    preceding the target, word-embedding models use the words contained in a symmetric
    window surrounding the target. In contrast, the bag-of-words model uses the entirety
    of documents as context and uses (weighted) counts to capture the cooccurrence
    of words rather than predictive vectors.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是通过训练一个浅层神经网络来预测给定上下文的单词而产生的。传统语言模型将上下文定义为目标词之前的单词，而单词嵌入模型使用包围目标的对称窗口中包含的单词。相比之下，词袋模型使用整个文档作为上下文，并使用（加权）计数来捕捉单词的共现关系，而不是预测向量。
- en: Earlier neural language models that were used included nonlinear hidden layers
    that increased the computational complexity. Word2vec and its extensions simplified
    the architecture to enable training on large datasets (Wikipedia, for example,
    contains over two billion tokens; see [Chapter 17](https://www.packtpub.com/sites/default/files/downloads/Deep_Learning.pdf),
    *Deep Learning*, for additional details on feed-forward networks).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 早期使用的神经语言模型包括增加了计算复杂性的非线性隐藏层。Word2vec 及其扩展简化了体系结构，以便在大型数据集上进行训练（例如，维基百科包含超过二十亿个标记；有关前馈网络的详细信息，请参阅[第
    17 章](https://www.packtpub.com/sites/default/files/downloads/Deep_Learning.pdf)
    *深度学习*）。
- en: The Word2vec model – learn embeddings at scale
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2vec 模型 – 大规模学习嵌入
- en: 'A Word2vec model is a two-layer neural net that takes a text corpus as input
    and outputs a set of embedding vectors for words in that corpus. There are two
    different architectures to learn word vectors efficiently using shallow neural
    networks depicted in the following figure:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 模型是一个将文本语料库作为输入并输出该语料库中单词的一组嵌入向量的两层神经网络。有两种不同的架构可以使用浅层神经网络有效地学习单词向量，如下图所示：
- en: The **Continuous-Bag-Of-Words** (**CBOW**) model predicts the target word using
    the average of the context word vectors as input so that their order does not
    matter. A CBOW model trains faster and tends to be slightly more accurate for
    frequent terms, but pays less attention to infrequent words.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Continuous-Bag-Of-Words**（**CBOW**）模型使用上下文词向量的平均值作为输入来预测目标词，因此它们的顺序并不重要。CBOW
    模型训练速度更快，对于频繁词汇来说稍微准确一些，但对不经常出现的单词关注较少。'
- en: 'The **Skip-Gram** (**SG**) model, by contrast, uses the target word to predict
    words sampled from the context. It works well with small datasets and finds good
    representations even for rare words or phrases:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Skip-Gram**（**SG**）模型与之相反，使用目标词来预测从上下文中抽样的词。它在小数据集上效果很好，并且即使对于罕见的单词或短语也能找到良好的表示：'
- en: '![](img/86b7aa9d-1435-458d-b352-da6d4097cea6.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86b7aa9d-1435-458d-b352-da6d4097cea6.png)'
- en: Hence, the Word2vec model receives an embedding vector as input and computes
    the dot product with another embedding vector. Note that, assuming normed vectors,
    the dot product is maximized (in absolute terms) when vectors are equal, and minimized
    when they are orthogonal.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Word2vec 模型接收一个嵌入向量作为输入，并与另一个嵌入向量计算点积。请注意，假设归一化向量，当向量相等时，点积被最大化（绝对值），当它们正交时被最小化。
- en: It then uses backpropagation to adjust the embedding weights in response to
    the loss computed by an objective function due to any classification errors. We
    will see in the next section how Word2vec computes the loss.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用反向传播来根据由任何分类错误造成的目标函数计算的损失来调整嵌入权重。在下一节中，我们将看到 Word2vec 如何计算损失。
- en: Training proceeds by sliding the context window over the documents, typically
    segmented into sentences. Each complete iteration over the corpus is called an
    **epoch**. Depending on the data, several dozen epochs may be necessary for vector
    quality to converge.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 训练通过将上下文窗口滑过文档进行，通常被分成句子。每次完整迭代整个语料库都被称为一个**时代**。根据数据，可能需要几十个时代才能使向量质量收敛。
- en: Technically, the SG model has been shown to factorize a word-context matrix
    that contains the pointwise mutual information of the respective word and context
    pairs implicitly (see references on GitHub).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，SG 模型已被证明可以因式分解包含相应单词和上下文对的点互信息的单词-上下文矩阵（请参阅 GitHub 上的参考文献）。
- en: Model objective – simplifying the softmax
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型目标 – 简化 softmax
- en: 'Word2vec models aim to predict a single word out of the potentially very large
    vocabulary. Neural networks often use the softmax function that maps any number
    of real values to an equal number of probabilities to implement the corresponding
    multiclass objective, where *h* refers to the embedding and *v* to the input vectors,
    and *c* is the context of word *w*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35620ce6-e09a-4043-a8ff-b5f2b6630334.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: 'However, the softmax complexity scales with the number of classes, as the denominator
    requires the computation of the dot product for all words in the vocabulary to
    standardize the probabilities. Word2vec models gain efficiency by using a simplified
    version of the softmax or sampling-based approaches (see references for details):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: The **hierarchical softmax** organizes the vocabulary as a binary tree with
    words as leaf nodes. The unique path to each node can be used to compute the word
    probability.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise-contrastive estimation** (**NCE**) samples out-of-context "noise words"
    and approximates the multiclass task by a binary classification problem. The NCE
    derivative approaches the softmax gradient as the number of samples increases,
    but as few as 25 samples can yield convergence similar to the softmax, at a rate
    that is 45 times faster.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Negative sampling** (**NEG**) omits the noise word samples to approximate
    NCE and directly maximizes the probability of the target word. Hence, NEG optimizes
    the semantic quality of embedding vectors (similar vectors for similar usage)
    rather than the accuracy on a test set. It may, however, produce poorer representations
    for infrequent words than the hierarchical softmax objective.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic phrase detection
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preprocessing typically involves phrase detection, that is, the identification
    of tokens that are commonly used together and should receive a single vector representation
    (for example, New York City, see the discussion of n-grams in [Chapter 13](461c4d2b-9349-4b77-baf9-7b4e60926c93.xhtml),
    *Working with Text Data*).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'The original Word2vec authors use a simple lift scoring method that identifies
    two words *w[i]*, *w[j]* as a bigram if their joint occurrence exceeds a given
    threshold relative to each word''s individual appearance, corrected by a discount
    factor *δ*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21e54b2d-361b-4c00-9c9f-5f06a332317c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: The scorer can be applied repeatedly to identify successively longer phrases.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative is the normalized point-wise mutual information score that is
    more accurate, but also more costly to compute. It uses the relative word frequency
    *P(w)* and varies between +1 and -1:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84b9a2c6-2ef4-4777-9dc3-1891145ae39c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: How to evaluate embeddings – vector arithmetic and analogies
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bag-of-words model creates document vectors that reflect the presence and
    relevance of tokens to the document. **Latent semantic analysis** reduces the
    dimensionality of these vectors and identifies what can be interpreted as latent
    concepts in the process. **Latent Dirichlet allocation** represents both documents
    and terms as vectors that contain the weights of latent topics.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型创建反映标记在文档中存在和相关性的文档向量。**潜在语义分析**减少了这些向量的维度，并在此过程中识别了可被解释为潜在概念的内容。**潜在狄利克雷分配**将文档和术语都表示为包含潜在主题权重的向量。
- en: The dimensions of the word and phrase vectors do not have an explicit meaning.
    However, the embeddings encode similar usage as proximity in the latent space
    in a way that carries over to semantic relationships. This results in the interesting
    properties that analogies can be expressed by adding and subtracting word vectors.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 词和短语向量的维度没有明确的含义。然而，嵌入在潜在空间中编码了相似用法，以一种体现在语义关系上的接近性的方式。这导致了有趣的性质，即类比可以通过添加和减去词向量来表达。
- en: 'The following figure shows how the vector connecting Paris and France (that
    is, the difference of their embeddings) reflects the capital of relationship.
    The analogous relationship, London: UK, corresponds to the same vector, that is,
    the UK is very close to the location obtained by adding the capital of vector
    to London:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了连接巴黎和法国的向量（即它们的嵌入之差）如何反映了首都关系。类似的关系，伦敦：英国，对应于相同的向量，即英国与通过将首都向量添加到伦敦得到的位置非常接近：
- en: '![](img/896d4b2a-33bd-4eeb-98eb-34b7653ae59e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/896d4b2a-33bd-4eeb-98eb-34b7653ae59e.png)'
- en: Just as words can be used in different contexts, they can be related to other
    words in different ways, and these relationships correspond to different directions
    in the latent space. Accordingly, there are several types of analogies that the
    embeddings should reflect if the training data permits.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如单词可以在不同的语境中使用一样，它们可以以不同的方式与其他单词相关联，而这些关系对应于潜在空间中的不同方向。因此，如果训练数据允许，嵌入应该反映出几种不同类型的类比关系。
- en: 'The Word2vec authors provide a list of several thousand relationships spanning
    aspects of geography, grammar and syntax, and family relationships to evaluate
    the quality of embedding vectors. As illustrated above, the test validates that
    the target word (UK) is closest to the result of adding the vector that represents
    an analogous relationship (Paris: France) to the target''s complement (London).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 的作者提供了一份涵盖地理、语法和句法以及家庭关系等多个方面的几千个关系的列表，用于评估嵌入向量的质量。如上所示，该测试验证了目标词（英国）最接近的是将代表类似关系（巴黎：法国）的向量添加到目标的补充（伦敦）后得到的结果。
- en: 'The following figure projects the 300-dimensional embeddings of the most closely
    related analogies for a Word2vec model trained on the Wikipedia corpus, with over
    2 billion tokens, into two dimensions using **principal component analysis** (**PCA**).
    A test of over 24,400 analogies from the following categories achieved an accuracy
    of over 73.5% (see notebook):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图将在维基百科语料库上训练的 Word2vec 模型的最相关类比的 300 维嵌入，具有超过 20 亿标记，通过**主成分分析**（**PCA**）投影到二维。从以下类别的超过
    24,400 个类比的测试实现了超过 73.5% 的准确率（参见笔记本）：
- en: '![](img/2871a658-5c0d-4506-a052-876002c88b8a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2871a658-5c0d-4506-a052-876002c88b8a.png)'
- en: Working with embedding models
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌入模型
- en: Similar to other unsupervised learning techniques, the goal of learning embedding
    vectors is to generate features for other tasks such as text classification or
    sentiment analysis.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他无监督学习技术类似，学习嵌入向量的目标是为其他任务生成特征，如文本分类或情感分析。
- en: 'There are several options to obtain embedding vectors for a given corpus of
    documents:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 获取给定文档语料库的嵌入向量有几种选项：
- en: Use embeddings learned from a generic large corpus such as Wikipedia or Google
    News
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用从通用大语料库（如维基百科或谷歌新闻）中学到的嵌入
- en: Train your own model using documents that reflect a domain of interest
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反映感兴趣领域的文档来训练自己的模型
- en: The less generic and more specialized the content of the subsequent text modeling
    task is, the more preferable is the second approach. However, quality word embeddings
    are data-hungry and require informative documents containing hundreds of millions
    of words.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于后续的文本建模任务，内容越专业化、越不通用，第二种方法就越可取。然而，高质量的词向量需要大量数据，并且需要包含数亿字的信息性文档。
- en: How to use pre-trained word vectors
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用预训练的词向量
- en: There are several sources for pretrained word embeddings. Popular options include
    Stanford's GloVE and spaCy's built-in vectors (see the notebook `using_trained_vectors`
    for details).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: GloVe – global vectors for word representation
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GloVe is an unsupervised algorithm developed at the Stanford NLP lab that learns
    vector representations for words from aggregated global word-word co-occurrence
    statistics (see references). Vectors pretrained on the following web-scale sources
    are available:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Common Crawl with 42B or 840B tokens and a vocabulary or 1.9M or 2.2M tokens
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wikipedia 2014 + Gigaword 5 with 6B tokens and a vocabulary of 400K tokens
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Twitter using 2B tweets, 27B tokens and a vocabulary of 1.2M tokens
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can use gensim to convert and load the vector text files into the `KeyedVector`
    object:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The Word2vec authors provide text files containing over 24,000 analogy tests
    that gensim uses to evaluate word vectors.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'The word vectors trained on the Wikipedia corpus cover all analogies and achieve
    an overall accuracy of 75.5% with some variation across categories:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '| **Category** | **Samples** | **Accuracy** | **Category** | **Samples** |
    **Accuracy** |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| capital-common-countries | 506 | 94.86% | comparative | 1,332 | 88.21% |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| capital-world | 8,372 | 96.46% | superlative | 1,056 | 74.62% |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| city-in-state | 4,242 | 60.00% | present-participle | 1,056 | 69.98% |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| currency | 752 | 17.42% | nationality-adjective | 1,640 | 92.50% |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| family | 506 | 88.14% | past-tense | 1,560 | 61.15% |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| adjective-to-adverb | 992 | 22.58% | plural | 1,332 | 78.08% |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| opposite | 756 | 28.57% | plural-verbs | 870 | 58.51% |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: The Common Crawl vectors for the 100,000 most common tokens cover about 80%
    of the analogies and achieve slightly higher accuracy at 78%, whereas the Twitter
    vectors cover only 25% with 62% accuracy.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: How to train your own word vector embeddings
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many tasks require embeddings or domain-specific vocabulary that pretrained
    models based on a generic corpus may not represent well or at all. Standard Word2vec
    models are not able to assign vectors to out-of-vocabulary words and instead use
    a default vector that reduces their predictive value.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: For example, when working with industry-specific documents, the vocabulary or
    its usage may change over time as new technologies or products emerge. As a result,
    the embeddings need to evolve as well. In addition, corporate earnings releases
    use nuanced language not fully reflected in GloVe vectors pretrained on Wikipedia
    articles.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: We will illustrate the Word2vec architecture using the Keras library that we
    will introduce in more detail in the next chapter and the more performant gensim
    adaptation of the code provided by the Word2vec authors. The notebook Word2vec
    contains additional implementation detail, including a reference of a TensorFlow
    implementation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The Skip-Gram architecture in Keras
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate the Word2vec network architecture, we use the TED Talk dataset
    with aligned English and Spanish subtitles that we first introduced in [Chapter
    13](https://cdp.packtpub.com/hands_on_machine_learning_for_algorithmic_trading/wp-admin/post.php?post=682&action=edit#post_584), *Working
    with Text Data*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明Word2vec网络架构，我们使用TED Talk数据集，该数据集具有对齐的英文和西班牙文字幕，我们首次在[第13章](https://cdp.packtpub.com/hands_on_machine_learning_for_algorithmic_trading/wp-admin/post.php?post=682&action=edit#post_584)中介绍了*处理文本数据*。
- en: The notebook contains the code to tokenize the documents and assign a unique
    ID to each item in the vocabulary. We require at least five occurrences in the
    corpus and keep a vocabulary of 31,300 tokens.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本包含将文档标记化并为词汇表中的每个项目分配唯一ID的代码。我们要求语料库中至少出现五次，并保留31300个标记的词汇表。
- en: Noise-contrastive estimation
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 噪声对比估计
- en: Keras includes a `make_sampling_table` method that allows us to create a training
    set as pairs of context and noise words with corresponding labels, sampled according
    to their corpus frequencies.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Keras包括一个`make_sampling_table`方法，允许我们创建一个训练集，其中上下文和噪声词与相应的标签配对，根据它们的语料库频率进行采样。
- en: The result is 27 million positive and negative examples of context and target
    pairs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是2700万个正面和负面的上下文和目标对的例子。
- en: The model components
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型组件
- en: The *Skip-Gram* model contains a 200-dimensional embedding vector for each vocabulary
    item, resulting in 31,300 x 200 trainable parameters, plus two for the sigmoid
    output.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*Skip-Gram*模型为词汇表中的每个项目包含一个200维嵌入向量，导致31300 x 200可训练参数，加上两个用于sigmoid输出。'
- en: In each iteration, the model computes the dot product of the context and the
    target-embedding vectors, passes the result through the sigmoid to produce a probability
    and adjusts the embedding based on the gradient of the loss.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，模型计算上下文和目标嵌入向量的点积，通过sigmoid产生概率，并根据损失的梯度调整嵌入。
- en: Visualizing embeddings using TensorBoard
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorBoard可视化嵌入
- en: TensorBoard is a visualization tool that permits the projection of the embedding
    vectors into three dimensions to explore the word and phrase locations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard是一种可视化工具，允许将嵌入向量投影到三维空间中，以探索单词和短语的位置。
- en: Word vectors from SEC filings using gensim
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用gensim从SEC备案中学习的单词向量
- en: In this section, we will learn word and phrase vectors from annual US **Securities
    and Exchange Commission** (**SEC**) filings using gensim to illustrate the potential
    value of word embeddings for algorithmic trading. In the following sections, we
    will combine these vectors as features with price returns to train neural networks
    to predict equity prices from the content of security filings.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用gensim从美国年度**证券交易委员会**(**SEC**)备案中学习单词和短语向量，以说明单词嵌入对算法交易的潜在价值。在接下来的几节中，我们将将这些向量与价格回报结合使用作为特征，训练神经网络来根据安全备案的内容预测股票价格。
- en: In particular, we use a dataset containing over 22,000 10-K annual reports from
    the period 2013-2016 that are filed by listed companies and contain both financial
    information and management commentary (see [Chapter 3](a7cec22f-095e-49c0-a2bb-e179f6e824a8.xhtml),
    *Alternative Data for Finance*). For about half of the 11-K filings for companies,
    we have stock prices to label the data for predictive modeling (see references
    about data sources and the notebooks in the `sec-filings` folder for details).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们使用一个数据集，其中包含2013-2016年间上市公司提交的超过22,000份10-K年度报告，其中包含财务信息和管理评论（参见[第3章](a7cec22f-095e-49c0-a2bb-e179f6e824a8.xhtml)，*金融替代数据*）。对于大约一半的公司的11-K备案，我们有股价以标记预测建模的数据（有关数据来源和`sec-filings`文件夹中笔记本的引用，请参阅详细信息）。
- en: Preprocessing
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理
- en: 'Each filing is a separate text file and a master index contains filing metadata.
    We extract the most informative sections, namely, the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 每个备案都是一个单独的文本文件，一个主索引包含备案元数据。我们提取最具信息量的部分，即以下部分：
- en: '**Items 1 and 1A**: Business and Risk Factors'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**项目1和1A**：业务和风险因素'
- en: '**Items 7 and 7A**: Management''s Discussion and Disclosures about Market Risks'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**项目7和7A**：管理对市场风险的讨论和披露'
- en: The notebook preprocessing shows how to parse and tokenize the text using spaCy,
    similar to the approach taken in [Chapter 14](beb6fa08-c790-47d5-82ef-f48a81dcf3d1.xhtml),
    *Topic Modeling*. We do not lemmatize the tokens to preserve the nuances of word
    usage.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本预处理显示如何使用spaCy解析和标记文本，类似于[第14章](beb6fa08-c790-47d5-82ef-f48a81dcf3d1.xhtml)中采用的方法，*主题建模*。我们不对单词进行词形还原处理，以保留单词用法的细微差别。
- en: Automatic phrase detection
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动短语检测
- en: 'We use `gensim` to detect phrases as previously introduced. The `Phrases` module
    scores the tokens and the `Phraser` class transforms the text data accordingly.
    The notebook shows how to repeat the process to create longer phrases:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`gensim`来检测短语，就像之前介绍的那样。`Phrases`模块对标记进行评分，而`Phraser`类相应地转换文本数据。笔记本展示了如何重复此过程以创建更长的短语：
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The most frequent bigrams include `common_stock`, `united_states`, `cash_flows`,
    `real_estate`, and `interest_rates`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最频繁的二元组包括`common_stock`，`united_states`，`cash_flows`，`real_estate`和`interest_rates`。
- en: Model training
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: The `gensim.models.Word2vec` class implements the SG and CBOW architectures
    introduced previously. The Word2vec notebook contains additional implementation
    detail.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim.models.Word2vec`类实现了之前介绍的SG和CBOW架构。Word2vec笔记本包含额外的实现细节。'
- en: 'To facilitate memory-efficient text ingestion, the `LineSentence` class creates
    a generator from individual sentences contained in the provided text file:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便内存高效的文本摄取，`LineSentence`类从提供的文本文件中创建一个生成器，其中包含单独的句子：
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `Word2vec` class offers the configuration options previously introduced:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`Word2vec`类提供了先前介绍的配置选项：'
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The notebook shows how to persist and reload models to continue training, or
    how to store the embedding vectors separately, for example, for use in ML models.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本展示了如何持久化和重新加载模型以继续训练，或者如何将嵌入向量单独存储，例如供ML模型使用。
- en: Model evaluation
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'Basic functionality includes identifying similar words:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 基本功能包括识别相似的单词：
- en: '[PRE4]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can also validate individual analogies using positive and negative contributions
    accordingly:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用正负贡献来验证单个类比：
- en: '[PRE5]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Performance impact of parameter settings
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数设置的性能影响
- en: 'We can use the analogies to evaluate the impact of different parameter settings.
    The following results stand out (see detailed results in the `models` folder):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用类比来评估不同参数设置的影响。以下结果表现突出（详细结果请参见`models`文件夹）：
- en: Negative sampling outperforms the hierarchical softmax, while also training
    faster
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负采样优于分层softmax，同时训练速度更快。
- en: The Skip-Gram architecture outperforms CBOW given the objective function
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skip-Gram架构优于CBOW给定的目标函数
- en: Different `min_count` settings have a smaller impact, with the midpoint of 50
    performing best
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的`min_count`设置影响较小，50为最佳表现的中间值。
- en: 'Further experiments with the best performing SG model, using negative sampling
    and a `min_count` of 50, show the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最佳表现的SG模型进行进一步实验，使用负采样和`min_count`为50，得到以下结果：
- en: Smaller context windows than five lower the performance
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比五更小的上下文窗口会降低性能。
- en: A higher negative sampling rate improves performance at the expense of slower
    training
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高的负采样率提高了性能，但训练速度慢了。
- en: Larger vectors improve performance, with a size of 600 yielding the best accuracy
    at 38.5%
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更大的向量提高了性能，大小为600时的准确率最高，为38.5％。
- en: Sentiment analysis with Doc2vec
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Doc2vec进行情感分析
- en: Text classification requires combining multiple word embeddings. A common approach
    is to average the embedding vectors for each word in the document. This uses information
    from all embeddings and effectively uses vector addition to arrive at a different
    location point in the embedding space. However, relevant information about the
    order of words is lost.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类需要组合多个词嵌入。一个常见的方法是对文档中每个词的嵌入向量进行平均。这使用所有嵌入的信息，并有效地使用向量加法来到达嵌入空间中的不同位置。但是，有关单词顺序的相关信息会丢失。
- en: By contrast, the state-of-the-art generation of embeddings for pieces of text
    such as a paragraph or a product review is to use the document-embedding model
    `Doc2vec`. This model was developed by the Word2vec authors shortly after publishing
    their original contribution.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，用于生成文本片段（如段落或产品评论）的嵌入的最先进模型是使用文档嵌入模型`Doc2vec`。这个模型是Word2vec作者在发布其原创贡献后不久开发的。
- en: 'Similar to Word2vec, there are also two flavors of `Doc2vec`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与Word2vec类似，`Doc2vec`也有两种类型：
- en: The **distributed bag of words** (**DBOW**) model corresponds to the Word2vec
    CBOW model. The document vectors result from training a network in the synthetic
    task of predicting a target word based on both the context word vectors and the
    document's doc vector.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式词袋**（**DBOW**）模型对应于Word2vec的CBOW模型。文档向量是通过训练网络在预测目标词的合成任务中产生的，该任务基于上下文词向量和文档的文档向量。'
- en: The **distributed memory** (**DM**) model corresponds to the Word2vec Skip-Gram
    architecture. The doc vectors result from training a neural net to predict a target
    word using the full document's doc vector.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式记忆**（**DM**）模型对应于 Word2vec Skip-Gram 架构。通过训练神经网络来预测目标单词，使用整个文档的文档向量。'
- en: Gensim's `Doc2vec` class implements this algorithm.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim 的 `Doc2vec` 类实现了这种算法。
- en: Training Doc2vec on yelp sentiment data
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Yelp 情感数据上训练 Doc2vec
- en: 'We use a random sample of 500,000 Yelp (see [Chapter 13](461c4d2b-9349-4b77-baf9-7b4e60926c93.xhtml), *Working
    with Text Data*) reviews with their associated star ratings (see notebook `yelp_sentiment`):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 500,000 条 Yelp 评论（参见[第 13 章](461c4d2b-9349-4b77-baf9-7b4e60926c93.xhtml)，*处理文本数据*）的随机样本，以及它们的相关星级评分（参见
    notebook `yelp_sentiment`）：
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We apply use simple pre-processing to remove stopwords and punctuation using
    `NLTK`''s tokenizer and drop reviews with fewer than 10 tokens:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用简单的预处理来删除停用词和标点符号，使用 `NLTK` 的分词器并删除少于 10 个标记的评论：
- en: '[PRE7]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Create input data
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建输入数据
- en: 'The `gensim.models.doc2vec` class processes documents in the `TaggedDocument`
    format that contains the tokenized documents alongside a unique tag that permits
    accessing the document vectors after training:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim.models.doc2vec` 类以 `TaggedDocument` 格式处理文档，其中包含标记化的文档以及允许在训练后访问文档向量的唯一标记：'
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The training interface works similar to `word2vec` with additional parameters
    to specify the Doc2vec algorithm:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 训练界面与 `word2vec` 类似，但有额外的参数来指定 Doc2vec 算法：
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can also use the `train()` method to continue the learning process and,
    for example, iteratively reduce the learning rate:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 `train()` 方法来继续学习过程，并且，例如，逐渐降低学习率：
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As a result, we can access the document vectors as features to train a sentiment
    classifier:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将文档向量作为特征来访问，以训练情感分类器：
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will train a `lightgbm` gradient boosting machine as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练一个 `lightgbm` 梯度提升机，如下所示：
- en: 'Create `lightgbm` `Dataset` objects from the train and test sets:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练集和测试集创建 `lightgbm` `Dataset` 对象：
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define the training parameters for a multiclass model with five classes (using
    defaults otherwise):'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义具有五个类别的多类模型的训练参数（否则使用默认值）：
- en: '[PRE13]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Train the model for 250 iterations and monitor the validation set error:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对模型进行 250 次迭代的训练，并监视验证集错误：
- en: '[PRE14]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Lightgbm predicts probabilities for all five classes. We obtain class predictions
    using `np.argmax()` to obtain the column index with the highest predicted probability:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lightgbm 对所有五个类别进行概率预测。我们使用 `np.argmax()` 来获取具有最高预测概率的列索引来获取类别预测：
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We compute the accuracy score to evaluate the result and see an improvement
    of more than 100% over the baseline of 20% for five balanced classes:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算准确率评分来评估结果，并看到相对于五个平衡类别的基线 20%，准确率提高了 100% 以上：
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we take a closer look at predictions for each class using the confusion
    matrix:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过混淆矩阵来更仔细地查看每个类别的预测：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And visualize the result as a `seaborn` heatmap:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并将结果可视化为 `seaborn` 热图：
- en: '[PRE18]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/c99b5339-66b3-4736-87ac-9fdc6ece26d3.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c99b5339-66b3-4736-87ac-9fdc6ece26d3.png)'
- en: In sum, the `doc2vec` method allowed us to achieve a very substantial improvement
    in test accuracy over a naive benchmark without much tuning. If we only select
    top and bottom reviews (with five and one stars, respectively) and train a binary
    classifier, the AUC score achieves over 0.86 using 250,000 samples from each class.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，`doc2vec` 方法使我们能够在不经过太多调整的情况下，比一个简单的基准模型取得了非常大的测试准确率改进。如果我们只选择顶部和底部的评论（分别为五星和一星），并训练一个二元分类器，则使用每个类别的
    250,000 个样本时，AUC 分数可以达到 0.86 以上。
- en: Bonus – Word2vec for translation
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附加内容 - 用于翻译的 Word2vec
- en: The notebook translation demonstrates that the relationships encoded in one
    language often correspond to similar relationships in another language.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 该 notebook 翻译演示了一个语言中编码的关系通常对应于另一种语言中的类似关系。
- en: It illustrates how word vectors can be used to translate words and phrases by
    projecting word vectors from the embedding space of one language into the space
    of another language using a translation matrix.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 它说明了如何使用单词向量通过将单词向量从一个语言的嵌入空间投影到另一个语言的空间使用翻译矩阵来翻译单词和短语。
- en: Summary
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter started with how word embeddings encode semantics for individual
    tokens more effectively than the bag-of-words model that we used in [Chapter 13](461c4d2b-9349-4b77-baf9-7b4e60926c93.xhtml),
    *Working with Text Data.* We also saw how to evaluated embedding by validating
    if semantic relationships among words are properly represented using linear vector
    arithmetic.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以词嵌入如何更有效地为个别标记编码语义开始，这比我们在[第13章](461c4d2b-9349-4b77-baf9-7b4e60926c93.xhtml)中使用的词袋模型更为有效。我们还看到了如何通过线性向量算术验证嵌入，以评估是否正确表示了单词之间的语义关系。
- en: To learn word embeddings, we use shallow neural networks that used to be slow
    to train at the scale of web data containing billions of tokens. The `word2vec`
    model combines several algorithmic innovations to dramatically speed up training
    and has established a new standard for text feature generation. We saw how to
    use pretrained word vectors using `spaCy` and `gensim`, and learned to train our
    own word vector embeddings. We then applied a `word2vec` model to SEC filings.
    Finally, we covered the `doc2vec` extension that learns vector representations
    for documents in a similar fashion as word vectors and applied it to Yelp business
    reviews.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 学习词嵌入时，我们使用的浅层神经网络曾经在包含数十亿标记的Web数据规模上训练速度很慢。`word2vec` 模型结合了几种算法创新，显著加速了训练，并为文本特征生成确立了新标准。我们看到了如何使用`spaCy`和`gensim`来使用预训练的词向量，并学会了训练自己的词向量嵌入。然后，我们将`word2vec`模型应用于SEC提交文件。最后，我们介绍了`doc2vec`扩展，它以与单词向量类似的方式学习文档的向量表示，并将其应用于Yelp商家评论。
- en: Now, we will begin part 4 on deep learning (available online as mentioned in
    the Preface), starting with an introduction to feed-forward networks, popular
    deep learning frameworks and techniques for efficient training at scale.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将开始第四部分关于深度学习（如前言所述，在线可用），从介绍前馈网络、流行的深度学习框架和大规模高效训练技术开始。
