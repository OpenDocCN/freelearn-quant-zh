["```py\npip install marbles\n\n```", "```py\nimport marbles.core                                 #1\nfrom marbles.mixins import mixins\n\nimport pandas as pd                                 #2\nimport numpy as np\nfrom datetime import datetime, timedelta\n\nclass TimeSeriesTestCase(marbles.core.TestCase,mixins.MonotonicMixins):                            #3\n    def setUp(self):                                            #4\n\n        self.df = pd.DataFrame({'dates':[datetime(2018,1,1),datetime(2018,2,1),datetime(2018,2,1)],'ireland_unemployment':[6.2,6.1,6.0]})   #5\n\n    def tearDown(self):\n        self.df = None                                          #6\n\n    def test_date_order(self):                                  #7\n\n        self.assertMonotonicIncreasing(sequence=self.df.dates,note = 'Dates need to increase monotonically')                                                 #8\n```", "```py\nif __name__ == '__main__':\n    marbles.core.main(argv=['first-arg-is-ignored'], exit=False)\n```", "```py\npython -m marbles marbles_test.py\n\n```", "```py\nF                                                     #1\n==================================================================\nFAIL: test_date_order (__main__.TimeSeriesTestCase)   #2\n------------------------------------------------------------------\nmarbles.core.marbles.ContextualAssertionError: Elements in 0   2018-01-01\n1   2018-02-01\n2   2018-02-01                                        #3\nName: dates, dtype: datetime64[ns] are not strictly monotonically increasing\n\nSource (<ipython-input-1-ebdbd8f0d69f>):              #4\n     19 \n >   20 self.assertMonotonicIncreasing(sequence=self.df.dates,\n     21                           note = 'Dates need to increase monotonically')\n     22 \nLocals:                                               #5\n\nNote:                                                 #6\n    Dates need to increase monotonically\n\n----------------------------------------------------------------------\n```", "```py\nRan 1 test in 0.007s\n\nFAILED (failures=1)\n\n```", "```py\npip install lime\n\n```", "```py\nfrom lime.lime_text import LimeTextExplainer               #1\nexplainer = LimeTextExplainer(class_names=class_names)     #2\nexp = explainer.explain_instance(test_example,             #3classifier.predict_proba, #4num_features=6)           #5\n\nexp.show_in_notebook()                                     #6\n```", "```py\npip install hyperas\n\n```", "```py\nfrom hyperopt import Trials, STATUS_OK, tpe        #1\nfrom hyperas import optim                          #2\nfrom hyperas.distributions import choice, uniform \n```", "```py\ndef data():                                      #1\n    import numpy as np                           #2\n    from keras.utils import np_utils\n\n    from keras.models import Sequential \n    from keras.layers import Dense, Activation, Dropout\n    from keras.optimizers import RMSprop\n\n    path = '../input/mnist.npz'                  #3\n    with np.load(path) as f:\n        X_train, y_train = f['x_train'], f['y_train']\n        X_test, y_test = f['x_test'], f['y_test']\n\n    X_train = X_train.reshape(60000, 784)        #4\n    X_test = X_test.reshape(10000, 784)\n    X_train = X_train.astype('float32')\n    X_test = X_test.astype('float32')\n    X_train /= 255\n    X_test /= 255\n    nb_classes = 10\n    y_train = np_utils.to_categorical(y_train, nb_classes)\n    y_test = np_utils.to_categorical(y_test, nb_classes)\n\n    return X_train, y_train, X_test, y_test      #5\n```", "```py\n    from keras.datasets import mnist\n    (Y_train, y_train), (X_test, y_test) = mnist.load_data() \n    ```", "```py\n    def model(X_train, y_train, X_test, y_test):                   #1\n        model = Sequential()                                       #2\n        model.add(Dense(512, input_shape=(784,)))\n\n        model.add(Activation('relu'))\n\n        model.add(Dropout({{uniform(0, 0.5)}}))                    #3\n\n        model.add(Dense({{choice([256, 512, 1024])}}))             #4\n\n        model.add(Activation({{choice(['relu','tanh'])}}))         #5\n\n        model.add(Dropout({{uniform(0, 0.5)}}))\n\n        model.add(Dense(10))\n        model.add(Activation('softmax'))\n\n        rms = RMSprop()\n        model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])\n\n        model.fit(X_train, y_train,                                #6batch_size={{choice([64, 128])}},epochs=1,verbose=2,validation_data=(X_test, y_test))\n        score, acc = model.evaluate(X_test, y_test, verbose=0)     #7\n        print('Test accuracy:', acc)\n        return {'loss': -acc, 'status': STATUS_OK, 'model': model} #8\n    ```", "```py\n    best_run, best_model = optim.minimize(model=model,\n    \t\t\t\t\t\t\t\t\tdata=data,\n    \t\t\t\t\t\t\t\t\talgo=tpe.suggest,\n    \t\t\t\t\t\t\t\t\tmax_evals=5,\n    \t\t\t\t\t\t\t\t\ttrials=Trials(),\n    \t\t\t\tnotebook_name='__notebook_source__')\n\n    ```", "```py\n{'Activation': 1,\n 'Dense': 1,\n 'Dropout': 0.3462695171578595,\n 'Dropout_1': 0.10640021656377913,\n 'batch_size': 0}\n\n```", "```py\ninit_lr = 1e-6                                            #1\nlosses = [] \nlrs = []\nfor i in range(20):                                       #2\n    model = Sequential()\n    model.add(Dense(512, input_shape=(784,)))\n    model.add(Activation('relu')) \n    model.add(Dropout(0.2))\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n\n    opt = Adam(lr=init_lr*2**i)                           #3\n    model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['acc'])\n\n    hist = model.fit(X_train, Y_train, batch_size = 128, epochs=1)                                                 #4\n\n    loss = hist.history['loss'][0]                        #5\n    losses.append(loss)\n    lrs.append(init_lr*2**i)\n```", "```py\nfig, ax = plt.subplots(figsize = (10,7))\nplt.plot(lrs,losses)\nax.set_xscale('log')\n```", "```py\ndef cosine_anneal_schedule(t):\n    lr_init = 1e-2                               #1\n    anneal_len = 5\n    if t >= anneal_len: t = anneal_len -1        #2\n    cos_inner = np.pi * (t % (anneal_len))       #3\n    cos_inner /= anneal_len\n    cos_out = np.cos(cos_inner) + 1\n    return float(lr_init / 2 * cos_out)\n```", "```py\nsrs = [cosine_anneal_schedule(t) for t in range(10)]\nplt.plot(srs)\n```", "```py\nfrom keras.callbacks import LearningRateScheduler\ncb = LearningRateScheduler(cosine_anneal_schedule)\n```", "```py\nmodel.fit(x_train,y_train,batch_size=128,epochs=5,callbacks=[cb])\n```", "```py\ndef cosine_anneal_schedule(t):\n    lr_init = 1e-2 \n    anneal_len = 10 \n    cos_inner = np.pi * (t % (anneal_len))  \n    cos_inner /= anneal_len\n    cos_out = np.cos(cos_inner) + 1\n    return float(lr_init / 2 * cos_out)\n```", "```py\nfrom keras.callbacks import TensorBoard\ntb = TensorBoard(log_dir='./logs/test2',           #1\n                 histogram_freq=1,                 #2\n                 batch_size=32,                    #3\n                 write_graph=True,                 #4\n                 write_grads=True, \n                 write_images=True, \n                 embeddings_freq=0,                #5\n                 embeddings_layer_names=None, \n                 embeddings_metadata=None)\n```", "```py\nhist = model.fit(x_train*255,y_train,batch_size=128,epochs=5,callbacks=[tb],validation_data=(x_test*255,y_test))\n```", "```py\ntensorboard --logdir=/full_path_to_your_logs\n\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.python import debug as tf_debug\nimport keras\n\nkeras.backend.set_session(tf_debug.TensorBoardDebugWrapperSession(tf.Session(), \"localhost:2018\"))\n```", "```py\ntensorboard --logdir=/full_path_to_your_logs --debugger_port 2018\n\n```", "```py\nfrom keras.optimizers import SGD\n\nclip_val_sgd = SGD(lr=0.01, clipvalue=0.5)\nclip_norm_sgd = SGD(lr=0.01, clipnorm=1.)\n```", "```py\nfrom keras.layers import BatchNormalization\nmodel.add(BatchNormalization())\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.python import keras\n\nfrom tensorflow.python.keras.models import Sequentialâ€©from tensorflow.python.keras.layers import Dense,Activation\n```", "```py\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train.shape = (60000, 28 * 28)\nx_train = x_train / 255\ny_train = keras.utils.to_categorical(y_train)\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(786, input_dim = 28*28))\nmodel.add(Activation('relu'))\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dense(160))\nmodel.add(Activation('relu'))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n\nmodel.compile(optimizer=keras.optimizers.SGD(lr=0.0001, momentum=0.9),loss='categorical_crossentropy',metric='accuracy')\n```", "```py\nestimator = keras.estimator.model_to_estimator(keras_model=model)\n```", "```py\nmodel.input_names\n['dense_1_input']\n```", "```py\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(x={'dense_1_input': x_train},y=y_train,num_epochs=1,shuffle=False)\n```", "```py\nestimator.train(input_fn=train_input_fn, steps=2000)\n```", "```py\nnvidia-smi -l 2\n\n```", "```py\n    model.fit_generator(generator, steps_per_epoch = 40, workers=4, use_multiprocessing=False)\n    ```", "```py\n    import threading\n\n    class thread_safe_iter:                   #1\n        def __init__(self, it):\n            self.it = it\n            self.lock = threading.Lock()\n\n        def __iter__(self):\n            return self\n\n        def next(self):                       #2\n            with self.lock:\n                return self.it.next()\n\n    def thread_safe_generator(f):             #3\n        def g(*a, **kw):\n            return thread_safe_iter(f(*a, **kw))\n        return g\n\n    @thread_safe_generator\n    def gen():\n    ```", "```py\n    dxtrain = tf.data.Dataset.from_tensor_slices(x_test)\n    dytrain = tf.data.Dataset.from_tensor_slices(y_train)\n    ```", "```py\n    def apply_one_hot(z):\n        return tf.one_hot(z,10)\n\n    dytrain = dytrain.map(apply_one_hot,num_parallel_calls=4)\n    ```", "```py\n    train_data = tf.data.Dataset.zip((dxtrain,dytrain)).shuffle(200).batch(32)\n    ```", "```py\n    model.fit(dataset, epochs=10, steps_per_epoch=60000 // 32)\n    ```", "```py\nfrom __future__ import print_function\ndef fib(n):\n    a, b = 0, 1\n    while b < n:\n        print(b, end=' ')\n        a, b = b, a + b\n    print()\n```", "```py\nfrom distutils.core import setup                   #1\nfrom Cython.Build import cythonize                 #2\n\nsetup(                                             #3ext_modules=cythonize(\"cython_fib_8_7.pyx\"),)\n```", "```py\npython 8_7_cython_setup.py build_ext --inplace\n\n```", "```py\nimport cython_fib_8_7\ncython_fib_8_7.fib(1000)\n```", "```py\ncython -a cython_fib_8_7.pyx\n\n```", "```py\nfrom __future__ import print_function\ndef fib(int n):\n    cdef int a = 0\n    cdef int b = 1\n    while b < n:\n        print(b, end=' ')\n        a, b = b, a + b\n    print()\n```"]