["```py\n    import pandas as pd \n    ```", "```py\n    df = pd.read_csv(\"../Datasets/credit_card_default.csv\", \n                     na_values=\"\")\n    df \n    ```", "```py\n    df.info() \n    ```", "```py\n    RangeIndex: 30000 entries, 0 to 29999\n    Data columns (total 24 columns):\n     #   Column                      Non-Null Count  Dtype  \n    ---  ------                      --------------  -----  \n     0   limit_bal                   30000 non-null  int64  \n     1   sex                         29850 non-null  object\n     2   education                   29850 non-null  object\n     3   marriage                    29850 non-null  object\n     4   age                         29850 non-null  float64\n     5   payment_status_sep          30000 non-null  object\n     6   payment_status_aug          30000 non-null  object\n     7   payment_status_jul          30000 non-null  object\n     8   payment_status_jun          30000 non-null  object\n     9   payment_status_may          30000 non-null  object\n     10  payment_status_apr          30000 non-null  object\n     11  bill_statement_sep          30000 non-null  int64  \n     12  bill_statement_aug          30000 non-null  int64  \n     13  bill_statement_jul          30000 non-null  int64  \n     14  bill_statement_jun          30000 non-null  int64  \n     15  bill_statement_may          30000 non-null  int64  \n     16  bill_statement_apr          30000 non-null  int64  \n     17  previous_payment_sep        30000 non-null  int64  \n     18  previous_payment_aug        30000 non-null  int64  \n     19  previous_payment_jul        30000 non-null  int64  \n     20  previous_payment_jun        30000 non-null  int64  \n     21  previous_payment_may        30000 non-null  int64  \n     22  previous_payment_apr        30000 non-null  int64  \n     23  default_payment_next_month  30000 non-null  int64  \n    dtypes: float64(1), int64(14), object(9)\n    memory usage: 5.5+ MB \n    ```", "```py\n    def  get_df_memory_usage(df, top_columns=5):\n        print(\"Memory usage ----\")\n        memory_per_column = df.memory_usage(deep=True) / (1024 ** 2)\n        print(f\"Top {top_columns} columns by memory (MB):\")\n        print(memory_per_column.sort_values(ascending=False) \\\n                               .head(top_columns))\n        print(f\"Total size: {memory_per_column.sum():.2f} MB\") \n    ```", "```py\n    get_df_memory_usage(df, 5) \n    ```", "```py\n    Memory usage ----\n    Top 5 columns by memory (MB):\n    education             1.965001\n    payment_status_sep    1.954342\n    payment_status_aug    1.920288\n    payment_status_jul    1.916343\n    payment_status_jun    1.904229\n    dtype: float64\n    Total size: 20.47 MB \n    ```", "```py\n    object_columns = df.select_dtypes(include=\"object\").columns\n    df[object_columns] = df[object_columns].astype(\"category\")\n    get_df_memory_usage(df) \n    ```", "```py\n    Memory usage ----\n    Top 5 columns by memory (MB):\n    bill_statement_sep      0.228882\n    bill_statement_aug      0.228882\n    previous_payment_apr    0.228882\n    previous_payment_may    0.228882\n    previous_payment_jun    0.228882\n    dtype: float64\n    Total size: 3.70 MB \n    ```", "```py\n    numeric_columns = df.select_dtypes(include=\"number\").columns\n    for col in numeric_columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n    get_df_memory_usage(df) \n    ```", "```py\n    Memory usage ----\n    Top 5 columns by memory (MB):\n    age                     0.228882\n    bill_statement_sep      0.114441\n    limit_bal               0.114441\n    previous_payment_jun    0.114441\n    previous_payment_jul    0.114441\n    dtype: float64\n    Total size: 2.01 MB \n    ```", "```py\n    df[\"age\"] = pd.to_numeric(df[\"age\"], downcast=\"float\")\n    get_df_memory_usage(df) \n    ```", "```py\nMemory usage ----\nTop 5 columns by memory (MB):\nbill_statement_sep      0.114441\nlimit_bal               0.114441\nprevious_payment_jun    0.114441\nprevious_payment_jul    0.114441\nprevious_payment_aug    0.114441\ndtype: float64\nTotal size: 1.90 MB \n```", "```py\ncolumn_dtypes = {\n    \"education\": \"category\",\n    \"marriage\": \"category\",\n    \"sex\": \"category\"\n}\ndf_cat = pd.read_csv(\"../Datasets/credit_card_default.csv\",\n                     na_values=\"\", dtype=column_dtypes) \n```", "```py\n    import pandas as pd\n    import numpy as np\n    import seaborn as sns \n    ```", "```py\n    df.describe().transpose().round(2) \n    ```", "```py\n    df.describe(include=\"object\").transpose() \n    ```", "```py\n    ax = sns.kdeplot(data=df, x=\"age\",\n                     hue=\"sex\", common_norm=False,\n                     fill=True)\n    ax.set_title(\"Distribution of age\") \n    ```", "```py\n    COLS_TO_PLOT = [\"age\", \"limit_bal\", \"previous_payment_sep\"]\n    pair_plot = sns.pairplot(df[COLS_TO_PLOT], kind=\"reg\",\n                             diag_kind=\"kde\", height=4,\n                             plot_kws={\"line_kws\":{\"color\":\"red\"}})\n    pair_plot.fig.suptitle(\"Pairplot of selected variables\") \n    ```", "```py\n    pair_plot = sns.pairplot(data=df,\n                             x_vars=COLS_TO_PLOT,\n                             y_vars=COLS_TO_PLOT,\n                             hue=\"sex\",\n                             height=4)\n    pair_plot.fig.suptitle(\"Pairplot of selected variables\") \n    ```", "```py\n    ax = sns.jointplot(data=df, x=\"age\", y=\"limit_bal\", \n                       hue=\"sex\", height=10)\n    ax.fig.suptitle(\"Age vs. limit balance\") \n    ```", "```py\n    def  plot_correlation_matrix(corr_mat):\n        sns.set(style=\"white\")\n        mask = np.zeros_like(corr_mat, dtype=bool)\n        mask[np.triu_indices_from(mask)] = True\n        fig, ax = plt.subplots()\n        cmap = sns.diverging_palette(240, 10, n=9, as_cmap=True)\n        sns.heatmap(corr_mat, mask=mask, cmap=cmap, \n                    vmax=.3, center=0, square=True, \n                    linewidths=.5, cbar_kws={\"shrink\": .5}, \n                    ax=ax)\n        ax.set_title(\"Correlation Matrix\", fontsize=16)\n        sns.set(style=\"darkgrid\")\n    corr_mat = df.select_dtypes(include=\"number\").corr()    \n    plot_correlation_matrix(corr_mat) \n    ```", "```py\n    ax = sns.boxplot(data=df, y=\"age\", x=\"marriage\", hue=\"sex\")\n    ax.set_title(\"Distribution of age\") \n    ```", "```py\n    ax = sns.violinplot(x=\"education\", y=\"limit_bal\", \n                        hue=\"sex\", split=True, data=df)\n    ax.set_title(\n        \"Distribution of limit balance per education level\", \n        fontsize=16\n    ) \n    ```", "```py\n    ax = sns.countplot(\"default_payment_next_month\", hue=\"sex\",\n                       data=df, orient=\"h\")\n    ax.set_title(\"Distribution of the target variable\", fontsize=16) \n    ```", "```py\n    ax = df.groupby(\"education\")[\"default_payment_next_month\"] \\\n           .value_counts(normalize=True) \\\n           .unstack() \\\n           .plot(kind=\"barh\", stacked=\"True\")\n    ax.set_title(\"Percentage of default per education level\",\n                 fontsize=16)\n    ax.legend(title=\"Default\", bbox_to_anchor=(1,1)) \n    ```", "```py\nfrom pandas_profiling import ProfileReport\nprofile = ProfileReport(df, title=\"Loan Default Dataset EDA\")\nprofile \n```", "```py\nprofile.to_file(\"loan_default_eda.html\") \n```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split \n    ```", "```py\n    X = df.copy()\n    y = X.pop(\"default_payment_next_month\") \n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    ) \n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, shuffle=False\n    ) \n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, stratify=y, random_state=42\n    ) \n    ```", "```py\n    print(\"Target distribution - train\")\n    print(y_train.value_counts(normalize=True).values)\n    print(\"Target distribution - test\")\n    print(y_test.value_counts(normalize=True).values) \n    ```", "```py\nTarget distribution - train\n[0.77879167 0.22120833]\nTarget distribution - test\n[0.77883333 0.22116667] \n```", "```py\nimport numpy as np\n\n# define the size of the validation and test sets\nVALID_SIZE = 0.1\nTEST_SIZE = 0.2\n\n# create the initial split - training and temp\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, \n    test_size=(VALID_SIZE + TEST_SIZE), \n    stratify=y, \n    random_state=42\n)\n\n# calculate the new test size\nnew_test_size = np.around(TEST_SIZE / (VALID_SIZE + TEST_SIZE), 2)\n\n# create the valid and test sets\nX_valid, X_test, y_valid, y_test = train_test_split(\n    X_temp, y_temp, \n    test_size=new_test_size, \n    stratify=y_temp, \n    random_state=42\n) \n```", "```py\nprint(\"Percentage of data in each set ----\")\nprint(f\"Train: {100 * len(X_train) / len(X):.2f}%\")\nprint(f\"Valid: {100 * len(X_valid) / len(X):.2f}%\")\nprint(f\"Test: {100 * len(X_test) / len(X):.2f}%\")\nprint(\"\")\nprint(\"Class distribution in each set ----\")\nprint(f\"Train: {y_train.value_counts(normalize=True).values}\")\nprint(f\"Valid: {y_valid.value_counts(normalize=True).values}\")\nprint(f\"Test: {y_test.value_counts(normalize=True).values}\") \n```", "```py\nPercentage of data in each set ----\nTrain: 70.00%\nValid: 9.90%\nTest: 20.10%\nClass distribution in each set ----\nTrain: [0.77879899 0.22120101]\nValid: [0.77878788 0.22121212]\nTest: [0.77880948 0.22119052] \n```", "```py\n    import pandas as pd\n    import missingno as msno\n    from sklearn.impute import SimpleImputer \n    ```", "```py\n    X.info() \n    ```", "```py\n    RangeIndex: 30000 entries, 0 to 29999\n    Data columns (total 23 columns):\n     #   Column                Non-Null Count  Dtype  \n    ---  ------                --------------  -----  \n     0   limit_bal             30000 non-null  int64  \n     1   sex                   29850 non-null  object\n     2   education             29850 non-null  object\n     3   marriage              29850 non-null  object\n     4   age                   29850 non-null  float64\n     5   payment_status_sep    30000 non-null  object\n     6   payment_status_aug    30000 non-null  object\n     7   payment_status_jul    30000 non-null  object \n    ```", "```py\n    msno.matrix(X) \n    ```", "```py\n    NUM_FEATURES = [\"age\"]\n    CAT_FEATURES = [\"sex\", \"education\", \"marriage\"] \n    ```", "```py\n    for col in NUM_FEATURES:\n        num_imputer = SimpleImputer(strategy=\"median\")\n        num_imputer.fit(X_train[[col]])\n        X_train.loc[:, col] = num_imputer.transform(X_train[[col]])\n        X_test.loc[:, col] = num_imputer.transform(X_test[[col]]) \n    ```", "```py\n    for col in CAT_FEATURES:\n        cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n        cat_imputer.fit(X_train[[col]])\n        X_train.loc[:, col] = cat_imputer.transform(X_train[[col]])\n        X_test.loc[:, col] = cat_imputer.transform(X_test[[col]]) \n    ```", "```py\n    import pandas as pd\n    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n    from sklearn.compose import ColumnTransformer \n    ```", "```py\n    COL = \"education\"\n    X_train_copy = X_train.copy()\n    X_test_copy = X_test.copy()\n    label_enc = LabelEncoder()\n    label_enc.fit(X_train_copy[COL])\n    X_train_copy.loc[:, COL] = label_enc.transform(X_train_copy[COL])\n    X_test_copy.loc[:, COL] = label_enc.transform(X_test_copy[COL])\n    X_test_copy[COL].head() \n    ```", "```py\n    6907     3\n    24575    0\n    26766    3\n    2156     0\n    3179     3\n    Name: education, dtype: int64 \n    ```", "```py\n    cat_features = X_train.select_dtypes(include=\"object\") \\\n                          .columns \\\n                          .to_list()\n    cat_features \n    ```", "```py\n    ['sex', 'education',  'marriage', 'payment_status_sep', 'payment_status_aug', 'payment_status_jul', 'payment_status_jun', 'payment_status_may', 'payment_status_apr'] \n    ```", "```py\n    one_hot_encoder = OneHotEncoder(sparse=False,\n                                    handle_unknown=\"error\",\n                                    drop=\"first\") \n    ```", "```py\n    one_hot_transformer = ColumnTransformer(\n        [(\"one_hot\", one_hot_encoder, cat_features)],\n        remainder=\"passthrough\",\n        verbose_feature_names_out=False\n    ) \n    ```", "```py\n    one_hot_transformer.fit(X_train) \n    ```", "```py\n    col_names = one_hot_transformer.get_feature_names_out()\n    X_train_ohe = pd.DataFrame(\n        one_hot_transformer.transform(X_train), \n        columns=col_names, \n        index=X_train.index\n    )\n    X_test_ohe = pd.DataFrame(one_hot_transformer.transform(X_test),\n                              columns=col_names,\n                              index=X_test.index) \n    ```", "```py\npd.get_dummies(X_train, prefix_sep=\"_\", drop_first=True) \n```", "```py\none_hot_encoder = OneHotEncoder(\n    categories=[[\"Male\", \"Female\", \"Unknown\"]],\n    sparse=False,\n    handle_unknown=\"error\",\n    drop=\"first\"\n)\none_hot_transformer = ColumnTransformer(\n    [(\"one_hot\", one_hot_encoder, [\"sex\"])]\n)\none_hot_transformer.fit(X_train)\none_hot_transformer.get_feature_names_out() \n```", "```py\narray(['one_hot__sex_Female', 'one_hot__sex_Unknown'], dtype=object) \n```", "```py\nimport category_encoders as ce \n```", "```py\none_hot_encoder_ce = ce.OneHotEncoder(use_cat_names=True) \n```", "```py\none_hot_encoder_ce.fit(X_train)\nX_train_ce = one_hot_encoder_ce.transform(X_train) \n```", "```py\n    from sklearn.tree import DecisionTreeClassifier, plot_tree\n    from sklearn import metrics\n    from chapter_13_utils import performance_evaluation_report \n    ```", "```py\n    tree_classifier = DecisionTreeClassifier(random_state=42)\n    tree_classifier.fit(X_train_ohe, y_train)\n    y_pred = tree_classifier.predict(X_test_ohe) \n    ```", "```py\n    LABELS = [\"No Default\", \"Default\"]\n    tree_perf = performance_evaluation_report(tree_classifier,\n                                              X_test_ohe,\n                                              y_test, labels=LABELS,\n                                              show_plot=True) \n    ```", "```py\n    {'accuracy': 0.7141666666666666,\n     'precision': 0.3656509695290859,\n     'recall': 0.39788997739261495,\n     'specificity': 0.8039803124331265,\n     'f1_score': 0.3810898592565861,\n     'cohens_kappa': 0.1956931046277427,\n     'matthews_corr_coeff': 0.1959883714391891,\n     'roc_auc': 0.601583581287813,\n     'pr_auc': 0.44877724015824927,\n     'average_precision': 0.2789754297204212} \n    ```", "```py\n    plot_tree(tree_classifier, max_depth=3, fontsize=10) \n    ```", "```py\nplot_tree(\n    tree_classifier,\n    max_depth=2,\n    feature_names=X_train_ohe.columns,\n    class_names=[\"No default\", \"Default\"],\n    rounded=True,\n    filled=True,\n    fontsize=10\n) \n```", "```py\ny_pred_prob = tree_classifier.predict_proba(X_test_ohe)[:, 1]\nprecision, recall, _ = metrics.precision_recall_curve(y_test,\n                                                      y_pred_prob) \n```", "```py\nax = plt.subplot()\nax.plot(recall, precision,\n        label=f\"PR-AUC = {metrics.auc(recall, precision):.2f}\")\nax.set(title=\"Precision-Recall Curve\",\n       xlabel=\"Recall\",\n       ylabel=\"Precision\")\nax.legend() \n```", "```py\nax = metrics.PrecisionRecallDisplay.from_estimator(\n    tree_classifier, X_test_ohe, y_test\n)\nax.ax_.set_title(\"Precision-Recall Curve\") \n```", "```py\nfrom dtreeviz.trees import * \n```", "```py\nsmall_tree = DecisionTreeClassifier(max_depth=3,\n                                    random_state=42)\nsmall_tree.fit(X_train_ohe, y_train) \n```", "```py\nviz = dtreeviz(small_tree,\n               x_data=X_train_ohe,\n               y_data=y_train,\n               feature_names=X_train_ohe.columns,\n               target_name=\"Default\",\n               class_names=[\"No\", \"Yes\"],\n               title=\"Decision Tree - Loan default dataset\")\nviz \n```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.impute import SimpleImputer\n    from sklearn.preprocessing import OneHotEncoder\n    from sklearn.compose import ColumnTransformer\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.pipeline import Pipeline\n    from chapter_13_utils import performance_evaluation_report \n    ```", "```py\n    df = pd.read_csv(\"../Datasets/credit_card_default.csv\", \n                     na_values=\"\")\n    X = df.copy()\n    y = X.pop(\"default_payment_next_month\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, \n        test_size=0.2, \n        stratify=y, \n        random_state=42\n    ) \n    ```", "```py\n    num_features = X_train.select_dtypes(include=\"number\") \\\n                          .columns \\\n                          .to_list()\n    cat_features = X_train.select_dtypes(include=\"object\") \\\n                          .columns \\\n                          .to_list() \n    ```", "```py\n    num_pipeline = Pipeline(steps=[\n        (\"imputer\", SimpleImputer(strategy=\"median\"))\n    ]) \n    ```", "```py\n    cat_list = [\n        list(X_train[col].dropna().unique()) for col in cat_features\n    ]\n\n    cat_pipeline = Pipeline(steps=[\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", OneHotEncoder(categories=cat_list, sparse=False, \n                                 handle_unknown=\"error\", \n                                 drop=\"first\"))\n    ]) \n    ```", "```py\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"numerical\", num_pipeline, num_features),\n            (\"categorical\", cat_pipeline, cat_features)\n        ],\n        remainder=\"drop\"\n    ) \n    ```", "```py\n    dec_tree = DecisionTreeClassifier(random_state=42)\n    tree_pipeline = Pipeline(steps=[\n        (\"preprocessor\", preprocessor),\n        (\"classifier\", dec_tree)\n    ]) \n    ```", "```py\n    tree_pipeline.fit(X_train, y_train) \n    ```", "```py\n    LABELS = [\"No Default\", \"Default\"]\n    tree_perf = performance_evaluation_report(tree_pipeline, X_test,\n                                              y_test, labels=LABELS,\n                                              show_plot=True) \n    ```", "```py\n    from sklearn.base import BaseEstimator, TransformerMixin\n    import numpy as np \n    ```", "```py\n    class  OutlierRemover(BaseEstimator, TransformerMixin):\n        def  __init__(self, n_std=3):\n            self.n_std = n_std\n\n        def  fit(self, X, y = None):\n            if np.isnan(X).any(axis=None):\n                raise ValueError(\"\"\"Missing values in the array! \n     Please remove them.\"\"\")\n\n            mean_vec = np.mean(X, axis=0)\n            std_vec = np.std(X, axis=0)\n\n            self.upper_band_ = pd.Series(\n                mean_vec + self.n_std * std_vec\n            )\n            self.upper_band_ = (\n                self.upper_band_.to_frame().transpose()\n            )\n            self.lower_band_ = pd.Series(\n                mean_vec - self.n_std * std_vec\n            )\n            self.lower_band_ = (\n                self.lower_band_.to_frame().transpose()\n            )\n            self.n_features_ = len(self.upper_band_.columns)\n\n            return self \n\n        def  transform(self, X, y = None):\n            X_copy = pd.DataFrame(X.copy())\n\n            upper_band = pd.concat(\n                [self.upper_band_] * len(X_copy), \n                ignore_index=True\n            )\n            lower_band = pd.concat(\n                [self.lower_band_] * len(X_copy), \n                ignore_index=True\n            )\n\n            X_copy[X_copy >= upper_band] = upper_band\n            X_copy[X_copy <= lower_band] = lower_band\n\n            return X_copy.values \n    ```", "```py\n    num_pipeline = Pipeline(steps=[\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"outliers\", OutlierRemover())\n    ]) \n    ```", "```py\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"numerical\", num_pipeline, num_features),\n            (\"categorical\", cat_pipeline, cat_features)\n        ],\n        remainder=\"drop\"\n    )\n    dec_tree = DecisionTreeClassifier(random_state=42)\n    tree_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor),\n                                    (\"classifier\", dec_tree)])\n    tree_pipeline.fit(X_train, y_train)\n    tree_perf = performance_evaluation_report(tree_pipeline, X_test,\n                                              y_test, labels=LABELS,\n                                              show_plot=True) \n    ```", "```py\ntree_pipeline.named_steps \n```", "```py\ntree_pipeline.named_steps[\"classifier\"] \n```", "```py\n(\n    tree_pipeline\n    .named_steps[\"preprocessor\"]\n    .named_transformers_[\"numerical\"][\"outliers\"]\n    .upper_band_\n) \n```", "```py\n    from sklearn.model_selection import (\n        GridSearchCV, cross_val_score, \n        RandomizedSearchCV, cross_validate, \n        StratifiedKFold\n    )\n    from sklearn import metrics \n    ```", "```py\n    k_fold = StratifiedKFold(5, shuffle=True, random_state=42) \n    ```", "```py\n    cross_val_score(tree_pipeline, X_train, y_train, cv=k_fold) \n    ```", "```py\n    array([0.72333333, 0.72958333, 0.71375, 0.723125, 0.72]) \n    ```", "```py\n    cv_scores = cross_validate(\n        tree_pipeline, X_train, y_train, cv=k_fold, \n        scoring=[\"accuracy\", \"precision\", \"recall\", \n                 \"roc_auc\"]\n    )\n    pd.DataFrame(cv_scores) \n    ```", "```py\n    param_grid = {\n        \"classifier__criterion\": [\"entropy\", \"gini\"],\n        \"classifier__max_depth\": range(3, 11),\n        \"classifier__min_samples_leaf\": range(2, 11),\n        \"preprocessor__numerical__outliers__n_std\": [3, 4]\n    } \n    ```", "```py\n    classifier_gs = GridSearchCV(tree_pipeline, param_grid,\n                                 scoring=\"recall\", cv=k_fold,\n                                 n_jobs=-1, verbose=1)\n    classifier_gs.fit(X_train, y_train) \n    ```", "```py\n    Fitting 5 folds for each of 288 candidates, totalling 1440 fits \n    ```", "```py\n    Best parameters: {'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__min_samples_leaf': 7, 'preprocessor__numerical__outliers__n_std': 4}\n    Recall (Training set): 0.3858\n    Recall (Test set): 0.3775 \n    ```", "```py\n    LABELS = [\"No Default\", \"Default\"]\n    tree_gs_perf = performance_evaluation_report(\n        classifier_gs, X_test, \n        y_test, labels=LABELS, \n        show_plot=True\n    ) \n    ```", "```py\n    classifier_rs = RandomizedSearchCV(tree_pipeline, param_grid, \n                                       scoring=\"recall\", cv=k_fold, \n                                       n_jobs=-1, verbose=1, \n                                       n_iter=100, random_state=42)\n    classifier_rs.fit(X_train, y_train)\n    print(f\"Best parameters: {classifier_rs.best_params_}\")\n    print(f\"Recall (Training set): {classifier_rs.best_score_:.4f}\")\n    print(f\"Recall (Test set): {metrics.recall_score(y_test, classifier_rs.predict(X_test)):.4f}\") \n    ```", "```py\n    Fitting 5 folds for each of 100 candidates, totalling 500 fits \n    ```", "```py\n    Best parameters: {'preprocessor__numerical__outliers__n_std': 3, 'classifier__min_samples_leaf': 7, 'classifier__max_depth': 10, 'classifier__criterion': 'gini'}\n    Recall (Training set): 0.3854\n    Recall (Test set): 0.3760 \n    ```", "```py\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV \n```", "```py\nclassifier_sh = HalvingGridSearchCV(tree_pipeline, param_grid,\n                                    scoring=\"recall\", cv=k_fold,\n                                    n_jobs=-1, verbose=1,\n                                    min_resources=\"exhaust\", factor=3)\nclassifier_sh.fit(X_train, y_train) \n```", "```py\nn_iterations: 6\nn_required_iterations: 6\nn_possible_iterations: 6\nmin_resources_: 98\nmax_resources_: 24000\naggressive_elimination: False\nfactor: 3\n----------\niter: 0\nn_candidates: 288\nn_resources: 98\nFitting 5 folds for each of 288 candidates, totalling 1440 fits\n----------\niter: 1\nn_candidates: 96\nn_resources: 294\nFitting 5 folds for each of 96 candidates, totalling 480 fits\n----------\niter: 2\nn_candidates: 32\nn_resources: 882\nFitting 5 folds for each of 32 candidates, totalling 160 fits\n----------\niter: 3\nn_candidates: 11\nn_resources: 2646\nFitting 5 folds for each of 11 candidates, totalling 55 fits\n----------\niter: 4\nn_candidates: 4\nn_resources: 7938\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n----------\niter: 5\nn_candidates: 2\nn_resources: 23814\nFitting 5 folds for each of 2 candidates, totalling 10 fits \n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier \n```", "```py\nparam_grid = [\n    {\"classifier\": [RandomForestClassifier(random_state=42)],\n     \"classifier__n_estimators\": np.linspace(100, 500, 10, dtype=int),\n     \"classifier__max_depth\": range(3, 11),\n     \"preprocessor__numerical__outliers__n_std\": [3, 4]},\n    {\"classifier\": [DecisionTreeClassifier(random_state=42)],\n     \"classifier__criterion\": [\"entropy\", \"gini\"],\n     \"classifier__max_depth\": range(3, 11),\n     \"classifier__min_samples_leaf\": range(2, 11),\n     \"preprocessor__numerical__outliers__n_std\": [3, 4]}\n] \n```", "```py\nclassifier_gs_2 = GridSearchCV(tree_pipeline, param_grid, \n                               scoring=\"recall\", cv=k_fold, \n                               n_jobs=-1, verbose=1)\n\nclassifier_gs_2.fit(X_train, y_train)\n\nprint(f\"Best parameters: {classifier_gs_2.best_params_}\") \nprint(f\"Recall (Training set): {classifier_gs_2.best_score_:.4f}\") \nprint(f\"Recall (Test set): {metrics.recall_score(y_test, classifier_gs_2.predict(X_test)):.4f}\") \n```", "```py\nBest parameters: {'classifier': DecisionTreeClassifier(max_depth=10, min_samples_leaf=7, random_state=42), 'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__min_samples_leaf': 7, 'preprocessor__numerical__outliers__n_std': 4}\nRecall (Training set): 0.3858\nRecall (Test set): 0.3775 \n```", "```py\npd.DataFrame(classifier_gs_2.cv_results_).sort_values(\"rank_test_score\") \n```"]