["```py\nimport statsmodels.tsa.api as tsa\nindustrial_production = web.DataReader('IPGMFN', 'fred', '1988', '2017-12').squeeze()\ncomponents = tsa.seasonal_decompose(industrial_production, model='additive')\nts = (industrial_production.to_frame('Original')\n      .assign(Trend=components.trend)\n      .assign(Seasonality=components.seasonal)\n      .assign(Residual=components.resid))\nts.plot(subplots=True, figsize=(14, 8)); \n```", "```py\nfor p1 in range(4):                # AR order\n    for q1 in range(4):            # MA order\n        for p2 in range(3):        # seasonal AR order\n            for q2 in range(3):    # seasonal MA order\n                y_pred = []\n                for i, T in enumerate(range(train_size, len(data))):\n                    train_set = data.iloc[T - train_size:T]\n                    model = tsa.SARIMAX(endog=train_set, # model specification\n                                        order=(p1, 0, q1),\n                                        seasonal_order=(p2, 0, q2, 12)).fit()\n                    preds.iloc[i, 1] = model.forecast(steps=1)[0]\n                mse = mean_squared_error(preds.y_true, preds.y_pred)\n                results[(p1, q1, p2, q2)] = [np.sqrt(mse),\n                    preds.y_true.sub(preds.y_pred).std(),\n                    np.mean(aic)] \n```", "```py\n RMSE         AIC         BIC\np1 q1 p2 q2                                  \n2  3  1  0   0.009323 -772.247023 -752.734581\n3  2  1  0   0.009467 -768.844028 -749.331586\n2  2  1  0   0.009540 -770.904835 -754.179884\n   3  0  0   0.009773 -760.248885 -743.523935\n   2  0  0   0.009986 -758.775827 -744.838368 \n```", "```py\nbest_model = tsa.SARIMAX(endog=industrial_production_log_diff, order=(2, 0, 3),\n                         seasonal_order=(1, 0, 0, 12)).fit()\nprint(best_model.summary()) \n```", "```py\nnasdaq = web.DataReader('NASDAQCOM', 'fred', '2000', '2020').squeeze()\nnasdaq_returns = np.log(nasdaq).diff().dropna().mul(100) # rescale to facilitate optimization \n```", "```py\nplot_correlogram(nasdaq_returns.sub(nasdaq_returns.mean()).pow(2), lags=120,                  title='NASDAQ Daily Volatility') \n```", "```py\ntrainsize = 10 * 252  # 10 years\ndata = nasdaq_returns.clip(lower=nasdaq_returns.quantile(.05),\n                           upper=nasdaq_returns.quantile(.95))\nT = len(nasdaq_returns)\nresults = {}\nfor p in range(1, 5):\n    for q in range(1, 5):\n        print(f'{p} | {q}')\n        result = []\n        for s, t in enumerate(range(trainsize, T-1)):\n            train_set = data.iloc[s: t]\n            test_set = data.iloc[t+1]  # 1-step ahead forecast\n            model = arch_model(y=train_set, p=p, q=q).fit(disp='off')\n            forecast = model.forecast(horizon=1)\n            mu = forecast.mean.iloc[-1, 0]\n            var = forecast.variance.iloc[-1, 0]\n            result.append([(test_set-mu)**2, var])\n        df = pd.DataFrame(result, columns=['y_true', 'y_pred'])\n        results[(p, q)] = np.sqrt(mean_squared_error(df.y_true, df.y_pred)) \n```", "```py\nam = ConstantMean(nasdaq_returns.clip(lower=nasdaq_returns.quantile(.05),\n                                      upper=nasdaq_returns.quantile(.95)))\nam.volatility = GARCH(2, 0, 2)\nam.distribution = Normal()\nbest_model = am.fit(update_freq=5)\nprint(best_model.summary()) \n```", "```py\ndf = web.DataReader(['UMCSENT', 'IPGMFN'],\n                     'fred', '1970', '2017-12').dropna()\ndf.columns = ['sentiment', 'ip'] \n```", "```py\ndf_transformed = pd.DataFrame({'ip': np.log(df.ip).diff(12),\n                              'sentiment': df.sentiment.diff(12)}).dropna()\ntest_unit_root(df_transformed) # see notebook for details and additional plots\n          p-value\nip          0.0003\nsentiment   0.0000 \n```", "```py\nmodel = VARMAX(df_transformed.loc[:'2017'], order=(1,1),\n               trend='c').fit(maxiter=1000) \n```", "```py\npreds = model.predict(start=480, end=len(df_transformed)-1) \n```", "```py\ndef compute_pair_metrics(security, candidates):\n    security = security.div(security.iloc[0])\n    ticker = security.name\n    candidates = candidates.div(candidates.iloc[0])\n    # compute heuristics\n    spreads = candidates.sub(security, axis=0)\n    n, m = spreads.shape\n    X = np.ones(shape=(n, 2))\n    X[:, 1] = np.arange(1, n + 1)\n    drift = ((np.linalg.inv(X.T @ X) @ X.T @ spreads).iloc[1]\n             .to_frame('drift'))\n    vol = spreads.std().to_frame('vol')\n    corr_ret = (candidates.pct_change()\n                .corrwith(security.pct_change())\n                .to_frame('corr_ret'))\n    corr = candidates.corrwith(security).to_frame('corr')\n    metrics = drift.join(vol).join(corr).join(corr_ret).assign(n=n)\n    tests = []\n    # compute cointegration tests\n    for candidate, prices in candidates.items():\n        df = pd.DataFrame({'s1': security, 's2': prices})\n        var = VAR(df)\n        lags = var.select_order() # select VAR order\n        k_ar_diff = lags.selected_orders['aic']\n        # Johansen Test with constant Term and estd. lag order\n        cj0 = coint_johansen(df, det_order=0, k_ar_diff=k_ar_diff)\n        # Engle-Granger Tests\n        t1, p1 = coint(security, prices, trend='c')[:2]\n        t2, p2 = coint(prices, security, trend='c')[:2]\n        tests.append([ticker, candidate, t1, p1, t2, p2, \n                      k_ar_diff, *cj0.lr1])\n\n    return metrics.join(tests) \n```", "```py\nspreads['trace_sig'] = ((spreads.trace0 > trace0_cv) &\n                        (spreads.trace1 > trace1_cv)).astype(int)\nspreads['eg_sig'] = (spreads.p < .05).astype(int) \n```", "```py\ndef KFSmoother(prices):\n    \"\"\"Estimate rolling mean\"\"\"\n\n    kf = KalmanFilter(transition_matrices=np.eye(1),\n                      observation_matrices=np.eye(1),\n                      initial_state_mean=0,\n                      initial_state_covariance=1,\n                      observation_covariance=1,\n                      transition_covariance=.05)\n    state_means, _ = kf.filter(prices.values)\n    return pd.Series(state_means.flatten(),\n                     index=prices.index) \n```", "```py\ndef KFHedgeRatio(x, y):\n    \"\"\"Estimate Hedge Ratio\"\"\"\n    delta = 1e-3\n    trans_cov = delta / (1 - delta) * np.eye(2)\n    obs_mat = np.expand_dims(np.vstack([[x], [np.ones(len(x))]]).T, axis=1)\n    kf = KalmanFilter(n_dim_obs=1, n_dim_state=2,\n                      initial_state_mean=[0, 0],\n                      initial_state_covariance=np.ones((2, 2)),\n                      transition_matrices=np.eye(2),\n                      observation_matrices=obs_mat,\n                      observation_covariance=2,\n                      transition_covariance=trans_cov)\n    state_means, _ = kf.filter(y.values)\n    return -state_means \n```", "```py\ndef estimate_half_life(spread):\n    X = spread.shift().iloc[1:].to_frame().assign(const=1)\n    y = spread.diff().iloc[1:]\n    beta = (np.linalg.inv(X.T@X)@X.T@y).iloc[0]\n    halflife = int(round(-np.log(2) / beta, 0))\n    return max(halflife, 1) \n```", "```py\ndef get_spread(candidates, prices):\n    pairs, half_lives = [], []\n    periods = pd.DatetimeIndex(sorted(candidates.test_end.unique()))\n    start = time()\n    for p, test_end in enumerate(periods, 1):\n        start_iteration = time()\n        period_candidates = candidates.loc[candidates.test_end == test_end, \n                                          ['y', 'x']]\n        trading_start = test_end + pd.DateOffset(days=1)\n        t = trading_start - pd.DateOffset(years=2)\n        T = trading_start + pd.DateOffset(months=6) - pd.DateOffset(days=1)\n        max_window = len(prices.loc[t: test_end].index)\n        print(test_end.date(), len(period_candidates))\n        for i, (y, x) in enumerate(zip(period_candidates.y, \n                                       period_candidates.x), 1):\n            pair = prices.loc[t: T, [y, x]]\n            pair['hedge_ratio'] = KFHedgeRatio(\n                y=KFSmoother(prices.loc[t: T, y]),\n                x=KFSmoother(prices.loc[t: T, x]))[:, 0]\n            pair['spread'] = pair[y].add(pair[x].mul(pair.hedge_ratio))\n            half_life = estimate_half_life(pair.spread.loc[t: test_end])\n            spread = pair.spread.rolling(window=min(2 * half_life, \n                                                    max_window))\n            pair['z_score'] = pair.spread.sub(spread.mean()).div(spread.\nstd())\n            pairs.append(pair.loc[trading_start: T].assign(s1=y, s2=x, period=p, pair=i).drop([x, y], axis=1))\n            half_lives.append([test_end, y, x, half_life])\n    return pairs, half_lives \n```", "```py\ndef get_trades(data):\n    pair_trades = []\n    for i, ((period, s1, s2), pair) in enumerate(\n             data.groupby(['period', 's1', 's2']), 1):\n        if i % 100 == 0:\n            print(i)\n        first3m = pair.first('3M').index\n        last3m = pair.last('3M').index\n        entry = pair.z_score.abs() > 2\n        entry = ((entry.shift() != entry)\n                 .mul(np.sign(pair.z_score))\n                 .fillna(0)\n                 .astype(int)\n                 .sub(2))\n        exit = (np.sign(pair.z_score.shift().fillna(method='bfill'))\n                != np.sign(pair.z_score)).astype(int) - 1\n        trades = (entry[entry != -2].append(exit[exit == 0])\n                  .to_frame('side')\n                  .sort_values(['date', 'side'])\n                  .squeeze())\n        trades.loc[trades < 0] += 2\n        trades = trades[trades.abs().shift() != trades.abs()]\n        window = trades.loc[first3m.min():first3m.max()]\n        extra = trades.loc[last3m.min():last3m.max()]\n        n = len(trades)\n        if window.iloc[0] == 0:\n            if n > 1:\n                print('shift')\n                window = window.iloc[1:]\n        if window.iloc[-1] != 0:\n            extra_exits = extra[extra == 0].head(1)\n            if extra_exits.empty:\n                continue\n            else:\n                window = window.append(extra_exits)\n        trades = (pair[['s1', 's2', 'hedge_ratio', 'period', 'pair']]\n                  .join(window. to_frame('side'), how='right'))\n        trades.loc[trades.side == 0, 'hedge_ratio'] = np.nan\n        trades.hedge_ratio = trades.hedge_ratio.ffill()\n        pair_trades.append(trades)\n    return pair_trades \n```", "```py\n@dataclass\nclass Pair:\n    period: int\n    s1: str\n    s2: str\n    size1: float\n    size2: float\n    long: bool\n    hr: float\n    p1: float\n    p2: float\n    entry_date: date = None\n    exit_date: date = None\n    entry_spread: float = np.nan\n    exit_spread: float = np.nan\n    def compute_spread(self, p1, p2):\n        return p1 * self.size1 + p2 * self.size2\n    def compute_spread_return(self, p1, p2):\n        current_spread = self.compute_spread(p1, p2)\n        delta = self.entry_spread - current_spread\n        return (delta / (np.sign(self.entry_spread) *\n                         self.entry_spread)) \n```"]