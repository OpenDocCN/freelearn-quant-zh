["```py\nclass ExperienceReplay(object):                                   #1\n    def __init__(self, max_memory=100, discount=.9):\n        self.max_memory = max_memory                              #2\n        self.memory = []\n        self.discount = discount\n\n    def remember(self, states, game_over):                        #3\n        self.memory.append([states, game_over])\n        if len(self.memory) > self.max_memory:\n            del self.memory[0]                                    #4\n\n    def get_batch(self, model, batch_size=10):                    #5\n        len_memory = len(self.memory)                             #6\n        num_actions = model.output_shape[-1]\n        env_dim = self.memory[0][0][0].shape[1]\n\n        inputs = np.zeros((min(len_memory, batch_size), env_dim)) #7\n        targets = np.zeros((inputs.shape[0], num_actions))\n\n        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])):           #8\n            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]                               #9\n            game_over = self.memory[idx][1]\n\n            inputs[i:i+1] = state_t                               #10\n\n            targets[i] = model.predict(state_t)[0]                #11\n\n            Q_sa = np.max(model.predict(state_tp1)[0])            #12\n\n            if game_over:                                         #13\n                targets[i, action_t] = reward_t\n            else:\n                targets[i, action_t] = reward_t + self.discount * Q_sa\n        return inputs, targets\n```", "```py\n    [...[experience, game_over][experience, game_over]...]\n    ```", "```py\nnum_actions = 3\ngrid_size = 10\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(100, input_shape=(grid_size**2,), activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(num_actions))\nmodel.compile(optimizer='sgd', loss='mse')\n```", "```py\ndef train(model,epochs):\n    win_cnt = 0                                                 #1\n\n    win_hist = []\n\n    for e in range(epochs):                                     #2\n        loss = 0.\n        env.reset()\n        game_over = False\n        input_t = env.observe()\n\n        while not game_over:                                    #3\n            input_tm1 = input_t                                 #4\n\n            if np.random.rand() <= epsilon:                     #5\n                action = np.random.randint(0, num_actions, size=1)\n            else:\n                q = model.predict(input_tm1)                    #6\n                action = np.argmax(q[0])\n\n            input_t, reward, game_over = env.act(action)        #7\n            if reward == 1:\n                win_cnt += 1\n\n            exp_replay.remember([input_tm1, action, reward, input_t],game_over)            #8\n\n            inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)          #9\n\n            batch_loss = model.train_on_batch(inputs, targets)\n\n            loss += batch_loss\n\n        win_hist.append(win_cnt)\n    return win_hist\n```", "```py\npip install\n\n```", "```py\nimport gym                                           #1\n\nimport numpy as np                                   #2\n\nfrom scipy.stats import norm                         #3\nfrom keras.layers import Dense, Input, Lambda\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras import backend as K\n\nfrom collections import deque                        #4\nimport random\n```", "```py\ndef __init__(self, state_size, action_size):\n\n    self.state_size = state_size                       #1\n    self.action_size = action_size\n    self.value_size = 1\n\n    self.exp_replay = deque(maxlen=2000)               #2\n\n    self.actor_lr = 0.0001                             #3\n    self.critic_lr = 0.001\n    self.discount_factor = .9\n\n    self.actor, self.critic = self.build_model()       #4\n\n    self.optimize_actor = self.actor_optimizer()       #5\n    self.optimize_critic = self.critic_optimizer()\n```", "```py\ndef build_model(self):\n\n    state = Input(batch_shape=(None, self.state_size))          #1\n\n    actor_input = Dense(30,                                     #2 \n                        activation='relu',\n                        kernel_initializer='he_uniform')(state)\n\n    mu_0 = Dense(self.action_size,                              #3\n                 activation='tanh',\n                 kernel_initializer='he_uniform')(actor_input)\n\n    mu = Lambda(lambda x: x * 2)(mu_0)                          #4\n\n    sigma_0 = Dense(self.action_size,                           #5\n                    activation='softplus',\n                    kernel_initializer='he_uniform')(actor_input)\n\n    sigma = Lambda(lambda x: x + 0.0001)(sigma_0)               #6\n\n    critic_input = Dense(30,                                    #7\n                         activation='relu',\n                         kernel_initializer='he_uniform')(state)\n\n    state_value = Dense(1, kernel_initializer='he_uniform')(critic_input)                                                          #8\n\n    actor = Model(inputs=state, outputs=(mu, sigma))            #9\n    critic = Model(inputs=state, outputs=state_value)          #10\n\n    actor._make_predict_function()                             #11\n    critic._make_predict_function() \n\n    actor.summary()                                            #12\n    critic.summary()\n\n    return actor, critic                                       #13\n```", "```py\ndef actor_optimizer(self):\n    action = K.placeholder(shape=(None, 1))                     #1\n    advantages = K.placeholder(shape=(None, 1))\n\n    mu, sigma_sq = self.actor.output                            #2\n\n    pdf = 1\\. / K.sqrt(2\\. * np.pi * sigma_sq) * \\\n                      K.exp(-K.square(action - mu) / (2\\. * sigma_sq))                          #3\n\n    log_pdf = K.log(pdf + K.epsilon())                          #4\n\n    exp_v = log_pdf * advantages                                #5\n\n    entropy = K.sum(0.5 * (K.log(2\\. * np.pi * sigma_sq) + 1.))  #6\n    exp_v = K.sum(exp_v + 0.01 * entropy)                       #7\n    actor_loss = -exp_v                                         #8\n\n    optimizer = Adam(lr=self.actor_lr)                          #9\n\n    updates = optimizer.get_updates(self.actor.trainable_weights, [], actor_loss)            #10\n\n    train = K.function([self.actor.input, action, advantages], [], updates=updates)                       #11\n\n    return train                                               #12\n```", "```py\ndef critic_optimizer(self):\n    discounted_reward = K.placeholder(shape=(None, 1))          #1\n\n    value = self.critic.output\n\n    loss = K.mean(K.square(discounted_reward - value))          #2\n\n    optimizer = Adam(lr=self.critic_lr)                         #3\n\n    updates = optimizer.get_updates(self.critic.trainable_weights, [], loss)\n\n    train = K.function([self.critic.input, discounted_reward], \n                       [], \n                       updates=updates)                         #4\n\n    return train\n```", "```py\ndef get_action(self, state):\n    state = np.reshape(state, [1, self.state_size])            #1\n    mu, sigma_sq = self.actor.predict(state)                   #2\n    epsilon = np.random.randn(self.action_size)                #3\n    action = mu + np.sqrt(sigma_sq) * epsilon                  #4\n    action = np.clip(action, -2, 2)                            #5\n    return action\n```", "```py\ndef train_model(self, state, action, reward, next_state, done):\n    self.exp_replay.append((state, action, reward, next_state, done))              #1\n\n     (state, action, reward, next_state, done) = random.sample(self.exp_replay,1)[0]             #2\n    target = np.zeros((1, self.value_size))                 #3\n    advantages = np.zeros((1, self.action_size))\n\n    value = self.critic.predict(state)[0]                   #4\n    next_value = self.critic.predict(next_state)[0]\n\n    if done:                                                #5\n        advantages[0] = reward - value\n        target[0][0] = reward\n    else:\n        advantages[0] = reward + self.discount_factor * (next_value) - value\n        target[0][0] = reward + self.discount_factor * next_value\n\n    self.optimize_actor([state, action, advantages])        #6\n    self.optimize_critic([state, target])\n```", "```py\ndef run_experiment(render=False, agent=None, epochs = 3000):\n    env = gym.make('Pendulum-v0')                                   #1\n\n    state_size = env.observation_space.shape[0]                     #2\n    action_size = env.action_space.shape[0]\n\n    if agent = None:                                                #3\n        agent = A2CAgent(state_size, action_size)\n\n    scores = []                                                     #4\n\n    for e in range(epochs):                                         #5\n        done = False                                                #6\n        score = 0\n        state = env.reset()\n        state = np.reshape(state, [1, state_size])\n\n        while not done:                                             #7\n            if render:                                              #8\n                env.render()\n\n            action = agent.get_action(state)                        #9\n            next_state, reward, done, info = env.step(action)      #10\n            reward /= 10                                           #11\n            next_state = np.reshape(next_state, [1, state_size])               #12\n            agent.train_model(state, action, reward, next_state, done)                    #13\n\n            score += reward                                        #14\n            state = next_state                                     #15\n\n            if done:                                               #16\n                scores.append(score) \n                print(\"episode:\", e, \"  score:\", score)\n\n                if np.mean(scores[-min(10, len(scores)):]) > -20:  #17\n                    print('Solved Pendulum-v0 after {} iterations'.format(len(scores)))\n                    return agent, scores\n```", "```py\nclass TradeEnv():\n    def reset(self):\n        self.data = self.gen_universe()                        #1\n        self.pos = 0                                           #2\n        self.game_length = self.data.shape[0]                  #3\n        self.returns = []                                      #4\n\n        return self.data[0,:-1,:]                              #5\n\n    def step(self,allocation):                                 #6\n        ret = np.sum(allocation * self.data[self.pos,-1,:])    #7\n        self.returns.append(ret)                               #8\n        mean = 0                                               #9\n        std = 1\n        if len(self.returns) >= 20:                            #10\n            mean = np.mean(self.returns[-20:])\n            std = np.std(self.returns[-20:]) + 0.0001\n\n        sharpe = mean / std                                    #11\n\n        if (self.pos +1) >= self.game_length:                  #12\n            return None, sharpe, True, {}\n        else:                                                  #13\n            self.pos +=1\n            return self.data[self.pos,:-1,:], sharpe, False, {}\n\n    def gen_universe(self):                                   #14\n        stocks = os.listdir(DATA_PATH) \n        stocks = np.random.permutation(stocks)\n        frames = [] \n        idx = 0\n        while len(frames) < 100:                              #15\n            try: \n                stock = stocks[idx]\n                frame = pd.read_csv(os.path.join(DATA_PATH,stock),index_col='Date')\n                frame = frame.loc['2005-01-01':].Close\n                frames.append(frame)\n            except:\n                e = sys.exc_info()[0]\n            idx += 1 \n\n        df = pd.concat(frames,axis=1,ignore_index=False)      #16\n        df = df.pct_change()\n        df = df.fillna(0)\n        batch = df.values\n        episodes = []                                         #17\n        for i in range(batch.shape[0] - 101):\n            eps = batch[i:i+101]\n            episodes.append(eps)\n        data = np.stack(episodes)\n        assert len(data.shape) == 3 \n        assert data.shape[-1] == 100\n        return data\n```", "```py\ndef build_model(self):\n\tstate = Input(batch_shape=(None, #1\n\t\t\tself.state_seq_length,\n\t\t\tself.state_size))\nx = LSTM(120,return_sequences=True)(state) #2\nx = LSTM(100)(x)\nactor_input = Dense(100, activation='relu', #3\n\t\tkernel_initializer='he_uniform')(x)\nmu = Dense(self.action_size, activation='tanh', #4\n\t\tkernel_initializer='he_uniform')(actor_input)\nsigma_0 = Dense(self.action_size, activation='softplus',\n\t\t\tkernel_initializer='he_uniform')\n\t\t\t(actor_input)\nsigma = Lambda(lambda x: x + 0.0001)(sigma_0)\ncritic_input = Dense(30, activation='relu',\n\t\t\t\t\t\tkernel_initializer='he_uniform')(x)\nstate_value = Dense(1, activation='linear',\n\t\tkernel_initializer='he_uniform')\n\t\t(critic_input)\nactor = Model(inputs=state, outputs=(mu, sigma))\ncritic = Model(inputs=state, outputs=state_value)\n\nactor._make_predict_function()\ncritic._make_predict_function()\n\nactor.summary()\ncritic.summary()\n\nreturn actor, critic\n\n```", "```py\nsolution = np.array([0.5, 0.1, -0.3])\ndef f(w):\n  reward = -np.sum(np.square(solution - w))\n  return reward\n```", "```py\nnpop =   50 #1\nsigma = 0.1 #2\nalpha = 0.1 #3\n```", "```py\nw = np.random.randn(3)                          #1\nfor i in range(300):                            #2\n  N = np.random.randn(npop, 3) * sigma          #3 \n  R = np.zeros(npop)\n  for j in range(npop):                         #4\n    w_try = w + N[j]\n    R[j] = f(w_try)\n\n  A = (R - np.mean(R)) / np.std(R)              #5\n  w = w + alpha * np.dot(N.T, A)/npop           #6\n```"]