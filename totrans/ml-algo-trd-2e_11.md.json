["```py\nfrom sklearn.tree import DecisionTreeRegressor\n# configure regression tree\nregression_tree = DecisionTreeRegressor(criterion='mse',      \n                                        max_depth=6,         \n                                        min_samples_leaf=50)\n# Create training data\ny = data.target\nX = data.drop(target, axis=1)\nX2 = X.loc[:, ['t-1', 't-2']]\n# fit model\nregression_tree.fit(X=X2, y=y)\n# fit OLS model\nols_model = sm.OLS(endog=y, exog=sm.add_constant(X2)).fit() \n```", "```py\n# randomize train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n# configure & train tree learner\nclf = DecisionTreeClassifier(criterion='gini',\n                            max_depth=5,\n                            random_state=42)\nclf.fit(X=X_train, y=y_train)\n# Output:\nDecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n            splitter='best') \n```", "```py\ndot_data = export_graphviz(classifier,\n                           out_file=None, # save to file and convert to png\n                           feature_names=X.columns,\n                           class_names=['Down', 'Up'],\n                           max_depth=3,\n                           filled=True,\n                           rounded=True,\n                           special_characters=True)\ngraphviz.Source(dot_data) \n```", "```py\n# only keep probabilities for pos. class\ny_score = classifier.predict_proba(X=X_test)[:, 1] \n```", "```py\nroc_auc_score(y_score=y_score, y_true=y_test)\n0.6341 \n```", "```py\nreg_tree = DecisionTreeRegressor(random_state=42)\nparam_grid = {'max_depth': [2, 3, 4, 5, 6, 7, 8, 10, 12, 15],\n              'min_samples_leaf': [5, 25, 50, 100],\n              'max_features': ['sqrt', 'auto']} \n```", "```py\ncv = MultipleTimeSeriesCV(n_splits=10,\n                          train_period_length=60,\n                          test_period_length=6,\n                          lookahead=1) \n```", "```py\ndef rank_correl(y, y_pred):\n    return spearmanr(y, y_pred)[0]\nic = make_scorer(rank_correl) \n```", "```py\ngridsearch_reg = GridSearchCV(estimator=reg_tree,\n                          param_grid=param_grid,\n                          scoring=ic,\n                          n_jobs=-1,\n                          cv=cv,  # custom MultipleTimeSeriesSplit\n                          refit=True,\n                          return_train_score=True)\ngridsearch_reg.fit(X=X, y=y) \n```", "```py\ndef get_leaves_count(tree):\n    t = tree.tree_\n    n = t.node_count\n    leaves = len([i for i in range(t.node_count) if t.children_left[i]== -1])\n    return leaves \n```", "```py\ntrain_scores, val_scores, leaves = {}, {}, {}\nfor max_depth in range(1, 26):\n    print(max_depth, end=' ', flush=True)\n    clf = DecisionTreeClassifier(criterion='gini', \n                                 max_depth=max_depth,\n                                 min_samples_leaf=10,\n                                 max_features='auto',\n                                 random_state=42)\n    train_scores[max_depth], val_scores[max_depth] = [], [] \n    leaves[max_depth] = []\n    for train_idx, test_idx in cv.split(X):\n        X_train, = X.iloc[train_idx], \n        y_train  = y_binary.iloc[train_ idx]\n        X_test, y_test = X.iloc[test_idx], y_binary.iloc[test_idx]\n        clf.fit(X=X_train, y=y_train)\n        train_pred = clf.predict_proba(X=X_train)[:, 1]\n        train_score = roc_auc_score(y_score=train_pred, y_true=y_train)\n        train_scores[max_depth].append(train_score)\n        test_pred = clf.predict_proba(X=X_test)[:, 1]\n        val_score = roc_auc_score(y_score=test_pred, y_true=y_test)\n        val_scores[max_depth].append(val_score)    \n        leaves[max_depth].append(get_leaves_count(clf)) \n```", "```py\nnoise = .5  # noise relative to std(y)\nnoise = y.std() * noise\nX_test = choice(x, size=test_size, replace=False)\nmax_depth = 10\nn_estimators=10\ntree = DecisionTreeRegressor(max_depth=max_depth)\nbagged_tree = BaggingRegressor(base_estimator=tree, n_estimators=n_estimators)\nlearners = {'Decision Tree': tree, 'Bagging Regressor': bagged_tree}\npredictions = {k: pd.DataFrame() for k, v in learners.items()}\nfor i in range(reps):\n    X_train = choice(x, train_size)\n    y_train = f(X_train) + normal(scale=noise, size=train_size)\n    for label, learner in learners.items():\n        learner.fit(X=X_train.reshape(-1, 1), y=y_train)\n        preds = pd.DataFrame({i: learner.predict(X_test.reshape(-1, 1))},\n                              index=X_test)\n        predictions[label] = pd.concat([predictions[label], preds], axis=1) \n```", "```py\nrf_clf = RandomForestClassifier(n_estimators=100,\n                                criterion='gini',\n                                max_depth=None,\n                                min_samples_split=2,\n                                min_samples_leaf=1,\n                                min_weight_fraction_leaf=0.0,\n                                max_features='auto',\n                                max_leaf_nodes=None,\n                                min_impurity_decrease=0.0,\n                                min_impurity_split=None,\n                                bootstrap=True, oob_score=False,\n                                n_jobs=-1, random_state=42) \n```", "```py\ncv = MultipleTimeSeriesCV(n_splits=10, train_period_length=60,\n                          test_period_length=6, lookahead=1)\nclf = RandomForestClassifier(random_state=42, n_jobs=-1)\nparam_grid = {'n_estimators': [50, 100, 250],\n              'max_depth': [5, 15, None],\n              'min_samples_leaf': [5, 25, 100]} \n```", "```py\ngridsearch_clf = GridSearchCV(estimator=clf,\n                              param_grid=param_grid,\n                              scoring='roc_auc',\n                              n_jobs=-1,\n                              cv=cv,\n                              refit=True,\n                              return_train_score=True,\n                              verbose=1) \n```", "```py\nDATA_DIR = Path('..', 'data')\nprices = (pd.read_hdf(DATA_DIR / 'assets.h5', 'stooq/jp/tse/stocks/prices')\n          .loc[idx[:, '2010': '2017'], :])\ndollar_vol = prices.close.mul(prices.volume)\ndollar_vol_rank = dollar_vol.groupby(level='date').rank(ascending=False)\nuniverse = dollar_vol_rank.groupby(level='symbol').mean().nsmallest(250).index \n```", "```py\ntrain_lengths = [1260, 756, 252, 126, 63]\ntest_lengths = [5, 21, 63]\ntest_params = list(product(train_lengths, test_lengths))\nn = len(test_params)\ntest_param_sample = np.random.choice(list(range(n)), \n                                     size=int(n), \n                                     replace=False)\ntest_params = [test_params[i] for i in test_param_sample] \n```", "```py\nbase_params = dict(boosting_type='rf',\n                   objective='regression',\n                   bagging_freq=1) \n```", "```py\nbagging_fraction_opts = [.5, .75, .95]\nfeature_fraction_opts = [.75, .95]\nmin_data_in_leaf_opts = [250, 500, 1000]\ncv_params = list(product(bagging_fraction_opts,\n                         feature_fraction_opts,\n                         min_data_in_leaf_opts))\nn_cv_params = len(cv_params) \n```", "```py\ncategoricals = ['year', 'weekday', 'month']\nfor feature in categoricals:\n    data[feature] = pd.factorize(data[feature], sort=True)[0] \n```", "```py\nfor train_length, test_length in test_params:\n    n_splits = int(2 * YEAR / test_length)\n    cv = MultipleTimeSeriesCV(n_splits=n_splits,\n                              test_period_length=test_length,\n                              lookahead=lookahead,\n                              train_period_length=train_length)\n    label = label_dict[lookahead]\n    outcome_data = data.loc[:, features + [label]].dropna()\n    lgb_data = lgb.Dataset(data=outcome_data.drop(label, axis=1),\n                           label=outcome_data[label],\n                           categorical_feature=categoricals,\n                           free_raw_data=False) \n```", "```py\n    for p, (bagging_fraction, feature_fraction, min_data_in_leaf) \\\n            in enumerate(cv_params_):\n        params = base_params.copy()\n        params.update(dict(bagging_fraction=bagging_fraction,\n                           feature_fraction=feature_fraction,\n                           min_data_in_leaf=min_data_in_leaf))\n        start = time()\n        cv_preds, nrounds = [], []\n        for i, (train_idx, test_idx) in \\\n                enumerate(cv.split(X=outcome_data)):\n            lgb_train = lgb_data.subset(train_idx.tolist()).construct()\n            lgb_test = lgb_data.subset(test_idx.tolist()).construct()\n            model = lgb.train(params=params,\n                              train_set=lgb_train,\n                              num_boost_round=num_boost_round,\n                              verbose_eval=False)\n            test_set = outcome_data.iloc[test_idx, :]\n            X_test = test_set.loc[:, model.feature_name()]\n            y_test = test_set.loc[:, label]\n            y_pred = {str(n): model.predict(X_test, num_iteration=n)\n                      for n in num_iterations}\n            cv_preds.append(y_test.to_frame('y_test')\n                            .assign(**y_pred).assign(i=i))\n            nrounds.append(model.best_iteration) \n    ```", "```py\n    df = [by_day.apply(lambda x: spearmanr(x.y_test,\n                                           x[str(n)])[0]).to_frame(n)\n          for n in num_iterations]\n    ic_by_day = pd.concat(df, axis=1)\n    daily_ic.append(ic_by_day.assign(bagging_fraction=bagging_fraction,\n                                     feature_fraction=feature_fraction,\n                                     min_data_in_leaf=min_data_in_leaf))\n    cv_ic = [spearmanr(cv_preds.y_test, cv_preds[str(n)])[0]\n             for n in num_iterations]\n    ic.append([bagging_fraction, feature_fraction,\n               min_data_in_leaf, lookahead] + cv_ic) \n    ```"]