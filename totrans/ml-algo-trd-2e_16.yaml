- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word Embeddings for Earnings Calls and SEC Filings
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the two previous chapters, we converted text data into a numerical format
    using the **bag-of-words model**. The result is sparse, fixed-length vectors that
    represent documents in high-dimensional word space. This allows the similarity
    of documents to be evaluated and creates features to train a model with a view
    to classifying a document's content or rating the sentiment expressed in it. However,
    these vectors ignore the context in which a term is used so that two sentences
    containing the same words in a different order would be encoded by the same vector,
    even if their meaning is quite different.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces an alternative class of algorithms that use **neural
    networks** to learn a vector representation of individual semantic units like
    a word or a paragraph. These vectors are dense rather than sparse, have a few
    hundred real-valued entries, and are called **embeddings** because they assign
    each semantic unit a location in a continuous vector space. They result from training
    a model to **predict tokens from their context** so that similar usage implies
    a similar embedding vector. Moreover, the embeddings encode semantic aspects like
    relationships among words by means of their relative location. As a result, they
    are powerful features for deep learning models for solving tasks that require
    semantic information, such as machine translation, question answering, or maintaining
    a dialogue.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: To develop a **trading strategy based on text data**, we are usually interested
    in the meaning of documents rather than individual tokens. For example, we might
    want to create a dataset that uses features representing a tweet or a news article
    with sentiment information (refer to *Chapter 14*, *Text Data for Trading – Sentiment
    Analysis*), or an asset's return for a given horizon after publication. Although
    the bag-of-words model loses plenty of information when encoding text data, it
    has the advantage of representing an entire document. However, word embeddings
    have been further developed to represent more than individual tokens. Examples
    include the **doc2vec** extension, which resorts to weighting word embeddings.
    More recently, the **attention** mechanism emerged to produce more context-sensitive
    sentence representations, resulting in **transformer** architectures such as the
    **BERT** family of models that has dramatically improved performance on numerous
    natural language tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, after working through this chapter and the companion notebooks,
    you will know about the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: What word embeddings are, how they work, and why they capture semantic information
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to obtain and use pretrained word vectors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which network architectures are most effective at training word2vec models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train a word2vec model using Keras, Gensim, and TensorFlow
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing and evaluating the quality of word vectors
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train a word2vec model on SEC filings to predict stock price moves
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How doc2vec extends word2vec and can be used for sentiment analysis
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: doc2vec如何扩展word2vec并用于情感分析
- en: Why the transformer's attention mechanism had such an impact on natural language
    processing
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么transformer的注意力机制对自然语言处理产生了如此大的影响
- en: How to fine-tune pretrained BERT models on financial data and extract high-quality
    embeddings
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在金融数据上微调预训练的BERT模型并提取高质量的嵌入
- en: You can find the code examples and links to additional resources in the GitHub
    directory for this chapter. This chapter uses neural networks and deep learning;
    if unfamiliar, you may want to first read *Chapter 17*, *Deep Learning for Trading,*
    which introduces key concepts and libraries.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本章的GitHub目录中找到代码示例和额外资源的链接。本章使用神经网络和深度学习；如果不熟悉，您可能需要先阅读*第17章*，*交易的深度学习*，介绍了关键概念和库。
- en: How word embeddings encode semantics
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词嵌入如何编码语义
- en: The bag-of-words model represents documents as sparse, high-dimensional vectors
    that reflect the tokens they contain. Word embeddings represent tokens as dense,
    lower-dimensional vectors so that the relative location of words reflects how
    they are used in context. They embody the **distributional hypothesis** from linguistics
    that claims words are best defined by the company they keep.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型将文档表示为稀疏的、高维度的向量，反映了它们包含的标记。而词嵌入则将标记表示为稠密、低维度的向量，以便于单词的相对位置反映它们在上下文中的使用方式。它们体现了语言学中的**分布假设**，该假设认为单词最好是通过其周围的上下文来定义。
- en: Word vectors are capable of capturing numerous semantic aspects; not only are
    synonyms assigned nearby embeddings, but words can have multiple degrees of similarity.
    For example, the word "driver" could be similar to "motorist" or to "factor."
    Furthermore, embeddings encode relationships among pairs of words like analogies
    (*Tokyo is to Japan* what *Paris is to France*, or *went is to go* what *saw is
    to see*), as we will illustrate later in this section.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 单词向量能够捕捉许多语义方面；不仅会为同义词分配附近的嵌入，而且单词可以具有多个相似度程度。例如，“driver”一词可能类似于“motorist”或“factor”。此外，嵌入还编码了词对之间的关系，如类比（*东京是日本*，*巴黎是法国*，或者*went是go的过去式*，*saw是see的过去式*），我们将在本节后面进行说明。
- en: Embeddings result from training a neural network to predict words from their
    context or vice versa. In this section, we will introduce how these models work
    and present successful approaches, including word2vec, doc2vec, and the more recent
    transformer family of models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是通过训练神经网络来预测单词与其上下文的关系或反之得到的。在本节中，我们将介绍这些模型的工作原理，并介绍成功的方法，包括word2vec、doc2vec以及更近期的transformer系列模型。
- en: How neural language models learn usage in context
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经语言模型如何学习上下文中的用法
- en: Word embeddings result from training a shallow neural network to predict a word
    given its context. Whereas traditional language models define context as the words
    preceding the target, word embedding models use the words contained in a symmetric
    window surrounding the target. In contrast, the bag-of-words model uses the entire
    document as context and relies on (weighted) counts to capture the co-occurrence
    of words.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入来自于训练一个浅层神经网络来预测给定上下文的单词。而传统的语言模型将上下文定义为目标单词之前的单词，而单词嵌入模型使用包围目标的对称窗口中的单词。相比之下，词袋模型使用整个文档作为上下文，并依赖（加权的）计数来捕捉单词的共现关系。
- en: Earlier neural language models used included nonlinear hidden layers that increased
    the computational complexity. **word2vec**, introduced by Mikolov, Sutskever,
    et al. (2013) and its extensions simplified the architecture to enable training
    on large datasets. The Wikipedia corpus, for example, contains over 2 billion
    tokens. (Refer to *Chapter 17*, *Deep Learning for Trading*, for additional details
    on feedforward networks.)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的神经语言模型使用了增加了计算复杂度的非线性隐藏层。而由Mikolov、Sutskever等人（2013）介绍的word2vec及其扩展简化了架构，使其能够在大型数据集上进行训练。例如，维基百科语料库包含超过20亿个标记。（有关前馈网络的详细信息，请参阅*第17章*，*交易的深度学习*。）
- en: word2vec – scalable word and phrase embeddings
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: word2vec – 可扩展的单词和短语嵌入
- en: 'A word2vec model is a two-layer neural net that takes a text corpus as input
    and outputs a set of embedding vectors for words in that corpus. There are two
    different architectures, shown in the following diagram, to efficiently learn
    word vectors using shallow neural networks (Mikolov, Chen, et al., 2013):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec模型是一个两层的神经网络，它以文本语料库作为输入，并为该语料库中的单词输出一组嵌入向量。有两种不同的架构，如下图所示，以有效地使用浅层神经网络学习单词向量（Mikolov、Chen等，2013）：
- en: The **continuous-bag-of-words** (**CBOW**) model predicts the target word using
    the average of the context word vectors as input so that their order does not
    matter. CBOW trains faster and tends to be slightly more accurate for frequent
    terms, but pays less attention to infrequent words.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续词袋**（**CBOW**）模型使用上下文词向量的平均值作为输入来预测目标词，因此它们的顺序并不重要。CBOW 训练速度更快，对于频繁出现的术语可能略微更准确，但对不常见的词注意力较少。'
- en: The **skip-gram** (**SG**) model, in contrast, uses the target word to predict
    words sampled from the context. It works well with small datasets and finds good
    representations even for rare words or phrases.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，**跳字**（**SG**）模型使用目标词来预测从上下文中采样的词。它在小型数据集上表现良好，并且即使对于罕见的词或短语也能找到良好的表示。
- en: '![](img/B15439_16_01.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_01.png)'
- en: 'Figure 16.1: Continuous-bag-of-words versus skip-gram processing logic'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.1：连续词袋与跳字处理逻辑
- en: The model receives an embedding vector as input and computes the dot product
    with another embedding vector. Note that, assuming normed vectors, the dot product
    is maximized (in absolute terms) when vectors are equal, and minimized when they
    are orthogonal.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 模型接收一个嵌入向量作为输入，并与另一个嵌入向量计算点积。请注意，假设向量已被规范化，则当向量相等时点积被最大化（绝对值），当它们正交时则被最小化。
- en: During training, the **backpropagation algorithm** adjusts the embedding weights
    in response to the loss computed by an objective function based on classification
    errors. We will see in the next section how word2vec computes the loss.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，**反向传播算法**根据基于分类错误的目标函数计算的损失调整嵌入权重。我们将在下一节中看到 word2vec 如何计算损失。
- en: Training proceeds by sliding the **context window** over the documents, typically
    segmented into sentences. Each complete iteration over the corpus is called an
    **epoch**. Depending on the data, several dozen epochs may be necessary for vector
    quality to converge.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 训练通过在文档上滑动**上下文窗口**进行，通常被分段为句子。对语料库的每次完整迭代称为一个**时代**。根据数据，可能需要几十个时代才能使向量质量收敛。
- en: The skip-gram model implicitly factorizes a word-context matrix that contains
    the pointwise mutual information of the respective word and context pairs (Levy
    and Goldberg, 2014).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 跳字模型隐式因式分解一个包含各个单词和上下文对的点间互信息的词-上下文矩阵（Levy 和 Goldberg，2014）。
- en: Model objective – simplifying the softmax
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型目标 – 简化 softmax
- en: 'Word2vec models aim to predict a single word out of a potentially very large
    vocabulary. Neural networks often use the softmax function as an output unit in
    the final layer to implement the multiclass objective because it maps an arbitrary
    number of real values to an equal number of probabilities. The softmax function
    is defined as follows, where *h* refers to the embedding and *v* to the input
    vectors, and *c* is the context of word *w*:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 模型旨在预测一个潜在非常庞大的词汇中的单个词。神经网络通常在最后一层使用 softmax 函数作为输出单元实现多类目标，因为它将任意数量的实值映射到相等数量的概率。softmax
    函数定义如下，其中 *h* 指代嵌入，*v* 指代输入向量，*c* 是单词 *w* 的上下文：
- en: '![](img/B15439_16_001.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_001.png)'
- en: 'However, the softmax complexity scales with the number of classes because the
    denominator requires computing the dot product for all words in the vocabulary
    to standardize the probabilities. Word2vec gains efficiency by using a modified
    version of the softmax or sampling-based approximations:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于 softmax 的复杂度与类别数量成比例，因为分母需要计算整个词汇表中所有单词的点积以标准化概率。Word2vec 通过使用 softmax
    的修改版本或基于采样的逼近来提高效率：
- en: The **hierarchical softmax** organizes the vocabulary as a binary tree with
    words as leaf nodes. The unique path to each node can be used to compute the word
    probability (Morin and Bengio, 2005).
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分层 softmax** 将词汇组织为具有单词作为叶节点的二叉树。到达每个节点的唯一路径可用于计算单词概率（Morin 和 Bengio，2005）。'
- en: '**Noise contrastive estimation** (**NCE**) samples out-of-context "noise words"
    and approximates the multiclass task by a binary classification problem. The NCE
    derivative approaches the softmax gradient as the number of samples increases,
    but as few as 25 samples can yield convergence similar to the softmax 45 times
    faster (Mnih and Kavukcuoglu, 2013).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声对比估计**（**NCE**）对上下文之外的“噪声词”进行采样，并将多类任务近似为二元分类问题。随着样本数量的增加，NCE 导数逼近 softmax
    梯度，但仅需 25 个样本即可获得与 softmax 相似的收敛速度增加 45 倍的收敛速度（Mnih 和 Kavukcuoglu，2013）。'
- en: '**Negative sampling** (**NEG**) omits the noise word samples to approximate
    NCE and directly maximizes the probability of the target word. Hence, NEG optimizes
    the semantic quality of embedding vectors (similar vectors for similar usage)
    rather than the accuracy on a test set. It may, however, produce poorer representations
    for infrequent words than the hierarchical softmax objective (Mikolov et al.,
    2013).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating phrase detection
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Preprocessing typically involves phrase detection, that is, the identification
    of tokens that are commonly used together and should receive a single vector representation
    (for example, New York City; refer to the discussion of n-grams in *Chapter 13*,
    *Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning*).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'The original word2vec authors (Mikolov et al., 2013) use a simple lift scoring
    method that identifies two words *w*[i], *w*[j] as a bigram if their joint occurrence
    exceeds a given threshold relative to each word''s individual appearance, corrected
    by a discount factor, *δ*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_16_002.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: The scorer can be applied repeatedly to identify successively longer phrases.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative is the normalized pointwise mutual information score, which
    is more accurate, but also more costly to compute. It uses the relative word frequency
    *P*(*w*) and varies between +1 and -1:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_16_003.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Evaluating embeddings using semantic arithmetic
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The bag-of-words model creates document vectors that reflect the presence and
    relevance of tokens to the document. As discussed in *Chapter 15*, *Topic Modeling
    – Summarizing Financial News*, **latent semantic analysis** reduces the dimensionality
    of these vectors and identifies what can be interpreted as latent concepts in
    the process. **Latent Dirichlet allocation** represents both documents and terms
    as vectors that contain the weights of latent topics.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: The word and phrase vectors produced by word2vec do not have an explicit meaning.
    However, the **embeddings encode similar usage as proximity** in the latent space
    created by the model. The embeddings also capture semantic relationships so that
    analogies can be expressed by adding and subtracting word vectors.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 16.2* shows how the vector that points from "Paris" to "France" (which
    measures the difference between their embedding vectors) reflects the "capital
    of" relationship. The analogous relationship between London and the UK corresponds
    to the same vector: the embedding for the term "UK" is very close to the location
    obtained by adding the "capital of" vector to the embedding for the term "London":'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_16_02.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.2: Embedding vector arithmetic'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Just as words can be used in different contexts, they can be related to other
    words in different ways, and these relationships correspond to different directions
    in the latent space. Accordingly, there are several types of analogies that the
    embeddings should reflect if the training data permits.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'The word2vec authors provide a list of over 25,000 relationships in 14 categories
    spanning aspects of geography, grammar and syntax, and family relationships to
    evaluate the quality of embedding vectors. As illustrated in the preceding diagram,
    the test validates that the target word "UK" is closest to the result of adding
    the vector that represents an analogous relationship "Paris: France" to the target''s
    complement "London".'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec 的作者提供了一个包含14个类别的超过25,000个关系列表，涵盖地理、语法和句法以及家庭关系的方面，以评估嵌入向量的质量。正如前面的图表所示，该测试验证了目标词“UK”与添加代表类似关系“巴黎：法国”的向量到目标的补集“伦敦”之间的最近距离。
- en: The following table shows the number of samples and illustrates some of the
    analogy categories. The test checks how close the embedding for *d* is to the
    location determined by *c + (b-a)*. Refer to the `evaluating_embeddings` notebook
    for implementation details.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了示例数，并说明了一些类比类别。该测试检查* d *的嵌入距离* c +（b-a）*确定的位置有多接近。有关实现细节，请参阅`evaluating_embeddings`笔记本。
- en: '| Category | # Samples | a | b | c | d |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | # 示例 | a | b | c | d |'
- en: '| Capital-Country | 506 | athens | greece | baghdad | iraq |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 首都-国家 | 506 | 雅典 | 希腊 | 巴格达 | 伊拉克 |'
- en: '| City-State | 4,242 | chicago | illinois | houston | texas |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 城市-州 | 4,242 | 芝加哥 | 伊利诺伊州 | 休斯顿 | 德克萨斯州 |'
- en: '| Past Tense | 1,560 | dancing | danced | decreasing | decreased |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 过去时 | 1,560 | 跳舞 | 跳舞 | 减少 | 减少 |'
- en: '| Plural | 1,332 | banana | bananas | bird | birds |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 复数 | 1,332 | 香蕉 | 香蕉 | 鸟 | 鸟类 |'
- en: '| Comparative | 1,332 | bad | worse | big | bigger |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 比较 | 1,332 | 坏 | 更坏 | 大 | 更大 |'
- en: '| Opposite | 812 | acceptable | unacceptable | aware | unaware |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 相反 | 812 | 可接受 | 不可接受 | 察觉到 | 未察觉到 |'
- en: '| Superlative | 1,122 | bad | worst | big | biggest |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 最高级 | 1,122 | 坏 | 最坏 | 大 | 最大 |'
- en: '| Plural (Verbs) | 870 | decrease | decreases | describe | describes |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 复数（动词） | 870 | 减少 | 减少 | 描述 | 描述 |'
- en: '| Currency | 866 | algeria | dinar | angola | kwanza |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 货币 | 866 | 阿尔及利亚 | 阿尔及利亚第纳尔 | 安哥拉 | 安哥拉宽扎 |'
- en: '| Family | 506 | boy | girl | brother | sister |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 家庭 | 506 | 男孩 | 女孩 | 兄弟 | 姐妹 |'
- en: 'Similar to other unsupervised learning techniques, the goal of learning embedding
    vectors is to generate features for other tasks, such as text classification or
    sentiment analysis. There are a couple of options to obtain embedding vectors
    for a given corpus of documents:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他无监督学习技术类似，学习嵌入向量的目标是为其他任务生成特征，例如文本分类或情感分析。有几种获得给定文档语料库的嵌入向量的选项：
- en: Use pretrained embeddings learned from a generic large corpus like Wikipedia
    or Google News
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用从通用大语料库（如维基百科或Google新闻）学习的预训练嵌入
- en: Train your own model using documents that reflect a domain of interest
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反映感兴趣领域的文档来训练您自己的模型
- en: The less generic and more specialized the content of the subsequent text modeling
    task, the more preferable the second approach. However, quality word embeddings
    are data-hungry and require informative documents containing hundreds of millions
    of words.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 后续文本建模任务的内容越不通用和更专业化，第二种方法就越可取。然而，高质量的单词嵌入需要数据丰富的信息性文档，其中包含数亿字的单词。
- en: We will first look at how you can use pretrained vectors and then demonstrate
    examples of how to build your own word2vec models using financial news and SEC
    filings data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将看看如何使用预训练向量，然后演示如何使用金融新闻和SEC备案数据构建自己的word2vec模型的示例。
- en: How to use pretrained word vectors
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用预训练单词向量
- en: There are several sources for pretrained word embeddings. Popular options include
    Stanford's GloVE and spaCy's built-in vectors (refer to the `using_pretrained_vectors`
    notebook for details). In this section, we will focus on GloVe.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个预训练单词嵌入的来源。流行的选项包括斯坦福的GloVE和spaCy内置的向量（有关详细信息，请参阅`using_pretrained_vectors`笔记本）。在本节中，我们将重点放在GloVe上。
- en: GloVe – Global vectors for word representation
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GloVe – 用于单词表示的全局向量
- en: 'GloVe (*Global Vectors for Word Representation*, Pennington, Socher, and Manning,
    2014) is an unsupervised algorithm developed at the Stanford NLP lab that learns
    vector representations for words from aggregated global word-word co-occurrence
    statistics (see resources linked on GitHub). Vectors pretrained on the following
    web-scale sources are available:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe（*全球单词表示向量*，Pennington、Socher和Manning，2014）是斯坦福NLP实验室开发的无监督算法，它从聚合的全局词-词共现统计中学习单词的向量表示（请参阅GitHub上链接的资源）。可用以下网络规模的预训练向量：
- en: '**Common Crawl** with 42 billion or 840 billion tokens and a vocabulary or
    1.9 million or 2.2 million tokens'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Common Crawl**，包含420亿或840亿令牌和190万或220万令牌的词汇量'
- en: '**Wikipedia** 2014 + Gigaword 5 with 6 billion tokens and a vocabulary of 400,000
    tokens'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**维基百科** 2014 + Gigaword 5，拥有 60 亿个标记和 40 万个标记的词汇表'
- en: '**Twitter** using 2 billion tweets, 27 billion tokens, and a vocabulary of
    1.2 million tokens'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Twitter** 使用了 20 亿条推文，27 亿个标记，以及一个包含 120 万个标记的词汇表。'
- en: 'We can use Gensim to convert the vector text files using `glove2word2vec` and
    then load them into the `KeyedVector` object:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Gensim 将文本文件转换为向量，使用 `glove2word2vec` 然后将它们加载到 `KeyedVector` 对象中：
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Gensim uses the **word2vec analogy tests** described in the previous section
    using text files made available by the authors to evaluate word vectors. For this
    purpose, the library has the `wv.accuracy` function, which we use to pass the
    path to the analogy file, indicate whether the word vectors are in binary format,
    and whether we want to ignore the case. We can also restrict the vocabulary to
    the most frequent to speed up testing:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim 使用了前面描述的 word2vec 类比测试，使用了作者提供的文本文件来评估词向量。为此，该库具有 `wv.accuracy` 函数，我们使用它传递类比文件的路径，指示词向量是否以二进制格式存储，以及是否要忽略大小写。我们还可以将词汇表限制为最常见的以加快测试速度。
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The word vectors trained on the Wikipedia corpus cover all analogies and achieve
    an overall accuracy of 75.44 percent with some variation across categories:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在维基百科语料库上训练的词向量覆盖了所有类比，并在各类别之间存在一定的变化，总体准确率为 75.44%：
- en: '| Category | # Samples | Accuracy |  | Category | # Samples | Accuracy |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | # 样本 | 准确度 |  | 类别 | # 样本 | 准确度 |'
- en: '| Capital-Country | 506 | 94.86% |  | Comparative | 1,332 | 88.21% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 国家首都 | 506 | 94.86% |  | 比较级 | 1,332 | 88.21% |'
- en: '| Capitals RoW | 8,372 | 96.46% |  | Opposite | 756 | 28.57% |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 非洲和南美洲首都 | 8,372 | 96.46% |  | 对立词 | 756 | 28.57% |'
- en: '| City-State | 4,242 | 60.00% |  | Superlative | 1,056 | 74.62% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 城市-州份 | 4,242 | 60.00% |  | 最高级 | 1,056 | 74.62% |'
- en: '| Currency | 752 | 17.42% |  | Present-Participle | 1,056 | 69.98% |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 货币 | 752 | 17.42% |  | 现在分词 | 1,056 | 69.98% |'
- en: '| Family | 506 | 88.14% |  | Past Tense | 1,560 | 61.15% |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 家庭 | 506 | 88.14% |  | 过去时 | 1,560 | 61.15% |'
- en: '| Nationality | 1,640 | 92.50% |  | Plural | 1,332 | 78.08% |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 国籍 | 1,640 | 92.50% |  | 复数 | 1,332 | 78.08% |'
- en: '| Adjective-Adverb | 992 | 22.58% |  | Plural Verbs | 870 | 58.51% |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 形容词-副词 | 992 | 22.58% |  | 复数动词 | 870 | 58.51% |'
- en: '*Figure 16.3* compares the performance for the three GloVe sources for the
    100,000 most common tokens. It shows that Common Crawl vectors, which cover about
    80 percent of the analogies, achieve slightly higher accuracy at 78 percent. The
    Twitter vectors cover only 25 percent, with 56.4 percent accuracy:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 16.3* 比较了三种 GloVe 源在 10 万个最常见标记上的性能。它显示了 Common Crawl 向量的准确度略高，达到了 78%，覆盖了约
    80% 的类比。而 Twitter 向量的覆盖率仅为 25%，准确度为 56.4%。'
- en: '![](img/B15439_16_03.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_03.png)'
- en: 'Figure 16.3: GloVe accuracy on word2vec analogies'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16.3: GloVe 在 word2vec 类比上的准确度'
- en: '*Figure 16.4* projects the 300-dimensional embeddings of the most closely related
    analogies for a word2vec model trained on the Wikipedia corpus with over 2 billion
    tokens into two dimensions using PCA. A test of over 24,400 analogies from the
    following categories achieved an accuracy of over 73.5 percent:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 16.4* 将在维基百科语料库上训练的 word2vec 模型的 300 维嵌入投影到两个维度上使用 PCA，测试了来自以下类别的 24,400
    个以上的类比，准确率超过 73.5%：'
- en: '![](img/B15439_16_04.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_04.png)'
- en: 'Figure 16.4: 2D visualization of selected analogy embeddings'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16.4: 选定类比嵌入的二维可视化'
- en: Custom embeddings for financial news
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义金融新闻嵌入
- en: Many tasks require embeddings of domain-specific vocabulary that models pretrained
    on a generic corpus may not be able to capture. Standard word2vec models are not
    able to assign vectors to out-of-vocabulary words and instead use a default vector
    that reduces their predictive value.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 许多任务需要领域特定词汇的嵌入，而预训练于通用语料库的模型可能无法捕捉到这些词汇。标准的 word2vec 模型无法为词汇表中不存在的词汇分配向量，而是使用一个默认向量，降低了它们的预测价值。
- en: For example, when working with **industry-specific documents**, the vocabulary
    or its usage may change over time as new technologies or products emerge. As a
    result, the embeddings need to evolve as well. In addition, documents like corporate
    earnings releases use nuanced language that GloVe vectors pretrained on Wikipedia
    articles are unlikely to properly reflect.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在处理**行业特定文件**时，词汇表或其用法可能随着新技术或产品的出现而发生变化。因此，嵌入也需要相应地演变。此外，像公司收益发布这样的文件使用了细微的语言，预训练于维基百科文章的
    GloVe 向量可能无法正确反映这些语言特点。
- en: In this section, we will train and evaluate domain-specific embeddings using
    financial news. We'll first show how to preprocess the data for this task, then
    demonstrate how the skip-gram architecture outlined in the first section works,
    and finally visualize the results. We also will introduce alternative, faster
    training methods.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用金融新闻训练和评估领域特定的嵌入。我们首先展示了如何为这项任务预处理数据，然后演示了第一节中概述的 skip-gram 架构的工作原理，并最终可视化结果。我们还将介绍替代的更快的训练方法。
- en: Preprocessing – sentence detection and n-grams
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理 - 句子检测和 n-gram
- en: To illustrate the word2vec network architecture, we'll use the financial news
    dataset with over 125,000 relevant articles that we introduced in *Chapter 15*,
    *Topic Modeling – Summarizing Financial News*, on topic modeling. We'll load the
    data as outlined in the `lda_financial_news.ipynb` notebook in that chapter. The
    `financial_news_preprocessing.ipynb` notebook contains the code samples for this
    section.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 word2vec 网络架构，我们将使用包含超过 12 万 5 千篇相关文章的金融新闻数据集，该数据集我们在第 15 章 *主题建模 - 总结金融新闻*
    中介绍了。我们将按照该章节中的 `lda_financial_news.ipynb` 笔记本中概述的方式加载数据。`financial_news_preprocessing.ipynb`
    笔记本包含了本节的代码示例。
- en: 'We use spaCy''s built-in **sentence boundary detection** to split each article
    into sentences, remove less informative items, such as numbers and punctuation,
    and keep the result if it is between 6 and 99 tokens long:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 spaCy 内置的**句子边界检测**来将每篇文章分割成句子，去除较少信息的项目，例如数字和标点符号，并保留结果（如果长度在 6 到 99 个标记之间）：
- en: '[PRE2]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We end up with 2.43 million sentences that, on average, contain 15 tokens.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到了 243 万个句子，平均每个句子包含 15 个标记。
- en: Next, we create n-grams to capture composite terms. Gensim lets us identify
    n-grams based on the relative frequency of joint versus individual occurrence
    of the components. The `Phrases` module scores the tokens, and the `Phraser` class
    transforms the text data accordingly.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建 n-gram 来捕捉复合术语。 Gensim 允许我们根据组件的联合与个别出现的相对频率来识别 n-gram。`Phrases` 模块对标记进行评分，并且
    `Phraser` 类相应地转换文本数据。
- en: 'It transforms our list of sentences into a new dataset that we can write to
    file as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 它将我们的句子列表转换为一个新的数据集，我们可以按如下方式写入文件：
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The notebook illustrates how we can repeat this process using the 2-gram file
    as input to create 3-grams. We end up with some 25,000 2-grams and 15,000 3- or
    4-grams. Inspecting the result shows that the highest-scoring terms are names
    of companies or individuals, suggesting that we might want to tighten our initial
    cleaning criteria. Refer to the notebook for additional details on the dataset.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本演示了如何使用 2-gram 文件作为输入来重复此过程以创建 3-gram。我们最终得到了大约 2 万 5 千个 2-gram 和 1 万 5
    千个 3-或 4-gram。检查结果显示，得分最高的术语是公司或个人的名称，这表明我们可能需要加强我们的初始清洁标准。有关数据集的其他详细信息，请参阅笔记本。
- en: The skip-gram architecture in TensorFlow 2
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 2 中的 skip-gram 架构
- en: In this section, we will illustrate how to build a word2vec model using the
    Keras interface of TensorFlow 2 that we will introduce in much more detail in
    the next chapter. The `financial_news_word2vec_tensorflow` notebook contains the
    code samples and additional implementation details.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何使用 TensorFlow 2 的 Keras 接口构建一个 word2vec 模型，我们将在下一章中更详细地介绍。`financial_news_word2vec_tensorflow`
    笔记本包含了代码示例和其他实现细节。
- en: 'We start by tokenizing the documents and assigning a unique ID to each item
    in the vocabulary. First, we sample a subset of the sentences created in the previous
    section to limit the training time:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对文档进行标记化，并为词汇表中的每个项目分配唯一的 ID。首先，我们从上一节创建的句子中抽样一部分来限制训练时间：
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We require at least 10 occurrences in the corpus, keep a vocabulary of 31,300
    tokens, and begin with the following steps:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要至少 10 次出现在语料库中，保留包含 31,300 个标记的词汇表，并从以下步骤开始：
- en: Extract the top *n* most common words to learn embeddings.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取前 *n* 个最常见的单词以学习嵌入。
- en: Index these *n* words with unique integers.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用唯一整数索引这些 *n* 个单词。
- en: 'Create an `{index: word}` dictionary.'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '创建一个 `{index: word}` 字典。'
- en: 'Replace the *n* words with their index, and a dummy value `''UNK''` elsewhere:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用它们的索引替换 *n* 个词，并在其他地方使用虚拟值 `'UNK'`：
- en: '[PRE5]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We end up with 17.4 million tokens and a vocabulary of close to 60,000 tokens,
    including up to 3-grams. The vocabulary covers around 72.5 percent of the analogies.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到了 1740 万个标记和接近 6 万个标记的词汇表，包括长达 3 个词的组合。词汇表涵盖了大约 72.5% 的类比。
- en: Noise-contrastive estimation – creating validation samples
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 噪声对比估计 - 创建验证样本
- en: 'Keras includes a `make_sampling_table` method that allows us to create a training
    set as pairs of context and noise words with corresponding labels, sampled according
    to their corpus frequencies. A lower factor increases the probability of selecting
    less frequent tokens; a chart in the notebook shows that the value of 0.1 limits
    sampling to the top 10,000 tokens:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 包括一个 `make_sampling_table` 方法，允许我们创建一个训练集，其中包含上下文和噪声词的一对对应标记，根据它们的语料库频率进行采样。较低的因子会增加选择不常见词汇的概率；笔记本中的图表显示，0.1
    的值将采样限制在前 10,000 个标记：
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Generating target-context word pairs
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成目标-上下文词对
- en: 'To train our model, we need pairs of tokens where one represents the target
    and the other is selected from the surrounding context window, as shown previously
    in the right panel of *Figure 16.1*. We can use Keras'' `skipgrams()` function
    as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练我们的模型，我们需要一对一对的标记，其中一个代表目标，另一个从周围上下文窗口中选择，如之前在 *图 16.1* 的右侧面板中所示。我们可以使用 Keras
    的 `skipgrams()` 函数如下所示：
- en: '[PRE7]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The result is 120.4 million context-target pairs, evenly split between positive
    and negative samples. The negative samples are generated according to the `sampling_table`
    probabilities we created in the previous step. The first five target and context
    word IDs with their matching labels appear as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是 1.204 亿个上下文-目标对，正负样本均匀分布。负样本是根据我们在前一步中创建的 `sampling_table` 概率生成的。前五个目标和上下文词
    ID 与它们匹配的标签如下所示：
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Creating the word2vec model layers
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 word2vec 模型层
- en: 'The word2vec model contains the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec 模型包括以下内容：
- en: An input layer that receives the two scalar values representing the target-context
    pair
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个接收表示目标-上下文对的两个标量值的输入层
- en: A shared embedding layer that computes the dot product of the vector for the
    target and context word
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个共享的嵌入层，计算目标和上下文词的向量的点积
- en: A sigmoid output layer
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 sigmoid 输出层
- en: 'The **input layer** has two components, one for each element of the target-context
    pair:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入层** 有两个组件，一个用于目标-上下文对的每个元素：'
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The **shared embedding layer** contains one vector for each element of the
    vocabulary that is selected according to the index of the target and context tokens,
    respectively:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**共享的嵌入层** 包含一个向量，每个词汇元素根据目标和上下文标记的索引进行选择：'
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The **output layer** measures the similarity of the two embedding vectors by
    their dot product and transforms the result using the `sigmoid` function that
    we encountered when discussing logistic regression in *Chapter 7*, *Linear Models
    – From Risk Factors to Return Forecasts*:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出层** 通过它们的点积测量两个嵌入向量的相似性，并使用 `sigmoid` 函数转换结果，我们在 *第 7 章，线性模型——从风险因素到收益预测*
    中讨论逻辑回归时遇到过这个函数：'
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This skip-gram model contains a 200-dimensional embedding layer that will assume
    different values for each vocabulary item. As a result, we end up with 59,617
    x 200 trainable parameters, plus two for the sigmoid output.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 skip-gram 模型包含一个 200 维的嵌入层，每个词汇项都将假设不同的值。结果是，我们得到了 59,617 x 200 个可训练参数，再加上两个用于
    sigmoid 输出的参数。
- en: In each iteration, the model computes the dot product of the context and the
    target embedding vectors, passes the result through the sigmoid to produce a probability,
    and adjusts the embedding based on the gradient of the loss.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，模型计算上下文和目标嵌入向量的点积，将结果通过 sigmoid 传递以产生概率，并根据损失的梯度调整嵌入。
- en: Visualizing embeddings using TensorBoard
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorBoard 可视化嵌入
- en: 'TensorBoard is a visualization tool that permits the projection of the embedding
    vectors into two or three dimensions to explore the word and phrase locations.
    After loading the embedding metadata file we created (refer to the notebook),
    you can also search for specific terms to view and explore its neighbors, projected
    into two or three dimensions using UMAP, t-SNE, or PCA (refer to *Chapter 13,
    Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning*). Refer
    to the notebook for a higher-resolution color version of the following screenshot:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 是一个可视化工具，允许将嵌入向量投影到二维或三维空间中，以探索单词和短语的位置。在加载我们创建的嵌入元数据文件后（参考笔记本），您还可以搜索特定术语以查看并探索其邻居，使用
    UMAP、t-SNE 或 PCA 将其投影到二维或三维空间中（参见 *第 13 章，使用无监督学习进行数据驱动的风险因素和资产配置*）。有关以下截图的更高分辨率彩色版本，请参考笔记本：
- en: '![](img/B15439_16_05.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_05.png)'
- en: 'Figure 16.5: 3D embeddings and metadata visualization'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.5：3D 嵌入和元数据可视化
- en: How to train embeddings faster with Gensim
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 Gensim 更快地训练嵌入
- en: The TensorFlow implementation is very transparent in terms of its architecture,
    but it is not particularly fast. The **natural language processing** (**NLP**)
    library Gensim, which we also used for topic modeling in the last chapter, offers
    better performance and more closely resembles the C-based word2vec implementation
    provided by the original authors.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的实现在其体系结构方面非常透明，但速度不是特别快。自然语言处理（NLP）库Gensim，我们在上一章中也用于主题建模，提供了更好的性能，并且更接近原始作者提供的基于C的word2vec实现。
- en: 'Usage is very straightforward. We first create a sentence generator that just
    takes the name of the file we produced in the preprocessing step as input (we''ll
    work with 3-grams again):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用非常简单。我们首先创建一个句子生成器，它只需将我们在预处理步骤中生成的文件名作为输入（我们将再次使用3-grams）：
- en: '[PRE12]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In a second step, we configure the word2vec model with the familiar parameters
    concerning the sizes of the embedding vector and the context window, the minimum
    token frequency, and the number of negative samples, among others:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，我们配置word2vec模型，使用熟悉的参数，涉及嵌入向量和上下文窗口的大小，最小标记频率以及负样本数量等：
- en: '[PRE13]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: One epoch of training takes a bit over 2 minutes on a modern 4-core i7 processor.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代4核i7处理器上，一次训练大约需要2分钟。
- en: 'We can persist both the model and the word vectors, or just the word vectors,
    as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以持久化模型和词向量，或仅持久化词向量，如下所示：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can validate model performance and continue training until we are satisfied
    with the results like so:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以验证模型性能，并继续训练，直到对结果满意为止：
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this case, training for six additional epochs yields the best results with
    an accuracy of 41.75 percent across all analogies covered by the vocabulary. The
    left panel of *Figure 16.6* shows the correct/incorrect predictions and accuracy
    breakdown per category.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，再训练六个额外的周期会产生最佳结果，所有涵盖词汇表的类比的准确率为41.75％。*图16.6*的左侧面板显示了正确/错误的预测和每个类别的准确度分布。
- en: 'Gensim also allows us to evaluate custom semantic algebra. We can check the
    popular `"woman"+"king"-"man" ~ "queen"` example as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim还允许我们评估自定义语义代数。我们可以检查流行的`"woman"+"king"-"man" ~ "queen"`示例如下：
- en: '[PRE16]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The right panel of the figure shows that "queen" is the third token, right
    after "monarch" and the less obvious "lewis", followed by several royalties:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的右侧面板显示，“queen”是第三个标记，紧随“monarch”和不太明显的“lewis”之后，然后是几个王室成员：
- en: '![](img/B15439_16_06.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_06.png)'
- en: 'Figure 16.6: Analogy accuracy by category and for a specific example'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6：类别和特定示例的类比准确度
- en: 'We can also evaluate the tokens most similar to a given target to gain a better
    understanding of the embedding characteristics. We randomly select based on log
    corpus frequency:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以评估与给定目标最相似的标记，以更好地理解嵌入特征。我们基于对数语料库频率进行随机选择：
- en: '[PRE17]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following table exemplifies the results that include several n-grams:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 下表举例说明了包含多个n-gram的结果：
- en: '| Target | Closest Match |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 最接近的匹配 |'
- en: '| 0 | 1 | 2 | 3 | 4 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 2 | 3 | 4 |'
- en: '| profiles | profile | users | political_consultancy_cambridge_analytica |
    sophisticated | facebook |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 档案 | 概要 | 用户 | 政治顾问剑桥分析 | 复杂 | Facebook |'
- en: '| divestments | divestitures | acquisitions | takeovers | bayer | consolidation
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 减持 | 转让 | 收购 | 接管 | 拜耳 | 合并 |'
- en: '| readiness | training | military | command | air_force | preparations |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 就绪性 | 训练 | 军事 | 指挥 | 空军 | 准备 |'
- en: '| arsenal | nuclear_weapons | russia | ballistic_missile | weapons | hezbollah
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 军火库 | 核武器 | 俄罗斯 | 弹道导弹 | 武器 | 黎巴嫩真主党 |'
- en: '| supply_disruptions | disruptions | raw_material | disruption | prices | downturn
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 供应中断 | 中断 | 原材料 | 中断 | 价格 | 下降 |'
- en: We will now proceed to develop an application more closely related to real-life
    trading using SEC filings.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将继续开发一个与实际交易更密切相关的应用程序，使用SEC文件。
- en: word2vec for trading with SEC filings
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于利用SEC文件进行交易的word2vec
- en: In this section, we will learn word and phrase vectors from annual SEC filings
    using Gensim to illustrate the potential value of word embeddings for algorithmic
    trading. In the following sections, we will combine these vectors as features
    with price returns to train neural networks to predict equity prices from the
    content of security filings.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Gensim从年度SEC文件中学习单词和短语向量，以说明词嵌入对算法交易的潜在价值。在接下来的章节中，我们将将这些向量与价格回报结合为特征，训练神经网络从安全文件的内容中预测股票价格。
- en: In particular, we will use a dataset containing over **22,000 10-K annual reports**
    from the period **2013-2016** that are filed by over 6,500 listed companies and
    contain both financial information and management commentary (see *Chapter 2*,
    *Market and Fundamental Data – Sources and Techniques*).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将使用一个包含来自**2013-2016年间的超过22,000份10-K年度报告**的数据集，这些报告由超过6,500家上市公司提交，并包含财务信息和管理评论（请参阅*第2章*，*市场和基本数据-来源和技术*）。
- en: For about 3,000 companies corresponding to 11,000 filings, we have stock prices
    to label the data for predictive modeling. (See data source details and download
    instructions and preprocessing code samples in the `sec_preprocessing` notebook
    in the `sec-filings` folder.)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对应于11,000份备案的大约3,000家公司，我们有股价数据来标记用于预测建模的数据。（在`sec-filings`文件夹中的`sec_preprocessing`笔记本中查看数据源详细信息和下载说明以及预处理代码示例。）
- en: Preprocessing – sentence detection and n-grams
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理-句子检测和n-grams
- en: 'Each filing is a separate text file, and a master index contains filing metadata.
    We extract the most informative sections, namely:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 每个备案都是一个单独的文本文件，主索引包含备案元数据。我们提取了最具信息量的部分，即：
- en: 'Item 1 and 1A: Business and Risk Factors'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目1和1A：业务和风险因素
- en: 'Item 7: Management''s Discussion'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目7：管理层讨论
- en: 'Item 7a: Disclosures about Market Risks'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目7a：关于市场风险的披露
- en: The `sec_preprocessing` notebook shows how to parse and tokenize the text using
    spaCy, similar to the approach in *Chapter 14.* We do not lemmatize the tokens
    to preserve nuances of word usage.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`sec_preprocessing`笔记本展示了如何使用spaCy解析和标记文本，类似于*第14章*中的方法。我们不对标记进行词形还原，以保留单词用法的细微差别。'
- en: Automatic phrase detection
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动短语检测
- en: As in the previous section, we use Gensim to detect phrases that consist of
    multiple tokens, or n-grams. The notebook shows that the most frequent bigrams
    include `common_stock`, `united_states`, `cash_flows`, `real_estate`, and `interest_rates`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一节一样，我们使用Gensim来检测由多个标记组成的短语，或n-grams。笔记本显示，最常见的二元组包括`common_stock`，`united_states`，`cash_flows`，`real_estate`和`interest_rates`。
- en: We end up with a vocabulary of slightly over 201,000 tokens with a median frequency
    of 7, suggesting substantial noise that we can remove by increasing the minimum
    frequency when training our word2vec model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到了一个词汇表，其中包含略多于201,000个标记，中位数频率为7，表明我们可以通过增加训练word2vec模型时的最小频率来消除相当大的噪音。
- en: Labeling filings with returns to predict earnings surprises
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用回报标记备案以预测盈利惊喜
- en: The dataset comes with a list of tickers and filing dates associated with the
    10,000 documents. We can use this information to select stock prices for a certain
    period surrounding the filing publication. The goal would be to train a model
    that uses word vectors for a given filing as input to predict post-filing returns.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集附带了与这10,000个文件相关的股票代码和备案日期的列表。我们可以利用这些信息选择围绕备案发布期间的某一时期的股价。目标是训练一个使用给定备案的词向量作为输入来预测备案后回报的模型。
- en: 'The following code example shows how to label individual filings with the 1-month
    return for the period after filing:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码示例显示了如何使用1个月的回报来标记单个备案：
- en: '[PRE18]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We will come back to this when we work with deep learning architectures in the
    following chapters.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在接下来的章节中使用深度学习架构时，我们将回到这一点。
- en: Model training
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练
- en: The `gensim.models.word2vec` class implements the skip-gram and CBOW architectures
    introduced previously. The notebook `word2vec` contains additional implementation
    details.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim.models.word2vec`类实现了先前介绍的skip-gram和CBOW架构。笔记本`word2vec`包含了额外的实现细节。'
- en: 'To facilitate memory-efficient text ingestion, the `LineSentence` class creates
    a generator from individual sentences contained in the text file provided:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了促进内存高效的文本摄取，`LineSentence`类从提供的文本文件中创建一个包含在单个句子中的生成器：
- en: '[PRE19]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `Word2Vec` class offers the configuration options introduced earlier in
    this chapter:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`Word2Vec`类提供了本章中介绍的配置选项：'
- en: '[PRE20]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The notebook shows how to persist and reload models to continue training, or
    how to store the embedding vectors separately, for example, for use in machine
    learning models.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本显示了如何持久保存和重新加载模型以继续训练，或者如何单独存储嵌入向量，例如用于机器学习模型中。
- en: Model evaluation
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'Basic functionality includes identifying similar words:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 基本功能包括识别相似的单词：
- en: '[PRE21]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can also validate individual analogies using positive and negative contributions
    accordingly:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以根据正面和负面的贡献来验证单个类比：
- en: '[PRE22]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Performance impact of parameter settings
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数设置的性能影响
- en: 'We can use the analogies to evaluate the impact of different parameter settings.
    The following results stand out (refer to the detailed results in the `models`
    folder):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用类比来评估不同参数设置的影响。以下结果非常突出（请参阅`models`文件夹中的详细结果）：
- en: Negative sampling outperforms the hierarchical softmax, while also training
    faster.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负采样优于分层softmax，同时训练速度更快。
- en: The skip-gram architecture outperforms CBOW.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: skip-gram架构优于CBOW。
- en: Different `min_count` settings have a smaller impact; the midpoint of 50 performs
    best.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的`min_count`设置影响较小；中间值50的性能最佳。
- en: 'Further experiments with the best-performing skip-gram model using negative
    sampling and a `min_count` of 50 show the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 使用负采样和`min_count`为50的性能最佳的skip-gram模型进行进一步实验，结果如下：
- en: Context windows smaller than 5 reduce performance.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小于5的上下文窗口减少了性能。
- en: A higher negative sampling rate improves performance at the expense of slower training.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高的负采样率提高了性能，但训练速度较慢。
- en: Larger vectors improve performance, with a `size` of 600 yielding the best accuracy
    at 38.5 percent.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更大的向量提高了性能，大小为600的向量在38.5%的准确率时表现最佳。
- en: Sentiment analysis using doc2vec embeddings
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用doc2vec嵌入进行情感分析
- en: Text classification requires combining multiple word embeddings. A common approach
    is to average the embedding vectors for each word in the document. This uses information
    from all embeddings and effectively uses vector addition to arrive at a different
    location point in the embedding space. However, relevant information about the
    order of words is lost.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类需要组合多个单词嵌入。常见方法是对文档中每个单词的嵌入向量进行平均。这使用了所有嵌入的信息，并有效地使用向量加法到达嵌入空间中的不同位置。然而，有关单词顺序的相关信息丢失了。
- en: 'In contrast, the document embedding model, doc2vec, developed by the word2vec
    authors shortly after publishing their original contribution, produces embeddings
    for pieces of text like a paragraph or a product review directly. Similar to word2vec,
    there are also two flavors of doc2vec:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，由word2vec作者在发布其原始贡献后不久开发的文档嵌入模型doc2vec直接为文本片段（如段落或产品评论）生成嵌入。与word2vec类似，doc2vec也有两种变体：
- en: The **distributed bag of words** (**DBOW**) model corresponds to the word2vec
    CBOW model. The document vectors result from training a network on the synthetic
    task of predicting a target word based on both the context word vectors and the
    document's doc vector.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式词袋**（**DBOW**）模型对应于word2vec的CBOW模型。文档向量是通过训练网络来预测目标词，该网络基于上下文单词向量和文档的文档向量的合成任务。'
- en: The **distributed memory** (**DM**) model corresponds to the word2wec skip-gram
    architecture. The doc vectors result from training a neural net to predict a target
    word using the full document's doc vector.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式记忆**（**DM**）模型对应于word2vec的skip-gram架构。文档向量是通过训练神经网络使用整个文档的文档向量来预测目标词而产生的。'
- en: Gensim's `Doc2Vec` class implements this algorithm. We'll illustrate the use
    of doc2vec by applying it to the Yelp sentiment dataset that we introduced in
    *Chapter 14*. To speed up training, we limit the data to a stratified random sample
    of 0.5 million Yelp reviews with their associated star ratings. The `doc2vec_yelp_sentiment`
    notebook contains the code examples for this section.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim的`Doc2Vec`类实现了这个算法。我们将通过将其应用于我们在*第14章*中介绍的Yelp情感数据集来说明doc2vec的用法。为了加快训练速度，我们将数据限制为具有相关星级评分的50万Yelp评论的分层随机样本。`doc2vec_yelp_sentiment`笔记本包含了本节的代码示例。
- en: Creating doc2vec input from Yelp sentiment data
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Yelp情感数据创建doc2vec输入
- en: 'We load the combined Yelp dataset containing 6 million reviews, as created
    in *Chapter 14*, *Text Data for Trading – Sentiment Analysis*, and sample 100,000
    reviews for each star rating:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了包含600万条评论的合并Yelp数据集，如*第14章*，*用于交易的文本数据-情感分析*中所创建的，并对每个星级评分的评论进行了10万次采样：
- en: '[PRE23]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We use nltk''s `RegexpTokenizer` for simple and quick text cleaning:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用nltk的`RegexpTokenizer`进行简单快速的文本清洗：
- en: '[PRE24]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: After we filter out reviews shorter than 10 tokens, we are left with 485,825
    samples. The left panel of *Figure 16.6* shows the distribution of the number
    of tokens per review.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们过滤掉长度小于10个标记的评论后，我们还剩下485,825个样本。*图16.6*的左面板显示了每个评论的标记数量的分布。
- en: 'The `gensim.models.Doc2Vec` class processes documents in the `TaggedDocument`
    format that contains the tokenized documents alongside a unique tag that permits
    the document vectors to be accessed after training:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim.models.Doc2Vec`类以`TaggedDocument`格式处理文档，其中包含标记化的文档以及一个唯一的标记，允许在训练后访问文档向量：'
- en: '[PRE25]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Training a doc2vec model
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training interface works in a similar fashion to word2vec and also allows
    continued training and persistence:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can query the *n* terms most similar to a given token as a quick way to
    evaluate the resulting word vectors as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The right panel of *Figure 16.7* displays the returned tokens and their similarity:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_16_07.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.7: Histogram of the number of tokens per review (left) and terms
    most similar to the token ''good'''
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Training a classifier with document vectors
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we can access the document vectors to create features for a sentiment
    classifier:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We create training and test sets as usual:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, we proceed to train a `RandomForestClassifier`, a LightGBM gradient boosting
    model, and a multinomial logistic regression. We use 500 trees for the random
    forest:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We use early stopping with the LightGBM classifier, but it runs for the full
    5,000 rounds because it continues to improve its validation performance:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, we build a multinomial logistic regression model as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'When we compute the accuracy for each model on the validation set, gradient
    boosting performs significantly better at 62.24 percent. *Figure 16.8* shows the
    confusion matrix and accuracy for each model:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_16_08.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.8: Confusion matrix and test accuracy for alternative models'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: The sentiment classification result in *Chapter 14*, *Text Data for Trading
    – Sentiment Analysis*, produced better accuracy for LightGBM (73.6 percent), but
    we used the full dataset and included additional features. You may want to test
    whether increasing the sample size or tuning the model parameters makes doc2vec
    perform equally well.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Lessons learned and next steps
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This example applied sentiment analysis using doc2vec to **product reviews rather
    than financial documents**. We selected product reviews because it is very difficult
    to find financial text data that is large enough for training word embeddings
    from scratch and also has useful sentiment labels or sufficient information for
    us to assign them labels, such as asset returns, ourselves.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'While product reviews allow us to demonstrate the workflow, we need to keep
    in mind **important structural differences**: product reviews are often short,
    informal, and specific to one particular object. Many financial documents, in
    contrast, are longer, more formal, and the target object may or may not be clearly
    identified. Financial news articles could concern multiple targets, and while
    corporate disclosures may have a clear source, they may also discuss competitors.
    An analyst report, for instance, may also discuss both positive and negative aspects
    of the same object or topic.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: In short, the interpretation of sentiment expressed in financial documents often
    requires a more sophisticated, nuanced, and granular approach that builds up an
    understanding of the content's meaning from different aspects. Decision makers
    also often care to understand how a model arrives at its conclusion.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: These challenges have not yet been solved and remain an area of very active
    research, complicated not least by the scarcity of suitable data sources. However,
    recent breakthroughs that significantly boosted performance on various NLP tasks
    since 2018 suggest that financial sentiment analysis may also become more robust
    in the coming years. We will turn to these innovations next.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战尚未解决，仍然是非常活跃的研究领域，其中最大的困难之一是适合的数据源的稀缺性。然而，自2018年以来显著提高了各种NLP任务性能的最新突破表明，金融情感分析在未来几年也可能变得更加健壮。我们将在接下来转向这些创新。
- en: New frontiers – pretrained transformer models
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新的前沿 – 预训练的变压器模型
- en: 'Word2vec and GloVe embeddings capture more semantic information than the bag-of-words
    approach. However, they allow only a single fixed-length representation of each
    token that does not differentiate between context-specific usages. To address
    unsolved problems such as multiple meanings for the same word, called **polysemy**,
    several new models have emerged that build on the **attention mechanism** designed
    to learn more contextualized word embeddings (Vaswani et al., 2017). The key characteristics
    of these models are as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec和GloVe嵌入捕捉到比词袋方法更多的语义信息。但是，它们只允许每个令牌有一个固定长度的表示，不区分上下文特定的用法。为了解决诸如同一个词的多重含义（称为**多义性**）等未解决的问题，出现了几种新模型，这些模型建立在旨在学习更多上下文化单词嵌入的**注意力机制**上（Vaswani等，2017）。这些模型的关键特征如下：
- en: The use of **bidirectional language models** that process text both left-to-right
    and right-to-left for a richer context representation
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**双向语言模型**同时处理文本的左到右和右到左以获得更丰富的上下文表示
- en: The use of **semi-supervised pretraining** on a large generic corpus to learn
    universal language aspects in the form of embeddings and network weights that
    can be used and fine-tuned for specific tasks (a form of **transfer learning**
    that we will discuss in more detail in *Chapter 18*, *CNNs for Financial Time
    Series and Satellite Images*)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**半监督预训练**在大型通用语料库上学习通用语言方面的嵌入和网络权重，这些可以用于特定任务的微调（一种我们将在*第18章*，*用于金融时间序列和卫星图像的CNNs*中更详细讨论的**迁移学习**形式）
- en: In this section, we briefly describe the attention mechanism, outline how the
    recent transformer models—starting with **Bidirectional Encoder Representation
    from Transformers** (**BERT**)—use it to improve performance on key NLP tasks,
    reference several sources for pretrained language models, and explain how to use
    them for financial sentiment analysis.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要描述了注意力机制，概述了最近的变压器模型——从**变压器中的双向编码器表示**（**BERT**）开始——如何利用它来提高关键NLP任务的性能，引用了几个预训练语言模型的来源，并解释了如何将它们用于金融情感分析。
- en: Attention is all you need
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力是你所需要的一切
- en: The **attention mechanism** explicitly models the relationships between words
    in a sentence to better incorporate the context. It was first applied to machine
    translation (Bahdanau, Cho, and Bengio, 2016), but has since become integral to
    neural language models for a wide variety of tasks.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力机制**明确地建模了句子中单词之间的关系，以更好地整合上下文。它最初被应用于机器翻译（Bahdanau，Cho和Bengio，2016），但此后已成为各种任务的神经语言模型的核心组成部分。'
- en: Until 2017, **recurrent neural networks** (**RNNs**), which sequentially process
    text left-to-right or right-to-left, represented the state of the art for NLP
    tasks like translation. Google, for example, has employed such a model in production
    since late 2016\. Sequential processing implies several steps to semantically
    connect words at distant locations and precludes parallel processing, which greatly
    speeds up computation on modern, specialized hardware like GPUs. (For more information
    on RNNs, refer to *Chapter 19*, *RNNs for Multivariate Time Series and Sentiment
    Analysis*.)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 直到2017年，**循环神经网络**（**RNNs**），它们按顺序从左到右或从右到左处理文本，代表了自然语言处理任务（如翻译）的最新技术。例如，谷歌自2016年末起在生产中就采用了这样的模型。顺序处理意味着需要多个步骤来语义连接远距离位置的单词，并且排除了并行处理，而这在现代专用硬件（如GPU）上会大大加速计算。（关于RNNs的更多信息，请参阅*第19章*，*多元时间序列和情感分析的RNNs*。）
- en: In contrast, the **Transformer** model, introduced in the seminal paper *Attention
    is all you need* (Vaswani et al., 2017), requires only a constant number of steps
    to identify semantically related words. It relies on a self-attention mechanism
    that captures links between all words in a sentence, regardless of their relative
    position. The model learns the representation of a word by assigning an attention
    score to every other word in the sentence that determines how much each of the
    other words should contribute to the representation. These scores then inform
    a weighted average of all words' representations, which is fed into a fully connected
    network to generate a new representation for the target word.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，**Transformer** 模型，由开创性论文 *Attention is all you need*（Vaswani等人，2017）引入，只需一个恒定数量的步骤来识别语义相关的单词。它依赖于一种自注意力机制，可以捕捉句子中所有单词之间的联系，而不考虑它们的相对位置。该模型通过给每个句子中的其他单词分配一个注意力分数来学习单词的表示，该分数决定其他单词应该对表示的贡献程度。然后，这些分数将指导所有单词表示的加权平均值，输入到一个全连接网络中，以生成目标单词的新表示。
- en: The Transformer model uses an encoder-decoder architecture with several layers,
    each of which uses several attention mechanisms (called **heads**) in parallel.
    It yielded large performance improvements on various translation tasks and, more
    importantly, inspired a wave of new research into neural language models addressing
    a broader range of tasks. The resources linked on GitHub contain various excellent
    visual explanations of how the attention mechanism works, so we won't go into
    more detail here.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型采用了一种编码器-解码器的架构，其中包括多个层，每个层并行使用了多个注意力机制（称为**头部**）。它在各种翻译任务上取得了巨大的性能提升，并且更重要的是，激发了一波新的研究，致力于神经语言模型解决更广泛范围的任务。在
    GitHub 上链接的资源中包含了关于注意力机制如何工作的各种优秀视觉解释，所以我们在这里不会详细介绍。
- en: BERT – towards a more universal language model
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT – 迈向更普适的语言模型
- en: In 2018, Google released the **BERT** model, which stands for **Bidirectional
    Encoder Representations from Transformers** (Devlin et al., 2019). In a major
    breakthrough for NLP research, it achieved groundbreaking results on eleven natural
    language understanding tasks, ranging from question answering and named entity
    recognition to paraphrasing and sentiment analysis, as measured by the **General
    Language Understanding Evaluation** (**GLUE**) benchmark (see GitHub for links
    to task descriptions and a leaderboard).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，Google 发布了 **BERT** 模型，全称为 **Bidirectional Encoder Representations from
    Transformers**（Devlin等人，2019）。对于自然语言理解任务，它在十一项任务上取得了突破性的成果，从问答和命名实体识别到释义和情感分析，都得到了
    **通用语言理解评估**（**GLUE**）基准的衡量（请参考 GitHub 获取任务描述和排行榜链接）。
- en: The new ideas introduced by BERT unleashed a flurry of new research that produced
    dozens of improvements that soon surpassed non-expert humans on the GLUE tasks
    and led to the more challenging **SuperGLUE** benchmark designed by DeepMind (Wang
    et al., 2019). As a result, 2018 is now considered a turning point for NLP research;
    both Google Search and Microsoft's Bing are now using variations of BERT to interpret
    user queries and provide more accurate results.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 引入的新思想引发了一系列新的研究，产生了数十项超越非专业人员的改进，很快超过了由 DeepMind 设计的更具挑战性的 **SuperGLUE**
    基准（Wang等人，2019）。因此，2018年现在被认为是自然语言处理研究的一个转折点；现在，Google 搜索和微软的必应都在使用 BERT 的变体来解释用户查询并提供更准确的结果。
- en: We will briefly outline BERT's key innovations and provide indications on how
    to get started using it and its subsequent enhancements with one of several open
    source libraries providing pretrained models.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要概述 BERT 的关键创新，并提供如何开始使用它及其后续增强版的指示，其中包括几个提供预训练模型的开源库。
- en: Key innovations – deeper attention and pretraining
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键创新 – 更深的注意力和预训练
- en: 'The BERT model builds on **two key ideas**, namely, the **transformer architecture**
    described in the previous section and **unsupervised pretraining** so that it
    doesn''t need to be trained from scratch for each new task; rather, its weights
    are fine-tuned:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型基于 **两个关键思想**，即前一节中描述的 **transformer 架构** 和 **无监督预训练**，这样它就不需要为每个新任务从头开始训练；相反，它的权重被微调：
- en: BERT takes the **attention mechanism** to a new (deeper) level by using 12 or
    24 layers, depending on the architecture, each with 12 or 16 attention heads.
    This results in up to 24 × 16 = 384 attention mechanisms to learn context-specific
    embeddings.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT通过使用12或24层（取决于架构），每层有12或16个注意头，将**注意机制**提升到一个新的（更深）水平。这导致最多24 × 16 = 384个注意机制来学习特定于上下文的嵌入。
- en: 'BERT uses **unsupervised, bidirectional pretraining** to learn its weights
    in advance on two tasks: **masked language modeling** (predicting a missing word
    given the left and right context) and **next sentence prediction** (predicting
    whether one sentence follows another).'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT使用**无监督的、双向的预训练**来提前在两个任务上学习其权重：**遮盖语言建模**（在左右上下文中给出缺失的单词）和**下一句预测**（预测一句话是否跟在另一句话后面）。
- en: '**Context-free** models such as word2vec or GloVe generate a single embedding
    for each word in the vocabulary: the word "bank" would have the same context-free
    representation in "bank account" and "bank of the river." In contrast, BERT learns
    to represent each word based on the other words in the sentence. As a **bidirectional
    model**, BERT is able to represent the word "bank" in the sentence "I accessed
    the bank account," not only based on "I accessed the" as a unidirectional contextual
    model, but also based on "account."'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**无上下文**模型（如word2vec或GloVe）为词汇表中的每个单词生成一个单一的嵌入：单词“bank”在“bank account”和“river
    bank”中具有相同的无上下文表示。相比之下，BERT学习根据句子中的其他单词来表示每个单词。作为**双向模型**，BERT能够表示句子“I accessed
    the bank account”中的单词“bank”，不仅基于“我访问了”作为单向上下文模型，还基于“account”。'
- en: BERT and its successors can be **pretrained on a generic corpus** like Wikipedia
    before adapting its final layers to a specific task and **fine-tuning its weights**.
    As a result, you can use large-scale, state-of-the-art models with billions of
    parameters, while only incurring a few hours rather than days or weeks of training
    costs. Several libraries offer such pretrained models that you can build on to
    develop a custom sentiment classifier for your dataset of choice.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: BERT及其后继版本可以**在通用语料库**（如维基百科）上进行预训练，然后调整其最终层以适应特定任务，并**微调其权重**。因此，您可以使用具有数十亿参数的大规模、最先进的模型，而只需支付几个小时而不是几天或几周的培训成本。几个库提供了这样的预训练模型，您可以构建一个定制的情感分类器，以适应您选择的数据集。
- en: Using pretrained state-of-the-art models
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用预训练的最先进模型
- en: The recent NLP breakthroughs described in this section have shown how to acquire
    linguistic knowledge from unlabeled text with networks large enough to represent
    the long tail of rare usage phenomena. The resulting Transformer architectures
    make fewer assumptions about word order and context; instead, they learn a much
    more subtle understanding of language from very large amounts of data, using hundreds
    of millions or even billions of parameters.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述的最新NLP突破展示了如何利用足够大的网络从未标记的文本中获取语言知识，这些Transformer体系结构减少了对单词顺序和上下文的假设；相反，它们从大量数据中学习语言的更微妙的理解，使用数亿甚至数十亿的参数。
- en: We will highlight several libraries that make pretrained networks, as well as
    excellent Python tutorials available.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点介绍几个使预训练网络和优秀的Python教程可用的库。
- en: The Hugging Face Transformers library
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hugging Face Transformers库
- en: Hugging Face is a US start-up developing chatbot applications designed to offer
    personalized AI-powered communication. It raised $15 million in late 2019 to further
    develop its very successful open source NLP library, Transformers.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face是一家美国初创公司，开发旨在提供个性化AI驱动通信的聊天机器人应用。2019年底，该公司筹集了1500万美元，以进一步开发其非常成功的开源NLP库Transformers。
- en: The library provides general-purpose architectures for natural language understanding
    and generation with more than 32 pretrained models in more than 100 languages
    and deep interoperability between TensorFlow 2 and PyTorch. It has excellent documentation.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 该库提供了用于自然语言理解和生成的通用架构，拥有超过32个预训练模型，涵盖100多种语言，并在TensorFlow 2和PyTorch之间具有深层的互操作性。它有很好的文档。
- en: The spacy-transformers library includes wrappers to facilitate the inclusion
    of the pretrained transformer models in a spaCy pipeline. Refer to the reference
    links on GitHub for more information.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: spacy-transformers库包含了用于在spaCy管道中方便地包含预训练变换器模型的包装器。有关更多信息，请参阅GitHub上的参考链接。
- en: AllenNLP
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: AllenNLP
- en: AllenNLP is built and maintained by the Allen Institute for AI, started by Microsoft
    cofounder Paul Allen, in close collaboration with researchers at the University
    of Washington. It has been designed as a research library for developing state-of-the-art
    deep learning models on a wide variety of linguistic tasks, built on PyTorch.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP 由微软联合创始人保罗·艾伦创建并维护，与华盛顿大学的研究人员密切合作。它被设计为一个用于在各种语言任务上开发最先进深度学习模型的研究库，基于
    PyTorch 构建。
- en: It offers solutions for key tasks from question answering to sentence annotation,
    including reading comprehension, named entity recognition, and sentiment analysis.
    A pretrained **RoBERTa** model (a more robust version of BERT; Liu et al., 2019)
    achieves over 95 percent accuracy on the Stanford sentiment treebank and can be
    used with just a few lines of code (see links to the documentation on GitHub).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供了从问答到句子标注等关键任务的解决方案，包括阅读理解、命名实体识别和情感分析。预训练的**RoBERTa**模型（BERT的更强大版本；Liu等，2019年）在斯坦福情感树库上实现了超过95%的准确率，并且只需几行代码即可使用（参见
    GitHub 上的文档链接）。
- en: Trading on text data – lessons learned and next steps
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交易文本数据——经验教训与下一步计划
- en: As highlighted at the end of the section *Sentiment analysis using doc2vec embeddings*,
    there are important structural characteristics of financial documents that often
    complicate their interpretation and undermine simple dictionary-based methods.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在“使用doc2vec嵌入进行情感分析”一节末尾所强调的那样，金融文件存在重要的结构特征，这些特征通常会使其解释变得复杂，并削弱基于简单词典的方法。
- en: In a recent survey of financial sentiment analysis, Man, Luo, and Lin (2019)
    found that most existing approaches only identify high-level polarities, such
    as positive, negative, or neutral. However, practical applications that lead to
    real decisions typically require a more nuanced and transparent analysis. In addition,
    the lack of large financial text datasets with relevant labels limits the potential
    for using traditional machine learning methods or neural networks for sentiment
    analysis.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近一项金融情感分析调查中，Man、Luo 和 Lin（2019年）发现，大多数现有方法只能识别高级极性，如积极、消极或中性。然而，导致实际决策的实际应用通常需要更细致和透明的分析。此外，缺乏具有相关标签的大型金融文本数据集限制了使用传统机器学习方法或神经网络进行情感分析的潜力。
- en: The pretraining approach just described, which, in principle, yields a deeper
    understanding of textual information, thus offers substantial promise. However,
    most applied research using transformers has focused on NLP tasks such as translation,
    question answering, logic, or dialog systems. Applications in relation to financial
    data are still in their infancy (see, for example, Araci 2019). This is likely
    to change soon given the availability of pretrained models and their potential
    to extract more valuable information from financial text data.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 刚才描述的预训练方法，原则上能够更深入地理解文本信息，因此具有相当大的潜力。然而，使用变换器进行的大多数应用研究都集中在诸如翻译、问答、逻辑或对话系统等自然语言处理任务上。与金融数据相关的应用仍处于初级阶段（例如，参见
    Araci 2019）。考虑到预训练模型的可用性以及它们从金融文本数据中提取更有价值信息的潜力，这种情况可能很快就会改变。
- en: Summary
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed a new way of generating text features that use
    shallow neural networks for unsupervised machine learning. We saw how the resulting
    word embeddings capture interesting semantic aspects beyond the meaning of individual
    tokens by capturing some of the context in which they are used. We also covered
    how to evaluate the quality of word vectors using analogies and linear algebra.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了一种利用浅层神经网络进行无监督机器学习的新型生成文本特征的方法。我们看到了由此产生的词嵌入如何捕捉到一些有趣的语义方面，超越了单个标记的含义，捕捉到了它们被使用的一些上下文。我们还介绍了如何使用类比和线性代数评估单词向量的质量。
- en: We used Keras to build the network architecture that produces these features
    and applied the more performant Gensim implementation to financial news and SEC
    filings. Despite the relatively small datasets, the word2vec embeddings did capture
    meaningful relationships. We also demonstrated how appropriate labeling with stock
    price data can form the basis for supervised learning.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Keras 构建了生成这些特征的网络架构，并将更高性能的 Gensim 实现应用于金融新闻和美国证券交易委员会的备案文件。尽管数据集相对较小，但
    word2vec 嵌入确实捕捉到了有意义的关系。我们还展示了如何通过股价数据进行适当标记，从而形成监督学习的基础。
- en: We applied the doc2vec algorithm, which produces a document rather than token
    vectors, to build a sentiment classifier based on Yelp business reviews. While
    this is unlikely to yield tradeable signals, it illustrates the process of how
    to extract features from relevant text data and train a model to predict an outcome
    that may be informative for a trading strategy.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用了doc2vec算法，该算法生成的是文档而不是令牌向量，以构建基于Yelp商业评论的情感分类器。虽然这不太可能产生可交易的信号，但它说明了如何从相关文本数据中提取特征并训练模型来预测可能对交易策略有信息意义的结果的过程。
- en: Finally, we outlined recent research breakthroughs that promise to yield more
    powerful natural language models due to the availability of pretrained architectures
    that only require fine-tuning. Applications to financial data, however, are still
    at the research frontier.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们概述了最近的研究突破，承诺通过可预先训练的架构的可用性来产生更强大的自然语言模型，这些模型仅需要微调即可。然而，对金融数据的应用仍处于研究前沿。
- en: In the next chapter, we will dive into the final part of this book, which covers
    how various deep learning architectures can be useful for algorithmic trading.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨本书的最后部分，该部分涵盖了各种深度学习架构如何对算法交易有用。
