- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Credit Risk Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problems such as credit scoring, fraud detection, churn prediction, credit limit
    definition, and financial behavior forecasting (among others) are constant challenges
    for banks and financial institutions, which permanently research for more accurate
    results and ways to decrease business-related risk when providing services. Most
    of these problems can be tackled by using machine learning to classify users who
    are likely to, for example, not pay their bills on time or commit fraud. In this
    chapter, the quantum machine learning side of these scenarios will be explored,
    using a permanent benchmark with classical counterparts for most of the cases.
  prefs: []
  type: TYPE_NORMAL
- en: In the current economic situation, where the stability of the markets is unpredictable
    and the way people work is always changing (thanks to the rise of the “gig economy”),
    it is harder to increase a credit product portfolio and cover a larger number
    of customer cohorts without increasing the risk for businesses. Exploring QML
    and ML methods side by side could leverage the current decision-making architectures
    of different banks, neobanks, fintechs, and other financial institutions that
    consider in their structures the possibility of lending money to companies or
    individuals.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, the ML alternative is very well suited for cases
    where we have evidence (data) but cannot fully grasp the underlying model or logic
    manually. One of the most well-known machine learning techniques to address these
    kinds of projects relates to supervised learning. Credit scoring systems are a
    common part of decision-making methods that are executed in finance. They use
    boosted decision trees (LGBM and XGBoost), random forests, support vector machines,
    clustering methods, and some regression models to get results that can be used
    to make decisions and automate scoring procedures. Similarly, QML algorithms can
    be tested and compared under the same scenario and eventually provide a business
    advantage. Typical classification algorithms in the quantum spectrum are **Quantum
    Neural Networks** (**QNNs**), **Quantum Support Vector Classifiers** (**QSVCs**),
    and **Variational Quantum Classifiers** (**VQCs**). The code and rationale expressed
    in this chapter can help data science departments or any machine learning-related
    professionals in the finance sector who are usually trying to improve their models,
    finding ways to determine the right approach to extract more benefit from the
    available computational power.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will compare machine learning and quantum machine learning
    algorithms using classical data represented in synthetic datasets, which are statistical
    replicas of real companies or data points. The aim is to assess their credit scores
    based on financial behavior. Additionally, we will analyze the characteristics
    of the dataset and examine the implications of using synthetic data. Finally,
    we will delve into model evaluation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter has the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The relevance of credit risk analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data exploration and preparation to execute both ML and QML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementation of classical and quantum machine learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantum Support Vector Machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The relevance of credit risk analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the objective of providing a broader context and understanding of the relevance
    of addressing classification problems in the finance sector, for this part of
    the book, it is important to define some core concepts, even from a high-level
    perspective. The term “credit risk” in the context of this chapter is the chance
    that a lender will lose money if a borrower doesn’t pay back a loan by a certain
    date. As the credit card business has grown quickly, as illustrated in *Figure
    6**.1*, and the financial players have grown over the years, the challenge of
    expanding the scope of targeted people requires more sophisticated underwriting
    systems. This puts a big portion of financial institutions at risk if the means
    to assess this risk are not accurate enough.
  prefs: []
  type: TYPE_NORMAL
- en: Given the situation previously described, it is often necessary to look at the
    credit risk of customers who have little or no credit history to expand the current
    client segments and find profitability in unexplored markets. Another challenge
    for credit decisioning systems is that most historical datasets related to credit
    card or loan clients are imbalanced because, usually, the portion of customers
    that do not pay on time is small compared with the whole sample. However, this
    could change, depending on the country and the way the target’s social and economic
    structure is set up by the financial institution. Therefore, doing a sensible
    data analysis before starting any project of predictive nature will become critical,
    as it will have a direct impact on the business revenue. Identifying unbalanced,
    biased, or incomplete information is a fundamental part of any data-driven project
    that you might endeavor (no matter whether it’s classical or quantum related).
  prefs: []
  type: TYPE_NORMAL
- en: Most of the machine learning systems on the market have been tested widely,
    and it is well known that, in some cases, they can accurately estimate the number
    of people who could default on their loans or credit card payments. Since the
    early 2000s, the field of machine learning has undergone tremendous growth, and
    the models for credit scoring strategies are now in a better position to help
    large and small banks by doing in-depth credit checks on their customers and prospects,
    mainly using technology to improve their decisioning structures so that assumed
    risk is within the acceptable limits, which may differ depending on the type of
    institution you belong to (see *Figure 6**.1* and [https://wolfstreet.com/2019/05/22/subprime-profit-machine-hiccups-credit-card-charge-offs-rise-in-the-banking-system/](https://wolfstreet.com/2019/05/22/subprime-profit-machine-hiccups-credit-card-charge-offs-rise-in-the-banking-system/)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – The yearly risk of new players trying to expand their portfolio](img/B19146_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – The yearly risk of new players trying to expand their portfolio
  prefs: []
  type: TYPE_NORMAL
- en: Data science and analytics departments in private corporations try to find more
    accurate and efficient ways to measure credit risk (Crouhy, et al., 2000; Crook,
    et al., 2007). Currently, a significant number of financial institutions are investigating
    quantum machine learning methods that can help them make better decisions and
    customize products for a large number of people. Banks such as CaixaBank (Spain),
    Itaú Unibanco (Brazil), alt.bank (Brazil), and Rauva (Portugal) specifically look
    for the benefit of **QML** methods for classification challenges related to risk
    or customer retention (also called *customer churn* in the industry literature)
    using hybrid quantum-classical algorithms. Also, banks such as BBVA (Spain), Crédit
    Agricole (France), Citi (United States), Barclays (United Kingdom), JP Morgan
    (United States), Royal Bank of Canada (Canada), Wells Fargo (United States), and
    HSBC (United Kingdom) look into and use different strategies for this new technology.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration and preparation to execute both ML and QML models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned before, in this chapter, we will walk you through the implementation
    of hybrid quantum-classical algorithms and how they behave in a real-world scenario
    in finance, but before you start playing with them in a professional setup, you
    should think – or at least review – some the following concepts.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data enrichment** refers to the process of enriching or supplementing an
    existing dataset with extra information. Data enrichment in the context of credit
    scoring systems is the use of additional data sources to supplement extra variables
    and features that could come from a credit bureau or a non-traditional source
    (e.g., mobile data mining) in order to increase the accuracy of credit risk assessments.'
  prefs: []
  type: TYPE_NORMAL
- en: By incorporating additional data sources like public records (digital footprints),
    social media behavior, financial history, open finance, and other alternative
    data sources, data enrichment can help bridge the gaps in information for a thorough
    analysis of customers. For instance, a lender might utilize a third-party service
    to verify a borrower’s job, income, and assets by obtaining data from financial
    institutions, tax authorities, and credit card institutions.
  prefs: []
  type: TYPE_NORMAL
- en: Creditors can make more informed choices regarding creditworthiness and lower
    the risk of default or delinquency by incorporating new data from credit bureau
    reports. Moreover, data enrichment may assist lenders in identifying new client
    categories and creating more customized solutions based on borrower profiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, the typical data sources of a credit scoring system can be aggregated
    into three main groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Internal data**: Most financial institutions will provide credit to current
    customers that use checking or current accounts. Analyzing this behavior should
    be the base for any further decision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Financial behavior data**: Retrieve all the financial data required to assess
    the financial behavior of an organization or individual, considering their payment
    history, risk scores, current debts, demographics, and current financial products
    in use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Out-of-the-box data**: This includes data that comes from different sources
    compared with the sources of traditional bureaus (for example, Equifax). It is
    well known that some financial institutions use psychological factors, smartphone
    metadata, and users’ digital footprints to add a significant number of variables
    and features in decisioning models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature analysis is the process of determining the most influential factors
    or features on the performance of a machine learning model. In the context of
    credit scoring systems, feature analysis is the process of discovering the most
    predictive characteristics that can be utilized to make correct credit decisions
    and discriminate correctly between potential good or bad payers.
  prefs: []
  type: TYPE_NORMAL
- en: Credit scoring models that employ machine learning often incorporate a number
    of characteristics or descriptive variables, including payment history, credit
    usage, credit tenure, and types of credit used. Nevertheless, not all of these
    characteristics may be equally significant in predicting credit risk, and some
    characteristics may have a greater influence than others.
  prefs: []
  type: TYPE_NORMAL
- en: Feature analysis aids in identifying the most influential variables that impact
    credit risk and prioritizing them in a model. Several approaches, including correlation
    analysis, decision trees, and gradient boosting algorithms, can be used to determine
    the characteristics that have the highest predictive potential.
  prefs: []
  type: TYPE_NORMAL
- en: By concentrating on the most essential variables, machine learning models for
    credit scoring can increase precision and lower the chance of default or delinquency.
    Feature analysis can also assist lenders in establishing more tailored risk management
    strategies by enhancing their understanding of the determinants of credit risk.
  prefs: []
  type: TYPE_NORMAL
- en: It is essential to remember that feature analysis techniques are an ongoing
    process, and the most relevant factors may change as economic conditions, customer
    behavior, and other variables alter. Thus, machine learning models for credit
    scoring must be continually updated and adjusted to account for the ever-changing
    nature of credit risk.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most well-known strategies and methods applied to execute the feature analysis
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature selection**: This is a non-trivial process that can have a tremendous
    impact on the final results, depending on the case. There is a myth about machine
    learning projects that more data and variables are always good, which is true,
    but not all of the information will be useful for the ML model. In some scenarios,
    it is actually better to reduce those features to allow a better prediction (Ji
    et al.). To execute these processes, there are a few techniques that consider
    the use of genetic algorithms, or simply analyze the importance, correlation,
    and variance of the features to decide which ones add more value to the predictive
    process. Typically, this stage is included in the data science procedure called
    **Exploratory Data Analysis** (**EDA**), which involves the investigation of datasets
    to extract the best data from them as input for subsequent operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: Once the data and the features are available, the
    original variables might not be enough to develop good results under your specific
    target or key demographics. If so, you may be required to build new features that
    can come as a result of a calculation from the original variables (e.g., if we
    have the customer’s transactions, additional features can be generated that consider
    the average value of the transactions and the same with the median, maximum amount,
    and minimum amount). These new columns inn the datasets can have a high impact
    on the machine learning model’s **Key Performance** **Indicators** (**KPIs**)
    later on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of modifying and preparing data for use in machine learning models
    is known as data preprocessing. In the context of credit scoring systems, data
    preparation entails cleaning, converting, and preparing credit data so that it
    may be efficiently utilized to train machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Credit data can sometimes be incomplete and disorganized, with missing numbers,
    inconsistent formats, and other difficulties that might hinder the effectiveness
    of machine learning models. Techniques for data preparation can assist in addressing
    these challenges and preparing data for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'By utilizing data pre-treatment approaches, machine learning models for credit
    scoring can be trained more efficiently and yield more accurate results. Here
    are two of the most relevant approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Oversampling**: It is common that financial datasets are imbalanced with
    regard to the target variable that a model is predicting. Having distributions
    of 95% non-defaulters and 5% defaulters could represent a significant difficulty
    for ML architectures. There are different techniques to mitigate this problem
    that will allow you to increase one of the classes or simply provide more statistical
    copies of your initial dataset. A typical strategy is to use the **Synthetic Minority
    Oversampling Technique** (**SMOTE**) or synthetic oversampling in general, with
    the application of several sub-strategies. By using these kinds of techniques,
    a financial dataset can be balanced to represent an equal portion of good and
    bad payers; therefore, the ML training process can be impacted positively in terms
    of the data points and patterns to be analyzed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Encoding**: Usually, data comes in different forms and data types. To be
    able to process them within an ML or QML algorithm, we need to ensure that all
    features are numerical, and frequently, a portion of the variables are originally
    categorical. To be able to continue the process, there are a few methods to encode
    categorical data as numerical data, such as one-hot encoding, label encoding,
    binary encoding, and feature hashing. Moreover, QML models pose the challenge
    of translating classical data samples into quantum states, which is a whole field
    by itself. We will explore some common embedding and feature maps within the regime
    of the techniques we will explore, but keep in mind that many different techniques
    exist on how encoding can be tackled, from basic feature mapping to **Quantum
    Generative Adversarial Networks** (**qGANs**), such as embeddings, as we saw in
    [*Chapter 4*](B19146_04.xhtml#_idTextAnchor079) with the stock price distribution
    case.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the upcoming subsections, as mentioned earlier, the primary focus will be
    on model implementations, and it should be assumed that the dataset has been prepared
    in accordance with all the previous concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Real business data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the barriers in QML, when applied to industry cases, is the lack of open
    source examples that use representative data to recreate a true business scenario.
    Most of the tutorials and open code repositories use demo datasets to show the
    architecture of the models, usually with interesting results that are difficult
    to replicate later. In the case of the exercise in this chapter, we will use a
    synthetic copy of real small and medium businesses’ financial behavior so that
    the results and the data treatment can truly mimic an authentic bank or fintech
    scenario at the time, classifying customers by their financial risk.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Synthetic data is meant to revolutionize artificial intelligence, according
    to experts such as Gartner ([https://www.gartner.com/en/newsroom/press-releases/2022-06-22-is-synthetic-data-the-future-of-ai](https://www.gartner.com/en/newsroom/press-releases/2022-06-22-is-synthetic-data-the-future-of-ai)).
    Trained on real-world data samples, data generators are able to create samples
    of realistic synthetic data points that are statistical replicas of the original
    ones. The model first discovers the patterns, correlations, and statistical characteristics
    of the sample data so that it can then fake samples similar to those. It is basically
    the same technology behind all the buzz there has been around deepfake technology
    but, in this case, used for corporate tabular data. One of the benefits of using
    synthetic data is the ability to curate the original dataset fixing bias, data
    sample balance, and missing data, as the generator is able to overcome these issues
    by learning the relationship between the features, composing a realistic data
    sample (Figueira et al.). It is particularly relevant when customer data is involved
    in the process, as this can pose privacy-related issues that may be a burden to
    an already complex analytic project. Luckily for us, given that synthetic data
    refers to no specific individual, if properly synthesized, it would be free of
    any regulatory restriction.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data looks and has the same meaning as the actual data sample used
    to train the algorithm. Since the synthetic dataset includes the same insights
    and correlations as the original, it is a great stand-in for the original. Taking
    this into account, the extracted information can be used safely as training data
    to build machine learning models and test data, such as when testing a credit
    scoring or fraud detection system (Assefa et al.).
  prefs: []
  type: TYPE_NORMAL
- en: 'The main benefits of this method for generating new data are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It avoids **General Data Protection Regulation** (**GDPR**) and other legal
    constraints when data is shared between institutions or is simply used to train
    ML models under different environments, without the risk of PII data leakage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It detects more efficiently outliers in cases such as fraud. Fraudulent actions
    are a small fraction of the total activities tracked by a bank. Due to the data
    size, it is difficult for a machine learning model to learn from this sort of
    dataset in order to identify new instances of fraud, and inaccurate findings may
    be a consequence. Undersampling and oversampling are two methods to deal with
    imbalanced datasets. Undersampling is the process of deleting (in this example)
    non-fraud observations to get a balanced dataset. In contrast, oversampling generates
    fresh examples of fraudulent behavior that mimic genuine fraud. The ML model may
    then be trained on the balanced dataset to provide more precise outcomes. In order
    to get a balanced dataset, synthetic data creation methods might be employed to
    generate fictitious cases of fraud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It improves existing machine learning models, since most of the algorithms related
    to supervised learning and deep learning are usually data-hungry. Even if a financial
    institution has sufficient data to train an ML model, data quantity has a significant
    impact on the accuracy of ML models. Synthetic data can be used to expand the
    size of a dataset dramatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The exercise that will be applied in this chapter is built on synthetic public
    data from **small and medium businesses** (**SMBs**) that has been released on
    the Kaggle data science platform by the company NayaOne ([https://www.kaggle.com/datasets/nayaone/sme-uk-businesses-financial-statistics](https://www.kaggle.com/datasets/nayaone/sme-uk-businesses-financial-statistics)).
    The dataframe’s structure is composed of 10 distinct CSV files, each of which
    contains information about a different aspect of a firm, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Account Receivable: Entails the money owed by clients for billed goods or services'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Businesses: A list of companies and their details'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'COVID: The financial data of firms during pandemic waves'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Credit Account History: Refers to the history of a credit account'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Credit Card History: Entails a record of the business’s credit card activity
    and associated debt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Credit Rating: A quantitative evaluation of a borrower’s creditworthiness in
    general or relative to a financial obligation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Director: An individual from the United Kingdom who holds a director role in
    the businesses presented in the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Factoring: Data related to the process when a firm sells its accounts receivable
    to a third party at a discount in a factoring transaction, which is a kind of
    debtor financing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loan: Details on paid and outstanding loans taken out by an organization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When all the CSV files are put together, they give a total of 269 variables.
    The target variable that determines whether a company is a “good” or “bad” payer
    is based on how the company actually behaved when it had debts, classifying them
    into four distinct groups, namely the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A potential defaulter in terms of loans**: In the context of loans, a debt
    is considered overdue if there was at least one delay in the payment of the loan
    installments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A potential defaulter in terms of credit cards**: A credit card account is
    deemed overdue if there was at least one payment delay'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A defaulter**: The corporation is deemed to have defaulted on the loan when
    the loan status variable is recorded as “default”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Late payers**: A delinquent mark was assigned to all firms with more than
    four late payments in the previous five years'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these previous rules defined good behavior in SMBs for 71.46% of the cases
    and bad behavior for the remaining 29.54%. Of course, as is customary, all the
    variables used to calculate the target class variables were dismissed from the
    dataset to avoid highly correlated features.
  prefs: []
  type: TYPE_NORMAL
- en: Provider of the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NayaOne is a disruptive start-up from the UK that was founded in 2019\. Its
    revolutionary goal is to provide a single point of access to hundreds or even
    thousands of synthetic data points, enabling machine learning models for the finance
    industry to be built without having to wait months for the proprietary data to
    be ready. Using NayaOne’s platform, every insurtech, fintech, or bank can prototype
    ML models or architectures in weeks rather than months or years.
  prefs: []
  type: TYPE_NORMAL
- en: On top of the benefits of faster prototyping to match an ideal time to market,
    the cost of experimentation can be reduced by at least 80%.
  prefs: []
  type: TYPE_NORMAL
- en: Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The areas of the features used for the model are the following, based on an
    EDA and feature selection analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '`primary_sector`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`factoring_provider`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revenue`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cant_invoices`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`payment_index`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_recruitments`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, 164 features were chosen as being very important to train the model
    after all the techniques were applied.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of classical and quantum machine learning algorithms for a credit
    scoring scenario
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applying machine learning and quantum machine learning for credit scoring challenges
    requires the development of a prediction model that can properly determine an
    individual’s or company’s creditworthiness. Typically, this procedure, as shown
    in the steps described previously, includes data collection, data enrichment,
    data preparation, feature engineering, feature selection, model selection, model
    training, model evaluation, and subsequently, deployment. In this section, we
    will cover most of the previous concepts and procedures, assuming that the data
    is already encoded to numerical variables and the feature has been selected.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, the data needs to be loaded. This data will come in one of the more well-known
    formats in the industry, which is CSV. The information that will load into the
    notebook, as previously detailed, is in a classical format, so we can handle the
    first steps of our architecture with pandas and scikit-learn without issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step one of the preprocessing is loading the CSV, defining the *X* and *y*
    values of the experiment, and later, splitting the train and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'At the moment, to load your data, it’s important to split the dataset into
    two groups – the dataset that will be used to train a model and the dataset that
    will be used to evaluate the model fitness. By having a dataset that has never
    been used to train a model, we aim to set realistic measures for when our models
    will face real incoming data coming when operating in the real world. This dataset
    already contains identifiers to split those two sets from the merged CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, it is simple to split those two sets into feature and target attributes.
    Columns not containing relevant information or that could fake our ability to
    predict over a further test set, given that the label information is encoded,
    need to be removed from the attributes that our model will be able to retrieve
    in reality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s important to always review the distribution of the target variable because
    if it is highly imbalanced, it could drive the model towards a simplified decision
    that all samples must be treated like the majority of them. In such cases, other
    oversampling techniques may be necessary to increase the performance of the model
    and balance it with the different sample groups within the dataset. In the example
    shown here, the dataset is balanced enough (70–30 between our two main classes
    of individuals) in preparation for the next sections, those related to the model’s
    architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The preprocessing stage is one of the most important steps in any machine learning
    project and model’s architecture. In hybrid quantum-classical algorithms, the
    input also needs to be transformed from classical features to quantum states,
    which are usually called quantum encodings or quantum embeddings. During this
    section, these ideas will be explained in detail, but first, we will focus on
    the classical preprocessing methods that will be extended during the quantum coding
    part.
  prefs: []
  type: TYPE_NORMAL
- en: After the train and test datasets have been obtained, the preprocessing step,
    especially the dimensionality reduction, will take place. As previously mentioned,
    not all attributes hold the same amount of information, and it is crucial to concentrate
    the majority of it to avoid burdening the model with unnecessary work in determining
    the relevance of features. Tree-based models, for example, do take into consideration
    the information gain each attribute provides (Tangirala 2020), and neural networks,
    for example, will render into zero value weights to those attributes that do not
    contribute to improving the decision. However, certain techniques, such as the
    ones we will explore, lack this capacity, and it is important we make a realistic
    exercise, easing the work required by the subsequent model training steps.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few techniques that can be used in this regard, and in the context
    of QML, they diminish the number of variables to be encoded into quantum devices,
    with fewer qubits to run classification tasks. Recall that this is relevant also
    because of the capacity of some quantum devices, which range from 10 or 100 qubits,
    while datasets can expand to thousands of features for each data point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two most utilized techniques are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal component analysis (PCA) (Bro & Smilde 2014)**: By analyzing the
    covariance matrix and expressing it in terms of linear combinations between various
    features, thereby generating orthogonal axes or principal components, PCA provides
    an ordering where the bottom principal components can be removed, as they encode
    a minimal amount of information. With this technique, the number of components
    or features that we want to extract from the process is a parameter that we can
    define, with the only constraint being that the features should not surpass the
    original number of variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear discriminant analysis (LDA) (Izenman 2013)**: LDA is a supervised
    algorithm that reduces the feature space by considering class labels. LDA focuses
    on target separability and tries to define the characteristics that better map
    this separable space. If LDA is used, there is a limitation on the number of features
    that could come from the reduction, since LDA can only provide N − 1, where N
    is the number of classes available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The diagram in *Figure 6**.2* highlights the main difference between the two
    techniques. In this case, LDA will be used because it has a proven record of surpassing
    PCA, with a reduced number of qubits and a binary classification (Mancilla and
    Pere 2022). The main constraint is that, previously, the database would have been
    split with the objective to extract more than one component per set and to have
    at least two components for the 2-qubit approach that followed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – A comparison between PCA and LDA](img/B19146_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – A comparison between PCA and LDA
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [https://sebastianraschka.com/Articles/2014_python_lda.html](https://sebastianraschka.com/Articles/2014_python_lda.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of simplicity, `X_train` and `X_test` will be split in half, allocating
    50% of the features to the `features_a` set and the rest to `features_b`. Since
    we have 167 features, 83 will be allocated in one half and 84 in the other. It
    is highly recommended that this process is replaced by clusters, variable types
    groups, data sources, or a correlation definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Another factor that is common is scale disparity. It is important that we make
    our models agnostic to scale and able to focus on the information encoded into
    our features. It is also convenient when dealing with neural networks, as normalization
    always helps with their convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we will replicate the information in different dataframes, adjusted
    to the target techniques we will use in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It is important to highlight that this transformation should only affect the
    scale of the data, as their distribution and the relationship between different
    attributes associated with each sample of our dataset should remain intact. It
    mostly affects the way in which each algorithm is trained and tries to make this
    task easier by scaling the information (Singh and Singh 2020).
  prefs: []
  type: TYPE_NORMAL
- en: Quantum Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Support Vector Classifier** (**SVC**) and **Quantum Support Vector Classifier**
    (**QSVC**) are the first models that will be used to look at our synthetic dataset,
    and we will see how a quantum algorithm versus a classical algorithm can work
    to find potential defaulters. One of the most widely used techniques is known
    as **Support Vector Machines** (**SVM**) (*Hearst et al., 1998*), which make use
    of hyperplanes in order to find separable spaces within our data regime. These
    hyperplanes are responsible for separating our N-dimensional information into
    different spaces, trying to maximize the margin between samples from the regions
    split by the hyperplane itself. By softening this margin constraint and allowing
    some samples to be misclassified, we allow the model to generalize from the dataset
    itself. This softened version is what we will call an SVC.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to the abstraction level that Python libraries such as scikit-learn
    provide, its usage is as simple as calling a fit function to pass the dataset
    and target data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As previously mentioned, it is important to pay attention to relevant metrics
    that will help us better understand the actual fitness of the model besides that
    single metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is when the relevance of unbalanced datasets shows its ugly face, as we
    can see that our accuracy is mostly driven by our ability to detect non-defaulting
    customers.
  prefs: []
  type: TYPE_NORMAL
- en: Given the nature of SVMs and their requirement to handle space separability,
    one of the initial proposals in the realm of QML essentially leverages the inherent
    high-dimensional encoding offered by quantum states. **Quantum Support Vector
    Machines** (**QSVMs**) (Rebentrost et al. 214) encode classical data into quantum
    states so that after a measurement is obtained, encoded samples show better separability
    – in this case, performed by classical means.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main benefits of the QSVM approach is how data points are represented
    in a quantum feature space, due to the use of quantum kernels (visualized in *Figure
    6**.3*). Usually, businesses can face non-linear separable data, and the use of
    classical kernel methods is not enough to properly divide two classes in the case
    of binary classification models. With the usage of quantum kernels, data points
    can be distributed in a Hilbert space so that they can be divided more efficiently
    with the algorithm (SVC). In *Figure 6**.3*, you can see a visual example of a
    workflow that presents both approaches (classical and quantum) and how data could
    be represented in the different feature spaces to gain linear separability ([https://quantum-journal.org/papers/q-2021-08-30-531/](https://quantum-journal.org/papers/q-2021-08-30-531/)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – A comparison of how classical and quantum kernels embed data
    points](img/B19146_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – A comparison of how classical and quantum kernels embed data points
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, one of the first steps we will face when attempting a QSVM classifier
    is encoding classical data into a feature map that performs the actual quantum
    encoding for us. There are several options we can choose from, and the gain we
    will obtain will come from arbitrary decisions on the number of repetitions each
    feature map is performed with. This is a common issue we can face in data science
    projects, where some decisions will be based on experience of given datasets or
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, a quantum feature map encodes conventional data into a quantum state
    space by employing a quantum circuit. The number of times a circuit is repeated
    during encoding is called “depth,” and it is a parameter that can be changed.
    QML on classical data requires the encoding of data into quantum states to be
    applied later to the quantum algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Qiskit provides a set of candidate feature maps we can use for our task of
    running a classification model. Most of them are based on Pauli operators, rotating
    according to our data features. That is one of the reasons why data needs to be
    preprocessed so that its numerical representation can be introduced as rotation
    angles in our feature map. Using combined Z rotations along the Z axis of each
    qubit is a common case that can be invoked, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: reps makes reference to the number of instances a feature map is repeated and
    is one of those hyperparameters that we might need to play with in order to find
    its best option. *Figure 6**.4* shows the representation of our selected feature
    map composed of a set of rations (P gates in pink) and entangling CNOT gates.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – The ZZFeature map with three repetition schemes](img/B19146_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – The ZZFeature map with three repetition schemes
  prefs: []
  type: TYPE_NORMAL
- en: The *X* variables within the boxes in *Figure 6**.4* represent the features
    of our dataset, which are used as a rotation weight that will drive the initial
    state of the circuit (|00> for the 2-qubit case) toward the one used as input
    in our SVC classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Qiskit also provides higher-level abstractions for these QML tasks, so it is
    not required to produce the whole code for known techniques, as is the case with
    QSVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though it may seem like the same result has been obtained, we could check
    the classification report as we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can see some of the decisions have moved between the different classes, losing
    precision and gaining recall in some cases, but we could, by simple encoding,
    improve our detection of defaulters by two percentual points.
  prefs: []
  type: TYPE_NORMAL
- en: It might seem like a small gain, but considering the figure we saw at the beginning
    of this chapter, a 2% increase in detection is a huge improvement in terms of
    revenue. It’s definitely a technique worth trying out, given how simple its adaptation
    from the classical regime is.
  prefs: []
  type: TYPE_NORMAL
- en: QNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Following the same structure as the previous comparison, we will now process
    our dataset of good customers and defaulters through quantum and classical neural
    networks. Neural networks of all kinds and shapes populate almost every machine
    learning initiative that a corporation may try (*Bishop 1994*). These models are
    versatile for a plethora of tasks, and their general framework allows for all
    kinds of architecture suitable for specific tasks – convolutional neural networks
    for artificial vision (*Khan et al., 2018*), recurrent neural networks for natural
    language understanding (*Yao et al., 2013*), or generative adversarial networks
    for synthetic data generation (*Park et* *al., 2018*).
  prefs: []
  type: TYPE_NORMAL
- en: Risk assessment is not absent from similar exercises (*Khashman 2010*), so given
    these are popular choices, we must try at least. Popular Python frameworks such
    as TensorFlow and its higher order abstraction, thanks to Keras, simplify much
    of the code needed to train a bioinspired trendy model. The code below follows
    an architecture where input data is passed through a series of layers in decreasing
    number of neurons (60, 40, 20 and 1), down to the point where the outcome of the
    last neuron determines the class input sample may belong to. That is why a Sigmoid
    function is defined for that last step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Densely connected layers are our choice for this example and a **Rectified
    Linear Unit** (**ReLU**) is the neuron of choice (*Agarap 2018*), but as previously
    discussed, these decisions are not driven by any specific process, other than
    experience and trials in similar scenarios where this architecture proved successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Training will yield a high accuracy score (above 90%), but the test dataset
    is the one that provides realistic expectations of the model’s ability to predict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Even though accuracy might seem to have increased, we have sacrificed the ability
    to identify non-defaulters and defaulters, which, even though they give a more
    balanced dataset, are not as efficient as we would like for our business needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that we already have an encoding sorted out, based on our previous exercise
    using ZZFeatureMap, it would be good to have a scheme where the whole model gets
    embedded into the quantum device so that once the data is represented in this
    feature map, the ability of quantum devices to disentangle complex relationships
    can be exploited fully. This can be done by QNNs (*Ezhov and Ventura, 2000, and
    Farhi and Neven, 2018*), a data embedding that is followed by a **Parameterized
    Quantum Circuit** (**PQC**) or ansatz, whose measurement sets the class that each
    sample belongs to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 6.5 – A QNN obtained by the preceding code](img/B19146_06_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – A QNN obtained by the preceding code
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in the obtained circuit, the feature map only has two inputs
    belonging to the features in our dataset, but the PQC has five parameters that
    we need to determine in order to proceed. The way to perform such a task is by
    variationally adapting the output of the QNN model so that it minimizes a cost
    function, the mismatch between our labeled data, and the measured classical bit
    in our case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Qiskit Machine Learning provides a whole machinery that will do the heavy lifting
    for us so that once the QNN structure is declared by these two steps (the feature
    embedding and the PQC), we just need to invoke a set of routines that will obtain
    the final model for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: VQC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we will analyze the benefits of using a VQC, which only exists on the
    quantum side of machine learning; therefore, a comparison can’t be made under
    the same principles. The VQC (*Havlíček et al. 2019*) is nothing more than a generalization
    of previously seen cases of QSVC and QNN. It allows for a broader description
    of the aforementioned concepts on data embedding and circuit parameterization
    but with a less restrictive setup, allowing any architecture to be deployed. The
    only restriction concerns the variational nature of obtaining the parameters for
    our ansatz and the outcome restricted to the task of classification.
  prefs: []
  type: TYPE_NORMAL
- en: Even though previously Qiskit-based approaches have been used, for this more
    generic setup, PennyLane is our framework of choice. This is mostly because of
    its functionality of differential programming that enables similar mechanisms
    for numerical gradient calculations, such as the ones popularized by TensorFlow
    and PyTorch, but also because of its access to gradient-based trainers such as
    Adam (*Kingma and* *Ba, 2014*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our circuit setup will follow a similar description as the previous example,
    with some differences in embedding and ansatz choices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`Angle` embedding encodes the dataset features into the angles of *Y* rotations
    and `StronglyEntanglingLayers` follows a scheme of single rotations and multiple
    CNOT entangling gate operations, which perform a circular link between all qubits
    (Schuld et al. 2020).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, the circuit will be linked to a device that PennyLane calls a `QNode`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'PennyLane allows fine-grained control over the different functions that will
    be used within the training of the model so that we can, for example, decide on
    the level of hybridization between classical and quantum means without much effort.
    In this example below, a classical bias neuron is added to the VQC scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The bias effect allows for a broader range of approximations when added, so
    it should work in our favor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Once our score loss function is defined, we need to define an accuracy metric
    that will also be used as the criteria for parameter selection within the main
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Our final score shows an increased ability to detect defaulters, but even though
    the number of low-risk individuals has decreased, it still reaches the levels
    of the initial SVM and QSVM approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Classification key performance indicators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In each one of the algorithms tested so far, you can see that there is a classification
    report that gives us the possibility to understand how good a model is in terms
    of predicting each class correctly. Deciding the **Key Performance Indicator**
    (**KPI**) to evaluate the model’s performance is not a trivial decision. Most
    people assume that accuracy is the most important measure to see whether a model
    is working properly or not, with regard to the objective of predicting a specific
    class. Imagine that your credit scoring dataset is imbalanced, with a proportion
    of 5% defaulters and 95% good customers that pay on time. If the model predicts
    that all the test set is good and there are no defaulters, you will have 95% accuracy,
    which is bad.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned before, it is common to face imbalanced datasets in the financial
    sector, so we usually need to look at a classification report to see which metric
    is the best to measure. The classification report shows the precision, recall,
    F1 score, and support per class. Digging deeper, it’s important to see first that
    there are four ways to evaluate whether the predictions are good enough or not.
    These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Negative (TN)**: The case was originally negative and the model predicted
    negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Positive (TP)**: The case was originally positive and the model predicted
    positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negative (FN)**: The case was originally positive but the model predicted
    negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive (FP)**: The case was originally negative but the model predicted
    positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These four measures are the result of the model, and usually, you can see them
    reflected in a data visualization format called a confusion matrix, as shown in
    *Figure 6**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – The confusion matrix](img/B19146_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – The confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'Using these metrics, we can calculate the classification report outcome, which
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision (TP/(TP + FP))**: This is the capacity of a classifier to avoid
    incorrectly labeling instances as positive when they are truly negative. It considers
    the set of rightfully predicted instances (**TP**) with respect to the number
    of samples defined as a positive class (**TP** + **FP**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall (TP/(TP+FN))**: This is a classifier’s capacity to locate all positive
    examples. Considering all samples in the positive class of the dataset or population,
    rates the correctly identified positives (**TP**) with respect to the positive
    population (**TP** + **FN**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1-score (2*(Recall * Precision) / (Recall + Precision))**: This is a weighted
    harmonic mean of accuracy and recall, where the highest possible score is 1.0
    and the lowest possible score is 0.0\. F1 scores are lower than accuracy measurements,
    since their computation includes precision and recall. As a general rule, the
    weighted average of F1 should be utilized to compare classifier models rather
    than global accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support**: This is the number of real instances of a class in a given dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these numbers are given automatically by libraries such as Scikit-Learn,
    but it is very important to understand them before deciding if the model is working
    well or not.
  prefs: []
  type: TYPE_NORMAL
- en: Balanced accuracy, or ROC-AUC score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will add another metric to the classification report that
    is related to the same baseline of the confusion matrix numbers, as can be seen
    in *Figure 6**.7*. To measure the performance of an imbalanced dataset in a test
    set, we cannot rely on accuracy, as it may provide inaccurate estimators, prioritizing
    the class that is predominant and drastically failing to detect the less present
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **receiver operating characteristic** (**ROC**) curve is a plot or graph
    representing the performance of the classification at all thresholds (the cut-off
    probability to declare an instance as part of a specific class). Basically, this
    curve is based on two parameters, based on the metrics mentioned previously, which
    are the **True Positive Rate** (**TPR**), equivalent to recall, and the **False
    Positive** **Rate** (**FPR**):'
  prefs: []
  type: TYPE_NORMAL
- en: '| True Positive Rate | False Positive Rate |'
  prefs: []
  type: TYPE_TB
- en: '| TPR =  TP _ TP + FN  | FPR =  FP _ FP + TN  |'
  prefs: []
  type: TYPE_TB
- en: Plotting the TPR against the FPR values in opposing axes, we obtain a plot,
    also known as a curve (hence the name).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – An ROC curve representation](img/B19146_06_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – An ROC curve representation
  prefs: []
  type: TYPE_NORMAL
- en: '**AUC** is an acronym for **Area Under Curve**, referring to the ROC curve.
    Thus, AUC measures the full two-dimensional area underneath the complete ROC curve
    (consider integral calculus) and has a score from 0 to 1\. There is no consensus
    about the specific AUC score number to define a model as good enough or not, but
    here are some considerations (assuming that higher is better):'
  prefs: []
  type: TYPE_NORMAL
- en: '**AUC score of 0.50**: Not useful'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AUC score 0.51–0.70**: Not enough discrimination to be a performant model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AUC score 0.71–0.80**: Acceptable as good enough discriminant'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AUC score 0.81–0.90**: Excellent discriminant'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AUC score >0.90**: An outstanding discriminant or an indicator of model overfitting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of binary classification models, the AUC score is equal to the balanced
    accuracy score. It represents the arithmetic mean between correctly classified
    samples in each category.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, even if accuracy is a common measure from the classification
    report that most people will look at, the way to treat this kind of imbalanced
    data scenario is to compare the models using a balanced accuracy score, or AUC
    score, which in this case are the same, since it is a binary classification challenge.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – A comparison of classification results between classical and
    hybrid quantum-classical methods](img/B19146_06_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – A comparison of classification results between classical and hybrid
    quantum-classical methods
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, the results do not appear to be conclusive about the benefits
    of using hybrid quantum-classical for classification problems that the finance
    sector may face. However, the purpose of the exercise in this chapter is to get
    people to think about their own business challenges and do more research, since
    we can see that quantum machine learning could be at least equal or slightly better
    than classical ML methods (e.g., QSVC versus SVC). When any incremental benefit
    is achieved in terms of the model’s capacity to detect each class properly, even
    if it seems a small increase, its impact on a business can actually mean high
    figures of saving or earnings significant in a competitive market.
  prefs: []
  type: TYPE_NORMAL
- en: 'On top of the architecture developed for this classification example, there
    are several more adjustments that can be explored to increase the performance
    of the hybrid algorithms. Some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Test different preprocessing methods such as normalization or dimensionality
    reductions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore better procedures to apply the division in the dataset for the LDA step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze different methods to execute feature selection in terms of the quantum
    enhancement of the architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate more feature maps to encode the data in quantum states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customize feature maps manually to measure the best approach for the individual
    problems faced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase the qubits used to detect whether there is an advantage or disadvantage
    when a higher number of quantum bits are used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train a quantum kernel to be optimized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change iterations and shots for the sake of finding the best parameters, with
    regard to the model classification result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execute the algorithms in different types of simulators and quantum hardware
    to evaluate the impact of the backends in the final result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After deciding to dive into these types of implementations, it’s critical to
    remember that the preprocessing and encoding steps of the quantum algorithms application
    process can be the key to retrieving successful results. We will see in [*Chapter
    7*](B19146_07.xhtml#_idTextAnchor145) how and which cloud provider we can use
    to run this type of project. We will learn how to use different hardware and how
    to access them through the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Agarap, A. F. (2018). Deep learning using rectified linear units (relu). arXiv*
    *preprint arXiv:1803.08375.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Assefa, S. A., Dervovic, D., Mahfouz, M., Tillman, R. E., Reddy, P., & Veloso,
    M. (2020, October). Generating synthetic data in finance: opportunities, challenges
    and pitfalls. In Proceedings of the First ACM International Conference on AI in
    Finance (**pp. 1–8).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bishop, C. M. (1994). Neural networks and their applications. Review of scientific
    instruments,* *65(6), 1803–1832.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bro, R., & Smilde, A. K. (2014). Principal component analysis. Analytical
    methods,* *6(9), 2812–2831.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Crook, J. N., Edelman, D. B., & Thomas, L. C. (2007). Recent developments
    in consumer credit risk assessment. European Journal of Operational Research,*
    *183(3), 1447–1465.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Crouhy, M., Galai, D., & Mark, R. (2000). A comparative analysis of current
    credit risk models. Journal of Banking & Finance,* *24(1-2), 59–117.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ezhov, A. A., & Ventura, D. (2000). Quantum neural networks. In Future directions
    for intelligent systems and information sciences (pp. 213–235).* *Physica, Heidelberg.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Farhi, E., & Neven, H. (2018). Classification with quantum neural networks
    on near term processors. arXiv* *preprint arXiv:1802.06002.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figueira, A., & Vaz, B. (2022). Survey on synthetic data generation, evaluation
    methods and GANs. Mathematics,* *10(15), 2733.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Havlíček, V., Córcoles, A. D., Temme, K., Harrow, A. W., Kandala, A., Chow,
    J. M., & Gambetta, J. M. (2019). Supervised learning with quantum-enhanced feature
    spaces. Nature,* *567(7747), 209–212.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hearst, M. A., Dumais, S. T., Osuna, E., Platt, J., & Scholkopf, B. (1998).
    Support vector machines. IEEE Intelligent Systems and their applications,* *13(4),
    18–28.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Izenman, A. J. (2013). Linear discriminant analysis. In Modern multivariate
    statistical techniques (pp. 237–280). Springer, New* *York, NY.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ji, G., & Zhu, Z. (2020). Knowledge distillation in wide neural networks:
    Risk bound, data efficiency and imperfect teacher. Advances in Neural Information
    Processing Systems,* *33, 20823–20833.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Khan, S., Rahmani, H., Shah, S. A. A., & Bennamoun, M. (2018). A guide to
    convolutional neural networks for computer vision. Synthesis lectures on computer
    vision,* *8(1), 1–207.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Khashman, A. (2010). Neural networks for credit risk evaluation: Investigation
    of different neural models and learning schemes. Expert Systems with Applications,*
    *37(9), 6233–6239.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization.
    arXiv* *preprint arXiv:1412.6980.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mancilla, J., & Pere, C. (2022). A Preprocessing Perspective for Quantum Machine
    Learning Classification Advantage in Finance Using NISQ Algorithms. Entropy,*
    *24(11), 1656.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Moritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., ...
    & Stoica, I. (2018). Ray: A distributed framework for emerging {AI} applications.
    In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI
    18) (**pp. 561–577).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Park, N., Mohammadi, M., Gorde, K., Jajodia, S., Park, H., & Kim, Y. (2018).
    Data synthesis based on generative adversarial networks. arXiv* *preprint arXiv:1806.03384.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rebentrost, P., Mohseni, M., & Lloyd, S. (2014). Quantum support vector machine
    for big data classification. Physical review letters,* *113(13), 130503.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Schuld, M., Bocharov, A., Svore, K. M., & Wiebe, N. (2020). Circuit-centric
    quantum classifiers. Physical Review A,* *101(3), 032308.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sergeev, A., & Del Balso, M. (2018). Horovod: fast and easy distributed deep
    learning in TensorFlow. arXiv* *preprint arXiv:1802.05799.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Singh, D., & Singh, B. (2020). Investigating the impact of data normalization
    on classification performance. Applied Soft Computing,* *97, 105524.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tangirala, S. (2020). Evaluating the impact of GINI index and information
    gain on classification using decision tree classifier algorithm. International
    Journal of Advanced Computer Science and Applications,* *11(2), 612–619.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Yao, K., Zweig, G., Hwang, M. Y., Shi, Y., & Yu, D. (2013, August). Recurrent
    neural networks for language understanding. In Interspeech (**pp. 2524–2528).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
