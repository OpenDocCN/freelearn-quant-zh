- en: Chapter 4. Understanding Time Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A time series is a form of data that has a temporal dimension and is easily
    the most iconic form of financial data out there. While a single stock quote is
    not a time series, take the quotes you get every day and line them up, and you
    get a much more interesting time series. Virtually all media materials related
    to finance sooner or later show a stock price gap; not a list of prices at a given
    moment, but a development of prices over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll often hear financial commenters discussing the movement of prices:
    "Apple Inc. is up 5%." But what does that mean? You''ll hear absolute values a
    lot less, such as, "A share of Apple Inc. is $137.74." Again, what does that mean?
    This occurs because market participants are interested in how things will develop
    in the future and they try to extrapolate these forecasts from how things developed
    in the past:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Time Series](img/B10354_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multiple time series graphs as seen on Bloomberg TV
  prefs: []
  type: TYPE_NORMAL
- en: Most forecasting that is done involves looking at past developments over a period
    of time. The concept of a time series set of data is an important element related
    to forecasting; for example, farmers will look at a time series dataset when forecasting
    crop yields. Because of this, a vast body of knowledge and tools for working with
    time series has developed within the fields of statistics, econometrics, and engineering.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be looking at a few classic tools that are still very
    much relevant today. We will then learn how neural networks can deal with time
    series, and how deep learning models can express uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: Before we jump into looking at time series, I need to set your expectations
    for this chapter. Many of you might have come to this chapter to read about stock
    market forecasting, but I need to warn you that this chapter is not about stock
    market forecasting, neither is any other chapter in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Economic theory shows that markets are somewhat efficient. The efficient market
    hypothesis states that all publicly available information is included in stock
    prices. This extends to information on how to process information, such as forecasting algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: If this book were to present an algorithm that could predict prices on the stock
    market and deliver superior returns, many investors would simply implement this
    algorithm. Since those algorithms would all buy or sell in anticipation of price
    changes, they would change the prices in the present, thus destroying the advantage
    that you would gain by using the algorithm. Therefore, the algorithm presented
    would not work for future readers.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, this chapter will use traffic data from Wikipedia. Our goal is to forecast
    traffic for a specific Wikipedia page. We can obtain the Wikipedia traffic data
    via the `wikipediatrend` CRAN package.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset that we are going to use here is the traffic data of around 145,000
    Wikipedia pages that has been provided by Google. The data can be obtained from Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The data can be found at the following links: [https://www.kaggle.com/c/web-traffic-time-series-forecasting](https://www.kaggle.com/c/web-traffic-time-series-forecasting)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/muonneutrino/wikipedia-traffic-data-exploratio](https://www.kaggle.com/muonneutrino/wikipedia-traffic-data-exploratio)'
  prefs: []
  type: TYPE_NORMAL
- en: Visualization and preparation in pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw in [Chapter 2](ch02.xhtml "Chapter 2. Applying Machine Learning to Structured
    Data"), *Applying Machine Learning to Structured Data*, it''s usually a good idea
    to get an overview of the data before we start training. You can achieve this
    for the data we obtained from Kaggle by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code will give us the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Page | 2015-07-01 | 2015-07-02 | … | 2016-12-31 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 2NE1_zh.wikipedia.org_all-access_spider | 18.0 | 11.0 | … | 20.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2PM_zh.wikipedia.org_all-access_spider | 11.0 | 14.0 | … | 20.0 |'
  prefs: []
  type: TYPE_TB
- en: The data in the **Page** column contains the name of the page, the language
    of the Wikipedia page, the type of accessing device, and the accessing agent.
    The other columns contain the traffic for that page on that date.
  prefs: []
  type: TYPE_NORMAL
- en: So, in the preceding table, the first row contains the page of 2NE1, a Korean
    pop band, on the Chinese version of Wikipedia, by all methods of access, but only
    for agents classified as spider traffic; that is, traffic not coming from humans.
    While most time series work is focused on local, time-dependent features, we can
    enrich all of our models by providing access to **global features**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we want to split up the page string into smaller, more useful features.
    We can achieve this by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We split the string by underscores. The name of a page could also include an
    underscore, so we separate off the last three fields and then join the rest to
    get the subject of the article.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the following code, the third-from-last element is the sub
    URL, for example, [en.wikipedia.org](http://en.wikipedia.org). The second-from-last
    element is the access, and the last element the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When we apply this function to every page entry in the training set, we obtain
    a list of tuples that we can then join together into a new DataFrame, as we can
    see in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we must add this new DataFrame back to our original DataFrame before
    removing the original page column, which we can do by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As a result of running this code, we have successfully finished loading the
    dataset. This means we can now move on to exploring it.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregate global feature statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After all of this hard work, we can now create some aggregate statistics on
    global features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pandas `value_counts()` function allows us to plot the distribution of
    global features easily. By running the following code, we will get a bar chart
    output of our Wikipedia dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result of running the previous code, we will output a bar chat that ranks
    the distributions of records within our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Aggregate global feature statistics](img/B10354_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of records by Wikipedia country page
  prefs: []
  type: TYPE_NORMAL
- en: The preceding plot shows the number of time series available for each subpage.
    Wikipedia has subpages for different languages, and we can see that our dataset
    contains pages from the English (en), Japanese (ja), German (de), French (fr),
    Chinese (zh), Russian (ru), and Spanish (es) Wikipedia sites.
  prefs: []
  type: TYPE_NORMAL
- en: In the bar chart we produced you may have also noted two non-country based Wikipedia
    sites. Both [commons.wikimedia.org](http://commons.wikimedia.org) and [www.mediawiki.org](http://www.mediawiki.org)
    are used to host media files such as images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run that command again, this time focusing on the type of access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, we''ll then see the following bar chart as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Aggregate global feature statistics](img/B10354_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of records by access type
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two possible access methods: **mobile** and **desktop**. There''s
    also a third option **all-access**, which combines the statistics for mobile and
    desktop access.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then plot the distribution of records by agent by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After running that code, we''ll output the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Aggregate global feature statistics](img/B10354_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of records by agent
  prefs: []
  type: TYPE_NORMAL
- en: There are time series available not only for spider agents, but also for all
    other types of access. In classic statistical modeling, the next step would be
    to analyze the effect of each of these global features and build models around
    them. However, this is not necessary if there's enough data and computing power
    available.
  prefs: []
  type: TYPE_NORMAL
- en: 'If that''s the case then a neural network is able to discover the effects of
    the global features itself and create new features based on their interactions.
    There are only two real considerations that need to be addressed for global features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Is the distribution of features very skewed?** If this is the case then there might
    only be a few instances that possess a global feature, and our model might overfit
    on this global feature. Imagine that there were only a small number of articles
    from the Chinese Wikipedia in the dataset. The algorithm might distinguish too
    much based on the feature then overfit the few Chinese entries. Our distribution
    is relatively even, so we do not have to worry about this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Can features be easily encoded?** Some global features cannot be one-hot
    encoded. Imagine that we were given the full text of a Wikipedia article with
    the time series. It would not be possible to use this feature straight away, as some
    heavy preprocessing would have to be done in order to use it. In our case, there
    are a few relatively straightforward categories that can be one-hot encoded. The
    subject names, however, cannot be one-hot encoded since there are too many of
    them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining the sample time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To examine the global features, of our dataset, we have to look at a few sample
    time series in order to get an understanding of the challenges that we may face.
    In this section, we will plot the views for the English language page of *Twenty
    One Pilots*, a musical duo from the USA.
  prefs: []
  type: TYPE_NORMAL
- en: 'To plot the actual page views together with a 10-day rolling mean. We can do this
    by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There is a lot going on in this code snippet, and it is worth going through
    it step by step. Firstly, we define which row we want to plot. The Twenty One
    Pilots article is row 39,457 in the training dataset. From there, we then define
    the window size for the rolling mean.
  prefs: []
  type: TYPE_NORMAL
- en: We separate the page view data and the name from the overall dataset by using
    the pandas  `iloc` tool. This allows us to index the data by row and column coordinates.
    Counting the days rather than displaying all the dates of the measurements makes
    the plot easier to read, therefore we are going to create a day counter for the
    *X*-axis.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we set up the plot and make sure it has the desired size by setting `figsize`.
    We also define the axis labels and the title. Next, we plot the actual page views.
    Our *X* coordinates are the days, and the *Y* coordinates are the page views.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the mean, we are going to use a **convolve** operation, which you
    might be familiar with as we explored convolutions in [Chapter 3](ch03.xhtml "Chapter 3. Utilizing
    Computer Vision"), *Utilizing Computer Vision*. This convolve operation creates
    a vector of ones divided by the window size, in this case 10\. The convolve operation
    slides the vector over the page view, multiplies 10-page views with 1/10, and
    then sums the resulting vector up. This creates a rolling mean with a window size
    10\. We plot this mean in black. Finally, we specify that we want to use a log
    scale for the *Y* axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Examining the sample time series](img/B10354_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Access statistics for the Twenty One Pilots Wikipedia page with a rolling mean
  prefs: []
  type: TYPE_NORMAL
- en: You can see there are some pretty large spikes in the Twenty One Pilots graph
    we just generated, even though we used a logarithmic axis. On some days, views
    skyrocket to 10 times what they were just days before. Because of that, it quickly
    becomes clear that a good model will have to be able to deal with such extreme spikes.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on, it's worth pointing out that it's also clearly visible that
    there are global trends, as the page views generally increase over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'For good measure, let''s plot the interest in Twenty One Pilots for all languages.
    We can do this by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet, we first set up the graph, as before. We then loop over the
    language codes and find the index of Twenty One Pilots. The index is an array
    wrapped in a tuple, so we have to extract the integer specifying the actual index.
    We then extract the page view data from the training dataset and plot the page
    views.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following chart, we can view the output of the code that we''ve just
    produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Examining the sample time series](img/B10354_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Access statistics for Twenty One Pilots by country
  prefs: []
  type: TYPE_NORMAL
- en: There is clearly some correlation between the time series. The English language
    version of Wikipedia (the top line) is, not surprisingly, by far the most popular.
    We can also see that the time series in our datasets are clearly not stationary;
    they change means and standard deviations over time.
  prefs: []
  type: TYPE_NORMAL
- en: A stationary process is one whose unconditional joint probability distribution
    stays constant over time. In other words, things such as the series mean or standard
    deviation should stay constant.
  prefs: []
  type: TYPE_NORMAL
- en: However, as you can see, between days 200-250 in the preceding graph, the mean views
    on the page changes dramatically. This result undermines some of the assumptions
    many classic modeling approaches make. Yet, financial time series are hardly ever
    stationary, so it is worthwhile dealing with these problems. By addressing these
    problems, we become familiar with several useful tools that can help us handle nonstationarity.
  prefs: []
  type: TYPE_NORMAL
- en: Different kinds of stationarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Stationarity can mean different things, and it is crucial to understand which
    kind of stationarity is required for the task at hand. For simplicity, we will
    just look at two kinds of stationarity here: mean stationarity and variance stationarity.
    The following image shows four time series with different degrees of (non-)stationarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Different kinds of stationarity](img/B10354_04_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Mean stationarity refers to the level of a series being constant. Here, individual
    data points can deviate, of course, but the long-run mean should be stable. Variance
    stationarity refers to the variance from the mean being constant. Again, there
    may be outliers and short sequences whose variance seems higher, but the overall
    variance should be at the same level. A third kind of stationarity, which is difficult
    to visualize and is not shown here, is covariance stationarity. This refers to
    the covariance between different lags being constant. When people refer to covariance
    stationarity, they usually mean the special condition in which mean, variance,
    and covariances are stationary. Many econometric models, especially in risk management,
    operate under this covariance stationarity assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Why stationarity matters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many classic econometric methods assume some form of stationarity. A key reason
    for this is that inference and hypothesis testing work better when time series
    are stationary. However, even from a pure forecasting point of view, stationarity
    helps because it takes some work away from our model. Take a look at the **Not
    Mean Stationary** series in the preceding charts. You can see that a major part
    of forecasting the series is to recognize the fact that the series moves upward.
    If we can capture this fact outside of the model, the model has to learn less
    and can use its capacity for other purposes. Another reason is that it keeps the
    values we feed into the model in the same range. Remember that we need to standardize
    data before using a neural network. If a stock price grows from $1 to $1,000,
    we end up with non-standardized data, which will in turn make training difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Making a time series stationary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The standard method to achieve mean stationarity in financial data (especially
    prices) is called differencing. It refers to computing the returns from prices.
    In the following image, you can see the raw and differenced versions of S&P 500\.
    The raw version is not mean stationary as the value grows, but the differenced
    version is roughly stationary.
  prefs: []
  type: TYPE_NORMAL
- en: '![Making a time series stationary](img/B10354_04_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another approach to mean stationarity is based on linear regression. Here,
    we fit a linear model to the data. A popular library for this kind of classical
    modeling is `statsmodels`, which has an inbuilt linear regression model. The following
    example shows how to use `statsmodels` to remove a linear trend from data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Making a time series stationary](img/B10354_04_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It is worth emphasizing that **stationarity is part of modeling and should be
    fit on the training set only**. This is not a big issue with differencing, but
    can lead to problems with linear detrending.
  prefs: []
  type: TYPE_NORMAL
- en: Removing variance non-stationarity is harder. A typical approach is to compute
    some rolling variance and divide new values by that variance. On the training
    set, you can also **studentize** the data. To do this, you need to compute the
    daily variance, and then divide all values by the root of it. Again, you may do
    this only on the training set, as the variance computation requires that you already
    know the values.
  prefs: []
  type: TYPE_NORMAL
- en: When to ignore stationarity issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are times when you should not worry about stationarity. When forecasting
    a sudden change, a so-called structural break, for instance. In the Wikipedia
    example, we are interested in knowing when the sites begin to be visited much
    more frequently than they were before. In this case, removing differences in level
    would stop our model from learning to predict such changes. Equally, we might
    be able to easily incorporate the non-stationarity into our model, or it can be
    ensured at a later stage in the pipeline. We usually only train a neural network
    on a small subsequence of the entire dataset. If we standardize each subsequence,
    the shift of mean within the subsequence might be negligible and we would not
    have to worry about it. Forecasting is a much more forgiving task than inference
    and hypothesis testing, so we might get away with a few non-stationarities if
    our model can pick up on them.
  prefs: []
  type: TYPE_NORMAL
- en: Fast Fourier transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another interesting statistic we often want to compute about time series is
    the Fourier transformation (FT). Without going into the math, a Fourier transformation
    will show us the amount of oscillation within a particular frequency in a function.
  prefs: []
  type: TYPE_NORMAL
- en: You can imagine this like the tuner on an old FM radio. As you turn the tuner,
    you search through different frequencies. Every once in a while, you find a frequency
    that gives you a clear signal of a particular radio station. A Fourier transformation
    basically scans through the entire frequency spectrum and records at what frequencies
    there is a strong signal. In terms of a time series, this is useful when trying
    to find periodic patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we found out that a frequency of one per week gave us a strong
    pattern. This would mean that knowledge about what the traffic was ton the same
    day one week ago would help our model.
  prefs: []
  type: TYPE_NORMAL
- en: When both the function and the Fourier transform are discrete, which is the
    case in a series of daily measurements, it is called the **discrete Fourier transform**
    (**DFT**). A very fast algorithm that is used for computing the DFT is known as
    the **Fast Fourier Transform** (**FFT**), which today has become an important
    algorithm in scientific computing. This theory was known to the mathematician
    Carl Gauss in 1805 but was brought to light more recently by American mathematicians
    James W. Cooley and John Tukey in 1965.
  prefs: []
  type: TYPE_NORMAL
- en: It's beyond the scope of this chapter to go into how and why the Fourier transformations
    work, so in this section we will only be giving a brief introduction. Imagine
    our function as a piece of wire. We take this wire and wrap it around a point,
    and if you wrap the wire so that the number of revolutions around the point matches
    the frequency of a signal, all of the signal peaks will be on one side of the
    pole. This means that the center of mass of the wire will move away from the point
    we wrapped the wire around.
  prefs: []
  type: TYPE_NORMAL
- en: In math, wrapping a function around a point can be achieved by multiplying the
    function *g*(*n*) with ![Fast Fourier transformations](img/B10354_04_002.jpg),
    where *f* is the frequency of wrapping, *n* is the number of the item from the
    series, and *i* is the imaginary square root of -1\. Readers that are not familiar
    with imaginary numbers can think of them as coordinates in which each number has
    a two-dimensional coordinate consisting of both a real and an imaginary number.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the center of mass, we average the coordinates of the points in
    our discrete function. The DFT formula is, therefore, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fast Fourier transformations](img/B10354_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here *y*[*f*] is the *f*th element in the transformed series, and *x*[*n*] is
    the *n*th element of the input series, *x*. *N* is the total number of points
    in the input series. Note that *y*[*f*] will be a number with a real and a discrete
    element.
  prefs: []
  type: TYPE_NORMAL
- en: To detect frequencies, we are only really interested in the overall magnitude
    of *y*[*f*]. To get this magnitude we need to so we compute the root of the sum
    of the squares of the imaginary and real parts. In Python, we do not have to worry
    about all the math as we can use `scikit-learn's fftpack`, which has an FFT function
    built in.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we first extract the time series measurements without the global features
    from our training set. Then we run the FFT algorithm, before finally computing
    the magnitudes of the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running that code, we now have the Fourier transformations for all the
    time series datasets. In order to allow us to get a better insight into the general
    behavior of the Fourier transformations we can average them by simply running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This first turns the magnitudes into a NumPy array before then computing the
    mean. We want to compute the mean per frequency, not just the mean value of all
    the magnitudes, therefore we need to specify the `axis` along which to take the
    mean value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the series are stacked in rows, so taking the mean column-wise
    (axis zero) will result in frequency-wise means. To better plot the transformation,
    we need to create a list of frequencies tested. The frequencies are in the form:
    day/all days in the dataset for each day, so 1/550, 2/550, 3/550, and so on. To
    create the list we need to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In this visualization, we only care about the range of frequencies in a weekly
    range, so we will remove the second half of the transformation, which we can do by
    running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can plot our transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon plotting the transformation, we will have successfully produced a chart
    similar to the one you see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fast Fourier transformations](img/B10354_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Fourier transformation of Wikipedia access statistics. Spikes marked by vertical
    lines
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the chart we produced, there are spikes at roughly 1/7 (0.14),
    2/7 (0.28), and 3/7 (0.42). As a week has seven days, that is a frequency of one
    time per week, two times per week, and three times per week. In other words, page
    statistics repeat themselves (approximately) every week, so that, for example,
    access on one Saturday correlates with access on the previous Saturday.
  prefs: []
  type: TYPE_NORMAL
- en: Autocorrelation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autocorrelation is the correlation between two elements of a series separated
    by a given interval. Intuitively, we would, for example, assume that knowledge
    about the last time step helps us in forecasting the next step. But how about
    knowledge from 2 time steps ago or from 100 time steps ago?
  prefs: []
  type: TYPE_NORMAL
- en: Running `autocorrelation_plot` will plot the correlation between elements with different
    lag times and can help us answer these questions. As a matter of fact, pandas
    comes with a handy autocorrelation plotting tool. To use it, we have to pass a series
    of data. In our case, we pass the page views of a page, selected at random.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will present us with the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Autocorrelation](img/B10354_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Autocorrelation of the Oh My Girl Chinese Wikipedia page
  prefs: []
  type: TYPE_NORMAL
- en: The plot in the preceding chart shows the correlation of page views for the
    Wikipedia page of *Oh My Girl*, a South Korean girl group, within the Chinese
    Wikipedia.
  prefs: []
  type: TYPE_NORMAL
- en: You can see that shorter time intervals between 1 and 20 days show a higher
    autocorrelation than longer intervals. Likewise there are also curious spikes,
    such as around 120 days and 280 days. It's possible that annual, quarterly, or
    monthly events could lead to an increase in the frequency of visits to the *Oh
    My Girl* Wikipedia page.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine the general pattern of these frequencies by drawing 1,000 of
    these autocorrelation plots. To do this we run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This code snippet first samples 1,000 random numbers between 0 and the number
    of series in our dataset, which in our case is around 145,000\. We use these as
    indices to randomly sample rows from our dataset for which we then draw the autocorrelation
    plot, which we can see in the following graphic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Autocorrelation](img/B10354_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Autocorrelations for 1,000 Wikipedia pages
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, autocorrelations can be quite different for different series
    and there is a lot of noise within the chart. There also seems to be a general
    trend toward higher correlations at around the 350-day mark.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it makes sense to incorporate annual lagged page views as a time-dependent
    feature as well as the autocorrelation for one-year time intervals as a global
    feature. The same is true for quarterly and half-year lag as these seem to have
    high autocorrelations, or sometimes quite negative autocorrelations, which makes
    them valuable as well.
  prefs: []
  type: TYPE_NORMAL
- en: Time series analysis, such as in the examples shown previously, can help us
    engineer features for our model. Complex neural networks could, in theory, discover
    all of these features by themselves. However, it is often much easier to help
    them a bit, especially with information about long periods of time.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a training and testing regime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even with lots of data available, we have to ask ourselves; How do we want
    to split data between *training*, *validation*, and *testing*. This dataset already
    comes with a test set of future data, therefore we don''t have to worry about
    the test set, but for the validation set, there are two ways of splitting: a walk-forward
    split, and a side-by-side split:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Establishing a training and testing regime](img/B10354_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Possible testing regimes
  prefs: []
  type: TYPE_NORMAL
- en: In a walk-forward split, we train on all 145,000 series. To validate, we are
    going to use more recent data from all the series. In a side-by-side split, we
    sample a number of series for training and use the rest for validation.
  prefs: []
  type: TYPE_NORMAL
- en: Both have advantages and disadvantages. The disadvantage of walk-forward splitting
    is that we cannot use all of the observations of the series for our predictions.
    The disadvantage of side-by-side splitting is that we cannot use all series for training.
  prefs: []
  type: TYPE_NORMAL
- en: If we have few series, but multiple data observations per series, a walk-forward
    split is preferable. However, if we have a lot of series, but few observations
    per series, then a side-by-side split is preferable.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a training and testing regime also aligns more nicely with the
    forecasting problem at hand. In side-by-side splitting, the model might overfit
    to global events in the prediction period. Imagine that Wikipedia was down for
    a week in the prediction period used in side-by-side splitting. This event would
    reduce the number of views for all the pages, and as a result the model would
    overfit to this global event.
  prefs: []
  type: TYPE_NORMAL
- en: We would not catch the overfitting in our validation set because the prediction
    period would also be affected by the global event. However, in our case, we have
    multiple time series, but only about 550 observations per series. Therefore there
    seems to be no global events that would have significantly impacted all the Wikipedia
    pages in that time period.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are some global events that impacted views for some pages, such
    as the Winter Olympics. Yet, this is a reasonable risk in this case, as the number
    of pages affected by such global events is still small. Since we have an abundance
    of series and only a few observations per series, a side-by-side split is more
    feasible in our case.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re focusing on forecasting traffic for 50 days. So, we
    must first split the last 50 days of each series from the rest, as seen in the
    following code, before splitting the training and validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: When splitting, we use `X.values` to only get the data, not a DataFrame containing
    the data. After splitting we are left with 130,556 series for training and 14,507
    for validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we are going to use the **mean absolute percentage error**
    (**MAPE**) as a loss and evaluation metric. MAPE can cause division-by-zero errors
    if the true value of `y` is zero. Thus, to prevent division by zero occurring,
    we''ll use a small-value epsilon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: A note on backtesting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The peculiarities of choosing training and testing sets are especially important
    in both systematic investing and algorithmic trading. The main way to test trading
    algorithms is a process called **backtesting**.
  prefs: []
  type: TYPE_NORMAL
- en: Backtesting means we train the algorithm on data from a certain time period
    and then test its performance on *older* data. For example, we could train on
    data from a date range of 2015 to 2018 and then test on data from 1990 to 2015\.
    By doing this, not only is the model's accuracy tested, but the backtested algorithm
    executes virtual trades so its profitability can be evaluated. Backtesting is
    done because there is plenty of past data available.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all that being said, backtesting does suffer from several biases. Let''s
    take a look at four of the most important biases that we need to be aware of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Look-ahead bias**: This is introduced if future data is accidentally included
    at a point in the simulation where that data would not have been available yet.
    This can be caused by a technical bug in the simulator, but it can also stem from
    a parameter calculation. If a strategy makes use of the correlation between two
    securities, for example, and the correlation is calculated once for all time,
    a look-ahead bias is introduced. The same goes for the calculation of maxima or
    minima.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Survivorship bias**: This is introduced if only stocks that still exist at
    the time of testing are included in the simulation. Consider, for example, the
    2008 financial crisis in which many firms went bankrupt. Leaving the stocks of
    these firms out when building a simulator in 2018 would introduce survivorship
    bias. After all, the algorithm could have invested in those stocks in 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Psychological tolerance bias**: What looks good in a backtest might not be
    good in real life. Consider an algorithm that loses money for four months in a
    row before making it all back in a backtest. We might feel satisfied with this
    algorithm. However, if the algorithm loses money for four months in a row in real
    life and we don''t know whether it will make that amount back, then will we sit
    tight or pull the plug? In the backtest, we know the final result, but in real
    life, we do not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting**: This is a problem for all machine learning algorithms, but
    in backtesting, overfitting is a persistent and insidious problem. Not only does the
    algorithm potentially overfit, but the designer of the algorithm might also use
    knowledge about the past and build an algorithm that overfits to it. It is easy
    to pick stocks in hindsight, and knowledge can be incorporated into models that
    then look great in backtests. While it might be subtle, such as relying on certain
    correlations that held up well in the past, but it is easy to build bias into
    models that are evaluated in backtesting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building good testing regimes is a core activity of any quantitative investment
    firm or anyone working intensively with forecasting. One popular strategy for
    testing algorithms, other than backtesting, testing models on data that is statistically
    similar to stock data but differs because it's generated. We might build a generator
    for data that looks like real stock data but is not real, thus avoiding knowledge
    about real market events creeping into our models.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to deploy models silently and test them in the future. The
    algorithm runs but executes only virtual trades so that if things go wrong, no
    money will be lost. This approach makes use of future data instead of past data.
    However, the downside to this method is that we have to wait for quite a while
    before the algorithm can be used.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, a combination regime is used. Statisticians carefully design regimes
    to see how an algorithm responds to different simulations. In our web traffic
    forecasting model, we will simply validate on different pages and then test on future data
    in the end.
  prefs: []
  type: TYPE_NORMAL
- en: Median forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A good sanity check and an often underrated forecasting tool is medians. A median
    is a value separating the higher half of a distribution from the lower half; it
    sits exactly in the middle of the distribution. Medians have the advantage of
    removing noise, coupled with the fact that they are less susceptible to outliers
    than means, and the way they capture the midpoint of distribution means that they
    are also easy to compute.
  prefs: []
  type: TYPE_NORMAL
- en: To make a forecast, we compute the median over a look-back window in our training
    data. In this case, we use a window size of 50, but you could experiment with
    other values. The next step is to select the last 50 values from our *X* values
    and compute the median.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a minute to note that in the NumPy median function, we have to set `keepdims=True`.
    This ensures that we keep a two-dimensional matrix rather than a flat array, which
    is important when computing the error. So, to make a forecast, we need to run
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output returned shows we obtain an error of about 68.1%; not bad given
    the simplicity of our method. To see how the medians work, let''s plot the *X*
    values, the true *y* values, and predictions for a random page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, our plotting consists of drawing three plots. For each plot,
    we must specify the *X* and *Y* values for the plot. For `X_train`, the *X* values
    range from 0 to 500, and for `y_train` and the forecast they range from 500 to
    550\. We then select the series we want to plot from our training data. Since
    we have only one median value, we repeat the median forecast of the desired series
    50 times in order to draw our forecast.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Median forecasting](img/B10354_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Median forecast and actual values for access of an image file. The True values
    are to the right-hand side of the plot, and the median forecast is the horizontal
    line in the center of them.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding output median forecast, the data for this page,
    in this case, an image of American actor Eric Stoltz, is very noisy, and the median
    cuts through all the noise. The median is especially useful here for pages that
    are visited infrequently and where there is no clear trend or pattern.
  prefs: []
  type: TYPE_NORMAL
- en: This is not all you can do with medians. Beyond what we've just covered, you
    could, for example, use different medians for weekends or use a median of medians from
    multiple look-back periods. A simple tool, such as median forecasting, is able
    to deliver good results with smart feature engineering. Therefore, it makes sense
    to spend a bit of time on implementing median forecasting as a baseline and performing
    a sanity check before using more advanced methods.
  prefs: []
  type: TYPE_NORMAL
- en: ARIMA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, in the section on exploratory data analysis, we talked about how seasonality
    and stationarity are important elements when it comes to forecasting time series.
    In fact, median forecasting has trouble with both. If the mean of a time series
    continuously shifts, then median forecasting will not continue the trend, and
    if a time series shows cyclical behavior, then the median will not continue with
    the cycle.
  prefs: []
  type: TYPE_NORMAL
- en: '**ARIMA** which stands for **Autoregressive Integrated Moving Average**, is
    made up of three core components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Autoregression**: The model uses the relationship between a value and a number
    of lagged observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrated**: The model uses the difference between raw observations to make
    the time series stationary. A time series going continuously upward will have
    a flat integral as the differences between points are always the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Moving Average**: The model uses residual errors from a moving average.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have to manually specify how many lagged observations we want to include,
    *p*, how often we want to differentiate the series, *d*, and how large the moving
    average window should be, *q*. ARIMA then performs a linear regression against
    all the included lagged observations and moving average residuals on the differentiated
    series.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use ARIMA in Python with `statsmodels`, a library with many helpful
    statistical tools. To do this, we simply run this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to create a new ARIMA model, we pass the data we want to fit, in this
    case from our earlier example of views for 2NE1 from the Chinese Wikipedia, as
    well as the desired values for *p*, *d,* and *q,* in that order. In this case,
    we want to include five lagged observations, differentiate once, and take a moving
    average window of five. In code, this works out as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then fit the model using `model.fit()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Running `model.summary()` at this point would output all the coefficients as
    well as significance values for statistical analysis. We, however, are more interested
    in how well our model does in forecasting. So, to complete this, and see the output,
    we simply run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the previous code, we''ll be able to output the results for 2NE1
    page views, as we can see in this graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ARIMA](img/B10354_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The residual error of the ARIMA forecast
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding chart, we can see that the model does very well in the beginning
    but really begins to struggle at around the 300-day mark. This could be because
    page views are harder to predict or because there is more volatility in this period.
  prefs: []
  type: TYPE_NORMAL
- en: In order for us to ensure that our model is not skewed, we need to examine the
    distribution of the residuals. We can do this by plotting a *kernel density estimator*,
    which is a mathematical method designed to estimate distributions without needing
    to model them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will then output the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ARIMA](img/B10354_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Approximately normally distributed residuals from ARIMA forecast
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, our model roughly represents a Gaussian distribution with a
    mean of zero. So, it's all good on that front, but then the question arises, "how
    do we make forecasts?"
  prefs: []
  type: TYPE_NORMAL
- en: 'To use this model for forecasting, all we have to do is to specify the number
    of days we want to forecast, which we can do with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This forecast not only gives us predictions but also the standard error and
    confidence interval, which is 95% by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s plot the projected views against the real views to see how we are doing.
    This graph shows the last 20 days for our prediction basis as well as the forecast
    to keep things readable. To produce this, we must execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will output the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ARIMA](img/B10354_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ARIMA forecast and actual access
  prefs: []
  type: TYPE_NORMAL
- en: You can see that ARIMA captures the periodicity of the series very well. Its
    forecast does steer off a bit toward the end, but in the beginning, it does a
    remarkable job.
  prefs: []
  type: TYPE_NORMAL
- en: Kalman filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kalman filters are a method of extracting a signal from either noisy or incomplete
    measurements. They were invented by Hungarian-born, American engineer, Rudolf
    Emil Kalman, for the purpose of electrical engineering, and were first used in
    the Apollo Space program in the 1960s.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea behind the Kalman filter is that there is some hidden state of
    a system that we cannot observe directly but for which we can obtain noisy measurements.
    Imagine you want to measure the temperature inside a rocket engine. You cannot
    put a measurement device directly into the engine, because it's too hot, but you
    can have a device on the outside of the engine.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, this measurement is not going to be perfect, as there are a lot of
    external factors occurring outside of the engine that make the measurement noisy.
    Therefore, to estimate the temperature inside the rocket, you need a method that
    can deal with the noise. We can think of the internal state in the page forecasting
    as the actual interest in a certain page, of which the page views represent only
    a noisy measurement.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea here is that the internal state, ![Kalman filters](img/B10354_04_004.jpg),
    at time *k* is a state transition matrix, *A,* multiplied with the previous internal
    state, ![Kalman filters](img/B10354_04_005.jpg), plus some process noise, ![Kalman
    filters](img/B10354_04_006.jpg). How interest in the Wikipedia page of 2NE1 develops
    is to some degree random. The randomness is assumed to follow a Gaussian normal
    distribution with mean zero and variance *Q*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kalman filters](img/B10354_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The obtained measurement at time *k*, ![Kalman filters](img/B10354_04_008.jpg),
    is an observation model, *H*, describing how states translate to measurements
    times the state, ![Kalman filters](img/B10354_04_009.jpg), plus some observation
    noise, ![Kalman filters](img/B10354_04_010.jpg). The observation noise is assumed
    to follow a Gaussian normal distribution with mean zero and variance *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kalman filters](img/B10354_04_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Roughly speaking, Kalman filters fit a function by estimating *A*, *H*, *Q,*
    and *R*. The process of going over a time series and updating the parameters is
    called smoothing. The exact mathematics of the estimation process is complicated
    and not very relevant if all we want to do is forecasting. Yet, what is relevant
    is that we need to provide priors to these values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should note that our state does not have to be only one number. In this
    case, our state is an eight-dimensional vector, with one hidden level as well
    as seven levels to capture weekly seasonality, as we can see in this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The transition matrix, *A,* looks like the following table, describing one
    hidden level, which we might interpret as the real interest as well as a seasonality
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The observation model, *H,* maps the general interest plus seasonality to a
    single measurement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The observation model looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The noise priors are just estimates scaled by a "smoothing factor," which allows
    us to control the update process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '`process_noise_cov` is an eight-dimensional vector, matching the eight-dimensional
    state vector. Meanwhile, `observation_noise_cov` is a single number, as we have
    only a single measurement. The only real requirement for these priors is that
    their shapes must allow the matrix multiplications described in the two preceding
    formulas. Other than that, we are free to specify transition models as we see
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: Otto Seiskari, a mathematician and 8th place winner in the original Wikipedia
    traffic forecasting competition, wrote a very fast Kalman filtering library, which
    we will be using here. His library allows for the vectorized processing of multiple
    independent time series, which is very handy if you have 145,000 time series to
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: The library''s repository can be found here: [https://github.com/oseiskar/simdkalman](https://github.com/oseiskar/simdkalman).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install his library using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'To import it, run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Although `simdkalman` is very sophisticated, it is quite simple to use. Firstly,
    we are going to specify a Kalman filter using the priors we just defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'From there we can then estimate the parameters and compute a forecast in one
    step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we make forecasts for 2NE1's Chinese page and create a forecast
    for 50 days. Take a minute to note that we could also pass multiple series, for example,
    the first 10 with `X_train[:10]`, and compute separate filters for all of them
    at once.
  prefs: []
  type: TYPE_NORMAL
- en: The result of the compute function contains the state and observation estimates
    from the smoothing process as well as predicted internal states and observations.
    States and observations are Gaussian distributions, so to get a plottable value,
    we need to access their mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our states are eight-dimensional, but we only care about the non-seasonal state
    value, so we need to index the mean, which we can achieve by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will then output the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kalman filters](img/B10354_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Predictions and inner states from the Kalman filter
  prefs: []
  type: TYPE_NORMAL
- en: We can clearly see in the preceding graph the effects of our prior modeling
    on the predictions. We can see the model predicts strong weekly oscillation, stronger
    than actually observed. Likewise, we can also see that the model does not anticipate
    any trends since we did not see model trends in our prior model.
  prefs: []
  type: TYPE_NORMAL
- en: Kalman filters are a useful tool and are used in many applications, from electrical
    engineering to finance. In fact, until relatively recently, they were the go-to
    tool for time series modeling. Smart modelers were able to create smart systems
    that described the time series very well. However, one weakness of Kalman filters
    is that they cannot discover patterns by themselves and need carefully engineered
    priors in order to work.
  prefs: []
  type: TYPE_NORMAL
- en: In the second half of this chapter, we will be looking at neural network-based
    approaches that can automatically model time series, and often with higher accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting with neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second half of the chapter is all about neural networks. In the first part,
    we will be building a simple neural network that only forecasts the next time
    step. Since the spikes in the series are very large, we will be working with log-transformed
    page views in input and output. We can use the short-term forecast neural network
    to make longer-term forecasts, too, by feeding its predictions back into the network.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can dive in and start building forecast models, we need to do some
    preprocessing and feature engineering. The advantage of neural networks is that
    they can take in both a high number of features in addition to very high-dimensional
    data. The disadvantage is that we have to be careful about what features we input.
    Remember how we discussed look-ahead bias earlier in the chapter, including future
    data that would not have been available at the time of forecasting, which is a
    problem in backtesting.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For each series, we will assemble the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '`log_view`: The natural logarithm of page views. Since the logarithm of zero
    is undefined, we will use `log1p`, which is the natural logarithm of page views
    plus one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`days`: One-hot encoded weekdays.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`year_lag`: The value of `log_view` from 365 days ago. `-1` if there is no
    value available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`halfyear_lag`: The value of `log_view` from 182 days ago. `-1` if there is
    no value available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quarter_lag`: The value of `log_view` from 91 days ago. `-1` if there is no
    value available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`page_enc`: The one-hot encoded subpage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`agent_enc`: The one-hot encoded agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`acc_enc`: The one-hot encoded access method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`year_autocorr`: The autocorrelation of the series of 365 days.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`halfyr_autocorr`: The autocorrelation of the series of 182 days.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quarter_autocorr`: The autocorrelation of the series of 91 days.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`medians`: The median of page views over the lookback period.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These features are assembled for each time series, giving our input data the
    shape (batch size, look back window size, 29).
  prefs: []
  type: TYPE_NORMAL
- en: Weekdays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The day of the week matters. Sundays may show different access behavior, when
    people are browsing from their couch, compared to Mondays, when people may be
    looking up things for work. So, we need to encode the weekday. A simple one-hot
    encoding will do the job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Firstly, we turn the date strings (such as 2017-03-02) into their weekday (Thursday).
    This is very simple to do, and can be done with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We then encode the weekdays into integers, so that "Monday" becomes `1`, "Tuesday"
    becomes `2`, and so on. We reshape the resulting array into a rank-2 tensor with
    shape (array length, 1) so that the one-hot encoder knows that we have many observations,
    but only one feature, and not the other way around:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we one-hot encode the days. We then add a new dimension to the tensor
    showing that we only have one "row" of dates. We will later repeat the array along
    this axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We will need the encoders for the agents later when we encode the agent of each series.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we first create a `LabelEncoder` instance that can transform the agent
    name strings into integers. We then transform all of the agents into such an integer
    string in order to set up a `OneHotEncoder` instance that can one-hot encode the
    agents. To save memory, we will then delete the already-encoded agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do the same for subpages and access methods by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we come to the lagged features. Technically, neural networks could discover
    what past events are relevant for forecasting themselves. However, this is pretty
    difficult because of the vanishing gradient problem, something that is covered
    in more detail later, in the *LSTM* section of this chapter. For now, let''s just
    set up a little function that creates an array lagged by a number of days:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This function first creates a new array that will fill up the "empty space"
    from the shift. The new array has as many rows as the original array but its series
    length, or width, is the number of days we want to lag. We then attach this array
    to the front of our original array. Finally, we remove elements from the back
    of the array in order to get back to the original array series length or width.     We want to inform our model about the amount of autocorrelation for different
    time intervals. To compute the autocorrelation for a single series, we shift the
    series by the amount of lag we want to measure the autocorrelation for. We then
    compute the autocorrelation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Weekdays](img/B10354_04_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this formula ![Weekdays](img/B10354_04_013.jpg) is the lag indicator. We
    do not just use a NumPy function since there is a real possibility that the divider
    is zero. In this case, our function will just return 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this function, which we wrote for a single series, to create a batch
    of autocorrelation features, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Firstly, we calculate the autocorrelations for each series in the batch. Then
    we fuse the correlations together into one NumPy array. Since autocorrelations
    are a global feature, we need to create a new dimension for the length of the
    series and another new dimension to show that this is only one feature. We then
    repeat the autocorrelations over the entire length of the series.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `get_batch` function utilizes all of these tools in order to provide us
    with one batch of data, as can be seen with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'That was a lot of code, so let''s take a minute to walk through the preceding
    code step by step in order to fully understand it:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensures there is enough data to create a lookback window and a target from the
    given starting point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separates the lookback window from the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separates the target and then takes the one plus logarithm of it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Takes the one plus logarithm of the lookback window and adds a feature dimension.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gets the days from the precomputed one-hot encoding of days and repeats it for
    each time series in the batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computes the lag features for year lag, half-year lag, and quarterly lag.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This step will encode the global features using the preceding defined encoders.
    The next two steps, 8 and 9, will echo the same role.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This step repeats step 7.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This step repeats step 7 and 8.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculates the year, half-year, and quarterly autocorrelation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculates the median for the lookback data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fuses all these features into one batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we can use our `get_batch` function to write a generator, just like
    we did in [Chapter 3](ch03.xhtml "Chapter 3. Utilizing Computer Vision"), *Utilizing
    Computer Vision*. This generator loops over the original training set and passes
    a subset into the `get_batch` function. It then yields the batch obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we choose random starting points to make the most out of our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This function is what we will train and validate on.
  prefs: []
  type: TYPE_NORMAL
- en: Conv1D
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might remember Convolution Neural Networks (ConvNets, or CNNs) from [Chapter
    3,](ch03.xhtml "Chapter 3. Utilizing Computer Vision") *Utilizing Computer Vision*,
    where we looked briefly at roofs and insurance. In computer vision, convolutional
    filters slide over the image two-dimensionally. There is also a version of convolutional
    filters that can slide over a sequence one-dimensionally. The output is another
    sequence, much like the output of a two-dimensional convolution was another image.
    Everything else about one-dimensional convolutions is exactly the same as two-dimensional
    convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we''re going to start by building a ConvNet that expects a
    fixed input length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Notice that next to `Conv1D` and `Activation`, there are two more layers in
    this network. `MaxPool1D` works exactly like `MaxPooling2D`, which we used earlier
    in the book. It takes a piece of the sequence with a specified length and returns
    the maximum element in the sequence. This is similar to how it returned the maximum
    element of a small window in two-dimensional convolutional networks.
  prefs: []
  type: TYPE_NORMAL
- en: Take note that max pooling always returns the maximum element for each channel.
    `Flatten` transforms the two-dimensional sequence tensor into a one-dimensional
    flat tensor. To use `Flatten` in combination with `Dense`, we need to specify
    the sequence length in the input shape. Here, we set it with the `max_len` variable.
    We do this because `Dense` expects a fixed input shape and `Flatten` will return
    a tensor based on the size of its input.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to using `Flatten` is `GlobalMaxPool1D`, which returns the maximum
    element of the entire sequence. Since the sequence is fixed in size, you can use
    a `Dense` layer afterward without fixing the input length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our model compiles just as you would expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We then train it on the generator that we wrote earlier. To obtain separate
    train and validation sets, we must first split the overall dataset and then create
    two generators based on the two datasets. To do this, run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can train our model on a generator, just like we did in computer
    vision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Your validation loss will still be quite high, around 12,798,928\. The absolute
    loss value is never a good guide for how well your model is doing. You'll find
    that it's better to use other metrics in order to see whether your forecasts are
    useful. However, please note that we will reduce the loss significantly later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Dilated and causal convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed in the section on backtesting, we have to make sure that our model
    does not suffer from look-ahead bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dilated and causal convolution](img/B10354_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Standard convolution does not take the direction of convolution into account
  prefs: []
  type: TYPE_NORMAL
- en: 'As the convolutional filter slides over the data, it looks into the future
    as well as the past. Causal convolution ensures that the output at time *t* derives
    only from inputs from time *t - 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dilated and causal convolution](img/B10354_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Causal convolution shifts the filter in the right direction
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras, all we have to do is set the `padding` parameter to `causal`. We
    can do this by executing the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Another useful trick is dilated convolutional networks. Dilation means that
    the filter only accesses every *n*th element, as we can see in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Dilated and causal convolution](img/B10354_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Dilated convolution skips over inputs while convolving
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, the upper convolutional layer has a dilation rate
    of 4 and the lower layer a dilation rate of 1\. We can set the dilation rate in
    Keras by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Simple RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another method to make order matter within neural networks is to give the network
    some kind of memory. So far, all of our networks have done a forward pass without
    any memory of what happened before or after the pass. It''s time to change that
    with a **recurrent neural network** (**RNN**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple RNN](img/B10354_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The scheme of an RNN
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs contain recurrent layers. Recurrent layers can remember their last activation
    and use it as their own input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple RNN](img/B10354_04_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A recurrent layer takes a sequence as an input. For each element, it then computes
    a matrix multiplication (*W * in*), just like a `Dense` layer, and runs the result
    through an activation function, such as `relu`. It then retains its own activation.
    When the next item of the sequence arrives, it performs the matrix multiplication
    as before, but this time it also multiplies its previous activation with a second
    matrix (![Simple RNN](img/B10354_04_015.jpg)). The recurrent layer adds the result
    of both operations together and passes it through the activation function again.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras, we can use a simple RNN as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The only parameter we need to specify is the size of the recurrent layer. This
    is basically the same as setting the size of a `Dense` layer, as `SimpleRNN` layers
    are very similar to `Dense` layers except that they feed their output back in
    as input. RNNs, by default, only return the last output of the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'To stack multiple RNNs, we need to set `return_sequences` to `True`, which
    we can do by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: As a result of this code, we'll be able to see that a simple RNN does much better
    than the convolutional model, with a loss of around 1,548,653\. You'll remember
    that previously our loss was at 12,793,928\. However, we can do much better using
    a more sophisticated version of the RNN.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, we learned about basic RNNs. In theory, simple RNNs should
    be able to retain even long-term memories. However, in practice, this approach
    often falls short because of the vanishing gradients problem.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of many timesteps, the network has a hard time keeping up meaningful
    gradients. While this is not the focus of this chapter, a more detailed exploration
    of why this happens can be read in the 1994 paper, *Learning long-term dependencies
    with gradient descent is difficult,* available at -[https://ieeexplore.ieee.org/document/279181](https://ieeexplore.ieee.org/document/279181)
    - by Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
  prefs: []
  type: TYPE_NORMAL
- en: In direct response to the vanishing gradients problem of simple RNNs, the **Long
    Short-Term Memory** (**LSTM**) layer was invented. This layer performs much better
    at longer time series. Yet, if relevant observations are a few hundred steps behind
    in the series, then even LSTM will struggle. This is why we manually included
    some lagged observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive into details, let''s look at a simple RNN that has been unrolled
    over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![LSTM](img/B10354_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A rolled out RNN
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this is the same as the RNN that we saw in [Chapter 2](ch02.xhtml
    "Chapter 2. Applying Machine Learning to Structured Data"), *Applying Machine
    Learning to Structured Data*, except that this has been unrolled over time.
  prefs: []
  type: TYPE_NORMAL
- en: The carry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The central addition of an LSTM over an RNN is the *carry*. The carry is like
    a conveyor belt that runs along the RNN layer. At each time step, the carry is
    fed into the RNN layer. The new carry gets computed from the input, RNN output,
    and old carry, in a separate operation from the RNN layer itself::'
  prefs: []
  type: TYPE_NORMAL
- en: '![The carry](img/B10354_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The LSTM schematic
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what the Compute Carry is, we should determine what should be
    added from the input and state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The carry](img/B10354_04_016.jpg)![The carry](img/B10354_04_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In these formulas ![The carry](img/B10354_04_018.jpg) is the state at time *t*
    (output of the simple RNN layer), ![The carry](img/B10354_04_019.jpg) is the input
    at time *t,* and *Ui*, *Wi*, *Uk*, and *Wk* are the model parameters (matrices)
    that will be learned. *a()* is an activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine what should be forgotten from the state and input, we need to
    use the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The carry](img/B10354_04_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The new carry is then computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The carry](img/B10354_04_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: While the standard theory claims that the LSTM layer learns what to add and
    what to forget, in practice, nobody knows what really happens inside an LSTM.
    However, LSTM models have been shown to be quite effective at learning long-term
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Take this time to note that LSTM layers do not need an extra activation function
    as they already come with a `tanh` activation function out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTMs can be used in the same way as `SimpleRNN`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'To stack layers, you also need to set `return_sequences` to `True`. Note that
    you can easily combine `LSTM` and `SimpleRNN` using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: If you are using a GPU and TensorFlow backend with Keras, use `CuDNNLSTM`
    instead of `LSTM`. It''s significantly faster while working in exactly the same
    way.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll now compile and run the model just as we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: This time, the loss went as low as 88,735, which is several orders of magnitude
    better than our initial model.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having read this far into the book, you''ve already encountered the concept
    of *dropout*. Dropout removes some elements of one layer of input at random. A
    common and important tool in RNNs is a *recurrent dropout*, which does not remove
    any inputs between layers but inputs between time steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent dropout](img/B10354_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Recurrent dropout scheme
  prefs: []
  type: TYPE_NORMAL
- en: Just as with regular dropout, recurrent dropout has a regularizing effect and
    can prevent overfitting. It's used in Keras by simply passing an argument to the
    LSTM or RNN layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the following code, recurrent dropout, unlike regular dropout,
    does not have its own layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Bayesian deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have a whole set of models that can make forecasts on time series. But are the
    point estimates that these models give sensible estimates or just random guesses?
    How certain is the model? Most classic probabilistic modeling techniques, such
    as Kalman filters, can give confidence intervals for predictions, whereas regular
    deep learning cannot do this. The field of Bayesian deep learning combines Bayesian approaches
    with deep learning to enable models to express uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: The key idea in Bayesian deep learning is that there is inherent uncertainty
    in the model. Sometimes this is done by learning a mean and standard deviation
    for weights instead of just a single weight value. However, this approach increases
    the number of parameters required, so it did not catch on. A simpler hack that
    allows us to turn regular deep networks into Bayesian deep networks is to activate
    dropout during prediction time and then make multiple predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will be using a simpler dataset than before. Our `X` values
    are 20 random values between -5 and 5, and our `y` values are just the sine function
    applied to these values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Our neural network is relatively straightforward, too. Note that Keras does
    not allow us to make a dropout layer the first layer, therefore we need to add
    a `Dense` layer that just passes through the input value. We can achieve this
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: To fit this function, we need a relatively low learning rate, so we import the
    Keras vanilla stochastic gradient descent optimizer in order to set the learning
    rate there. We then train the model for 10,000 epochs. Since we are not interested
    in the training logs, we set `verbose` to `0`, which makes the model train "quietly."
  prefs: []
  type: TYPE_NORMAL
- en: 'We do this by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to test our model over a larger range of values, so we create a test
    dataset with 200 values ranging from -10 to 10 in 0.1 intervals. We can imitate
    the test by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: And now comes the magic trick! Using `keras.backend`, we can pass settings to
    TensorFlow, which runs the operations in the background. We use the backend to
    set the learning phase parameter to `1`. This makes TensorFlow believe that we
    are training, and so it will apply dropout. We then make 100 predictions for our
    test data. The result of these 100 predictions is a probability distribution for
    the `y` value at every instance of `X`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: For this example to work, you have to load the backend, clear the
    session, and set the learning phase before defining and training the model, as
    the training process will leave the setting in the TensorFlow graph. You can also
    save the trained model, clear the session, and reload the model. See the code
    for this section for a working implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start this process, we first run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we can obtain our distributions with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we can calculate the mean and standard deviation for our distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot the model''s predictions with one, two, and four standard
    deviations (corresponding to different shades of blue):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result of running this code, we will see the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian deep learning](img/B10354_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Predictions with uncertainty bands
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the model is relatively confident around areas where it had
    data and becomes less and less confident the further away it gets from the data
    points.
  prefs: []
  type: TYPE_NORMAL
- en: Getting uncertainty estimates from our model increases the value we can get
    from it. It also helps in improving the model if we can detect where the model
    is over or under confident. Right now, Bayesian deep learning is only in its infancy,
    and we will certainly see many advances in the next few years.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we''re at the end of the chapter, why not try some of the following exercises?
    You''ll find guides on how to complete them all throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A good trick is to use LSTMs on top of one-dimensional convolution, as one-dimensional
    convolution can go over large sequences while using fewer parameters. Try to implement
    an architecture that first uses a few convolutional and pooling layers and then
    a few LSTM layers. Try it out on the web traffic dataset. Then try adding (recurrent)
    dropout. Can you beat the LSTM model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add uncertainty to your web traffic forecasts. To do this, remember to run your
    model with dropout turned on at inference time. You will obtain multiple forecasts
    for one time step. Think about what this would mean in the context of trading
    and stock prices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Visit the Kaggle datasets page and search for time series data. Make a forecasting
    model. This involves feature engineering with autocorrelation and Fourier transformation,
    picking the right model from the ones introduced (for example, ARIMA versus neural
    networks), and then training your model. A hard task, but you will learn a lot!
    Any dataset will do, but I suggest that you may want to try the stock market dataset
    here: [https://www.kaggle.com/szrlee/stock-time-series-20050101-to-20171231](https://www.kaggle.com/szrlee/stock-time-series-20050101-to-20171231),
    or the electric power consumption dataset here: [https://www.kaggle.com/uciml/electric-power-consumption-data-set](https://www.kaggle.com/uciml/electric-power-consumption-data-set).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about a wide range of conventional tools for dealing
    with time series data. You also learned about one-dimensional convolution and
    recurrent architectures, and finally, you learned a simple way to get your models
    to express uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time series are the most iconic form of financial data. This chapter has given
    you a rich toolbox for dealing with time series. Let''s recap all of the things that
    we''ve covered on the example of forecasting web traffic for Wikipedia:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic data exploration to understand what we are dealing with
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fourier transformation and autocorrelation as tools for feature engineering
    and understanding data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a simple median forecast as a baseline and sanity check
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and using ARIMA and Kalman filters as classic prediction models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing features, including building a data loading mechanism for all our
    time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using one-dimensional convolutions and variants such as causal convolutions
    and dilated convolutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the purpose and use of RNNs and their more powerful variant, LSTMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting to grips with understanding how to add uncertainty to our forecasts
    with the dropout trick, taking our first step into Bayesian learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This rich toolbox of time series techniques comes in especially handy in the
    next chapter, where we will cover natural language processing. Language is basically
    a sequence, or time series, of words. This means we can reuse many tools from
    time series modeling for natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to find company names in text, how to
    group text by topic, and even how to translate text using neural networks.
  prefs: []
  type: TYPE_NORMAL
