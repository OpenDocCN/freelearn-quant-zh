- en: Chapter 8. Time-Series Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In finance and economics, a huge amount of our data is in the format of time-series,
    such as stock prices and **Gross Domestic Products** (**GDP**). From [Chapter
    4](ch04.html "Chapter 4. Sources of Data"), *Sources of Data*, it is shown that
    from Yahoo!Finance, we could download daily, weekly, and monthly historical price
    time-series. From **Federal Reserve Bank's Economics Data Library** (**FRED**),
    we could retrieve many historical time-series such as GDP. For time-series, there
    exist many issues, such as how to estimate returns from historical price data,
    how to merge datasets with the same or different frequencies, seasonality, and
    detect auto-correlation. Understanding those properties is vitally important for
    our knowledge development.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to time-series analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design a good date variable, and merging different datasets by date
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normal distribution and normality test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Term structure of interest rates, 52-week high, and low trading strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return estimation and converting daily returns to monthly or annual returns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T-test, F-test, and Durbin-Watson test for autocorrelation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fama-MacBeth regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roll (1984) spread, Amihud's (2002) illiquidity, and Pastor and Stambaugh's
    (2003) liquidity measure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: January effect and weekday effect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieving high-frequency data from Google Finance and Prof. Hasbrouck's TORQ
    database (Trade, Order, Report, and Quotation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to CRSP (Center for Research in Security Prices) database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to time-series analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most finance data is in the format of time-series, see the following several
    examples. The first one shows how to download historical, daily stock price data
    from Yahoo!Finance for a given ticker''s beginning and ending dates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to time-series analysis](img/B06175_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The type of the data is `numpy.recarray` as the `type(x)` would show. The second
    example prints the first several observations from two datasets called `ffMonthly.pkl`
    and `usGDPquarterly.pkl`, and both are available from the author''s website, such
    as [http://canisius.edu/~yany/python/ffMonthly.pkl](http://canisius.edu/~yany/python/ffMonthly.pkl):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The related output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to time-series analysis](img/B06175_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There is one end of chapter problem which is designed to *merge* discrete data
    with the daily data. The following program retrieves the daily price data from
    Google finance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to time-series analysis](img/B06175_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To get the current stock quote, we have the following program. Note that the
    output is for January 21, 2017:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Introduction to time-series analysis](img/B06175_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By using the next Python program, the **Gross Domestic Product** (**GDP**)
    data from January 1947 to June 2016 would be retrieved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to time-series analysis](img/B06175_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Merging datasets based on a date variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make our time-series more manageable, it is a great idea to generate a `date`
    variable. When talking about such a variable, readers could think about year (YYYY),
    year and month (YYYYMM) or year, month, and day (YYYYMMDD). For just the year,
    month, and day combination, we could have many forms. Using January 20, 2017 as
    an example, we could have 2017-1-20, 1/20/2017, 20Jan2017, 20-1-2017, and the
    like. In a sense, a true date variable, in our mind, could be easily manipulated.
    Usually, the true `date` variable takes a form of *year-month-day* or other forms
    of its variants. Assume the date variable has a value of 2000-12-31\. After adding
    one day to its value, the result should be 2001-1-1.
  prefs: []
  type: TYPE_NORMAL
- en: Using pandas.date_range() to generate one dimensional time-series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We could easily use the `pandas.date_range()` function to generate our time-series;
    refer to the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding program, since the `sp.random.seed()` function is applied,
    readers should get the same output if he/she uses the same seed. The output is
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To better facilitate working with time-series data, in the following program,
    the `pandas.read_csv()` function is used, see the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using pandas.date_range() to generate one dimensional time-series](img/B06175_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To see the format of date, we have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Using pandas.date_range() to generate one dimensional time-series](img/B06175_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Using pandas.date_range() to generate one dimensional time-series](img/B06175_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following program, the `matplotlib.finance.quotes_historical_yahoo_ochl()`
    function is applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using pandas.date_range() to generate one dimensional time-series](img/B06175_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the index is in a form of date format, see the following code. For
    the meaning of `.strftime("%Y")`, see *Table 8.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are several ways to define a `date` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description | Examples |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `pandas.date_range` | 1\. For a range of dates | `pd.date_range(''1/1/2017'',
    periods=252)` |'
  prefs: []
  type: TYPE_TB
- en: '| `datetime.date` | 2\. One day | `>>>from datetime import datetime``>>>datetime.date(2017,1,20)`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `datetime.date.today()` | 3\. Get today''s value | `>>>datetime.date.today()``datetime.date(2017,
    1, 26)` |'
  prefs: []
  type: TYPE_TB
- en: '| `datetime.now()` | 4\. Get the current time | `>>>from datetime import datetime``>>>datetime.now()``datetime.datetime(2017,
    1, 26, 8, 58, 6, 420000)` |'
  prefs: []
  type: TYPE_TB
- en: '| `relativedelta()` | 5\. Add certain numbers of days, months, or years to
    a date variable | `>>>from datetime import datetime``>>>today=datetime.today().date()``>>>print(today)``2017-01-26``>>>print(today+relativedelta(days=31))``2017-02-26`
    |'
  prefs: []
  type: TYPE_TB
- en: 'Retrieving the year, month, and day from a `date` variable is used quite frequently
    when dealing with time-series—see the following Python program by using the `strftime()`
    function. The corresponding output is in the following right panel. The format
    of those results of year, month, and day, is string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table summarizes its usages. For more details, see the link at:
    [http://strftime.org/](http://strftime.org/):'
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description | Examples |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `.strftime("%Y")` | 1\. 4-digit year string | `a=datetime.date(2017,1,2)``a.strftime("%Y")`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `.strftime("%y")` | 2\. 2-digit year string | `a.strftime("%y")` |'
  prefs: []
  type: TYPE_TB
- en: '| `.strftime("%m")` | 3\. Month string | `a.strftime("%m")` |'
  prefs: []
  type: TYPE_TB
- en: '| `.strftime("%d")` | 4\. Day string | `a.strftime("%d")` |'
  prefs: []
  type: TYPE_TB
- en: Return estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With price data, we could calculate returns. In addition, sometimes we have
    to convert daily returns to weekly or monthly, or convert monthly returns to quarterly
    or annual ones. Thus, understanding how to estimate returns and their conversion
    is vital. Assume that we have the following four prices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to know how these prices are sorted. If the first price happened
    before the second price, then the first return should be *(1.1-1)/1=10%*. Next,
    we learn how to retrieve the first *n-1* and the last *n-1* records from an *n*
    record array. To list the first *n-1* price, we use `p[:-1]`, while for the last
    three prices we use `p[1:]` as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To estimate returns, we could use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When given two prices of *x1* and *x2* and assume that *x2* is behind *x1*,
    we could use *ret=(x2-x1)/x1*. Alternatively, we could use *ret=x2/x1-1*. Thus,
    for the preceding example, we could use `ret=p[1:]/p[:-1]-1`. Obviously, this
    second method would avoid certain typing errors. On the other hand, if the prices
    are arranged in the reverse order, for example, the first one is the latest price
    and the last one is the oldest price, then we have to estimate returns in the
    following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As it is mentioned in [Chapter 7](ch07.html "Chapter 7. Multifactor Models
    and Performance Measures"), *Multifactor Models and Performance Measures* we could
    use `.diff()` and `.shift()` functions to estimate returns. See the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows how to download daily price data from Yahoo!Finance
    and estimate daily returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line uploads a function from `matplotlib.finance`. We define the
    beginning and ending dates using a `tuple` data type. The downloaded historical
    daily price data is assigned to `x`. To verify that our returns are correctly
    estimated, we can print a few prices to our screen. Then, we could manually verify
    one or two return values as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Yes, the last result confirms that our first return is correctly estimated.
  prefs: []
  type: TYPE_NORMAL
- en: Converting daily returns to monthly ones
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes, we need to convert daily returns to monthly or annual ones. Here
    is our procedure. First, we estimate the daily log returns. We then take a summation
    of all daily log returns within each month to find out the corresponding monthly
    log returns. The final step is to convert a log monthly return to a monthly percentage
    return. Assume that we have the price data of *p0, p1, p2, …., p20*, where *p0*
    is the last trading price of the last month, *p1* is the first price of this month,
    and *p20* is the last price of this month. Thus, this month''s percentage return
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting daily returns to monthly ones](img/B06175_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The monthly log return is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting daily returns to monthly ones](img/B06175_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The relationship between a monthly percentage and a log monthly return is given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting daily returns to monthly ones](img/B06175_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The daily log return is defined similarly, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting daily returns to monthly ones](img/B06175_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the following summation of log returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting daily returns to monthly ones](img/B06175_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on the previous procedure, the following Python program converts daily
    returns into monthly returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting daily returns to monthly ones](img/B06175_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Merging datasets by date
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following program merges the daily adjusted closing price of IBM with the
    daily Fama-French 3-factor time-series. The `ffMonthly.pkl` is available at: [http://canisius.edu/~yany/python/ffDaily.pkl](http://canisius.edu/~yany/python/ffDaily.pkl):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Understanding the interpolation technique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Interpolation is a technique used quite frequently in finance. In the following
    example, we have to replace two missing values, `NaN`, between 2 and 6\. The `pandas.interpolate()`
    function, for a linear interpolation, is used to fill in the two missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding method is a linear interpolation. Actually, we could estimate
    a Δ and calculate those missing values manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the interpolation technique](img/B06175_08_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *v2*(*v1*) is the second (first) value and *n* is the number of intervals
    between those two values. For the preceding case, Δ is *(6-2)/3=1.33333*. Thus,
    the next value will be *v1+Δ=2+1.33333=3.33333*. This way, we could continually
    estimate all missing values. Note that if we have several periods with missing
    values, then the delta for each period has to be calculated manually to verify
    the methodology. From the Yahoo!Finance bond page at [http://finance.yahoo.com/bonds](http://finance.yahoo.com/bonds),
    we could get the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Maturity | Yield | Yesterday | Last week | Last month |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 3 Month | 0.05 | 0.05 | 0.04 | 0.03 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 Month | 0.08 | 0.07 | 0.07 | 0.06 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 Year | 0.29 | 0.29 | 0.31 | 0.33 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 Year | 0.57 | 0.54 | 0.59 | 0.61 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 Year | 1.34 | 1.32 | 1.41 | 1.39 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 Year | 2.7 | 2.66 | 2.75 | 2.66 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 Year | 3.8 | 3.78 | 3.85 | 3.72 |'
  prefs: []
  type: TYPE_TB
- en: Table 8.3 Term structure interest rate
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Based on the tabular data, we have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Merging data with different frequencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following Python program merges two datasets: US **Gross Domestic Product**
    (**GDP**) data with a quarterly frequency and `ffMonthly`, [http://canisius.edu/~yany/python/ffMonthly.pkl](http://canisius.edu/~yany/python/ffMonthly.pkl)
    with a monthly frequency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The interpolation methodology discussed previously is applied to the missing
    months in terms of GDP data. The `ffMonthly` dataset is assumed to be saved in
    the `c:/temp/` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The outputs are shown here. Since there is no data for GDP before 1947 and
    the `ffMonthly` time-series starts from July 1926, the last several observations
    of the merged data are more informative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'For the second example, we merge a business cycle indicator, called `businessCycle.pkl`,
    available at [http://canisius.edu/~yany/python/businessCycle.pkl](http://canisius.edu/~yany/python/businessCycle.pkl),
    with a monthly frequency and GDP (quarterly frequency). See the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We could print a few lines to see the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Tests of normality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In finance, knowledge about normal distribution is very important for two reasons.
    First, stock returns are assumed to follow a normal distribution. Second, the
    error terms from a good econometric model should follow a normal distribution
    with a zero mean. However, in the real world, this might not be true for stocks.
    On the other hand, whether stocks or portfolios follow a normal distribution could
    be tested by various so-called normality tests. The Shapiro-Wilk test is one of
    them. For the first example, random numbers are drawn from a normal distribution.
    As a consequence, the test should confirm that those observations follow a normal
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Assume that our confidence level is 95%, that is, alpha=0.05\. The first value
    of the result is the test statistic, and the second one is its corresponding P-value.
    Since the P-value is so big, much bigger than 0.05, we accept the null hypothesis
    that the returns follow a normal distribution. For the second example, random
    numbers are drawn from a uniform distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the P-value is close to zero, we reject the null hypothesis. In other
    words, those observations do not follow a normal distribution. The third example
    verifies whether IBM''s returns follow a normal distribution. The last five year''s
    daily data from Yahoo! Finance is used for the test. The null hypothesis is that
    IBM''s daily returns are drawn from a normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this P-value is so close to zero, we reject the null hypothesis. In other
    words, we conclude that IBM''s daily returns do not follow a normal distribution.
    For a normality test, we could also apply the Anderson-Darling test, which is
    a modification of the Kolmogorov-Smirnov test, to verify whether the observations
    follow a particular distribution. See the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have three sets of values: the Anderson-Darling test statistic, a
    set of critical values, and a set of corresponding confidence levels, such as
    15 percent, 10 percent, 5 percent, 2.5 percent, and 1 percent, as shown in the
    previous output. If we choose a 1 percent confidence level—the last value of the
    third set—the critical value is 1.089, the last value of the second set. Since
    our testing statistic is 12.61, which is much higher than the critical value of
    1.089, we reject the null hypothesis. Thus, our Anderson-Darling test leads to
    the same conclusion as our Shapiro-Wilk test. One of the beauties of the `scipy.stats.anderson()`
    test is that we can test for other distributions. After applying the `help()`
    function, we would get the following list. The default distribution is for the
    normality test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Estimating fat tails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the important properties of a normal distribution is that we could use
    mean and standard deviation, the first two moments, to fully define the whole
    distribution. For *n* returns of a security, its first four moments are defined
    in equation (1). The mean or average is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating fat tails](img/B06175_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Its (sample) variance is defined by the following equation. The standard deviation,
    that is, σ, is the square root of the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating fat tails](img/B06175_08_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The skewness defined by the following formula indicates whether the distribution
    is skewed to the left or to the right. For a symmetric distribution, its skewness
    is zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating fat tails](img/B06175_08_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The kurtosis reflects the impact of extreme values because of its power of
    four. There are two types of definitions with and without minus three; refer to
    the following two equations. The reason behind the deduction of three in equation
    (10B), is that for a normal distribution, its kurtosis based on equation (10A)
    is three:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating fat tails](img/B06175_08_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Some books distinguish these two equations by calling equation (10B) excess
    kurtosis. However, many functions based on equation (10B) are still named kurtosis.
    We know that a standard normal distribution has a zero mean, unit standard deviation,
    zero skewness, and zero kurtosis (based on equation 10B). The following output
    confirms these facts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The related output is shown here. Note that since the `scipy.random.seed()`
    function is applied, readers should get the same results if the same seed of 12345
    is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating fat tails](img/B06175_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The mean, skewness, and kurtosis are all close to zero, while the standard
    deviation is close to one. Next, we estimate the four moments for S&P500 based
    on its daily returns as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for those five values, including the number of observations, is
    given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating fat tails](img/B06175_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This result is very close to the result in the paper titled *Study of Fat Tail
    Risk* by Cook Pine Capital (2008). Using the same argument, we conclude that the
    SP500 daily returns are skewed to the left, that is, a negative skewness, and
    have fat tails (kurtosis is 20.81 instead of zero).
  prefs: []
  type: TYPE_NORMAL
- en: T-test and F-test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In finance, a T-test could be viewed as one of the most widely used statistical
    hypothesis tests in which the test statistic follows a student''s t distribution
    if the null hypothesis is supported. We know that the mean for a standard normal
    distribution is zero. In the following program, we generate 1,000 random numbers
    from a standard normal distribution. Then, we conduct two tests: test whether
    the mean is 0.5, and test whether the mean is zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For the first test, in which we test whether the time-series has a mean of
    *0.5*, we reject the null hypothesis since the T-value is *49.76* and the P-value
    is 0\. For the second test, we accept the null hypothesis since the T-value is
    close to -0.26 and the P-value is 0.79\. In the following program, we test whether
    the mean of the daily returns from IBM in 2013 is zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: From the previous results, we know that the average daily returns for IBM is
    0.00004 percent. The T-value is -0.049 while the P-value is 0.96\. Thus, we accept
    the null hypothesis, that is, the daily mean return is statistically the same
    as zero.
  prefs: []
  type: TYPE_NORMAL
- en: Tests of equal variances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we test whether two variances for IBM and DELL are the same or not over
    a five-year period from 2012 to 2016\. The function called `sp.stats.bartlet()`
    performs Bartlett''s test for equal variances with a null hypothesis that all
    input samples are from populations with equal variances. The outputs are the T-value
    and P-value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: With a T-value of 108 and a P-value of 0, we conclude that these two stocks
    will have different variances for their daily stock returns from 2012 to 2016
    for any significance level.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the January effect
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we use IBM''s data to test the existence of the so-called
    **January** effect, which states that stock returns in January are statistically
    different from those in other months. First, we collect the daily price for IBM
    from Yahoo! Finance. Then, we convert daily returns to monthly ones. After that,
    we classify all monthly returns into two groups: returns in January versus returns
    in other months.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we test the equality of group means as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the T-value is 1.89 and P-value is 0.058, we conclude that there is no
    January effect if we use IBM as an example and choose a 5 percent significance
    level. A word of caution: we should not generalize this result since it is based
    on just one stock. In terms of the weekday effect, we could apply the same procedure
    to test its existence. One end of chapter problems is designed to test the weekday
    effect based on the same logic.'
  prefs: []
  type: TYPE_NORMAL
- en: 52-week high and low trading strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some investors/researchers argue that we could adopt a 52-week high and low
    trading strategy by taking a long position if today''s price is close to the maximum
    price achieved in the past 52 weeks and taking an opposite position if today''s
    price is close to its 52-week low. Let''s randomly choose a day of 12/31/2016\.
    The following Python program presents this 52-week''s range and today''s position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![52-week high and low trading strategy](img/B06175_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: According to the 52-week high and low trading strategy, we have more incentive
    to buy IBM's stock today. This example is just an illustration on how to make
    a decision. There is nothing done to test whether this is a profitable trading
    strategy. If a reader is interested in testing this 52-week high and low trading
    strategy, he/she should use all stocks to form two portfolios. For more details,
    see George and Huang (2004).
  prefs: []
  type: TYPE_NORMAL
- en: Estimating Roll's spread
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Liquidity is defined as how quickly we can dispose of our asset without losing
    its intrinsic value. Usually, we use spread to represent liquidity. However, we
    need high-frequency data to estimate spread. Later in the chapter, we show how
    to estimate spread directly by using high-frequency data. To measure spread indirectly
    based on daily observations, Roll (1984) shows that we can estimate it based on
    the serial covariance in price changes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating Roll''s spread](img/B06175_08_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *S* is the Roll spread, *Pt* is the closing price of a stock on day,
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating Roll''s spread](img/B06175_08_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is Pt-Pt-1, and
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating Roll''s spread](img/B06175_08_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ', *t* is the average share price in the estimation period. The following Python
    code estimates Roll''s spread for IBM, using one year''s daily price data from
    Yahoo! Finance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating Roll''s spread](img/B06175_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, during that period, Roll's spread for IBM is $1.136\. See the following
    for the major assumption for Roll's model,
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating Roll''s spread](img/B06175_08_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating Roll''s spread](img/B06175_08_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 'The covariance between them is negative. When its value is positive, Roll''s
    model would fail. In a real world, it could occur for many cases. Usually, practitioners
    adopt two approaches: when the spread is negative, we just ignore those cases
    or use other methods to estimate spread. The second approach is to add a negative
    sign in front of a positive covariance.'
  prefs: []
  type: TYPE_NORMAL
- en: Estimating Amihud's illiquidity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'According to Amihud (2002), liquidity reflects the impact of order flow on
    price. His illiquidity measure is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating Amihud''s illiquidity](img/B06175_08_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *illiq(t)* is the Amihud''s illiquidity measure for month *t*, *Ri* is
    the daily return at day *i*, *Pi* is the closing price at *i*, and *Vi* is the
    daily dollar trading volume at *i*. Since the illiquidity is the reciprocal of
    liquidity, the lower the illiquidity value, the higher the liquidity of the underlying
    security. First, let''s look at an item-by-item division:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we estimate Amihud''s illiquidity for IBM based on trading
    data in October 2013\. The value is *1.21*10-11*. It seems that this value is
    quite small. Actually, the absolute value is not important; the relative value
    matters. If we estimate the illiquidity for WMT over the same period, we would
    find a value of *1.52*10-11*. Since 1.21 is less than 1.52, we conclude that IBM
    is more liquid than WMT. This correlation is represented in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Estimating Pastor and Stambaugh (2003) liquidity measure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on the methodology and empirical evidence in Campbell, Grossman, and
    Wang (1993), Pastor and Stambaugh (2003) designed the following model to measure
    individual stock''s liquidity and the market liquidity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating Pastor and Stambaugh (2003) liquidity measure](img/B06175_08_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *yt* is the excess stock return, *Rt-Rf* , *t*, on day *t*, *Rt* is the
    return for the stock, *Rf,t* is the risk-free rate, *x1*,*t* is the market return,
    and *x2*,*t* is the signed dollar trading volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating Pastor and Stambaugh (2003) liquidity measure](img/B06175_08_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*pt* is the stock price, and volume, *t* is the trading volume. The regression
    is run based on daily data for each month. In other words, for each month, we
    get one *β2* that is defined as the liquidity measure for individual stock. The
    following code estimates the liquidity for IBM. First, we download the IBM and
    S&P500 daily price data, estimate their daily returns, and merge them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous program, *y* is IBM''s excess return at time *t+1*, *x1* is
    the market excess return at time *t*, and *x2* is the signed dollar trading volume
    at time *t*. The coefficient before *x2* is Pastor and Stambaugh''s liquidity
    measure. The corresponding output is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Fama-MacBeth regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s look at the OLS regression by using the `pandas.ols` function
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'For the Fama-MacBeth regression, we have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Durbin-Watson
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Durbin-Watson statistic is related auto-correlation. After we run a regression,
    the error term should have no correlation, with a mean zero. Durbin-Watson statistic
    is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Durbin-Watson](img/B06175_08_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *et* is the error term at time *t*, *T* is the total number of error
    term. The Durbin-Watson statistic tests the null hypothesis that the residuals
    from an ordinary least-squares regression are not auto-correlated against the
    alternative that the residuals follow an AR1 process. The Durbin-Watson statistic
    ranges in value from 0 to 4\. A value near 2 indicates non-autocorrelation; a
    value toward 0 indicates positive autocorrelation; a value toward 4 indicates
    negative autocorrelation, see the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Durbin-Watson Test | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ![Durbin-Watson](img/B06175_08_41.jpg) | No autocorrelation |'
  prefs: []
  type: TYPE_TB
- en: '| Towards 0 | Positive auto-correlation |'
  prefs: []
  type: TYPE_TB
- en: '| Towards 4 | Negative auto-correlation |'
  prefs: []
  type: TYPE_TB
- en: Table 8.3 Durbin-Watson Test
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The following Python program runs a CAPM first by using daily data for IBM.
    The S&P500 is used as the index. The time period is from 1/1/2012 to 12/31/2016,
    a 5-year window. The risk-free rate is ignored in this case. For the residual
    from the regression, a Durbin-Watson test is run to test its autocorrelation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Durbin-Watson](img/B06175_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A positive of 1.82 close to 2 indicates the autocorrelation might be zero for
    the residuals from the CAPM for IBM. We would have a more definitive answer. Alternatively,
    we simply type the command of `print(result.summary())`, see the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Durbin-Watson](img/B06175_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding result shows the number of observations is 1,257 and Durbin-Watson
    test is 1.82\. Based on lower (upper) bounds (dL and dU) at: [https://web.stanford.edu/~clint/bench/dwcrit.htm](https://web.stanford.edu/~clint/bench/dwcrit.htm),
    we conclude that 1.82 is not close enough to 2\. Thus, the residuals are still
    positively correlated. The **Akaike Information Criterion** (**AIC**) is a measure
    of the relative quality of statistical models for a given set of data. It has
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Durbin-Watson](img/B06175_08_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *k* is the number of coefficients to be estimated in the model and *L*
    is the value of the log-likelihood. In the preceding example, *k=1* and *L=4089.0*.
    Thus, AIC will be *2*1-2*4089.9=8177.8*. AIC would test whether this is a good
    model in an absolute term. However, given several candidate models, the preferred
    model is the one with the minimum AIC value. AIC rewards goodness of fit (as assessed
    by the likelihood function), but it also includes a penalty that is an increasing
    function of the number of estimated parameters (*k*). BIC stands for Bayesian
    Information Criterion and it is defined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Durbin-Watson](img/B06175_08_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *n* is the number of observations and *k* is the number of parameters
    to be estimated including the intercept. The Jarque–Bera test is a goodness-of-fit
    test of whether our data has the skewness and kurtosis matching a normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Durbin-Watson](img/B06175_08_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *S* is the skewness and *C* is the kurtosis. The null hypothesis is a
    joint hypothesis of the skewness being zero and the excess kurtosis being zero.
    From the preceding result, since Prob. (*JB*) is zero, we reject the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Python for high-frequency data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'High-frequency data is referred to as second-by-second or millisecond-by-millisecond
    transaction and quotation data. The New York Stock Exchange''s **Trade and Quotation**
    (**TAQ**) database is a typical example ([http://www.nyxdata.com/data-products/daily-taq](http://www.nyxdata.com/data-products/daily-taq)).
    The following program can be used to retrieve high-frequency data from Google
    Finance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding program, we have two input variables: `ticker` and `path`.
    After we choose `path` with an embedded variable called `ttt`, we replace it with
    our `ticker` using the `string.replace()` function. The first and last five lines
    are shown as follows using the `.head()` and `.tail()` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Python for high-frequency data](img/B06175_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The related web page for the intra-day high-frequency data from Google is located
    at [https://www.google.com/finance/getprices?q=AAPL&i=300&p=10d&f=d,o,%20h,l,c,v](https://www.google.com/finance/getprices?q=AAPL&i=300&p=10d&f=d,o,%20h,l,c,v)
    and its header (first 10) lines are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The objective of the following program is to add a timestamp:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the program, we can observe the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the first and last several lines, we could use the `.head()` and `.tail()`
    functions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Since the TAQ database is quite expensive, potentially, most readers might not
    be able to access the data. Fortunately, we have a database called **Trade, Order,
    Report, and Quotation** (**TORQ**). Thanks to Prof. Hasbrouck, the database can
    be downloaded from [http://people.stern.nyu.edu/jhasbrou/Research/](http://people.stern.nyu.edu/jhasbrou/Research/).
  prefs: []
  type: TYPE_NORMAL
- en: 'From the same web page, we could download the TORQ manual as well. Based on
    Prof. Hasbrouck''s binary datasets, we generate a few corresponding datasets in
    the pickle format of pandas. The **Consolidated Trade** (**CT**) dataset can be
    downloaded from [http://canisius.edu/~yany/python/TORQct.pkl](http://canisius.edu/~yany/python/TORQct.pkl).
    After saving this dataset in `C:\temp`, we can issue the following two lines of
    Python code to retrieve it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the first and last couple of lines, we use the `.head()` and `.tail()`
    functions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the `ticker` is used as an index, we could list all unique index values
    to find out the names of stocks contained in the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Spread estimated based on high-frequency data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on the **Consolidated Quote** (**CQ**) dataset supplied by Prof. Hasbrouck,
    we generate a dataset with the pickle format of pandas, that can be downloaded
    from [http://canisius.edu/~yany/python/TORQcq.pkl](http://canisius.edu/~yany/python/TORQcq.pkl).
    Assume that the following data is located under `C:\temp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we could use the `unique()` function to find out all tickers. Assume
    that we are interested in a stock with an `MO` ticker as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'It is a good idea to check a few observations. From the first line of the following
    output, we know that spread should be 0.125 (47.125-47.000):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'To find the mean spread and the mean relative spread, we have the following
    code. The complete program is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we didn't process or clean the data. Usually, we have
    to process data by adding various filters, such as delete quotes with negative
    spread, `bidsiz` is zero, or `ofrsiz` is zero, before we estimate spread and do
    other estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to CRSP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this book, our focus is free public data. Thus, we only discuss a few financial
    databases since some readers might from schools with valid subscription. CRSP
    is the one. In this chapter, we mention just three Python datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '**Center for Research in Security Prices** (**CRSP**). It contains all trading
    data, such as closing price, trading volume, and shares outstanding for all listed
    stocks in the US from 1926 onward. Because of its quality and long history, it
    has been used intensively by academic researchers and practitioners. The first
    dataset is called `crspInfo.pkl`, see the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The related output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The `PERMNO` is the stock `ID`, `PERMCO` is the company `ID`, `CUSIP` is security
    `ID`, `FIRMNAME` is the company header name, that is, today''s name, `EXCHANGE`
    is the exchange code, `BEGDATE (ENDDATE)` is when the data is available. The second
    dataset is for market indices, see the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The last dataset is for monthly stocks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please refer to the following articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Amihud and Yakov, 2002*, *Illiquidity and stock returns: cross-section and
    time-series effects*, *Journal of Financial Markets, 5, 31–56*, [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.9505&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.9505&rep=rep1&type=pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bali, T. G., Cakici, N., and Whitelaw, R. F., 2011*, *Maxing out: Stocks as
    lotteries and the cross-section of expected returns*, *Journal of Financial Economics,
    99(2), 427–446* [http://www.sciencedirect.com/science/article/pii/S0304405X1000190X](http://www.sciencedirect.com/science/article/pii/S0304405X1000190X)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cook Pine Capital LLC, November 26, 2008, Study of Fat-tail Risk*, [http://www.cookpinecapital.com/pdf/Study%20of%20Fat-tail%20Risk.pdf](http://www.cookpinecapital.com/pdf/Study%20of%20Fat-tail%20Risk.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CRSP web site, [http://crsp.com/](http://crsp.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CRSP user manual, [http://www.crsp.com/documentation](http://www.crsp.com/documentation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*George, T.J., and Hwang, C., 2004*, *The 52-Week High and Momentum Investing,
    Journal of Finance 54(5), 2145–2176*, [http://www.bauer.uh.edu/tgeorge/papers/gh4-paper.pdf](http://www.bauer.uh.edu/tgeorge/papers/gh4-paper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hasbrouck, Joel, 1992*, *Using the TORQ database, New York University*, [http://people.stern.nyu.edu/jhasbrou/Research/Working%20Papers/TORQDOC3.PDF](http://people.stern.nyu.edu/jhasbrou/Research/Working%20Papers/TORQDOC3.PDF)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jegadeesh, N., and Titman, S., 1993, Returns to Buying Winners and Selling
    Losers: Implications for Stock Market Efficiency, Journal of Finance 48(1), 65–91*,
    [http://www.e-m-h.org/JeTi93.pdf](http://www.e-m-h.org/JeTi93.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Moskowitz, T., and Grinblatt, M., 1999, Do industries explain momentum? Journal
    of Finance 54(4), 2017–2069,* [http://faculty.som.yale.edu/Tobiasmoskowitz/documents/DoIndustriesExplainMomentum.pdf](http://faculty.som.yale.edu/Tobiasmoskowitz/documents/DoIndustriesExplainMomentum.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pastor and Stambaugh, 2003, Liqudity measure and expected stock returns, Journal
    of Political Economy, 642-685,* [http://people.stern.nyu.edu/lpederse/courses/LAP/papers/TransactionCosts/PastorStam.pdf](http://people.stern.nyu.edu/lpederse/courses/LAP/papers/TransactionCosts/PastorStam.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Roll. R., 1984, A Simple Measure of the Effective Bid-Ask Spread in an Efficient
    Market, Journal of Finance, 39, 1127-1139,* [http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.1984.tb03897.x/pdf](http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.1984.tb03897.x/pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A – Python program to generate GDP dataset usGDPquarterly2.pkl
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first program generates a Python dataset with a `.pkl` extension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'To retrieve the dataset, we use the `pandas.read_pickle()` function. See the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Appendix B – critical values of F for the 0.05 significance level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first row is for the degree of freedom for the denominator while the first
    column is for the degree of freedom for the numerator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Appendix B – critical values of F for the 0.05 significance level](img/B06175_08_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The key part of the program used to generate the preceding table is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Appendix C – data case #4 - which political party manages the economy better?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the US, people have been seeing many presidential debates among potential
    presidential nominees for the Republican and Democratic parties. One question
    a potential voter likes to ask is, which party could manage the economy better?
    With this term project, we try to ask this question: which party could manage
    the economy better in terms of the performance of the stock market? According
    to the web page of [http://www.enchantedlearning.com/history/us/pres/list.shtml](http://www.enchantedlearning.com/history/us/pres/list.shtml),
    we could find which party a US president belongs to:'
  prefs: []
  type: TYPE_NORMAL
- en: '| President which party time period |'
  prefs: []
  type: TYPE_TB
- en: '| ![Appendix C – data case #4 - which political party manages the economy better?](img/B06175_08_43.jpg)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Thus, we could generate the following table. The PARTY and RANGE variables
    are from the web page. YEAR2 is the second number of RANGE minus 1, except for
    the last row:'
  prefs: []
  type: TYPE_NORMAL
- en: '| PARTY | RANGE | YEAR1 | YEAR2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Republican | 1923-1929 | 1923 | 1928 |'
  prefs: []
  type: TYPE_TB
- en: '| Republican | 1929-1933 | 1929 | 1932 |'
  prefs: []
  type: TYPE_TB
- en: '| Democrat | 1933-1945 | 1933 | 1944 |'
  prefs: []
  type: TYPE_TB
- en: '| Democrat | 1945-1953 | 1945 | 1952 |'
  prefs: []
  type: TYPE_TB
- en: '| Republican | 1953-1961 | 1953 | 1960 |'
  prefs: []
  type: TYPE_TB
- en: '| Democrat | 1961-1963 | 1961 | 1962 |'
  prefs: []
  type: TYPE_TB
- en: '| Democrat | 1963-1969 | 1963 | 1968 |'
  prefs: []
  type: TYPE_TB
- en: '| Republican | 1969-1974 | 1969 | 1973 |'
  prefs: []
  type: TYPE_TB
- en: '| Republican | 1974-1977 | 1974 | 1976 |'
  prefs: []
  type: TYPE_TB
- en: '| Democrat | 1977-1981 | 1977 | 1980 |'
  prefs: []
  type: TYPE_TB
- en: '| Republican | 1981-1989 | 1981 | 1988 |'
  prefs: []
  type: TYPE_TB
- en: '| Republican | 1989-1993 | 1989 | 1992 |'
  prefs: []
  type: TYPE_TB
- en: '| Democrat | 1993-2001 | 1993 | 2000 |'
  prefs: []
  type: TYPE_TB
- en: '| Republican | 2001-2009 | 2001 | 2008 |'
  prefs: []
  type: TYPE_TB
- en: '| Democrat | 2009-2017 | 2009 | 2016 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Parties and Presidents since 1923'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Retrieve monthly stock data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Classify returns into two groups according to YEAR1 and YEAR2: under Republican
    and under Democratic.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Test the null hypothesis: two group means are equal:![Appendix C – data case
    #4 - which political party manages the economy better?](img/B06175_08_35.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Discuss your results and answer the following question: are the monthly mean
    returns under both parties equal? Based on the preceding table, readers could
    sort all monthly mean returns into two categories: under Democratic Party and
    under the Republican Party.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: For readers from schools without CRSP subscription, they could download the
    S&P500 market index from Yahoo! Finance. On the other hand, for readers from schools
    with CRSP subscriptions, they could use both **value-weighted market returns**
    (**VWRETD**) and **equal-weighted market index** (**EWRETD**).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which module contains the function called rolling_kurt? How can you use the
    function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on daily data downloaded from Yahoo! Finance, find whether Wal-Mart's
    daily returns follow a normal distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on daily returns in 2016, are the mean returns for IBM and DELL the same?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: You can use Yahoo! Finance as your source of data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How many dividends distributed or stock splits happened over the past 10 years
    for IBM and DELL based on the historical data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a Python program to estimate rolling beta on a 3-year window for a few
    stocks such as IBM, WMT, C and MSFT.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Assume that we just downloaded the prime rate from the Federal Banks'' data
    library from: [http://www.federalreserve.gov/releases/h15/data.htm](http://www.federalreserve.gov/releases/h15/data.htm).
    We downloaded the time-series for Financial 1-month business day. Write a Python
    program to merge them using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go to the web page: [http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html](http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Click on Fama-French Factor, and download their monthly factors named `F-F_Research_Data_Factors.zip`.
    Unzip the `.zip` file and estimate market monthly returns.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, for July 1926, *market return = 2.65/100+0.22/100*. This file was
    created by `CMPT_ME_BEME_RETS` using the 201212 CRSP database.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Download the monthly and daily Fama-French factors from Prof. French''s data
    library at: [http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html](http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html).
    Assume that you are holding an SMB portfolio. Answer the following three questions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the total return from January 1, 1989 to December 31, 2016 using daily
    data?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the total return from January 1, 1989, to December 31, 2016, using monthly
    data?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Are they the same? If they are different, explain some reasons that lead to
    their differences.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How to replicate Jagadeech and Tidman (1993) momentum strategy by using Python
    and CRSP data? [Assume that your school has CRSP subscription].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Write a Python program to estimate returns. The format of your function could
    be `dailyRet(data,sorted=0)`. Then sorted is for how the price is sorted. For
    example, the default value could be from the oldest to the newest, while `sorted=1`
    for the opposite. One related Python program is given here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note that there are two sorting: p1 is before p2 or p1 is after p2.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Replicate the table for the critical values of F for the `0.05` significant
    level in Appendix B. The following Python program is offered:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In addition, generate the similar tables for 0.01 and 0.10 significant levels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the program to test the January effect, write a Python program to test
    week-day effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generate a business cycle indicator. The business cycle data is from the National
    Bureau of Economic Research center. The original starting date is June 1854, [http://www.nber.org/cycles/cyclesmain.html](http://www.nber.org/cycles/cyclesmain.html).
    Since stock data starts from 1926, we could remove data before 1923\. For a peak,
    we assign a positive 1, while for a trough, we assign a negative 1\. Any months
    between those peaks and troughs, we linearly interpolate, see the following Panel
    B. *P* for peak and *T* for trough. *T(t-1)* is for the previous trough and *P(t-1)*
    is for the previous peak:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '|   |   | Contraction | Expansion | Cycle |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Peak (P) | Trough (T) | P to T | T(t-1) to P | T(-1) to T | P(t-1) to P |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| May 1923(II) | July 1924 (III) | 14 | 22 | 36 | 40 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| October 1926(III) | November 1927 (IV) | 13 | 27 | 40 | 41 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| August 1929(III) | March 1933 (I) | 43 | 21 | 64 | 34 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| May 1937(II) | June 1938 (II) | 13 | 50 | 63 | 93 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| February 1945(I) | October 1945 (IV) | 8 | 80 | 88 | 93 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| November 1948(IV) | October 1949 (IV) | 11 | 37 | 48 | 45 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| July 1953(II) | May 1954 (II) | 10 | 45 | 55 | 56 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| August 1957(III) | April 1958 (II) | 8 | 39 | 47 | 49 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| April 1960(II) | February 1961 (I) | 10 | 24 | 34 | 32 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| December 1969(IV) | November 1970 (IV) | 11 | 106 | 117 | 116 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| November 1973(IV) | March 1975 (I) | 16 | 36 | 52 | 47 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| January 1980(I) | July 1980 (III) | 6 | 58 | 64 | 74 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| July 1981(III) | November 1982 (IV) | 16 | 12 | 28 | 18 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| July 1990(III) | March 1991(I) | 8 | 92 | 100 | 108 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| March 2001(I) | November 2001 (IV) | 8 | 120 | 128 | 128 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| December 2007 (IV) | June 2009 (II) | 18 | 73 | 91 | 81 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Write a Python program to download daily price and estimate daily returns.
    Then convert daily returns into monthly ones. The date variable for the monthly
    returns should be the last trading days of the month. A Python dataset at: [http://canisius.edu/~yany/python/tradingDaysMonthly.pkl](http://canisius.edu/~yany/python/tradingDaysMonthly.pkl),
    could be used, see the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Write a Python program to generate quarterly returns from historical daily price
    or historical monthly price data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, many concepts and issues associated with time-series are discussed
    in detail. Topics include how to design a true date variable, how to merge datasets
    with different frequencies, how to download historical prices from Yahoo! Finance;
    also, different ways to estimate returns, estimate the Roll (1984) spread, Amihud's
    (2002) illiquidity, Pastor and Stambaugh's (2003) liquidity, and how to retrieve
    high-frequency data from Prof. Hasbrouck's TORQ database (Trade, Oder, Report
    and Quotation). In addition, two datasets from CRSP are shown. Since this book
    is focusing on open and publicly available finance, economics, and accounting
    data, we could mention a few financial databases superficially.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we discuss many concepts and theories related to portfolio
    theory such as how to measure portfolio risk, how to estimate the risk of 2-stock
    and n-stock portfolio, the trade-off between risk and return by using various
    measures of Sharpe ratio, Treynor ratio, and Sortino ratio, how to minimize portfolio
    risk based on those measures (ratios), how to set up an objective function, how
    to choose an efficient portfolio for a given set of stocks, and how to construct
    an efficient frontier.
  prefs: []
  type: TYPE_NORMAL
