["```py\npath = '../input/adult.csv'\ninput_data = pd.read_csv(path, na_values=\"?\")\ninput_data = input_data[input_data['race'].isin(['White', 'Black'])]\n```", "```py\nsensitive_attribs = ['race', 'gender']\nA = input_data[sensitive_attribs]\nA = pd.get_dummies(A,drop_first=True)\nA.columns = sensitive_attribs\n```", "```py\ny = (input_data['income'] == '>50K').astype(int)\n```", "```py\nX = input_data.drop(labels=['income', 'race', 'gender'],axis=1)\n\nX = X.fillna('Unknown')\n\nX = pd.get_dummies(X, drop_first=True)\n```", "```py\nX_train, X_test, y_train, y_test, A_train, A_test = \\\ntrain_test_split(X, y, A, test_size=0.5, stratify=y, random_state=7)\n```", "```py\nscaler = StandardScaler().fit(X_train)\n\nX_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns, index=X_train.index)\n\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n```", "```py\ndef p_rule(y_pred, a_values, threshold=0.5):\n    y_a_1 = y_pred[a_values == 1] > threshold if threshold else y_pred[a_values == 1]                                           #1\n    y_a_0 = y_pred[a_values == 0] > threshold if threshold else y_pred[a_values == 0] \n    odds = y_a_1.mean() / y_a_0.mean()                          #2\n    return np.min([odds, 1/odds]) * 100\n```", "```py\nn_features=X_train.shape[1]\nn_sensitive=A_train.shape[1]\n```", "```py\nclf_inputs = Input(shape=(n_features,))\nx = Dense(32, activation='relu')(clf_inputs)\nx = Dropout(0.2)(x)\nx = Dense(32, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(32, activation='relu')(x)\nx = Dropout(0.2)(x)\noutputs = Dense(1, activation='sigmoid', name='y')(x)\nclf_net = Model(inputs=[clf_inputs], outputs=[outputs])\n```", "```py\nadv_inputs = Input(shape=(1,))\nx = Dense(32, activation='relu')(adv_inputs)\nx = Dense(32, activation='relu')(x)\nx = Dense(32, activation='relu')(x)\nout_race = Dense(1, activation='sigmoid')(x)\nout_gender = Dense(1, activation='sigmoid')(x)\nadv_net = Model(inputs=[adv_inputs], outputs=[out_race,out_gender])\n```", "```py\ndef make_trainable_fn(net):              #1\n    def make_trainable(flag):            #2\n        net.trainable = flag             #3\n        for layer in net.layers:\n            layer.trainable = flag\n    return make_trainable                #4\n```", "```py\ntrainable_clf_net = make_trainable_fn(clf_net)\ntrainable_adv_net = make_trainable_fn(adv_net)\n```", "```py\ntrainable_clf_net(True)\n```", "```py\nclf = clf_net\nclf.compile(loss='binary_crossentropy', optimizer='adam')\n```", "```py\nadv_out = adv_net(clf_net(clf_inputs))\n```", "```py\nclf_out = clf_net(clf_inputs)\n```", "```py\nclf_w_adv = Model(inputs=[clf_inputs], outputs=[clf_out]+adv_out)\n```", "```py\ntrainable_clf_net(True)\ntrainable_adv_net(False)\n```", "```py\nloss_weights = [1.]+[-lambda_param for lambda_param in lambdas]\n```", "```py\nclf_w_adv.compile(loss='binary_crossentropy'), loss_weights=loss_weights,optimizer='adam')\n```", "```py\nadv = Model(inputs=[clf_inputs], outputs=adv_net(clf_net(clf_inputs)))\n```", "```py\ntrainable_clf_net(False)\ntrainable_adv_net(True)\n```", "```py\nadv.compile(loss='binary_crossentropy', optimizer='adam')\n```", "```py\ntrainable_clf_net(True)\nclf.fit(X_train.values, y_train.values, epochs=10)\n```", "```py\ny_pred = clf.predict(X_test)\n```", "```py\nacc = accuracy_score(y_test,(y_pred>0.5))* 100\nprint('Clf acc: {:.2f}'.format(acc))\n\nfor sens in A_test.columns:\n    pr = p_rule(y_pred,A_test[sens])\n    print('{}: {:.2f}%'.format(sens,pr))\n```", "```py\nout:\nClf acc: 85.44\nrace: 41.71%\ngender: 29.41%\n\n```", "```py\ntrainable_clf_net(False)\ntrainable_adv_net(True)\n```", "```py\nclass_weight_adv = compute_class_weights(A_train)\n```", "```py\nadv.fit(X_train.values, np.hsplit(A_train.values, A_train.shape[1]), class_weight=class_weight_adv, epochs=10)\n```", "```py\nn_iter=250\nbatch_size=128\nn_sensitive = A_train.shape[1]\n```", "```py\nclass_weight_clf_w_adv = [{0:1., 1:1.}]+class_weight_adv\n```", "```py\nval_metrics = pd.DataFrame()\nfairness_metrics = pd.DataFrame()\n```", "```py\nfor idx in range(n_iter):\n```", "```py\ntrainable_clf_net(False)\ntrainable_adv_net(True)\nadv.fit(X_train.values, np.hsplit(A_train.values, A_train.shape[1]), batch_size=batch_size, class_weight=class_weight_adv, epochs=1, verbose=0)\n```", "```py\ntrainable_clf_net(True)\ntrainable_adv_net(False)\n```", "```py\nindices = np.random.permutation(len(X_train))[:batch_size]\nX_batch = X_train.values[indices]\ny_batch = y_train.values[indices]\nA_batch = A_train.values[indices]\n```", "```py\nclf_w_adv.train_on_batch(X_batch, [y_batch]+\\np.hsplit(A_batch, n_sensitive),class_weight=class_weight_clf_w_adv)\n```", "```py\ny_pred = pd.Series(clf.predict(X_test).ravel(), index=y_test.index)\n```", "```py\nroc_auc = roc_auc_score(y_test, y_pred)\nacc = accuracy_score(y_test, (y_pred>0.5))*100\n\nval_metrics.loc[idx, 'ROC AUC'] = roc_auc\nval_metrics.loc[idx, 'Accuracy'] = acc\n```", "```py\nfor sensitive_attr :n A_test.columns:\n    fairness_metrics.loc[idx, sensitive_attr] =\\\n    p_rule(y_pred,A_test[sensitive_attr])\n```", "```py\npip install linearmodels\n\n```", "```py\nfrom linearmodels.iv import IV2SLS\niv = IV2SLS(dependent=y,exog=X,endog=P],instruments=Z).fit(cov_type='unadjusted')\n```", "```py\nexplainer = shap.KernelExplainer(f, X.iloc[:100,:])\n```", "```py\nshap_values = explainer.shap_values(X.iloc[350,:], nsamples=500)\n```", "```py\nshap.force_plot(explainer.expected_value, shap_values)\n```", "```py\nshap_values = explainer.shap_values(X.iloc[167,:], nsamples=500)\nshap.force_plot(explainer.expected_value, shap_values)\n```", "```py\nshap_values = explainer.shap_values(X.iloc[100:330,:], nsamples=500)\n```", "```py\nshap.force_plot(explainer.expected_value, shap_values)\n```", "```py\nshap.summary_plot(shap_values, X.iloc[100:330,:])\n```", "```py\nshap.dependence_plot(\"marital-status\", shap_values, X.iloc[100:330,:], display_features=X_display.iloc[100:330,:])\n```"]