- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: 'Applied Machine Learning: Identifying Credit Default'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用机器学习：识别信用违约
- en: In recent years, we have witnessed machine learning gaining more and more popularity
    in solving traditional business problems. Every so often, a new algorithm is published,
    beating the current state of the art. It is only natural for businesses (in all
    industries) to try to leverage the incredible powers of machine learning in their
    core functionalities.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，我们见证了机器学习在解决传统商业问题方面越来越受到欢迎。时不时就会有新的算法发布，超越当前的最先进技术。各行各业的企业试图利用机器学习的强大能力来改进其核心功能，似乎是自然而然的事情。
- en: 'Before specifying the task we will be focusing on in this chapter, we provide
    a brief introduction to the field of machine learning. The machine learning domain
    can be broken down into two main areas: supervised learning and unsupervised learning.
    In the former, we have a target variable (label), which we try to predict as accurately
    as possible. In the latter, there is no target, and we try to use different techniques
    to draw some insights from the data.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始具体讨论我们将专注的任务之前，我们首先简要介绍一下机器学习领域。机器学习可以分为两个主要方向：监督学习和无监督学习。在监督学习中，我们有一个目标变量（标签），我们尽力预测其尽可能准确的值。在无监督学习中，没有目标变量，我们试图利用不同的技术从数据中提取一些洞见。
- en: We can further break down supervised problems into regression problems (where
    a target variable is a continuous number, such as income or the price of a house)
    and classification problems (where the target is a class, either binary or multi-class).
    An example of unsupervised learning is clustering, which is often used for customer
    segmentation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以进一步将监督学习问题细分为回归问题（目标变量是连续数值，比如收入或房价）和分类问题（目标是类别，可能是二分类或多分类）。无监督学习的一个例子是聚类，通常用于客户细分。
- en: In this chapter, we tackle a binary classification problem set in the financial
    industry. We work with a dataset contributed to the UCI Machine Learning Repository,
    which is a very popular data repository. The dataset used in this chapter was
    collected in a Taiwanese bank in October 2005\. The study was motivated by the
    fact that—at that time—more and more banks were giving credit (either cash or
    via credit cards) to willing customers. On top of that, more people, regardless
    of their repayment capabilities, accumulated significant amounts of debt. All
    of this led to situations in which some people were unable to repay their outstanding
    debts. In other words, they defaulted on their loans.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们解决的是一个金融行业中的二分类问题。我们使用的数据集贡献自UCI机器学习库，这是一个非常流行的数据存储库。本章使用的数据集是在2005年10月由一家台湾银行收集的。研究的动机是——当时——越来越多的银行开始向愿意的客户提供信用（无论是现金还是信用卡）。此外，越来越多的人无论其还款能力如何，积累了大量债务。这一切导致了部分人无法偿还未结清的债务，换句话说，他们违约了。
- en: The goal of the study was to use some basic information about customers (such
    as gender, age, and education level), together with their past repayment history,
    to predict which of them were likely to default. The setting can be described
    as follows—using the previous 6 months of repayment history (April-September 2005),
    we try to predict whether the customer will default in October 2005\. Naturally,
    such a study could be generalized to predict whether a customer will default in
    the next month, within the next quarter, and so on.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究的目标是利用一些基本的客户信息（如性别、年龄和教育水平），结合他们的过往还款历史，来预测哪些客户可能会违约。该设置可以描述如下——使用前6个月的还款历史（2005年4月到9月），我们尝试预测该客户是否会在2005年10月违约。自然，这样的研究可以推广到预测客户是否会在下个月、下个季度等时段违约。
- en: By the end of this chapter, you will be familiar with a real-life approach to
    a machine learning task, from gathering and cleaning data to building and tuning
    a classifier. Another takeaway is understanding the general approach to machine
    learning projects, which can then be applied to many different tasks, be it churn
    prediction or estimating the price of new real estate in a neighborhood.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将熟悉一个机器学习任务的实际操作流程，从数据收集和清理到构建和调优分类器。另一个收获是理解机器学习项目的一般方法，这可以应用于许多不同的任务，无论是客户流失预测，还是估算某个区域新房地产的价格。
- en: 'In this chapter, we focus on the following recipes:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点介绍以下内容：
- en: Loading data and managing data types
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据与管理数据类型
- en: Exploratory data analysis
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: Splitting data into training and test sets
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集
- en: Identifying and dealing with missing values
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别和处理缺失值
- en: Encoding categorical variables
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码类别变量
- en: Fitting a decision tree classifier
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拟合决策树分类器
- en: Organizing the project with pipelines
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用管道组织项目
- en: Tuning hyperparameters using grid search and cross-validation
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网格搜索和交叉验证调优超参数
- en: Loading data and managing data types
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据并管理数据类型
- en: In this recipe, we show how to load a dataset from a CSV file into Python. The
    very same principles can be used for other file formats as well, as long as they
    are supported by `pandas`. Some popular formats include Parquet, JSON, XLM, Excel,
    and Feather.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们展示了如何将数据集从 CSV 文件加载到 Python 中。相同的原则也可以应用于其他文件格式，只要它们被 `pandas` 支持。一些常见的格式包括
    Parquet、JSON、XLM、Excel 和 Feather。
- en: '`pandas` has a very consistent API, which makes finding its functions much
    easier. For example, all functions used for loading data from various sources
    have the syntax `pd.read_xxx`, where `xxx` should be replaced by the file format.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas` 拥有非常一致的 API，这使得查找其函数变得更加容易。例如，所有用于从各种来源加载数据的函数都有 `pd.read_xxx` 这样的语法，其中
    `xxx` 应替换为文件格式。'
- en: We also show how certain data type conversions can significantly reduce the
    size of DataFrames in the memory of our computers. This can be especially important
    when working with large datasets (GBs or TBs), which can simply not fit into memory
    unless we optimize their usage.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了如何通过某些数据类型转换显著减少 DataFrame 在我们计算机内存中的大小。这在处理大型数据集（GB 或 TB 级别）时尤为重要，因为如果不优化其使用，它们可能根本无法适应内存。
- en: In order to present a more realistic scenario (including messy data, missing
    values, and so on) we applied some transformations to the original dataset. For
    more information on those changes, please refer to the accompanying GitHub repository.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了呈现更现实的场景（包括杂乱的数据、缺失值等），我们对原始数据集应用了一些转换。有关这些更改的更多信息，请参阅随附的 GitHub 仓库。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Execute the following steps to load a dataset from a CSV file into Python:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤将数据集从 CSV 文件加载到 Python 中：
- en: 'Import the libraries:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE0]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the data from the CSV file:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 CSV 文件加载数据：
- en: '[PRE1]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Running the snippet generates the following preview of the dataset:'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段会生成数据集的以下预览：
- en: '![](../Images/B18112_13_01.png)'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_01.png)'
- en: 'Figure 13.1: Preview of the dataset. Not all columns were displayed'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.1：数据集预览。并非所有列都被显示
- en: The DataFrame has 30,000 rows and 24 columns. It contains a mix of numeric and
    categorical variables.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该 DataFrame 有 30,000 行和 24 列。它包含数字型和类别型变量的混合。
- en: 'View the summary of the DataFrame:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看 DataFrame 的摘要：
- en: '[PRE2]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Running the snippet generates the following summary:'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段会生成以下摘要：
- en: '[PRE3]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the summary, we can see information about the columns and their data types,
    the number of non-null (in other words, non-missing) values, the memory usage,
    and so on.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在摘要中，我们可以看到关于列及其数据类型、非空（换句话说，非缺失）值的数量、内存使用情况等信息。
- en: 'We can also observe a few distinct data types: floats (floating-point numbers,
    such as 3.42), integers, and objects. The last ones are the `pandas` representation
    of string variables. The number next to `float` and `int` indicates how many bits
    this type uses to represent a particular value. The default types use 64 bits
    (or 8 bytes) of memory.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还可以观察到几种不同的数据类型：浮点数（如 3.42）、整数和对象。最后一种是 `pandas` 对字符串变量的表示。紧挨着 `float` 和 `int`
    的数字表示该类型用于表示特定值时所使用的位数。默认类型使用 64 位（或 8 字节）的内存。
- en: 'The basic `int8` type covers integers in the following range: -128 to 127\.
    `uint8` stands for unsigned integer and covers the same total span, but only the
    non-negative values, that is, 0 to 255\. By knowing the range of values covered
    by specific data types (please refer to the link in the *See also* section), we
    can try to optimize allocated memory. For example, for features such as the month
    of purchase (represented by numbers in the range 1-12), there is no point in using
    the default `int64`, as a much smaller type would suffice.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基本的 `int8` 类型覆盖的整数范围是：-128 到 127。`uint8` 表示无符号整数，覆盖相同的范围，但仅包含非负值，即 0 到 255。通过了解特定数据类型覆盖的值范围（请参见
    *另见* 部分中的链接），我们可以尝试优化内存分配。例如，对于表示购买月份（范围为 1-12 的数字）这样的特征，使用默认的 `int64` 类型没有意义，因为一个更小的数据类型就足够了。
- en: 'Define a function for inspecting the exact memory usage of a DataFrame:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来检查 DataFrame 的准确内存使用情况：
- en: '[PRE4]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can now apply the function to our DataFrame:'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们现在可以将该函数应用于我们的 DataFrame：
- en: '[PRE5]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Running the snippet generates the following output:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段会生成以下输出：
- en: '[PRE6]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the output, we can see that the 5.5+ MB reported by the `info` method turned
    out to be almost 4 times more. This is still very small in terms of current machines’
    capabilities, however, the memory-saving principles we show in this chapter apply
    just as well to DataFrames measured in gigabytes.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在输出中，我们可以看到`info`方法报告的5.5+ MB实际上几乎是4倍的值。虽然这在当前机器的能力范围内仍然非常小，但本章中展示的节省内存的原则同样适用于以GB为单位测量的DataFrame。
- en: 'Convert the columns with the `object` data type into the `category` type:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据类型为`object`的列转换为`category`类型：
- en: '[PRE7]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Running the snippet generates the following overview:'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段会生成以下概览：
- en: '[PRE8]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Just by converting the `object` columns into a `pandas`-native categorical representation,
    we managed to reduce the size of the DataFrame by ~80%!
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仅通过将`object`列转换为`pandas`原生的分类表示，我们成功地将DataFrame的大小减少了约80%！
- en: 'Downcast the numeric columns to integers:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数字列降级为整数：
- en: '[PRE9]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Running the snippet generates the following overview:'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段会生成以下概览：
- en: '[PRE10]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the summary, we can see that after a few data type conversions, the column
    that takes up the most memory is the one containing customers’ ages (you can see
    that in the output of `df.info()`, not shown here for brevity). That is because
    it is encoded using a `float` data type and downcasting using the `integer` setting
    was not applied to `float` columns.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在总结中，我们可以看到，在进行几次数据类型转换后，占用最多内存的列是包含顾客年龄的那一列（你可以在`df.info()`的输出中看到这一点，这里为了简洁未显示）。这是因为它使用了`float`数据类型，并且未对`float`列应用`integer`类型的降级。
- en: 'Downcast the `age` column using the `float` data type:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`float`数据类型降级`age`列：
- en: '[PRE11]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Running the snippet generates the following overview:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码片段会生成以下概览：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Using various data type conversions, we have managed to reduce the memory size
    of our DataFrame from 20.5 MB to 1.9 MB, which is a 91% reduction.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过各种数据类型转换，我们将DataFrame的内存大小从20.5 MB减少到了1.9 MB，减少了91%。
- en: How it works...
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: After importing `pandas`, we loaded the CSV file by using the `pd.read_csv`
    function. When doing so, we indicated that empty strings should be interpreted
    as missing values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`pandas`后，我们使用`pd.read_csv`函数加载了CSV文件。在此过程中，我们指示空字符串应视为缺失值。
- en: 'In *Step 3*, we displayed a summary of the DataFrame to inspect its contents.
    To get a better understanding of the dataset, we provide a simplified description
    of the variables:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤3*中，我们展示了DataFrame的摘要以检查其内容。为了更好地理解数据集，我们提供了变量的简化描述：
- en: '`limit_bal`—the amount of the given credit (NT dollars)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`limit_bal`—授予的信用额度（新台币）'
- en: '`sex`—biological sex'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sex`—生物性别'
- en: '`education`—level of education'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`education`—教育水平'
- en: '`marriage`— marital status'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`marriage`—婚姻状况'
- en: '`age`—age of the customer'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`age`—顾客的年龄'
- en: '`payment_status_{month}`—status of payments in one of the previous 6 months'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`payment_status_{month}`—前6个月中某个月的支付状态'
- en: '`bill_statement_{month}`—the number of bill statements (NT dollars) in one
    of the previous 6 months'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bill_statement_{month}`—前6个月中某个月的账单金额（新台币）'
- en: '`previous_payment_{month}`—the number of previous payments (NT dollars) in
    one of the previous 6 months'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`previous_payment_{month}`—前6个月中某个月的支付金额（新台币）'
- en: '`default_payment_next_month`—the target variable indicating whether the customer
    defaulted on the payment in the following month'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`default_payment_next_month`—目标变量，表示顾客是否在下个月出现违约'
- en: In general, `pandas` tries to load and store data as efficiently as possible.
    It automatically assigns data types (which we can inspect by using the `dtypes`
    method of a `pandas` DataFrame). However, there are some tricks that can lead
    to much better memory allocation, which definitely makes working with larger tables
    (in hundreds of MBs, or even GBs) easier and more efficient.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，`pandas`会尽可能高效地加载和存储数据。它会自动分配数据类型（我们可以通过`pandas` DataFrame的`dtypes`方法查看）。然而，有些技巧可以显著改善内存分配，这无疑使得处理更大的表格（以百MB甚至GB为单位）更加容易和高效。
- en: In *Step 4*, we defined a function for inspecting the exact memory usage of
    a DataFrame. The `memory_usage` method returns a `pandas` Series with the memory
    usage (in bytes) for each of the DataFrame’s columns. We converted the output
    into MBs to make it easier to understand.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤4*中，我们定义了一个函数来检查DataFrame的确切内存使用情况。`memory_usage`方法返回一个`pandas`的Series，列出每个DataFrame列的内存使用量（以字节为单位）。我们将输出转换为MB，以便更易理解。
- en: When using the `memory_usage` method, we specified `deep=True`. That is because
    the `object` data type, unlike other dtypes (short for data types), does not have
    a fixed memory allocation for each cell. In other words, as the `object` dtype
    usually corresponds to text, it means that the amount of memory used depends on
    the number of characters in each cell. Intuitively, the more characters in a string,
    the more memory that cell uses.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`memory_usage`方法时，我们指定了`deep=True`。这是因为与其他数据类型（dtypes）不同，`object`数据类型对于每个单元格并没有固定的内存分配。换句话说，由于`object`数据类型通常对应文本，它的内存使用量取决于每个单元格中的字符数。直观来说，字符串中的字符越多，该单元格使用的内存就越多。
- en: In *Step 5*, we leveraged a special data type called `category` to reduce the
    DataFrame’s memory usage. The underlying idea is that string variables are encoded
    as integers, and `pandas` uses a special mapping dictionary to decode them back
    into their original form. This is especially useful when dealing with a limited
    number of distinct values, for example, certain levels of education, country of
    origin, and so on. To save memory, we first identified all the columns with the
    `object` data type using the `select_dtypes` method. Then, we changed the data
    type of those columns from `object` to `category`. We did so using the `astype`
    method.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们利用了一种特殊的数据类型`category`来减少DataFrame的内存使用。其基本思想是将字符串变量编码为整数，`pandas`使用一个特殊的映射字典将其解码回原始形式。这个方法在处理有限的不同值时尤其有效，例如某些教育水平、原籍国家等。为了节省内存，我们首先使用`select_dtypes`方法识别出所有`object`数据类型的列。然后，我们将这些列的数据类型从`object`更改为`category`。这一过程是通过`astype`方法完成的。
- en: We should know when it is actually profitable (from the memory’s perspective)
    to use the `category` data type. A rule of thumb is to use it for variables with
    a ratio of unique observations to the overall number of observations lower than
    50%.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该知道什么时候使用`category`数据类型从内存角度上来说是有利的。一个经验法则是，当唯一观测值与总观测值的比例低于50%时，使用该数据类型。
- en: In *Step 6*, we used the `select_dtypes` method to identify all numeric columns.
    Then, using a `for` loop iterating over the identified columns, we converted the
    values to numeric using the `pd.to_numeric` function. This might strike as odd,
    given that we first identified the numeric columns and then converted them to
    numeric again. However, the crucial part is the `downcast` argument of the function.
    By passing the `"integer"` value, we have optimized the memory usage of all the
    integer columns by downcasting the default `int64` data type to smaller alternatives
    (`int32` and `int8`).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 6*中，我们使用`select_dtypes`方法识别了所有数值列。然后，通过一个`for`循环遍历已识别的列，使用`pd.to_numeric`函数将值转换为数值。虽然看起来有些奇怪，因为我们首先识别了数值列，然后又将它们转换为数值，但关键点在于该函数的`downcast`参数。通过传递`"integer"`值，我们通过将默认的`int64`数据类型降级为更小的替代类型（`int32`和`int8`），优化了所有整数列的内存使用。
- en: Even though we applied the function to all numeric columns, only the ones that
    contained integers were successfully transformed. That is why in *Step 7* we additionally
    downcasted the `float` column containing the clients’ ages.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们将该函数应用于所有数值列，但只有包含整数的列成功进行了转换。这就是为什么在*步骤 7*中，我们额外将包含客户年龄的`float`列进行了降级处理。
- en: There’s more…
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多方法……
- en: In this recipe, we have mentioned how to optimize the memory usage of a `pandas`
    DataFrame. We first loaded the data into Python, then we inspected the columns,
    and at the end we converted the data types of some columns to reduce memory usage.
    However, such an approach might not be possible, as the data might simply not
    fit into memory in the first place.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们提到如何优化`pandas` DataFrame的内存使用。我们首先将数据加载到Python中，然后检查了各列，最后我们将一些列的数据类型转换以减少内存使用。然而，这种方法可能并不总是可行，因为数据可能根本无法适配内存。
- en: 'If that is the case, we can also try the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是这种情况，我们还可以尝试以下方法：
- en: Read the dataset in chunks (by using the `chunk` argument of `pd.read_csv`).
    For example, we could load just the first 100 rows of data.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按块读取数据集（通过使用`pd.read_csv`的`chunk`参数）。例如，我们可以仅加载前100行数据。
- en: Read only the columns we actually need (by using the `usecols` argument of `pd.read_csv`).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只读取我们实际需要的列（通过使用`pd.read_csv`的`usecols`参数）。
- en: While loading the data, use the `column_dtypes` argument to define the data
    types used for each of the columns.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在加载数据时，使用`column_dtypes`参数定义每一列的数据类型。
- en: 'To illustrate, we can use the following snippet to load our dataset and while
    doing so indicate that the selected three columns should have a `category` data
    type:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，我们可以使用以下代码片段加载数据集，并在加载时指定选中的三列应具有 `category` 数据类型：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If all of those approaches fail, we should not give up. While `pandas` is definitely
    the gold standard of working with tabular data in Python, we can leverage the
    power of some alternative libraries, which were built specifically for such a
    case. Below you can find a list of libraries you could use when working with large
    volumes of data:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以上方法都无效，我们也不应该放弃。虽然 `pandas` 无疑是 Python 中操作表格数据的黄金标准，但我们可以借助一些专门为此类情况构建的替代库。下面是你在处理大数据量时可以使用的一些库的列表：
- en: '`Dask`: an open-source library for distributed computing. It facilitates running
    many computations at the same time, either on a single machine or on clusters
    of CPUs. Under the hood, the library breaks down a single large data processing
    job into many smaller tasks, which are then handled by `numpy` or `pandas`. As
    the last step, the library reassembles the results into a coherent whole.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dask`：一个开源的分布式计算库。它可以同时在单台机器或者 CPU 集群上运行多个计算任务。库内部将一个大数据处理任务拆分成多个小任务，然后由 `numpy`
    或 `pandas` 处理。最后一步，库会将结果重新组合成一个一致的整体。'
- en: '`Modin`: a library designed to parallelize `pandas` DataFrames by automatically
    distributing the computation across all of the system’s available CPU cores. The
    library divides an existing DataFrame into different parts such that each part
    can be sent to a different CPU core.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Modin`：一个旨在通过自动将计算任务分配到系统中所有可用 CPU 核心上来并行化 `pandas` DataFrame 的库。该库将现有的 DataFrame
    分割成不同的部分，使得每个部分可以被发送到不同的 CPU 核心处理。'
- en: '`Vaex`: an open-source DataFrame library specializing in lazy out-of-core DataFrames.
    Vaex requires negligible amounts of RAM for inspecting and interacting with a
    dataset of arbitrary size, all thanks to combining the concepts of lazy evaluations
    and memory mapping.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Vaex`：一个开源的 DataFrame 库，专门用于懒加载的外部数据框架。Vaex 通过结合懒加载评估和内存映射的概念，能够在几乎不占用 RAM
    的情况下，检查和操作任意大小的数据集。'
- en: '`datatable`: an open-source library for manipulating 2-dimensional tabular
    data. In many ways, it is similar to `pandas`, with special emphasis on speed
    and the volume of data (up to 100 GB) while using a single-node machine. If you
    have worked with R, you might already be familiar with the related package called
    `data.table`, which is R users’ go-to package when it comes to the fast aggregation
    of large data.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datatable`：一个开源库，用于操作二维表格数据。在许多方面，它与 `pandas` 相似，特别强调速度和数据量（最多支持 100 GB），并且可以在单节点机器上运行。如果你曾使用过
    R，可能已经熟悉相关的 `data.table` 包，这是 R 用户在进行大数据快速聚合时的首选工具。'
- en: '`cuDF`: a GPU DataFrame library that is part of NVIDIA’s RAPIDS, a data science
    ecosystem spanning multiple open-source libraries and leveraging the power of
    GPUs. `cuDF` allows us to use a `pandas`-like API to benefit from the performance
    boost without going into the details of CUDA programming.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cuDF`：一个 GPU DataFrame 库，是 NVIDIA RAPIDS 生态系统的一部分，RAPIDS 是一个涉及多个开源库并利用 GPU
    强大计算能力的数据科学生态系统。`cuDF` 允许我们使用类似 `pandas` 的 API，在无需深入了解 CUDA 编程的情况下，享受性能提升的好处。'
- en: '`polars`: an open-source DataFrame library that achieves phenomenal computation
    speed by leveraging Rust (programming language) with Apache Arrow as its memory
    model.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`polars`：一个开源的 DataFrame 库，通过利用 Rust 编程语言和 Apache Arrow 作为内存模型，达到了惊人的计算速度。'
- en: See also
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Additional resources:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 额外资源：
- en: 'Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dua, D. 和 Graff, C.（2019）。UCI 机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州尔湾：加利福尼亚大学信息与计算机科学学院。
- en: Yeh, I. C. & Lien, C. H. (2009). “The comparisons of data mining techniques
    for the predictive accuracy of probability of default of credit card clients.”
    *Expert Systems with Applications*, 36(2), 2473-2480\. [https://doi.org/10.1016/j.eswa.2007.12.020](https://doi.org/10.1016/j.eswa.2007.12.020).
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeh, I. C. 和 Lien, C. H.（2009）。"数据挖掘技术比较对信用卡客户违约概率预测准确性的影响。" *Expert Systems
    with Applications*, 36(2), 2473-2480。 [https://doi.org/10.1016/j.eswa.2007.12.020](https://doi.org/10.1016/j.eswa.2007.12.020)。
- en: 'List of different data types used in Python: [https://numpy.org/doc/stable/user/basics.types.html#.](https://numpy.org/doc/stable/user/basics.types.html#.)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python中使用的不同数据类型列表：[https://numpy.org/doc/stable/user/basics.types.html#.](https://numpy.org/doc/stable/user/basics.types.html#.)
- en: Exploratory data analysis
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: The second step of a data science project is to carry out **Exploratory Data
    Analysis** (**EDA**). By doing so, we get to know the data we are supposed to
    work with. This is also the step during which we test the extent of our domain
    knowledge. For example, the company we are working for might assume that the majority
    of its customers are people between the ages of 18 and 25\. But is this actually
    the case? While doing EDA we might also run into some patterns that we do not
    understand, which are then a starting point for a discussion with our stakeholders.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学项目的第二步是进行**探索性数据分析**（**EDA**）。通过这样做，我们可以了解需要处理的数据。这也是我们检验自己领域知识深度的阶段。例如，我们所服务的公司可能假设其大多数客户年龄在18到25岁之间。但事实真的是这样吗？在进行EDA时，我们可能会发现一些自己无法理解的模式，这时就可以成为与利益相关者讨论的起点。
- en: 'While doing EDA, we can try to answer the following questions:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行EDA时，我们可以尝试回答以下问题：
- en: What kind of data do we actually have, and how should we treat different data
    types?
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们到底拥有何种类型的数据，应该如何处理不同的数据类型？
- en: What is the distribution of the variables?
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量的分布情况如何？
- en: Are there outliers in the data and how can we treat them?
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中是否存在离群值，我们该如何处理它们？
- en: Are any transformations required? For example, some models work better with
    (or require) normally distributed variables, so we might want to use techniques
    such as log transformation.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否需要进行任何变换？例如，一些模型在处理（或要求）服从正态分布的变量时表现更好，因此我们可能需要使用诸如对数变换之类的技术。
- en: Does the distribution vary per group (for example, sex or education level)?
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同群体（例如性别或教育水平）之间的分布是否存在差异？
- en: Do we have cases of missing data? How frequent are these, and in which variables
    do they occur?
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否有缺失数据？这些数据的频率是多少？它们出现在哪些变量中？
- en: Is there a linear relationship (correlation) between some variables?
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某些变量之间是否存在线性关系（相关性）？
- en: Can we create new features using the existing set of variables? An example might
    be deriving an hour/minute from a timestamp, a day of the week from a date, and
    so on.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否可以使用现有的变量集创建新的特征？例如，可能从时间戳中推导出小时/分钟，或者从日期中推导出星期几，等等。
- en: Are there any variables that we can remove as they are not relevant for the
    analysis? An example might be a randomly generated customer identifier.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有一些变量可以删除，因为它们与分析无关？例如，随机生成的客户标识符。
- en: Naturally, this list is non-exhaustive and carrying out the analysis might spark
    more questions than we initially had. EDA is extremely important in all data science
    projects, as it enables analysts to develop an understanding of the data, facilitates
    asking better questions, and makes it easier to pick modeling approaches suitable
    for the type of data being dealt with.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，这个列表并不详尽，进行分析时可能会引发比最初更多的问题。EDA在所有数据科学项目中都极为重要，因为它使分析师能够深入理解数据，有助于提出更好的问题，并且更容易选择适合所处理数据类型的建模方法。
- en: In real-life cases, it makes sense to first carry out a univariate analysis
    (one feature at a time) for all relevant features to get a good understanding
    of them. Then, we can proceed to multivariate analysis, that is, comparing distributions
    per group, correlations, and so on. For brevity, we only show selected analysis
    approaches to selected features, but a deeper analysis is highly encouraged.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际案例中，通常先对所有相关特征进行单变量分析（一次分析一个特征），以便深入理解它们。然后，可以进行多变量分析，即比较每组的分布，相关性等。为了简洁起见，我们这里只展示对某些特征的选定分析方法，但强烈建议进行更深入的分析。
- en: Getting ready
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: We continue with exploring the data we loaded in the previous recipe.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续探索在上一个步骤中加载的数据。
- en: How to do it...
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何进行...
- en: 'Execute the following steps to carry out the EDA of the loan default dataset:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以进行贷款违约数据集的探索性数据分析（EDA）：
- en: 'Import the libraries:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE14]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Get summary statistics of the numeric variables:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数值变量的摘要统计：
- en: '[PRE15]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Running the snippet generates the following summary table:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行该代码片段会生成以下摘要表格：
- en: '![](../Images/B18112_13_02.png)'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_02.png)'
- en: 'Figure 13.2: Summary statistics of the numeric variables'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.2：数值变量的摘要统计
- en: 'Get summary statistics of the categorical variables:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取分类变量的摘要统计：
- en: '[PRE16]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Running the snippet generates the following summary table:'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段生成了以下汇总表：
- en: '![](../Images/B18112_13_03.png)'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_03.png)'
- en: 'Figure 13.3: Summary statistics of the categorical variables'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.3：分类变量的汇总统计
- en: 'Plot the distribution of age and split it by sex:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制年龄分布并按性别划分：
- en: '[PRE17]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Running the snippet generates the following plot:'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段生成了以下图：
- en: '![](../Images/B18112_13_04.png)'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_04.png)'
- en: 'Figure 13.4: The KDE plot of age, grouped by sex'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.4：按性别分组的年龄KDE图
- en: By analyzing the **kernel density estimate** (**KDE**) plot, we can say there
    is not much difference in the shape of the distribution per sex. The female sample
    is slightly younger, on average.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过分析**核密度估计**（**KDE**）图，我们可以看出，每个性别的分布形态差异不大。女性样本的年龄略微偏小。
- en: 'Create a pairplot of selected variables:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建选定变量的对角图：
- en: '[PRE18]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Running the snippet generates the following plot:'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段生成了以下图：
- en: '![](../Images/B18112_13_05.png)'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_05.png)'
- en: 'Figure 13.5: A pairplot with KDE plots on the diagonal and fitted regression
    lines in each scatterplot'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.5：带有KDE图的对角图和每个散点图中的回归线拟合
- en: 'We can make a few observations from the created pairplot:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以从创建的对角图中得出一些观察结果：
- en: The distribution of `previous_payment_sep` is highly skewed—it has a very long
    tail.
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`previous_payment_sep`的分布高度偏斜——它有一个非常长的尾巴。'
- en: Connected to the previous point, we can observe some very extreme values of
    `previous_payment_sep` in the scatterplots.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与前述内容相关，我们可以在散点图中观察到`previous_payment_sep`的极端值。
- en: It is difficult to draw conclusions from the scatterplots, as there are 30,000
    observations on each of them. When plotting such volumes of data, we could use
    transparent markers to better visualize the density of the observation in certain
    areas.
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从散点图中很难得出结论，因为每个散点图中都有30,000个观察值。当绘制如此大量的数据时，我们可以使用透明标记来更好地可视化某些区域的观察密度。
- en: The outliers can have a significant impact on the regression lines.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离群值可能对回归线产生显著影响。
- en: 'Additionally, we can separate the sexes by specifying the `hue` argument:'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，我们可以通过指定`hue`参数来区分性别：
- en: '[PRE19]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Running the snippet generates the following plot:'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段生成了以下图：
- en: '![](../Images/B18112_13_06.png)'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_06.png)'
- en: 'Figure 13.6: The pairplot with separate markers for each sex'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.6：每个性别分别标记的对角图
- en: While we can gain some more insights from the diagonal plots with the split
    per sex, the scatterplots are still quite unreadable due to the sheer volume of
    plotted data.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管通过性别划分后的对角图能提供更多的见解，但由于绘制数据量庞大，散点图仍然相当难以解读。
- en: As a potential solution, we could randomly sample from the entire dataset and
    only plot the selected observations. A possible downside of that approach is that
    we might miss some observations with extreme values (outliers).
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作为潜在解决方案，我们可以从整个数据集中随机抽样，并只绘制选定的观察值。该方法的一个可能缺点是，我们可能会遗漏一些具有极端值（离群值）的观察数据。
- en: 'Analyze the relationship between age and limit balance:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析年龄与信用额度余额之间的关系：
- en: '[PRE20]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Running the snippet generates the following plot:'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段生成了以下图：
- en: '![](../Images/B18112_13_07.png)'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_07.png)'
- en: 'Figure 13.7: A joint plot showing the relationship between age and limit balance,
    grouped by sex'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.7：显示年龄与信用额度余额关系的联合图，按性别分组
- en: A joint plot contains quite a lot of useful information. First of all, we can
    see the relationship between two variables on the scatterplot. Then, we can also
    investigate the distributions of the two variables individually using the KDE
    plots along the axes (we can also plot histograms instead).
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联合图包含了大量有用的信息。首先，我们可以在散点图中看到两个变量之间的关系。接下来，我们还可以使用沿坐标轴的KDE图来分别调查两个变量的分布（我们也可以选择绘制直方图）。
- en: 'Define and run a function for plotting the correlation heatmap:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并运行一个绘制相关性热图的函数：
- en: '[PRE21]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Running the snippet generates the following plot:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段生成了以下图：
- en: '![](../Images/B18112_13_08.png)'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_08.png)'
- en: 'Figure 13.8: Correlation heatmap of the numeric features'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.8：数值特征的相关性热图
- en: We can see that age seems to be uncorrelated to any of the other features.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，年龄似乎与其他特征没有显著的相关性。
- en: 'Analyze the distribution of age in groups using box plots:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用箱型图分析分组后的年龄分布：
- en: '[PRE22]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Running the snippet generates the following plot:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段生成了以下图：
- en: '![](../Images/B18112_13_09.png)'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_09.png)'
- en: 'Figure 13.9: Distribution of age by marital status and sex'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.9：按婚姻状况和性别分组的年龄分布
- en: The distributions seem quite similar within marital groups, with men always
    having a higher median age.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从分布来看，婚姻状况组内似乎相似，男性的中位数年龄总是较高。
- en: 'Plot the distribution of limit balance for each sex and education level:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制每个性别和教育水平的信用额度分布：
- en: '[PRE23]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Running the snippet generates the following plot:'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段会生成以下图表：
- en: '![](../Images/B18112_13_10.png)'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_10.png)'
- en: 'Figure 13.10: Distribution of limit balance by education level and sex'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.10：按教育水平和性别划分的信用额度分布
- en: 'Inspecting the plot reveals a few interesting patterns:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检查图表可以揭示一些有趣的模式：
- en: The largest balance appears in the group with the *Graduate school* level of
    education.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大的余额出现在*研究生*教育水平的组别中。
- en: 'The shape of the distribution is different per education level: the *Graduate
    school* level resembles the *Others* category, while the *High school* level is
    similar to the *University* level.'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个教育水平的分布形状不同：*研究生*水平类似于*其他*类别，而*高中*水平则与*大学*水平相似。
- en: In general, there are few differences between the sexes.
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体来说，性别之间的差异较小。
- en: 'Investigate the distribution of the target variable per sex and education level:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调查按性别和教育水平划分的目标变量分布：
- en: '[PRE24]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Running the snippet generates the following plot:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段会生成以下图表：
- en: '![](../Images/B18112_13_11.png)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_11.png)'
- en: 'Figure 13.11: Distribution of the target variable by sex'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.11：按性别划分的目标变量分布
- en: By analyzing the plot, we can say that the percentage of defaults is higher
    among male customers.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过分析图表，我们可以得出结论：男性客户的违约比例较高。
- en: 'Investigate the percentage of defaults per education level:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调查每个教育水平的违约百分比：
- en: '[PRE25]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Running the snippet generates the following plot:'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码片段会生成以下图表：
- en: '![](../Images/B18112_13_12.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_12.png)'
- en: 'Figure 13.12: Percentage of defaults by education level'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12：按教育水平划分的违约百分比
- en: Relatively speaking, most defaults happen among customers with a high-school
    education, while the fewest defaults happen in the *Others* category.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 相对而言，大多数违约发生在高中教育的客户中，而违约最少的则出现在*其他*类别中。
- en: How it works...
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In the previous recipe, we already explored two DataFrame methods that are
    useful for starting exploratory data analysis: `shape` and `info`. We can use
    them to quickly learn the shape of the dataset (number of rows and columns), what
    data types are used for representing each feature, and so on.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们已经探索了两个在开始探索性数据分析时非常有用的DataFrame方法：`shape`和`info`。我们可以使用它们快速了解数据集的形状（行数和列数）、每个特征的数据类型等。
- en: In this recipe, we are mostly using the `seaborn` library, as it is the go-to
    library when it comes to exploring data. However, we could use alternative plotting
    libraries. The `plot` method of a `pandas` DataFrame is quite powerful and allows
    for quickly visualizing our data. Alternatively, we could use `plotly` (and its
    `plotly.express` module) to create fully interactive data visualizations.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们主要使用了`seaborn`库，因为它是探索数据时最常用的库。然而，我们也可以使用其他绘图库。`pandas` DataFrame的`plot`方法非常强大，可以快速可视化数据。作为替代，我们也可以使用`plotly`（及其`plotly.express`模块）来创建完全交互的数据可视化。
- en: In this recipe, we started the analysis by using a very simple yet powerful
    method of a `pandas` DataFrame—`describe`. It printed summary statistics, such
    as the count, mean, min/max, and quartiles of all the numeric variables in the
    DataFrame. By inspecting these metrics, we could infer the value range of a certain
    feature, or whether the distribution is skewed (by looking at the difference between
    the mean and median). Also, we could easily spot values outside the plausible
    range, for example, a negative or very young/old age.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过使用`pandas` DataFrame中的一种非常简单但强大的方法——`describe`，开始了分析。它打印了所有数值变量的汇总统计信息，如计数、均值、最小值/最大值和四分位数。通过检查这些指标，我们可以推断出某个特征的值范围，或者分布是否偏斜（通过查看均值和中位数的差异）。此外，我们还可以轻松发现不合常理的值，例如负数或过年轻/年老的年龄。
- en: We can include additional percentiles in the `describe` method by passing an
    extra argument, for example, `percentiles=[.99]`. In this case, we added the 99th
    percentile.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过传递额外的参数（例如，`percentiles=[.99]`）在`describe`方法中包含更多的百分位数。在这种情况下，我们添加了第99百分位。
- en: The count metric represents the number of non-null observations, so it is also
    a way to determine which numeric features contain missing values. Another way
    of investigating the presence of missing values is by running `df.isnull().sum()`.
    For more information on missing values, please see the *Identifying and dealing
    with missing values* recipe.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 计数度量表示非空观察值的数量，因此它也是确定哪些数值特征包含缺失值的一种方法。另一种检查缺失值存在的方法是运行`df.isnull().sum()`。有关缺失值的更多信息，请参见*识别和处理缺失值*的配方。
- en: 'In *Step 3*, we added the `include="object"` argument while calling the `describe`
    method to inspect the categorical features separately. The output was different
    from the numeric features: we could see the count, the number of unique categories,
    which one was the most frequent, and how many times it appeared in the dataset.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3步*中，我们在调用`describe`方法时添加了`include="object"`参数，以便单独检查分类特征。输出与数值特征不同：我们可以看到计数、唯一类别的数量、最常见的类别以及它在数据集中出现的次数。
- en: We can use `include="all"`to display the summary metrics for all features—only
    the metrics available for a given data type will be present, while the rest will
    be filled with `NA` values.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`include="all"`来显示所有特征的汇总度量——只有给定数据类型可用的度量会出现，其余则会填充为`NA`值。
- en: In *Step 4*, we showed a way of investigating the distribution of a variable,
    in this case, the age of the customers. To do so, we created a KDE plot. It is
    a method of visualizing the distribution of a variable, very similar to a traditional
    histogram. KDE represents the data using a continuous probability density curve
    in one or more dimensions. One of its advantages over a histogram is that the
    resulting plot is less cluttered and easier to interpret, especially when considering
    multiple distributions at once.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4步*中，我们展示了调查变量分布的一种方法，在这种情况下是顾客的年龄。为此，我们创建了一个KDE图。它是一种可视化变量分布的方法，非常类似于传统的直方图。KDE通过在一个或多个维度中使用连续的概率密度曲线来表示数据。与直方图相比，它的一个优点是生成的图形更加简洁，且更容易解释，特别是在同时考虑多个分布时。
- en: A common source of confusion around the KDE plots is about the units on the
    density axis. In general, the kernel density estimation results in a probability
    distribution. However, the height of the curve at each point gives a density,
    instead of the probability. We can obtain a probability by integrating the density
    across a certain range. The KDE curve is normalized so that the integral over
    all possible values is equal to 1\. This means that the scale of the density axis
    depends on the data values. To take it a step further, we can decide how to normalize
    the density when we are dealing with multiple categories in one plot. If we use
    `common_norm=True`, each density is scaled by the number of observations so that
    the total area under all curves sums to 1\. Otherwise, the density of each category
    is normalized independently.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 关于KDE图，常见的困惑来源于密度轴的单位。一般而言，核密度估计结果是一个概率分布。然而，曲线在每一点的高度给出的是密度，而不是概率。我们可以通过对密度在某一范围内进行积分来获得概率。KDE曲线已被归一化，使得所有可能值的积分总和等于1。这意味着密度轴的尺度取决于数据值。更进一步地，如果我们在一个图中处理多个类别，我们可以决定如何归一化密度。如果我们使用`common_norm=True`，每个密度都会根据观察值的数量进行缩放，使得所有曲线下的总面积之和为1。否则，每个类别的密度会独立归一化。
- en: Together with a histogram, the KDE plot is one of the most popular methods of
    inspecting the distribution of a single feature. To create a histogram, we can
    use the `sns.histplot` function. Alternatively, we can use the `plot` method of
    a `pandas` DataFrame, while specifying `kind="hist"`. We show examples of creating
    histograms in the accompanying Jupyter notebook (available on GitHub).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 与直方图一起，KDE图是检查单一特征分布的最流行方法之一。要创建直方图，我们可以使用`sns.histplot`函数。或者，我们也可以使用`pandas`
    DataFrame的`plot`方法，并指定`kind="hist"`。我们在附带的Jupyter笔记本中展示了创建直方图的示例（可在GitHub上找到）。
- en: An extension of this analysis can be done by using a pairplot. It creates a
    matrix of plots, where the diagonal shows the univariate histograms or KDE plots,
    while the off-diagonal plots are scatterplots of two features. This way, we can
    also try to see if there is a relationship between the two features. To make identifying
    the potential relationships easier, we have also added the regression lines.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用成对图（pairplot），可以扩展此分析。它创建一个图矩阵，其中对角线显示单变量直方图或核密度估计图（KDE），而非对角线的图为两特征的散点图。通过这种方式，我们还可以尝试查看两个特征之间是否存在关系。为了更容易识别潜在的关系，我们还添加了回归线。
- en: In our case, we only plotted three features. That is because with 30,000 observations
    it can take quite some time to render the plot for all numeric columns, not to
    mention losing readability with so many small plots in one matrix. When using
    pairplots, we can also specify the `hue` argument to add a split for a category
    (such as sex, or education level).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们只绘制了三个特征。这是因为对于30,000个观测值，绘制所有数值列的图表可能会耗费相当长的时间，更不用说在一个矩阵中包含如此多小图时会导致图表难以读取。当使用成对图时，我们还可以指定`hue`参数来为某一类别（如性别或教育水平）进行拆分。
- en: We can also zoom into a relationship between two variables using a joint plot
    (`sns.jointplot`). It is a type of plot that combines a scatterplot to analyze
    the bivariate relationship and KDE plots or histograms to analyze the univariate
    distribution. In *Step 6*, we analyzed the relationship between age and limit
    balance.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过联合图（`sns.jointplot`）来放大查看两个变量之间的关系。这是一种结合了散点图和核密度估计图或直方图的图，既可以分析双变量关系，也可以分析单变量分布。在*步骤
    6*中，我们分析了年龄与限额余额之间的关系。
- en: In *Step 7*, we defined a function for plotting a heatmap representing the correlation
    matrix. In the function, we used a couple of operations to mask the upper triangular
    matrix and the diagonal (all diagonal elements of the correlation matrix are equal
    to 1). This way, the output is much easier to interpret. Using the `annot` argument
    of `sns.heatmap`, we could add the underlying numbers to the heatmap. However,
    we should only do so when the number of analyzed features is not too high. Otherwise,
    they will become unreadable.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 7*中，我们定义了一个绘制热图表示相关矩阵的函数。在该函数中，我们使用了一些操作来遮蔽上三角矩阵和对角线（相关矩阵的所有对角元素都为1）。这样，输出结果更容易解读。使用`annot`参数的`sns.heatmap`，我们可以在热图中添加底层数字。然而，当分析的特征数量过多时，我们应该避免这么做，否则数字将变得难以阅读。
- en: To calculate the correlations, we used the `corr` method of a DataFrame, which
    by default calculates the **Pearson’s correlation coefficient**. We did this only
    for numeric features. There are also methods for calculating the correlation of
    categorical features; we mention some of them in the *There’s more…* section.
    Inspecting correlations is crucial, especially when using machine learning algorithms
    that assume linear independence of the features.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算相关性，我们使用了DataFrame的`corr`方法，默认计算**皮尔逊相关系数**。我们只对数值特征进行了此操作。对于分类特征，也有计算相关性的方法；我们在*更多内容...*部分提到了一些方法。检查相关性至关重要，特别是在使用假设特征线性独立的机器学习算法时。
- en: 'In *Step 8*, we used box plots to investigate the distribution of age by marital
    status and sex. A box plot (also called a box-and-whisker plot) presents the distribution
    of data in such a way that facilitates comparisons between levels of a categorical
    variable. A box plot presents the information about the distribution of the data
    using a 5-number summary:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 8*中，我们使用箱型图来研究按婚姻状况和性别划分的年龄分布。箱型图（也叫箱形图）通过一种便于比较分类变量各个层级之间的分布方式呈现数据分布。箱型图通过5个数值摘要展示数据分布信息：
- en: Median (50th percentile)—represented by the horizontal black line within the
    boxes.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中位数（第50百分位数）—由箱体内的水平黑线表示。
- en: '**Interquartile range** (**IQR**)—represented by the box. It spans the range
    between the first quartile (25th percentile) and the third quartile (75th percentile).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**四分位距**（**IQR**）—由箱体表示。它表示第一四分位数（25百分位数）和第三四分位数（75百分位数）之间的范围。'
- en: The whiskers—represented by the lines stretched from the box. The extreme values
    of the whiskers (marked as horizontal lines) are defined as the first quartile
    − 1.5 IQR and the third quartile + 1.5 IQR.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 须须线—由从箱体延伸出的线表示。须须线的极值（标记为水平线）定义为第一四分位数 - 1.5 IQR 和第三四分位数 + 1.5 IQR。
- en: 'We can use the box plots to gather the following insights about our data:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用箱型图从数据中获取以下见解：
- en: The points marked outside of the whiskers can be considered outliers. This method
    is called **Tukey’s fences** andis one of the simplest outlier detection techniques.
    In short, it assumes that observations lying outside of the [Q1 – 1.5 IQR, Q3
    + 1.5 IQR] range are outliers.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记在须外的点可以视为异常值。这种方法被称为**Tukey’s fences**，是最简单的异常值检测技术之一。简而言之，它假设位于[Q1 - 1.5
    IQR, Q3 + 1.5 IQR]范围之外的观测值为异常值。
- en: The potential skewness of the distribution. A right-skewed (positive skewness)
    distribution can be observed when the median is closer to the lower bound of the
    box, and the upper whisker is longer than the lower one. *Vice versa* for the
    left-skewed distributions. *Figure 13.13* illustrates this.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布的潜在偏斜度。当中位数接近箱子的下限，而上须比下须长时，可以观察到右偏（正偏）的分布。*反之*，左偏分布则相反。*图13.13*展示了这一点。
- en: '![](../Images/B18112_13_13.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_13.png)'
- en: 'Figure 13.13: Determining the skewness of distribution using box plots'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.13：使用箱线图确定分布的偏斜度
- en: In *Step 9*, we used violin plots to investigate the distribution of the limit
    balance feature per education level and sex. We created them by using `sns.violinplot`.
    We indicated the education level with the `x` argument. Additionally, we set `hue="sex"`
    and `split=True`. By doing so, each half of the violin represented a different
    sex.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤9*中，我们使用小提琴图来研究限额余额特征在教育水平和性别上的分布。我们通过使用`sns.violinplot`来创建这些图表。我们用`x`参数表示教育水平，并且设置了`hue="sex"`和`split=True`。这样，小提琴的每一半就代表不同的性别。
- en: 'In general, violin plots are very similar to box plots and we can find the
    following information in them:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，小提琴图与箱线图非常相似，我们可以在其中找到以下信息：
- en: The median, represented by a white dot.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中位数，用白色点表示。
- en: The interquartile range, represented as the black bar in the center of a violin.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四分位距，表示为小提琴中央的黑色条形。
- en: The lower and upper adjacent values, represented by the black lines stretched
    from the bar. The lower adjacent value is defined as the first quartile − 1.5
    IQR, while the upper one is defined as the third quartile + 1.5 IQR. Again, we
    can use the adjacent values as a simple outlier detection technique.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下邻值和上邻值，由从条形延伸出的黑线表示。下邻值定义为第一四分位数 - 1.5 IQR，而上邻值定义为第三四分位数 + 1.5 IQR。同样，我们可以使用邻值作为简单的异常值检测技术。
- en: Violin plots are a combination of a box plot and a KDE plot. A definite advantage
    of a violin plot over a box plot is that the former enables us to clearly see
    the shape of the distribution. This is especially useful when dealing with multimodal
    distributions (distributions with multiple peaks), such as the limit balance violin
    in the *Graduate school* education category.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 小提琴图是箱线图和核密度估计图（KDE图）的结合。与箱线图相比，小提琴图的一个明显优点是它能够清晰地展示分布的形状。当处理多峰分布（具有多个峰值的分布）时，尤其有用，比如在*研究生*教育类别中的限额余额小提琴图。
- en: In the last two steps, we investigated the distribution of the target variable
    (default) per sex and education. In the first case, we used `sns.countplot` to
    display the count of occurrences of both possible outcomes for each sex. In the
    second case, we opted for a different approach. We wanted to plot the percentage
    of defaults per education level, as comparing percentages between groups is easier
    than comparing nominal values. To do so, we first grouped by education level,
    selected the variable of interest, calculated the percentages per group (using
    the `value_counts(normalize=True)` method), unstacked (to remove multi-index),
    and generated a plot using the already familiar `plot` method.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后两步中，我们分析了目标变量（违约）在性别和教育水平上的分布。在第一种情况下，我们使用了`sns.countplot`来显示每种性别下两种可能结果的出现次数。在第二种情况下，我们选择了不同的方法。我们想要绘制每个教育水平的违约百分比，因为比较不同组之间的百分比比比较名义值更容易。为此，我们首先按教育水平分组，选择感兴趣的变量，计算每组的百分比（使用`value_counts(normalize=True)`方法），去除多重索引（通过unstack），然后使用已经熟悉的`plot`方法生成图表。
- en: There’s more...
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: In this recipe, we introduced a range of possible approaches to investigate
    the data at hand. However, this requires many lines of code (quite a lot of them
    boilerplate) each time we want to carry out the EDA. Thankfully, there is a Python
    library that simplifies the process. The library is called `pandas_profiling`
    and with a single line of code, it generates a comprehensive summary of the dataset
    in the form of an HTML report.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们介绍了一些可能的方法来调查手头的数据。然而，每次进行探索性数据分析（EDA）时，我们需要编写许多行代码（其中有相当多是模板代码）。幸运的是，有一个
    Python 库简化了这一过程。这个库叫做 `pandas_profiling`，只需一行代码，它就能生成数据集的全面概述，并以 HTML 报告的形式呈现。
- en: 'To create a report, we need to run the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成报告，我们需要运行以下代码：
- en: '[PRE26]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We could also create a profile using the new (added by `pandas_profiling`) `profile_report`
    method of a `pandas` DataFrame.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过 `pandas_profiling` 新添加的 `profile_report` 方法，基于一个 `pandas` DataFrame
    创建个人资料。
- en: 'For practical reasons, we might prefer to save the report as an HTML file and
    inspect it in a browser instead of the Jupyter notebook. We can easily do so using
    the following snippet:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 出于实际考虑，我们可能更倾向于将报告保存为 HTML 文件，并在浏览器中查看，而不是在 Jupyter notebook 中查看。我们可以使用以下代码片段轻松实现：
- en: '[PRE27]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The report is very exhaustive and contains a lot of useful information. Please
    see the following figure for an example.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 报告非常详尽，包含了许多有用的信息。请参见以下图例作为示例。
- en: '![](../Images/B18112_13_14.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_14.png)'
- en: 'Figure 13.14: Example of a deep-dive into the limit balance feature'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.14：深入分析限额平衡特征的示例
- en: 'For brevity’s sake, we will only discuss selected elements of the report:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们将只讨论报告中的选定部分：
- en: An overview giving information about the size of the DataFrame (number of features/rows,
    missing values, duplicated rows, memory size, breakdown per data type).
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概述提供了有关 DataFrame 的信息（特征/行的数量、缺失值、重复行、内存大小、按数据类型的划分）。
- en: Alerts warning us about potential issues with our data, including a high percentage
    of duplicated rows, highly correlated (and potentially redundant) features, features
    that have a high percentage of zero values, highly skewed features, etc.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 警告我们关于数据中潜在问题的警报，包括重复行的高比例、高度相关（且可能冗余）的特征、具有高比例零值的特征、高度偏斜的特征等。
- en: 'Different measures of correlation: Spearman’s ![](../Images/B18112_13_008.png),
    Pearson’s r, Kendall’s ![](../Images/B18112_13_009.png), Cramér’s V, and Phik
    (![](../Images/B18112_13_010.png)). The last one is especially interesting, as
    it is a recently developed correlation coefficient that works consistently between
    categorical, ordinal, and interval variables. On top of that, it captures non-linear
    dependencies. Please see the *See also* section for a reference paper describing
    the metric.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的相关性度量：Spearman’s ![](../Images/B18112_13_008.png)、Pearson’s r、Kendall’s ![](../Images/B18112_13_009.png)、Cramér’s
    V 和 Phik (![](../Images/B18112_13_010.png))。最后一个特别有趣，因为它是一个最近开发的相关系数，可以在分类、顺序和区间变量之间始终如一地工作。此外，它还能够捕捉非线性依赖性。有关该度量的参考论文，请参见
    *See also* 部分。
- en: Detailed analysis of missing values.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 详细分析缺失值。
- en: Detailed univariate analysis of each feature (more details are available by
    clicking *Toggle details* in the report).
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个特征的详细单变量分析（更多细节可以通过点击报告中的 *Toggle details* 查看）。
- en: '`pandas-profiling` is the most popular auto-EDA tool in Python’s vast ecosystem
    of libraries. However, it is definitely not the only one. You can also investigate
    the following:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas-profiling` 是 Python 库生态系统中最流行的自动化 EDA 工具，但它绝对不是唯一的。你还可以使用以下工具进行调查：'
- en: '`sweetviz`—[https://github.com/fbdesignpro/sweetviz](https://github.com/fbdesignpro/sweetviz)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sweetviz`—[https://github.com/fbdesignpro/sweetviz](https://github.com/fbdesignpro/sweetviz)'
- en: '`autoviz`—[https://github.com/AutoViML/AutoViz](https://github.com/AutoViML/AutoViz)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`autoviz`—[https://github.com/AutoViML/AutoViz](https://github.com/AutoViML/AutoViz)'
- en: '`dtale`—[https://github.com/man-group/dtale](https://github.com/man-group/dtale)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtale`—[https://github.com/man-group/dtale](https://github.com/man-group/dtale)'
- en: '`dataprep`—[https://github.com/sfu-db/dataprep](https://github.com/sfu-db/dataprep)'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataprep`—[https://github.com/sfu-db/dataprep](https://github.com/sfu-db/dataprep)'
- en: '`lux`—[https://github.com/lux-org/lux](https://github.com/lux-org/lux)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lux`—[https://github.com/lux-org/lux](https://github.com/lux-org/lux)'
- en: Each one of them approaches EDA a bit differently. Hence, it is best to explore
    them all and pick the tool that works best for your needs.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 每个工具对 EDA 的处理方式有所不同。因此，最好是探索它们所有，并选择最适合你需求的工具。
- en: See also
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For more information about Phik (![](../Images/B18112_13_010.png)), please
    see the following paper:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Phik 的更多信息（![](../Images/B18112_13_010.png)），请参阅以下论文：
- en: Baak, M., Koopman, R., Snoek, H., & Klous, S. (2020). “A new correlation coefficient
    between categorical, ordinal and interval variables with Pearson characteristics.”
    *Computational Statistics & Data Analysis*, *152*, 107043\. [https://doi.org/10.1016/j.csda.2020.107043](https://doi.org/10.1016/j.csda.2020.107043).
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baak, M., Koopman, R., Snoek, H., & Klous, S. (2020). “一种新的相关系数，适用于具有皮尔逊特征的分类、序数和区间变量。”
    *计算统计与数据分析*，*152*，107043。 [https://doi.org/10.1016/j.csda.2020.107043](https://doi.org/10.1016/j.csda.2020.107043)。
- en: Splitting data into training and test sets
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集
- en: 'Having completed the EDA, the next step is to split the dataset into training
    and test sets. The idea is to have two separate datasets:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 完成EDA后，下一步是将数据集分为训练集和测试集。这个思路是将数据分为两个独立的数据集：
- en: Training set—on this part of the data, we train a machine learning model
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集——在这部分数据上，我们训练机器学习模型
- en: Test set—this part of the data was not seen by the model during training and
    is used to evaluate its performance
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试集——这部分数据在训练过程中没有被模型看到，用于评估模型的性能
- en: By splitting the data this way, we want to prevent overfitting. **Overfitting**
    is a phenomenon that occurs when a model finds too many patterns in data used
    for training and performs well only on that particular data. In other words, it
    fails to generalize to unseen data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式拆分数据，我们希望防止过拟合。**过拟合**是指当模型在训练数据上找到过多模式，并且仅在这些数据上表现良好时发生的现象。换句话说，它无法泛化到看不见的数据。
- en: This is a very important step in the analysis, as doing it incorrectly can introduce
    bias, for example, in the form of **data leakage**. Data leakage can occur when,
    during the training phase, a model observes information to which it should not
    have access. We follow up with an example. A common scenario is that of imputing
    missing values with the feature’s average. If we had done this before splitting
    the data, we would have also used data from the test set to calculate the average,
    introducing leakage. That is why the proper order would be to split the data into
    training and test sets first and then carry out the imputation, using the data
    observed in the training set. The same goes for setting up rules for identifying
    outliers.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这是分析中的一个非常重要的步骤，因为如果操作不当，可能会引入偏差，例如**数据泄漏**。数据泄漏是指在训练阶段，模型观察到它本不应该接触到的信息。我们接下来举个例子。一个常见的情况是使用特征的均值填补缺失值。如果我们在拆分数据之前就进行了填补，测试集中的数据也会被用来计算均值，从而引入泄漏。这就是为什么正确的顺序应该是先将数据拆分为训练集和测试集，然后进行填补，使用在训练集上观察到的数据。同样的规则也适用于识别异常值的设置。
- en: Additionally, splitting the data ensures consistency, as unseen data in the
    future (in our case, new customers that will be scored by the model) will be treated
    in the same way as the data in the test set.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，拆分数据确保了一致性，因为未来看不见的数据（在我们的案例中，是模型将要评分的新客户）将与测试集中的数据以相同的方式处理。
- en: How to do it...
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Execute the following steps to split the dataset into training and test sets:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤将数据集分为训练集和测试集：
- en: 'Import the libraries:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE28]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Separate the target from the features:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目标与特征分开：
- en: '[PRE29]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Split the data into training and test sets:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集：
- en: '[PRE30]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Split the data into training and test sets without shuffling:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在不打乱顺序的情况下将数据分为训练集和测试集：
- en: '[PRE31]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Split the data into training and test sets with stratification:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分层法将数据分为训练集和测试集：
- en: '[PRE32]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Verify that the ratio of the target is preserved:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证目标比例是否保持一致：
- en: '[PRE33]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Running the snippet generates the following output:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码片段会生成以下输出：
- en: '[PRE34]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In both sets, the percentage of payment defaults is around 22.12%.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个数据集中，支付违约的比例大约为22.12%。
- en: How it works...
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: After importing the libraries, we separated the target from the features using
    the `pop` method of a `pandas` DataFrame.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入库之后，我们使用`pandas` DataFrame的`pop`方法将目标与特征分开。
- en: In *Step 3*, we showed how to do the most basic split. We passed `X` and `y`
    objects to the `train_test_split` function. Additionally, we specified the size
    of the test set, as a fraction of all observations. For reproducibility, we also
    specified the random state. We had to assign the output of the function to four
    new objects.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤3*中，我们展示了如何进行最基本的拆分。我们将`X`和`y`对象传递给了`train_test_split`函数。此外，我们还指定了测试集的大小，以所有观察值的一个比例表示。为了可重复性，我们还指定了随机状态。我们必须将函数的输出分配给四个新对象。
- en: In *Step 4*, we took a different approach. By specifying `test_size=0.2` and
    `shuffle=False`, we assigned the first 80% of the data to the training set and
    the remaining 20% to the test set. This might come in handy when we want to preserve
    the order in which the observations are becoming available.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们采用了不同的方法。通过指定`test_size=0.2`和`shuffle=False`，我们将数据的前80%分配给训练集，剩下的20%分配给测试集。当我们希望保持观察数据的顺序时，这种方法很有用。
- en: In *Step 5*, we also specified the stratification argument by passing the target
    variable (`stratify=y`). Splitting the data with stratification means that both
    the training and test sets will have an almost identical distribution of the specified
    variable. This parameter is very important when dealing with imbalanced data,
    such as in cases of fraud detection. If 99% of data is normal and only 1% covers
    fraudulent cases, a random split can result in the training set not having any
    fraudulent cases. That is why when dealing with imbalanced data, it is crucial
    to split it correctly.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们还通过传递目标变量（`stratify=y`）指定了分层划分的参数。使用分层划分数据意味着训练集和测试集将具有几乎相同的指定变量分布。这个参数在处理不平衡数据时非常重要，例如欺诈检测的情况。如果99%的数据是正常的，只有1%的数据是欺诈案件，随机划分可能导致训练集中没有欺诈案例。因此，在处理不平衡数据时，正确划分数据至关重要。
- en: In the very last step, we verified if the stratified train/test split resulted
    in the same ratio of defaults in both datasets. To verify that, we used the `value_counts`
    method of a `pandas` DataFrame.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们验证了分层的训练/测试划分是否在两个数据集中产生了相同的违约比例。为此，我们使用了`pandas` DataFrame的`value_counts`方法。
- en: In the rest of the chapter, we will use the data obtained from the stratified
    split.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将使用从分层划分中获得的数据。
- en: There’s more...
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'It is also common to split data into three sets: training, validation, and
    test. The **validation set** is used for frequent evaluation and tuning of the
    model’s hyperparameters. Suppose we want to train a decision tree classifier and
    find the optimal value of the `max_depth` hyperparameter, which decides the maximum
    depth of the tree.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据划分为三个数据集也是常见做法：训练集、验证集和测试集。**验证集**用于频繁评估和调整模型的超参数。假设我们想训练一个决策树分类器，并找到`max_depth`超参数的最优值，该超参数决定树的最大深度。
- en: To do so, we can train the model multiple times using the training set, each
    time with a different value of the hyperparameter. Then, we can evaluate the performance
    of all these models, using the validation set. We pick the best model of those,
    and then ultimately evaluate its performance on the test set.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以多次使用训练集训练模型，每次使用不同的超参数值。然后，我们可以使用验证集评估所有这些模型的表现。我们选择其中表现最好的模型，并最终在测试集上评估其性能。
- en: 'In the following snippet, we illustrate a possible way of creating a train-validation-test
    split, using the same `train_test_split` function:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，我们演示了使用相同的`train_test_split`函数创建训练集-验证集-测试集划分的一种可能方法：
- en: '[PRE35]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We basically ran `train_test_split` twice. What is important is that we had
    to adjust the sizes of the `test_size` input in such a way that the initially
    defined proportions (70-10-20) were preserved.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上执行了两次`train_test_split`。重要的是，我们必须调整`test_size`输入的大小，以确保最初定义的比例（70-10-20）得以保持。
- en: 'We also verify that everything went according to plan: that the size of the
    datasets corresponds to the intended split and that the ratio of defaults is the
    same in each set. We do so using the following snippet:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还验证了所有操作是否按计划进行：数据集的大小是否与预定的划分一致，且每个数据集中的违约比例是否相同。我们使用以下代码片段来完成验证：
- en: '[PRE36]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Executing the snippet generates the following output:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码片段将生成以下输出：
- en: '[PRE37]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We have indeed verified that the original dataset was split with the intended
    70-10-20 ratio and that the distribution of defaults (target variable) was preserved
    due to stratification. Sometimes, we do not have enough data to split it into
    three sets, either because we do not have that many observations in general or
    because the data can be highly imbalanced, and we would remove valuable training
    samples from the training set. That is why practitioners often use a method called
    cross-validation, which we describe in the *Tuning hyperparameters using grid
    search* *and cross-validation* recipe.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经验证原始数据集确实按预期的 70-10-20 比例进行了拆分，并且由于分层，违约（目标变量）的分布得以保持。有时，我们没有足够的数据将其拆分成三组，要么是因为我们总共有的数据样本不够，要么是因为数据高度不平衡，导致我们会从训练集中移除有价值的训练样本。因此，实践中经常使用一种叫做交叉验证的方法，具体内容请参见
    *使用网格搜索和交叉验证调整超参数* 配方。
- en: Identifying and dealing with missing values
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别和处理缺失值
- en: 'In most real-life cases, we do not work with clean, complete data. One of the
    potential problems we are bound to encounter is that of missing values. We can
    categorize missing values by the reason they occur:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实际情况中，我们并不处理干净、完整的数据。我们可能会遇到的一个潜在问题就是缺失值。我们可以根据缺失值发生的原因对其进行分类：
- en: '**Missing completely at random** (**MCAR**)—The reason for the missing data
    is unrelated to the rest of the data. An example could be a respondent accidentally
    missing a question in a survey.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全随机缺失**（**MCAR**）——缺失数据的原因与其他数据无关。一个例子可能是受访者在调查中不小心漏掉了一个问题。'
- en: '**Missing at random** (**MAR**)—The missingness of the data can be inferred
    from data in another column(s). For example, a missing response to a certain survey
    question can to some extent be determined conditionally by other factors such
    as sex, age, lifestyle, and so on.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机缺失**（**MAR**）——缺失数据的原因可以从另一列（或多列）数据中推断出来。例如，某个调查问题的缺失回答在某种程度上可以由性别、年龄、生活方式等其他因素条件性地确定。'
- en: '**Missing not at random** (**MNAR**)—When there is some underlying reason for
    the missing values. For example, people with very high incomes tend to be hesitant
    about revealing it.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非随机缺失**（**MNAR**）——缺失值背后存在某种潜在原因。例如，收入非常高的人往往不愿透露收入。'
- en: '**Structurally missing data**—Often a subset of MNAR, the data is missing because
    of a logical reason. For example, when a variable representing the age of a spouse
    is missing, we can infer that a given person has no spouse.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构性缺失数据**——通常是 MNAR 的一个子集，数据缺失是由于某种逻辑原因。例如，当一个表示配偶年龄的变量缺失时，我们可以推测该人没有配偶。'
- en: Some machine learning algorithms can account for missing data, for example,
    decision trees can treat missing values as a separate and unique category. However,
    many algorithms either cannot do so or their popular implementations (such as
    the ones in `scikit-learn`) do not incorporate this functionality.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习算法可以处理缺失数据，例如，决策树可以将缺失值视为一个独立且独特的类别。然而，许多算法要么无法做到这一点，要么它们的流行实现（如 `scikit-learn`）并未包含此功能。
- en: We should only impute features, not the target variable!
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该只对特征进行填充，而不是目标变量！
- en: 'Some popular solutions to handling missing values include:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的处理缺失值的解决方案包括：
- en: Drop observations with one or more missing values—while this is the easiest
    approach, it is not always a good one, especially in the case of small datasets.
    We should also be aware of the fact that even if there is only a small fraction
    of missing values per feature, they do not necessarily occur for the same observations
    (rows), so the actual number of rows we might need to remove can be much higher.
    Additionally, in the case of data missing not at random, removing such observations
    from the analysis can introduce bias into the results.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除包含一个或多个缺失值的观测值——虽然这是最简单的方法，但并不总是最佳选择，特别是在数据集较小的情况下。我们还需要注意，即使每个特征中只有很小一部分缺失值，它们也不一定出现在相同的观测（行）中，因此我们可能需要删除的行数会远高于预期。此外，在数据缺失并非随机的情况下，删除这些观测可能会引入偏差。
- en: We can opt to drop the entire column (feature) if it is mostly populated with
    missing values. However, we need to be cautious as this can already be an informative
    signal for our model.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果某一列（特征）大部分值都缺失，我们可以选择删除整列。然而，我们需要小心，因为这可能已经是我们模型的一个有信息的信号。
- en: Replace the missing values with a value far outside the possible range, so that
    algorithms such as decision trees can treat it as a special value, indicating
    missing data.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用远远超出可能范围的值来替换缺失值，这样像决策树这样的算法可以将其视为特殊值，表示缺失数据。
- en: In the case of dealing with time series, we can use forward-filling (take the
    last-known observation before the missing one), backward-filling (take the first-known
    observation after the missing one), or interpolation (linear or more advanced).
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理时间序列时，我们可以使用前向填充（取缺失值之前的最后一个已知观测值）、后向填充（取缺失值之后的第一个已知观测值）或插值法（线性或更高级的插值）。
- en: '**Hot-deck imputation**—in this simple algorithm, we first select one or more
    of the other features correlated with the one containing missing values. Then,
    we sort the rows of the dataset by these selected features. Lastly, we iterate
    over the rows from top to bottom and replace each missing value with the previous
    non-missing value in the same feature.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**热备填充法**——在这个简单的算法中，我们首先选择一个或多个与包含缺失值的特征相关的其他特征。然后，我们按这些选定特征对数据集的行进行排序。最后，我们从上到下遍历行，将每个缺失值替换为同一特征中前一个非缺失的值。'
- en: Replace the missing values with an aggregate metric—for continuous data, we
    can use the mean (when there are no clear outliers in the data) or median (when
    there are outliers). In the case of categorical variables, we can use mode (the
    most common value in the set). Potential disadvantages of mean/median imputation
    include the reduction of variance in the dataset and distorting the correlations
    between the imputed features and the rest of the dataset.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用聚合指标替换缺失值——对于连续数据，我们可以使用均值（当数据中没有明显的异常值时）或中位数（当数据中存在异常值时）。对于分类变量，我们可以使用众数（集合中最常见的值）。均值/中位数填充的潜在缺点包括减少数据集的方差并扭曲填充特征与数据集其余部分之间的相关性。
- en: Replace the missing values with aggregate metrics calculated per group—for example,
    when dealing with body-related metrics, we can calculate the mean or median per
    sex, to more accurately replace the missing data.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用按组计算的聚合指标替换缺失值——例如，在处理与身体相关的指标时，我们可以按性别计算均值或中位数，以更准确地替代缺失数据。
- en: ML-based approaches—we can treat the considered feature as a target, and use
    complete cases to train a model and predict values for the missing observations.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于机器学习的方法——我们可以将考虑的特征作为目标，使用完整的数据训练一个模型并预测缺失观测值的值。
- en: In general, exploring the missing values is part of the EDA. We briefly touched
    upon it when analyzing the report generated with `pandas_profiling`. But we deliberately
    left it without much coverage until now, as it is crucial to carry out any kind
    of missing value imputation after the train/test split. Otherwise, we cause data
    leakage.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，探索缺失值是探索性数据分析（EDA）的一部分。在分析使用`pandas_profiling`生成的报告时，我们简要提到了这一点。但我们故意在现在之前没有详细讨论，因为在训练/测试集拆分后进行任何类型的缺失值填充非常关键。否则，我们会导致数据泄漏。
- en: In this recipe, we show how to identify the missing values in our data and how
    to impute them.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们展示了如何识别数据中的缺失值以及如何填补它们。
- en: Getting ready
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we assume that we have the outputs of the stratified train/test
    split from the previous recipe, *Splitting data into training and test sets*.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个方案，我们假设已经有了前一个方案“*将数据拆分为训练集和测试集*”中的分层训练/测试集拆分输出。
- en: How to do it...
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Execute the following steps to investigate and deal with missing values in
    the dataset:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以调查并处理数据集中的缺失值：
- en: 'Import the libraries:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE38]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Inspect the information about the DataFrame:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查DataFrame的信息：
- en: '[PRE39]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Executing the snippet generates the following summary (abbreviated):'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码段会生成以下摘要（缩略版）：
- en: '[PRE40]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Our dataset has more columns, however, the missing values are only present in
    the 4 columns visible in the summary. For brevity, we have not included the rest
    of the output.
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的数据集有更多的列，但缺失值仅存在于摘要中可见的四列中。为了简洁起见，我们没有包括其余的输出。
- en: 'Visualize the nullity of the DataFrame:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化DataFrame的空值情况：
- en: '[PRE41]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Running the line of code results in the following plot:'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行这行代码会生成以下图表：
- en: '![A picture containing chart  Description automatically generated](../Images/B18112_13_15.png)'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![一张包含图表的图片，自动生成的描述](../Images/B18112_13_15.png)'
- en: 'Figure 13.15: The nullity matrix plot of the loan default dataset'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.15：贷款违约数据集的空值矩阵图
- en: The white bars visible in the columns represent missing values. We should keep
    in mind that when working with large datasets with only a few missing values,
    those white bars might be quite difficult to spot.
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在列中可见的白色条形代表缺失值。我们应该记住，在处理大数据集且仅有少量缺失值时，这些白色条形可能相当难以察觉。
- en: The line on the right side of the plot describes the shape of data completeness.
    The two numbers indicate the maximum and minimum nullity in the dataset. When
    an observation has no missing values, the line will be at the maximum right position
    and have a value equal to the number of columns in the dataset (23 in this case).
    As the number of missing values starts to increase within an observation, the
    line moves towards the left. The nullity value of 21 indicates that there is a
    row with 2 missing values in it, as the maximum value for this dataset is 23 (the
    number of columns).
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图表右侧的线条描述了数据完整性的形状。这两组数字表示数据集中的最大和最小空值数量。当一个观察值没有缺失值时，线条将位于最右边的位置，并且值等于数据集中的列数（此例中为23）。随着缺失值数量在一个观察值中开始增加，线条会向左移动。空值为21表示数据集中有一行包含2个缺失值，因为该数据集的最大值是23（列数）。
- en: 'Define columns with missing values per data type:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按数据类型定义包含缺失值的列：
- en: '[PRE42]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Impute numerical features:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填充数值特征：
- en: '[PRE43]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Impute categorical features:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填充类别特征：
- en: '[PRE44]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We can verify that neither the training nor test sets contain missing values
    using the `info` method.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`info`方法验证训练集和测试集都不包含缺失值。
- en: How it works...
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 1*, we imported the required libraries. Then, we used the `info` method
    of a `pandas` DataFrame to view information about the columns, such as their type
    and the number of non-null observations. The difference between the total number
    of observations and the non-null ones corresponds to the number of missing observations.
    Another way of inspecting the number of missing values per column is to run `X.isnull().sum()`.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 1*中，我们导入了所需的库。然后，我们使用`pandas` DataFrame的`info`方法查看列的信息，如其类型和非空观测值的数量。总观测值数与非空观测值数的差值对应缺失观测值的数量。检查每列缺失值数量的另一种方式是运行`X.isnull().sum()`。
- en: Instead of imputing, we could also drop the observations (or even columns) containing
    missing values. To drop all rows containing any missing value, we could use `X_train.dropna(how="any",
    inplace=True)`. In our sample case, the number of missing values is not large,
    however, in a real-life dataset it can be considerable or the dataset might be
    too small for the analysts to be able to remove observations. Alternatively, we
    could also specify the `thresh` argument of the `dropna` method to indicate in
    how many columns an observation (row) needs to have missing values in order to
    be dropped from the dataset.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 除了填充，我们也可以删除包含缺失值的观测值（甚至是列）。要删除所有包含任何缺失值的行，我们可以使用`X_train.dropna(how="any",
    inplace=True)`。在我们的示例中，缺失值的数量不大，但在实际数据集中，缺失值可能会很多，或者数据集太小，分析人员无法删除观测值。或者，我们还可以指定`dropna`方法的`thresh`参数，指明一个观测值（行）需要在多少列中缺失值才会被删除。
- en: In *Step 3*, we visualized the nullity of the DataFrame, with the help of the
    `missingno` library.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 3*中，我们利用`missingno`库可视化了DataFrame的空值情况。
- en: In *Step 4*, we defined lists containing features we wanted to impute, one list
    per data type. The reason for this is the fact that numeric features are imputed
    using different strategies than categorical features. For basic imputation, we
    used the `SimpleImputer` class from `scikit-learn`.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们定义了包含我们希望填充的特征的列表，每种数据类型一个列表。这样做的原因是，数值特征的填充策略与类别特征的填充策略不同。对于基本的填充，我们使用了来自`scikit-learn`的`SimpleImputer`类。
- en: In *Step 5*, we iterated over the numerical features (in this case, only the
    age feature), and used the median to replace the missing values. Inside the loop,
    we defined the imputer object with the correct strategy (`"``median"`), fitted
    it to the given column of the training data, and transformed both the training
    and test data. This way, the median was estimated by using only the training data,
    preventing potential data leakage.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们遍历了数值特征（在此案例中仅为年龄特征），并使用中位数来替换缺失值。在循环内，我们定义了具有正确策略（`"median"`）的填充对象，将其拟合到训练数据的指定列，并对训练数据和测试数据进行了转换。这样，中位数的估算仅使用了训练数据，从而防止了潜在的数据泄露。
- en: In this recipe, we used `scikit-learn` to deal with the imputation of missing
    values. However, we can also do this manually. To do so, for each column with
    any missing values (either in the training or test set), we need to calculate
    the given statistic (mean/median/mode) using the training set, for example, `age_median
    = X_train.age.median()`. Afterward, we need to use this median to fill in the
    missing values for the age column (in both the training and test sets) using the
    `fillna` method. We show how to do it in the Jupyter notebook available in the
    book’s GitHub repository.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用`scikit-learn`处理缺失值的填充方法。然而，我们也可以手动处理。为此，对于每个包含缺失值的列（无论是在训练集还是测试集），我们需要使用训练集计算给定的统计量（均值/中位数/众数），例如，`age_median
    = X_train.age.median()`。然后，我们需要使用这个中位数来填充年龄列中的缺失值（在训练集和测试集中都使用`fillna`方法）。我们将在书籍的GitHub仓库中的Jupyter笔记本里展示如何操作。
- en: '*Step 6* is analogous to *Step 5*, where we used the same approach to iterate
    over categorical columns. The difference lies in the selected strategy—we used
    the most frequent value (`"most_frequent"`) in the given column. This strategy
    can be used for both categorical and numerical features. In the latter case, it
    corresponds to the mode.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤6*与*步骤5*类似，都是使用相同的方法遍历分类列。不同之处在于所选的策略——我们使用了给定列中最频繁的值（`"most_frequent"`）。该策略适用于分类特征和数值特征。在后者情况下，它对应的是众数。'
- en: There’s more...
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: There are a few more things worth mentioning when covering handling missing
    values.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理缺失值时，还有一些值得注意的事项。
- en: More visualizations available in the missingno library
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在missingno库中有更多的可视化工具
- en: 'In this recipe, we have already covered the nullity matrix representation of
    missing values in a dataset. However, the `missingno` library offers a few more
    helpful visualizations:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经覆盖了数据集中缺失值的空值矩阵表示。然而，`missingno`库提供了更多有用的可视化工具：
- en: '`msno.bar`—generates a bar chart representing the nullity in each column. Might
    be easier to quickly interpret than the nullity matrix.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`msno.bar`——生成一个条形图，表示每一列的空值情况。这可能比空值矩阵更容易快速解释。'
- en: '`msno.heatmap`—visualizes the nullity correlation, that is, how strongly the
    presence/absence of one feature impacts the presence of another. The interpretation
    of the nullity correlation is very similar to the standard Pearson’s correlation
    coefficient. It ranges from -1 (when one feature occurs, then the other one certainly
    does not) through 0 (features appearing or not appearing have no effect on each
    other) to 1 (if one feature occurs, then the other one certainly does too).'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`msno.heatmap`——可视化空值相关性，也就是一个特征的存在/缺失如何影响另一个特征的存在。空值相关性的解释与标准的皮尔逊相关系数非常相似。它的取值范围从-1（当一个特征出现时，另一个特征肯定不出现）到0（特征的出现或缺失彼此之间没有任何影响），再到1（如果一个特征出现，则另一个特征也一定会出现）。'
- en: '`msno.dendrogram`—allows us to better understand the correlations between variable
    completion. Under the hood, it uses hierarchical clustering to bin features against
    one another by their nullity correlation.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`msno.dendrogram`——帮助我们更好地理解变量之间的缺失相关性。在底层，它使用层次聚类方法，通过空值相关性将特征进行分箱。'
- en: '![](../Images/B18112_13_16.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_16.png)'
- en: 'Figure 13.16: Example of the nullity dendrogram'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.16：空值树状图示例
- en: To interpret the figure, we need to analyze it from a top-down perspective.
    First, we should look at cluster leaves, which are linked at a distance of zero.
    Those fully predict each other’s presence, that is, one feature might always be
    missing when the other is present, or they might always both be present or missing,
    and so on. Cluster leaves with a split close to zero predict each other very well.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释该图，我们需要从上到下分析它。首先，我们应该查看聚类叶节点，它们在距离为零时被连接在一起。这些特征可以完全预测彼此的存在，也就是说，当一个特征存在时，另一个特征可能总是缺失，或者它们可能总是同时存在或同时缺失，依此类推。距离接近零的聚类叶节点能够很好地预测彼此。
- en: In our case, the dendrogram links together the features that are present in
    every observation. We know this for certain, as we have introduced the missing
    observations by design in only four features.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，树状图将每个观测值中都存在的特征联系在一起。我们对此非常确定，因为我们设计上仅在四个特征中引入了缺失值。
- en: ML-based approaches to imputing missing values
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于机器学习的缺失值填充方法
- en: In this recipe, we mentioned how to impute missing values. Approaches such as
    replacing the missing values with one large value or the mean/median/mode are
    called **single imputation approaches**, as they replace missing values with one
    specific value. On the other hand, there are also **multiple imputation approaches**,
    and one of those is **Multiple Imputation by Chained Equations** (**MICE**).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们提到过如何填补缺失值。像用一个大值或均值/中值/众数替换缺失值的方法被称为**单次填补方法**，因为它们用一个特定的值来替代缺失值。另一方面，还有**多次填补方法**，其中之一是**链式方程的多重填补**（**MICE**）。
- en: In short, the algorithm runs multiple regression models, and each missing value
    is determined conditionally on the basis of the non-missing data points. A potential
    benefit of using an ML-based approach to imputation is the reduction of bias introduced
    by single imputation. The MICE algorithm is available in `scikit-learn` under
    the name of `IterativeImputer`.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，该算法运行多个回归模型，每个缺失值是基于非缺失数据点的条件值来确定的。使用基于机器学习的方法进行填补的潜在好处是减少了单次填补方法带来的偏差。MICE算法可以在`scikit-learn`中找到，命名为`IterativeImputer`。
- en: Alternatively, we could use the **nearest neighbors imputation** (implemented
    in `scikit-learn`'s `KNNImputer`). The underlying assumption of the KNN imputation
    is that a missing value can be approximated by the values of the same feature
    coming from the observations that are closest to it. The closeness to the other
    observations is determined using other features and some form of a distance metric,
    for example, the Euclidean distance.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以使用**最近邻填补**（在`scikit-learn`的`KNNImputer`中实现）。KNN填补的基本假设是，缺失的值可以通过来自最接近观测值的同一特征的其他观测值来近似。观测值之间的接近度是通过其他特征和某种距离度量来确定的，例如欧几里得距离。
- en: 'As the algorithm uses KNN, it comes with some of its drawbacks:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该算法使用KNN，它也有一些缺点：
- en: Requires tuning of the *k* hyperparameter for best performance
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要调节超参数*k*以获得最佳性能
- en: We need to scale the data and preprocess categorical features
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要对数据进行标准化，并预处理类别特征
- en: We need to pick an appropriate distance metric (especially in cases when we
    have a mix of categorical and numerical features)
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要选择一个合适的距离度量（特别是在我们有类别和数值特征混合的情况下）
- en: The algorithm is sensitive to outliers and noise in data
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法对异常值和数据中的噪声较为敏感
- en: Can be computationally expensive as it requires calculating the distances between
    every pair of observations
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于需要计算每一对观测值之间的距离，因此计算开销可能较大
- en: Another of the available ML-based algorithms is called **MissForest** (available
    in the `missingpy` library). Without going into too much detail, the algorithm
    starts by filling in the missing values with the median or mode imputation. Then,
    it trains a Random Forest model to predict the feature that is missing using the
    other known features. The model is trained using the observations for which we
    know the values of the target (so the ones that were not imputed in the first
    step) and then makes predictions for the observations with the missing feature.
    In the next step, the initial median/mode prediction is replaced with the one
    coming from the RF model. The process of looping through the missing data points
    is repeated several times, and each iteration tries to improve upon the previous
    one. The algorithm stops when some stopping criterion is met or we exhaust the
    allowed number of iterations.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可用的基于机器学习的算法叫做**MissForest**（可在`missingpy`库中找到）。简而言之，该算法首先用中值或众数填补缺失值。然后，使用随机森林模型来预测缺失的特征，模型通过其他已知特征训练得到。该模型使用我们知道目标值的观测数据进行训练（即在第一步中未填补的观测数据），然后对缺失特征的观测数据进行预测。在下一步中，初始的中值/众数预测将被来自随机森林模型的预测值替代。这个过程会循环多次，每次迭代都试图改进前一次的结果。当满足某个停止准则或耗尽允许的迭代次数时，算法停止。
- en: 'Advantages of MissForest:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: MissForest的优点：
- en: Can handle missing values in both numeric and categorical features
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理数值型和类别型特征中的缺失值
- en: Does not require data preprocessing (such as scaling)
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要数据预处理（如标准化）
- en: Robust to noisy data, as Random Forest makes little to no use of uninformative
    features
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对噪声数据具有鲁棒性，因为随机森林几乎不使用无信息特征
- en: Non-parametric—it does not make assumptions about the relationship between the
    features (MICE assumes linearity)
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非参数化——它不对特征之间的关系做任何假设（而MICE假设线性关系）
- en: Can leverage non-linear and interaction effects between features to improve
    imputation performance
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以利用特征之间的非线性和交互效应来提高插补性能。
- en: 'Disadvantages of MissForest:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: MissForest的缺点：
- en: Imputation time increases with the number of observations, features, and the
    number of features containing missing values
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插补时间随观测值数量、特征数量以及包含缺失值的特征数量的增加而增加。
- en: Similar to Random Forest, not very easy to interpret
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与随机森林类似，解释起来不太容易。
- en: It is an algorithm and not a model object we can store somewhere (for example,
    as a pickle file) and reuse whenever we need to impute missing values
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一种算法，而不是我们可以存储在某个地方（例如，作为pickle文件）并在需要时重新使用的模型对象，用于插补缺失值。
- en: See also
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Additional resources are available here:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的资源可以在这里找到：
- en: 'Azur, M. J., Stuart, E. A., Frangakis, C., & Leaf, P. J. (2011). “Multiple
    imputation by chained equations: what is it and how does it work?” *International
    Journal of Methods in Psychiatric Research*, 20(1), 40-49\. [https://doi.org/10.1002/mpr.329](https://doi.org/10.1002/mpr.329).'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azur, M. J., Stuart, E. A., Frangakis, C., & Leaf, P. J. (2011). “链式方程法的多重插补：它是什么？它是如何工作的？”
    *国际精神病学研究方法杂志*，20(1)，40-49\。 [https://doi.org/10.1002/mpr.329](https://doi.org/10.1002/mpr.329)。
- en: 'Buck, S. F. (1960). “A method of estimation of missing values in multivariate
    data suitable for use with an electronic computer.” *Journal of the Royal Statistical
    Society: Series B* (Methodological), 22(2), 302-306\. [https://www.jstor.org/stable/2984099](https://www.jstor.org/stable/2984099).'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buck, S. F. (1960). “一种适用于电子计算机的多元数据缺失值估算方法。” *皇家统计学会学报：B系列（方法论）*，22(2)，302-306\。
    [https://www.jstor.org/stable/2984099](https://www.jstor.org/stable/2984099)。
- en: Stekhoven, D. J. & Bühlmann, P. (2012). “MissForest—non-parametric missing value
    imputation for mixed-type data.” *Bioinformatics*, *28*(1), 112-118.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stekhoven, D. J. & Bühlmann, P. (2012). “MissForest——适用于混合数据类型的非参数缺失值插补。” *生物信息学*，*28*(1)，112-118。
- en: 'van Buuren, S. & Groothuis-Oudshoorn, K. (2011). “MICE: Multivariate Imputation
    by Chained Equations in R.” *Journal of Statistical Software* 45 (3): 1–67\.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'van Buuren, S. & Groothuis-Oudshoorn, K. (2011). “MICE：基于链式方程的多重插补（R语言）。” *统计软件杂志*
    45 (3): 1–67\。'
- en: Van Buuren, S. (2018). *Flexible Imputation of Missing Data*. CRC press.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Buuren, S. (2018). *缺失数据的灵活插补*。CRC出版社。
- en: '`miceforest`—a Python library for fast, memory-efficient MICE with LightGBM.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`miceforest`——一个用于快速、内存高效的MICE与LightGBM的Python库。'
- en: '`missingpy`—a Python library containing the implementation of the MissForest
    algorithm.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`missingpy`——一个Python库，包含MissForest算法的实现。'
- en: Encoding categorical variables
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码类别变量
- en: In the previous recipes, we have seen that some features are categorical variables
    (originally represented as either `object` or `category` data types). However,
    most machine learning algorithms work exclusively with numeric data. That is why
    we need to encode categorical features into a representation compatible with the
    ML models.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的配方中，我们看到一些特征是类别变量（最初表示为`object`或`category`数据类型）。然而，大多数机器学习算法只能处理数字数据。这就是为什么我们需要将类别特征编码为与机器学习模型兼容的表示形式。
- en: 'The first approach to encoding categorical features is called **label encoding**.
    In this approach, we replace the categorical values of a feature with distinct
    numeric values. For example, with three distinct classes, we use the following
    representation: `[0, 1, 2]`.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 编码类别特征的第一种方法叫做**标签编码**。在这种方法中，我们用不同的数字值替代特征的类别值。例如，对于三种不同的类别，我们使用以下表示：[0, 1,
    2]。
- en: This is already very similar to the outcome of converting to the `category`
    data type in `pandas`. Let’s assume we have a DataFrame called `df_cat`, which
    has a feature called `feature_1`. This feature is encoded as the `category` data
    type. We can then access the codes of the categories by running `df_cat["feature_1"].cat.codes`.
    Additionally, we can recover the mapping by running `dict(zip(df_cat["feature_1"].cat.codes,
    df_cat["feature_1"]))`. We can also use the `pd.factorize` function to arrive
    at a very similar representation.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这与转换为`pandas`中的`category`数据类型的结果非常相似。假设我们有一个名为`df_cat`的DataFrame，它有一个叫做`feature_1`的特征。这个特征被编码为`category`数据类型。我们可以通过运行`df_cat["feature_1"].cat.codes`来访问类别的编码值。此外，我们可以通过运行`dict(zip(df_cat["feature_1"].cat.codes,
    df_cat["feature_1"]))`来恢复映射。我们也可以使用`pd.factorize`函数得到一个非常相似的表示。
- en: 'One potential issue with label encoding is that it introduces a relationship
    between the categories, while often there is none. In a three-classes example,
    the relationship looks as follows: 0 < 1 < 2\. This does not make much sense if
    the categories are, for example, countries. However, this can work for features
    that represent some kind of order (ordinal variables). For example, label encoding
    could work well with a rating of service received, on a scale of Bad-Neutral-Good.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码的一个潜在问题是，它会在类别之间引入一种关系，而实际中这种关系可能并不存在。在一个三类的例子中，关系如下：0 < 1 < 2。如果这些类别是例如国家，这就没有多大意义。然而，这对表示某种顺序的特征（有序变量）来说是有效的。例如，标签编码可以很好地用于服务评级，评级范围为差-中等-好。
- en: To overcome the preceding problem, we can use **one-hot encoding**. In this
    approach, for each category of a feature, we create a new column (sometimes called
    a dummy variable) with binary encoding to denote whether a particular row belongs
    to this category. A potential drawback of this method is that it significantly
    increases the dimensionality of the dataset (**curse of dimensionality**). First,
    this can increase the risk of overfitting, especially when we do not have that
    many observations in our dataset. Second, a high-dimensional dataset can be a
    significant problem for any distance-based algorithm (for example, k-Nearest Neighbors),
    as—on a very high level—a large number of dimensions causes all the observations
    to appear equidistant from each other. This can naturally render the distance-based
    models useless.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决前述问题，我们可以使用**独热编码**。在这种方法中，对于特征的每个类别，我们都会创建一个新的列（有时称为虚拟变量），通过二进制编码表示某行是否属于该类别。该方法的一个潜在缺点是，它会显著增加数据集的维度（**维度灾难**）。首先，这会增加过拟合的风险，特别是在数据集中观测值不多时。其次，高维数据集对于任何基于距离的算法（例如k-最近邻）来说都是一个重大问题，因为在高维情况下，多个维度会导致所有观测值彼此之间看起来等距。这自然会使基于距离的模型变得无效。
- en: Another thing we should be aware of is that creating dummy variables introduces
    a form of redundancy to the dataset. In fact, if a feature has three categories,
    we only need two dummy variables to fully represent it. That is because if an
    observation is neither of the two, it must be the third one. This is often referred
    to as the **dummy-variable trap**, and it is best practice to always remove one
    column (known as the reference value) from such an encoding. This is especially
    important in unregularized linear models.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的问题是，创建虚拟变量会给数据集引入一种冗余的形式。事实上，如果某个特征有三个类别，我们只需要两个虚拟变量就能完全表示它。因为如果一个观测值不是其中两个，它就必须是第三个。这通常被称为**虚拟变量陷阱**，最佳实践是总是从这种编码中删除一列（称为参考值）。在没有正则化的线性模型中，这一点尤其重要。
- en: Summing up, we should avoid label encoding as it introduces false order to the
    data, which can lead to incorrect conclusions. Tree-based methods (decision trees,
    Random Forest, and so on) can work with categorical data and label encoding. However,
    one-hot encoding is the natural representation of categorical features for algorithms
    such as linear regression, models calculating distance metrics between features
    (such as k-means clustering or k-Nearest Neighbors), or **Artificial Neural Networks**
    (**ANN**).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们应该避免使用标签编码，因为它会给数据引入虚假的顺序，这可能导致错误的结论。基于树的方法（决策树、随机森林等）可以与分类数据和标签编码一起工作。然而，对于线性回归、计算特征之间距离度量的模型（例如k-means聚类或k-最近邻）或**人工神经网络**（**ANN**）等算法，独热编码是分类特征的自然表示。
- en: Getting ready
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: For this recipe, we assume that we have the outputs of the imputed training
    and test sets from the previous recipe, *Identifying and dealing with missing
    values*.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本食谱，我们假设我们已经得到了前一个食谱*识别和处理缺失值*的填充训练集和测试集的输出。
- en: How to do it...
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Execute the following steps to encode categorical variables with label encoding
    and one-hot encoding:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤，使用标签编码和独热编码对分类变量进行编码：
- en: 'Import the libraries:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE45]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Use Label Encoder to encode a selected column:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标签编码器对选定的列进行编码：
- en: '[PRE46]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Running the snippet generates the following preview of the transformed column:'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行该代码片段会生成转换后的列的以下预览：
- en: '[PRE47]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We created a copy of `X_train` and `X_test`, just to show how to work with `LabelEncoder`,
    but we do not want to modify the actual DataFrames we intend to use later.
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们创建了`X_train`和`X_test`的副本，仅仅是为了展示如何使用`LabelEncoder`，但我们不希望修改我们稍后打算使用的实际数据框。
- en: We can access the labels stored within the fitted `LabelEncoder` by using the
    `classes_` attribute.
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以通过使用`classes_`属性访问在已拟合的`LabelEncoder`中存储的标签。
- en: 'Select categorical features for one-hot encoding:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择进行独热编码的类别特征：
- en: '[PRE48]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We will apply one-hot encoding to the following columns:'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将对以下列应用独热编码：
- en: '[PRE49]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Instantiate the `OneHotEncoder` object:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`OneHotEncoder`对象：
- en: '[PRE50]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Create the column transformer using the one-hot encoder:'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用独热编码器创建列转换器：
- en: '[PRE51]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Fit the transformer:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 适配转换器：
- en: '[PRE52]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Executing the snippet prints the following preview of the column transformer:'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会打印出以下列转换器的预览：
- en: '![](../Images/B18112_13_17.png)'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_17.png)'
- en: 'Figure 13.17: Preview of the column transformer with one-hot encoding'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.17：使用独热编码的列转换器预览
- en: 'Apply the transformations to both training and test sets:'
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练集和测试集应用转换：
- en: '[PRE53]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: As we have mentioned before, one-hot encoding comes with the potential disadvantage
    of increasing the dimensionality of the dataset. In our case, we started with
    23 columns. After applying one-hot encoding, we ended up with 72 columns.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所提到的，独热编码可能带来增加数据集维度的缺点。在我们的例子中，我们一开始有23列，应用独热编码后，最终得到了72列。
- en: How it works...
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we imported the required libraries. In the second step, we selected the
    column we wanted to encode using label encoding, instantiated the `LabelEncoder`,
    fitted it to the training data, and transformed both the training and the test
    data. We did not want to keep the label encoding, and for that reason we operated
    on copies of the DataFrames.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入了所需的库。第二步中，我们选择了要进行标签编码的列，实例化了`LabelEncoder`，将其拟合到训练数据上，并转换了训练集和测试集。我们不想保留标签编码，因此我们对数据框进行了副本操作。
- en: 'We demonstrated using label encoding as it is one of the available options,
    however, it comes with quite serious drawbacks. So in practice, we should refrain
    from using it. Additionally, `scikit-learn`''s documentation warns us with the
    following statement: *This transformer should be used to encode target values,
    i.e. y, and not the input X.*'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了使用标签编码，因为它是可用选项之一，但它有相当严重的缺点。因此在实际应用中，我们应避免使用它。此外，`scikit-learn`的文档警告我们以下声明：*此转换器应当用于编码目标值，即y，而非输入X。*
- en: In *Step 3*, we started the preparations for one-hot encoding by creating a
    list of all the categorical features. We used the `select_dtypes` method to select
    all features with the `object` data type.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 3*中，我们开始为独热编码做准备，通过创建一个包含所有类别特征的列表。我们使用`select_dtypes`方法选择所有`object`数据类型的特征。
- en: In *Step 4*, we created an instance of `OneHotEncoder`. We specified that we
    did not want to work with a sparse matrix (a special kind of data type, suitable
    for storing matrices with a very high percentage of zeros), we dropped the first
    column per feature (to avoid the dummy variable trap), and we specified what to
    do if the encoder finds an unknown value while applying the transformation (`handle_unknown='error'`).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们创建了`OneHotEncoder`的实例。我们指定不希望使用稀疏矩阵（这是一种特殊的数据类型，适用于存储具有较高零值比例的矩阵），我们删除了每个特征的第一列（以避免虚拟变量陷阱），并指定了当编码器在应用转换时遇到未知值时该如何处理（`handle_unknown='error'`）。
- en: In *Step 5*, we defined the `ColumnTransformer`, which is a convenient approach
    to applying the same transformation (in this case, the one-hot encoder) to multiple
    columns. We passed a list of steps, where each step was defined by a tuple. In
    this case, it was a single tuple with the name of the step (`"one_hot"`), the
    transformation to be applied, and the features to which we wanted to apply the
    transformation.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们定义了`ColumnTransformer`，这是一种方便的方法，可以将相同的转换（在本例中是独热编码器）应用于多列。我们传递了一个步骤列表，其中每个步骤都由一个元组定义。在这个例子中，它是一个包含步骤名称（`"one_hot"`）、要应用的转换以及我们希望应用该转换的特征的元组。
- en: When creating the `ColumnTransformer`, we also specified another argument, `remainder="passthrough"`,
    which has effectively fitted and transformed only the specified columns, while
    leaving the rest intact. The default value for the `remainder` argument was `"drop"`,
    which dropped the unused columns. We also specified the value of the `verbose_feature_names_out`
    argument as `False`. This way, when we use the `get_feature_names_out` method
    later, it will not prefix all feature names with the name of the transformer that
    generated that feature.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建`ColumnTransformer`时，我们还指定了另一个参数`remainder="passthrough"`，这实际上仅对指定的列进行了拟合和转换，同时保持其余部分不变。`remainder`参数的默认值是`"drop"`，会丢弃未使用的列。我们还将`verbose_feature_names_out`参数的值设置为`False`。这样，稍后使用`get_feature_names_out`方法时，它将不会在所有特征名称前加上生成该特征的转换器名称。
- en: If we had not changed it, some features would have the `"one_hot__"` prefix,
    while the others would have `"remainder__"`.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有更改它，某些特征将具有`"one_hot__"`前缀，而其他特征将具有`"remainder__"`前缀。
- en: In *Step 6*, we fitted the column transformer to the training data using the
    `fit` method. Lastly, we applied the transformations using the `transform` method
    to both training and test sets. As the `transform` method returns a `numpy` array
    instead of a `pandas` DataFrame, we had to convert them ourselves. We started
    by extracting the names of the features using the `get_feature_names_out`. Then,
    we created a `pandas` DataFrame using the transformed features, the new column
    names, and the old indices (to keep everything in order).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤6*中，我们使用`fit`方法将列转换器拟合到训练数据。最后，我们使用`transform`方法对训练集和测试集应用了转换。由于`transform`方法返回的是`numpy`数组而不是`pandas`
    DataFrame，我们必须自己进行转换。我们首先使用`get_feature_names_out`提取特征名称。然后，我们使用转换后的特征、新的列名和旧的索引（以保持顺序）创建了一个`pandas`
    DataFrame。
- en: Similar to handling missing values or detecting outliers, we fit all the transformers
    (including one-hot encoding) to the training data only, and then we apply the
    transformations to both training and test sets. This way, we avoid potential data
    leakage.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于处理缺失值或检测异常值，我们仅将所有转换器（包括独热编码）拟合到训练数据，然后将转换应用于训练集和测试集。通过这种方式，我们可以避免潜在的数据泄漏。
- en: There’s more...
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: We would like to mention a few more things regarding the encoding of categorical
    variables.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还想提及一些关于类别变量编码的其他事项。
- en: Using pandas for one-hot encoding
  id: totrans-433
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用pandas进行独热编码
- en: 'Alternatively to `scikit-learn`, we can use `pd.get_dummies` for one-hot encoding
    categorical features. The example syntax looks like the following:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`scikit-learn`，我们还可以使用`pd.get_dummies`对类别特征进行独热编码。示例语法如下：
- en: '[PRE54]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: It’s good to know this alternative approach, as it can be easier to work with
    (column names are automatically accounted for), especially when creating a quick
    Proof of Concept (PoC). However, when productionizing the code, the best approach
    would be to use the `scikit-learn` variant and create the dummy variables within
    a pipeline.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这种替代方法是有益的，因为它可能更易于使用（列名会自动处理），特别是在创建快速概念验证（PoC）时。然而，在将代码投入生产时，最佳方法是使用`scikit-learn`的变体，并在管道中创建虚拟变量。
- en: Specifying possible categories for OneHotEncoder
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为OneHotEncoder指定可能的类别
- en: 'When creating `ColumnTransformer`, we could have additionally provided a list
    of possible categories for all the considered features. A simplified example follows:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建`ColumnTransformer`时，我们本可以另外提供一个包含所有考虑特征的可能类别的列表。以下是一个简化的示例：
- en: '[PRE55]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Executing the snippet returns the following:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 执行该代码片段将返回以下内容：
- en: '[PRE56]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: By passing a list (of lists) containing possible categories for each feature,
    we are taking into account the possibility that the specific value does not appear
    in the training set, but might appear in the test set (or as part of the batch
    of new observations in the production environment). If this were the case, we
    would run into errors.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递一个包含每个特征可能类别的列表（列表的列表），我们考虑到了一种可能性，即某个特定值在训练集中可能没有出现，但可能会出现在测试集中（或在生产环境中新观测值的批次中）。如果是这种情况，我们就会遇到错误。
- en: In the preceding code block, we added an extra category called `"Unknown"` to
    the column representing sex. As a result, we will end up with an extra “dummy”
    column for that category. The male category was dropped as the reference one.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们向表示性别的列添加了一个名为`"Unknown"`的额外类别。因此，我们会为该类别生成一个额外的“虚拟”列。男性类别被作为参考类别而被删除。
- en: Category Encoders library
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类别编码器库
- en: Aside from using `pandas` and `scikit-learn`, we can also use another library
    called `Category Encoders`. It belongs to a set of libraries compatible with `scikit-learn`
    and provides a selection of encoders using a similar fit-transform approach. That
    is why it is also possible to use them together with `ColumnTransformer` and `Pipeline`.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用`pandas`和`scikit-learn`，我们还可以使用另一个名为`Category Encoders`的库。它属于一组与`scikit-learn`兼容的库，并提供了一些编码器，采用类似的fit-transform方法。这也是为什么它可以与`ColumnTransformer`和`Pipeline`一起使用的原因。
- en: We show an alternative implementation of the one-hot encoder.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了一个替代的一热编码实现。
- en: 'Import the library:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE57]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Create the encoder object:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 创建编码器对象：
- en: '[PRE58]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Additionally, we could specify an argument called `drop_invariant`, to indicate
    that we want to drop columns with no variance, so for example columns filled with
    only one distinct value. This could help with reducing the number of features.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以指定一个名为`drop_invariant`的参数，表示我们希望丢弃没有方差的列，例如仅填充一个唯一值的列。这可以帮助减少特征数量。
- en: 'Fit the encoder, and transform the data:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合编码器，并转换数据：
- en: '[PRE59]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This implementation of the one-hot encoder automatically encodes only the columns
    containing strings (unless we specify only a subset of categorical columns by
    passing a list to the `cols` argument). By default, it also returns a `pandas`
    DataFrame (in comparison to the `numpy` array, in the case of `scikit-learn`'s
    implementation) with the adjusted column names. The only drawback of this implementation
    is that it does not allow for dropping the one redundant dummy column of each
    feature.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 这种一热编码器的实现会自动编码仅包含字符串的列（除非我们通过将类别列的列表传递给`cols`参数来指定只编码部分列）。默认情况下，它还返回一个`pandas`
    DataFrame（与`scikit-learn`实现中的`numpy`数组相比），并调整了列名。这种实现的唯一缺点是，它不允许丢弃每个特征的一个冗余虚拟变量列。
- en: A warning about one-hot encoding and decision tree-based algorithms
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于一热编码和基于决策树的算法的警告
- en: While regression-based models can naturally handle the OR condition of one-hot-encoded
    features, the same is not that simple with decision tree-based algorithms. In
    theory, decision trees are capable of handling categorical features without the
    need for encoding.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于回归的模型自然能处理一热编码特征的OR条件，但决策树算法却不那么简单。从理论上讲，决策树可以在不需要编码的情况下处理类别特征。
- en: However, its popular implementation in `scikit-learn` still requires all features
    to be numerical. Without going into too much detail, such an approach favors continuous
    numerical features over one-hot-encoded dummies, as a single dummy can only bring
    a fraction of the total feature information into the model. A possible solution
    is to use either a different kind of encoding (label/target encoding) or an implementation
    that handles categorical features, such as Random Forest in the `h2o` library
    or the LightGBM model.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`scikit-learn`中流行的实现仍然要求所有特征必须是数值型的。简单来说，这种方法偏向于连续数值型特征，而不是一热编码的虚拟变量，因为单个虚拟变量只能将特征信息的一部分带入模型。一个可能的解决方案是使用另一种编码方式（标签/目标编码）或使用能够处理类别特征的实现，例如`h2o`库中的随机森林或LightGBM模型。
- en: Fitting a decision tree classifier
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合决策树分类器
- en: A **decision tree** classifier is a relatively simple yet very important machine
    learning algorithm used for both regression and classification problems. The name
    comes from the fact that the model creates a set of rules (for example, `if x_1
    > 50 and x_2 < 10 then y = 'default'`), which taken together can be visualized
    in the form of a tree. The decision trees segment the feature space into a number
    of smaller regions, by repeatedly splitting the features at a certain value. To
    do so, they use a **greedy algorithm** (together with some heuristics) to find
    a split that minimizes the combined impurity of the children nodes. The impurity
    in classification tasks is measured using the Gini impurity or entropy, while
    for regression problems the trees use the mean squared error or the mean absolute
    error as the metric.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**分类器是一种相对简单但非常重要的机器学习算法，既可以用于回归问题，也可以用于分类问题。这个名字来源于模型创建一组规则（例如，`if x_1
    > 50 and x_2 < 10 then y = ''default''`），这些规则加起来可以被可视化为一棵树。决策树通过在某个值处反复划分特征，将特征空间划分为若干较小的区域。为此，它们使用**贪心算法**（结合一些启发式方法）来找到一个分裂点，以最小化子节点的总体杂质。在分类任务中，杂质通过基尼杂质或熵来衡量，而在回归问题中，树使用均方误差或均绝对误差作为衡量标准。'
- en: In the case of a binary classification problem, the algorithm tries to obtain
    nodes that contain as many observations from one class as possible, thus minimizing
    the impurity. The prediction in a terminal node (leaf) is made on the basis of
    mode in the case of classification, and mean for regression problems.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类问题中，算法试图获得包含尽可能多来自同一类别的观测数据的节点，从而最小化不纯度。分类问题中，终端节点（叶子节点）的预测是基于众数，而回归问题则是基于均值。
- en: Decision trees are a base for many complex algorithms, such as Random Forest,
    Gradient Boosted Trees, XGBoost, LightGBM, CatBoost, and so on.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是许多复杂算法的基础，例如随机森林、梯度提升树、XGBoost、LightGBM、CatBoost等。
- en: 'The advantages of decision trees include the following:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的优点包括以下几点：
- en: Easily visualized in the form of a tree—high interpretability
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以树形结构轻松可视化——具有很高的可解释性
- en: Fast training and prediction stages
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速的训练和预测阶段
- en: A relatively small number of hyperparameters to tune
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要调整的超参数相对较少
- en: Support numerical and categorical features
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持数值型和分类特征
- en: Can handle non-linearity in data
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理数据中的非线性关系
- en: Can be further improved with feature engineering, though there is no explicit
    need to do so
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特征工程可以进一步改善，但并不需要强制这样做
- en: Do not require scaling or normalization of features
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要对特征进行缩放或标准化
- en: Incorporate their version of feature selection by choosing the features on which
    to split the sample
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择划分样本的特征，整合其特征选择的版本
- en: Non-parametric model—no assumptions about the distribution of the features/target
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非参数模型——不对特征/目标的分布做任何假设
- en: 'On the other hand, the disadvantages of decision trees include the following:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，决策树的缺点包括以下几点：
- en: Instability—the trees are very sensitive to the noise in input data. A small
    change in the data can change the model significantly.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不稳定性——决策树对输入数据中的噪声非常敏感。数据中的一个小变化可能会显著改变模型。
- en: Overfitting—if we do not provide maximum values or stopping criteria, the trees
    tend to grow very deep and do not generalize well.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合——如果我们没有提供最大值或停止准则，决策树可能会过度生长，导致模型的泛化能力差。
- en: The trees can only interpolate, but not extrapolate—they make constant predictions
    for observations that lie beyond the boundary region established on the feature
    space of the training data.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树只能进行内插，但不能进行外推——对于训练数据的特征空间边界之外的观测数据，它们做出恒定的预测。
- en: The underlying greedy algorithm does not guarantee the selection of a globally
    optimal decision tree.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底层的贪心算法无法保证选择全局最优的决策树。
- en: Class imbalance can lead to biased trees.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别不平衡可能导致决策树出现偏差。
- en: Information gain (a decrease in entropy) in a decision tree with categorical
    variables results in a biased outcome for features with a higher number of categories.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树中类别变量的信息增益（熵的减少）会导致具有较多类别的特征结果偏向。
- en: Getting ready
  id: totrans-479
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we assume that we have the outputs of the one-hot-encoded training
    and test sets from the previous recipe, *Encoding categorical variables*.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，我们假设已经得到了前一个食谱《*编码分类变量*》中的一热编码训练集和测试集的输出。
- en: How to do it...
  id: totrans-481
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Execute the following steps to fit a decision tree classifier:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来拟合决策树分类器：
- en: 'Import the libraries:'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE60]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: In this recipe and the following ones, we will be using the `performance_evaluation_report`
    helper function. It plots useful metrics (confusion matrix, ROC curve) used for
    evaluating binary classification models. Also, it returns a dictionary containing
    more metrics, which we cover in the *How it works…* section.
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本食谱及随后的食谱中，我们将使用`performance_evaluation_report`辅助函数。它绘制了用于评估二分类模型的有用指标（混淆矩阵、ROC曲线）。此外，它还返回一个字典，包含更多的评估指标，我们将在*工作原理*部分进行介绍。
- en: 'Create the instance of the model, fit it to the training data, and create predictions:'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建模型实例，将其拟合到训练数据，并生成预测：
- en: '[PRE61]'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Evaluate the results:'
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估结果：
- en: '[PRE62]'
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下图表：
- en: '![](../Images/B18112_13_18.png)'
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_18.png)'
- en: 'Figure 13.18: The performance evaluation report of the fitted decision tree
    classifier'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.18：拟合后的决策树分类器的性能评估报告
- en: 'The `tree_perf` object is a dictionary containing more relevant metrics, which
    can further help us with evaluating the performance of our model. We present those
    metrics below:'
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`tree_perf`对象是一个字典，包含更多相关的评估指标，可以进一步帮助我们评估模型的性能。我们在下面展示这些指标：'
- en: '[PRE63]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: For more insights into the interpretation of the evaluation metrics, please
    refer to the *How it works…* section.
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要获取更多关于评估指标解释的见解，请参考*它是如何工作的……*部分。
- en: 'Plot the first few levels of the fitted decision tree:'
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制拟合决策树的前几层：
- en: '[PRE64]'
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下图形：
- en: '![](../Images/B18112_13_19.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_19.png)'
- en: 'Figure 13.19: The fitted decision tree, capped at a max depth of 3'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.19：拟合的决策树，最大深度限制为 3
- en: Using the one-liner, we can already visualize quite a lot of information. We
    decided to plot only the 3 levels of the decision tree, as the fitted tree actually
    reached the depth of 44 levels. As we have mentioned, not restricting the `max_depth`
    hyperparameter can lead to such cases, which are also very likely to overfit.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单行代码，我们已经能够可视化出大量信息。我们决定只绘制决策树的 3 层，因为拟合的树实际上达到了 44 层的深度。正如我们所提到的，不限制`max_depth`超参数可能会导致这种情况，这也很可能会导致过拟合。
- en: 'In the tree, we can see the following information:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 在树中，我们可以看到以下信息：
- en: Which feature is used to split the tree and at which value. Unfortunately, with
    the default settings, we only see the column number instead of the feature’s name.
    We will fix that soon.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于拆分树的特征以及拆分的值。不幸的是，使用默认设置时，我们只看到列号，而不是特征名称。我们将很快修正这个问题。
- en: The value of the Gini impurity.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基尼不纯度的值。
- en: The number of samples in each node/leaf.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点/叶子中的样本数量。
- en: The number of observations of each class within the node/leaf.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点/叶子中各类的观察数量。
- en: 'We can add more information to the plot with a few additional arguments of
    the `plot_tree` function:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`plot_tree`函数的几个附加参数向图形中添加更多信息：
- en: '[PRE65]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下图形：
- en: '![](../Images/B18112_13_20.png)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_20.png)'
- en: 'Figure 13.20: The fitted decision tree, capped at a max depth of 2'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.20：拟合的决策树，最大深度限制为 2
- en: 'In *Figure 13.20*, we see some additional information:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 13.20*中，我们看到了一些额外的信息：
- en: The name of the feature used for creating the split
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于创建拆分的特征名称
- en: The name of the class dominating in each node/leaf
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点/叶子中占主导地位的类别名称
- en: Visualizing decision trees has many benefits. First, we can gain insights into
    which features are used for creating the model (a possible measure of feature
    importance) and what values were used to create the splits. Provided that the
    features have clear interpretation, this could work as a form of a sanity check
    to see if our initial hypotheses about the data and the considered problem came
    true and are aligned with common sense or domain knowledge. It could also help
    with presenting a clear and coherent story to the business stakeholders, who can
    quite easily understand such a simple representation of the model. We will discuss
    the feature importance and model explainability in depth in the following chapter.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化决策树有许多好处。首先，我们可以深入了解哪些特征用于创建模型（这可能是特征重要性的一种衡量标准），以及哪些值用于创建拆分。前提是这些特征具有清晰的解释，这可以作为一种理智检查，看看我们关于数据和所考虑问题的初步假设是否成立，是否符合常识或领域知识。它还可以帮助向业务利益相关者呈现清晰、一致的故事，他们可以很容易地理解这种简单的模型表示。我们将在下一章深入讨论特征重要性和模型可解释性。
- en: How it works...
  id: totrans-516
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In *Step 2*, we used the typical `scikit-learn` approach to training a machine
    learning model. First, we created the object of the `DecisionTreeClassifier` class
    (using all the default settings and a fixed random state). Then, we fitted the
    model to the training data (we needed to pass both the features and the target)
    using the `fit` method. Lastly, we obtained the predictions by using the `predict`
    method.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 2*中，我们使用了典型的`scikit-learn`方法来训练机器学习模型。首先，我们创建了`DecisionTreeClassifier`类的对象（使用所有默认设置和固定的随机状态）。然后，我们使用`fit`方法将模型拟合到训练数据（需要传入特征和目标）。最后，我们通过`predict`方法获得预测结果。
- en: Using the `predict` method results in an array of predicted classes (in this
    case, it is either a 0 or a 1). However, there are cases when we are interested
    in the assigned probabilities or scores. To obtain those, we can use the `predict_proba`
    method, which returns an array of size `n_test_observations` by `n_classes`. Each
    row contains all the possible class probabilities (they sum up to 1). In the case
    of binary classification, the `predict` method automatically assigns a positive
    class when the corresponding probability is above 50%.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`predict`方法会返回一个预测类别的数组（在本例中是0或1）。然而，也有一些情况我们对预测的概率或分数感兴趣。为了获取这些值，我们可以使用`predict_proba`方法，它返回一个`n_test_observations`行，`n_classes`列的数组。每一行包含所有可能类别的概率（这些概率的总和为1）。在二分类的情况下，当对应的概率超过50%时，`predict`方法会自动将正类分配给该观察值。
- en: In *Step 3*, we evaluated the performance of the model. We used a custom function
    to display all the results. We will not go deeper into its specifics, as it is
    quite standard and is built using functions from the `metrics` module of `scikit-learn`.
    For a detailed description of the function, please refer to the accompanying GitHub
    repository.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤3*中，我们评估了模型的表现。我们使用了一个自定义函数来展示所有的结果。我们不会深入探讨它的具体细节，因为它是标准的，且是通过使用`scikit-learn`库中的`metrics`模块的函数构建的。有关该函数的详细描述，请参考附带的GitHub仓库。
- en: 'The **confusion matrix** summarizes all possible combinations of the predicted
    values as opposed to the actual target. The possible values are as follows:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '**混淆矩阵**总结了所有可能的预测值与实际目标值之间的组合。可能的值如下：'
- en: '**True positive** (**TP**): The model predicts a default, and the client defaulted'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正阳性**（**TP**）：模型预测为违约，且客户违约了。'
- en: '**False positive** (**FP**): The model predicts a default, but the client did
    not default'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性**（**FP**）：模型预测为违约，但客户未违约。'
- en: '**True negative** (**TN**): The model predicts a good customer, and the client
    did not default'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正阴性**（**TN**）：模型预测为好客户，且客户未违约。'
- en: '**False negative** (**FN**): The model predicts a good customer, but the client
    defaulted'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性**（**FN**）：模型预测为好客户，但客户违约了。'
- en: In the scenarios presented above, we assumed that default is represented by
    the positive class. It does not mean that the outcome (client defaulting) is good
    or positive, just that an event occurred. Most frequently, the majority class
    is the “uninteresting” case and is assigned the negative label. This is a typical
    convention used in data science projects.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面呈现的场景中，我们假设违约是由正类表示的。这并不意味着结果（客户违约）是好或正面的，仅仅表示某事件发生了。通常情况下，多数类是“无关”的情况，会被赋予负标签。这是数据科学项目中的典型约定。
- en: 'Using the presented values, we can further build multiple evaluation criteria:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以上呈现的值，我们可以进一步构建多个评估标准：
- en: '**Accuracy** [expressed as (*TP* + *TN*) / (*TP* + *FP* + *TN* + *FN*)]—measures
    the model’s overall ability to correctly predict the class of the observation.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率** [表示为 (*TP* + *TN*) / (*TP* + *FP* + *TN* + *FN*)]——衡量模型正确预测观察值类别的整体能力。'
- en: '**Precision** [expressed as *TP* / (*TP* + *FP*)]—measures what fraction of
    all predictions of the positive class (in our case, the default) indeed were positive.
    In our project, it answers the question: *Out of all predictions of default, how
    many clients actually defaulted*? Or in other words: *When the model predicts
    default, how often is it correct?*'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度** [表示为 *TP* / (*TP* + *FP*)]——衡量所有正类预测（在我们的案例中是违约）中，实际为正类的比例。在我们的项目中，它回答了这个问题：*在所有违约预测中，实际违约的客户有多少*？或者换句话说：*当模型预测违约时，它的准确率有多高？*'
- en: '**Recall** [expressed as *TP* / (*TP* + *FN*)]—measures what fraction of all
    positive cases were predicted correctly. Also called sensitivity or the true positive
    rate. In our case, it answers the question: *What fraction of all observed defaults
    did we predict correctly?*'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率** [表示为 *TP* / (*TP* + *FN*)]——衡量所有正类样本中被正确预测的比例。也叫做敏感度或真正阳性率。在我们的案例中，它回答了这个问题：*我们正确预测了多少实际发生的违约事件？*'
- en: '**F-1 Score—**a harmonic average of precision and recall. The reason for using
    the harmonic mean instead of arithmetic average is that it takes into account
    the harmony (similarity) between the two scores. Thus, it punishes extreme outcomes
    and discourages highly unequal values. For example, a classifier with precision
    = 1 and recall = 0 would score a 0.5 using a simple average, but a 0 when using
    the harmonic mean.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F-1分数**—精确度和召回率的调和平均数。使用调和平均数而非算术平均数的原因是它考虑了两个分数之间的协调性（相似性）。因此，它惩罚极端结果，并避免高度不平衡的值。例如，一个精确度为1而召回率为0的分类器，在使用简单平均时得分为0.5，而在使用调和平均时得分为0。'
- en: '**Specificity** [expressed as *TN* / (*TN* + *FP*)]—measures what fraction
    of negative cases (clients without a default) actually did not default. A helpful
    way of thinking about specificity is to consider it the recall of the negative
    class.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特异性** [表示为 *TN* / (*TN* + *FP*)]—衡量负类案例（没有违约的客户）中实际上没有违约的比例。理解特异性的一个有用方法是将其视为负类的召回率。'
- en: 'Understanding the subtleties behind these metrics is very important for the
    correct evaluation of the model’s performance. Accuracy can be highly misleading
    in the case of class imbalance. Imagine a case when 99% of data is not fraudulent
    and only 1% is fraudulent. Then, a naïve model classifying each observation as
    non-fraudulent achieves 99% accuracy, while it is actually worthless. That is
    why, in such cases, we should refer to precision or recall:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些指标背后的细微差别对于正确评估模型性能非常重要。在类别不平衡的情况下，准确率可能会非常具有误导性。假设99%的数据不是欺诈的，只有1%是欺诈的。那么，一个将每个观察值都分类为非欺诈的天真模型可以达到99%的准确率，但实际上它是毫无价值的。这就是为什么在这种情况下，我们应该参考精确度或召回率：
- en: When we try to achieve as high precision as possible, we will get fewer false
    positives, at the cost of more false negatives. We should optimize for precision
    when the cost of a false positive is high, for example, in spam detection.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们尽力实现尽可能高的精确度时，我们将减少假阳性，但代价是增加假阴性。当假阳性的代价很高时，我们应该优化精确度，例如在垃圾邮件检测中。
- en: When optimizing for recall, we will achieve fewer false negatives, at the cost
    of more false positives. We should optimize for recall when the cost of a false
    negative is high, for example, in fraud detection.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在优化召回率时，我们将减少假阴性，但代价是增加假阳性。当假阴性的代价很高时，我们应该优化召回率，例如在欺诈检测中。
- en: There is no one-size-fits-all rule about which metric is best. The metric that
    we try to optimize for should be selected based on the use case.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 关于哪种指标最好，没有一刀切的规则。我们试图优化的指标应根据使用场景来选择。
- en: The second plot contains the **Receiver Operating Characteristic** (**ROC**)
    curve. The ROC curve presents a trade-off between the true positive rate (TPR,
    recall) and the false positive rate (FPR, which is equal to 1 minus specificity)
    for different probability thresholds. A probability threshold determines the predicted
    probability above which we decide that the observation belongs to the positive
    class (by default, it is 50%).
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 第二张图包含**接收者操作特征**（**ROC**）曲线。ROC曲线展示了不同概率阈值下真实正类率（TPR，召回率）与假阳性率（FPR，即1减去特异性）之间的权衡。概率阈值决定了当预测概率超过某一值时，我们判断观察结果属于正类（默认值为50%）。
- en: 'An ideal classifier would have a false positive rate of 0 and a true positive
    rate of 1\. Thus, the sweet spot in the ROC plot is the (0,1) point in the plot.
    A skillful model’s curve would be as close to it as possible. On the other hand,
    a model with no skill will have a line close to the diagonal (45°) line. To better
    understand the ROC curve, please consider the following:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 一个理想的分类器将具有0的假阳性率和1的真实正类率。因此，ROC图中的最佳点是图中的(0,1)点。一个有技巧的模型曲线将尽可能接近这个点。另一方面，一个没有技巧的模型将会有一条接近对角线（45°）的线。为了更好地理解ROC曲线，请考虑以下内容：
- en: Let’s assume that we pick the decision threshold to be 0, that is, all observations
    are classified as defaults. This leads to two conclusions. First, no actual defaults
    are predicted as the negative class (false negatives), which means that the true
    positive rate (recall) is 1\. Second, no good customers are classified as such
    (true negatives), which means that the false positive rate is also 1\. This corresponds
    to the top-right corner of the ROC curve.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们将决策阈值设置为0，也就是说，所有观察结果都被分类为违约。这得出两个结论。首先，没有实际的违约被预测为负类（假阴性），这意味着真实正类率（召回率）为1。其次，没有良好的客户被分类为正类（真实负类），这意味着假阳性率也为1。这对应于ROC曲线的右上角。
- en: Let’s move to the other extreme and assume that the decision threshold is 1,
    that is, all customers are classified as good customers (no default, that is,
    the negative class). As there are no positive predictions at all, this leads to
    the following conclusions. First, there are no true positives (TPR = 0). Second,
    there are no false positives (FPR = 0). Such a scenario corresponds to the bottom
    left of the curve.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们假设另一种极端情况，假设决策阈值为 1，也就是说，所有客户都被分类为优质客户（无违约，即负类）。由于完全没有正预测，这将导致以下结论：首先，没有真正例（TPR
    = 0）。其次，没有假正例（FPR = 0）。这种情况对应于曲线的左下角。
- en: As such, all the points on the curve correspond to the scores of a classifier
    for thresholds between the two extremes (0 and 1). The curve should approach the
    ideal point, in which the true positive rate is 1 and the false positive rate
    is 0\. That is, no defaulting client is classified as a good customer and no good
    customer is classified as likely to default. In other words, a perfect classifier.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，曲线上的所有点都对应于分类器在两个极端（0 和 1）之间的各个阈值下的得分。该曲线应接近理想点，即真正例率为 1，假正例率为 0。也就是说，所有违约客户都不被分类为优质客户，所有优质客户都不被分类为可能违约。换句话说，这是一个完美的分类器。
- en: If the performance approaches the diagonal line, the model is classifying roughly
    as many defaulting and non-defaulting customers as defaulting. In other words,
    this would be a classifier as good as random guessing.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果性能接近对角线，说明模型对违约和非违约客户的分类大致相同，等同于随机猜测。换句话说，这个分类器与随机猜测一样好。
- en: A model with a curve below the diagonal line is possible and is actually better
    than the “no-skill” one, as its predictions can be simply inverted to obtain better
    performance.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 一个位于对角线下方的模型是可能的，实际上比“无技能”模型更好，因为它的预测可以简单地反转，从而获得更好的性能。
- en: To summarize the performance of a model with one number, we can look at the
    **area under the ROC curve** (**AUC**). It is an aggregate measure of performance
    across all possible decision thresholds. It is a metric with values between 0
    and 1 and it tells us how much the model is capable of distinguishing between
    the classes. A model with an AUC of 0 is always wrong, while a model with an AUC
    of 1 is always correct. An AUC of 0.5 indicates a model with no skill that is
    pretty much equal to random guessing.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 总结模型性能时，我们可以通过一个数字来查看**ROC 曲线下的面积**（**AUC**）。它是衡量所有可能决策阈值下的综合性能的指标。AUC 的值介于
    0 和 1 之间，它告诉我们模型区分各类的能力。AUC 为 0 的模型总是错误的，而 AUC 为 1 的模型总是正确的。AUC 为 0.5 表示模型没有任何技能，几乎等同于随机猜测。
- en: We can interpret the AUC in probabilistic terms. In short, it indicates how
    well the probabilities from the positive classes are separated from the negative
    classes. AUC represents the probability that a model ranks a random positive observation
    more highly than a random negative one.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用概率的角度来解读 AUC。简而言之，它表示正类概率与负类概率的分离程度。AUC 代表一个模型将一个随机正类样本排得比一个随机负类样本更高的概率。
- en: An example might make it a bit easier to understand. Let’s assume we have predictions
    obtained from some model, ranked in ascending order by their score/probability.
    *Figure 13.21* illustrates this. An AUC of 75% means that if we take one random
    positive observation and one random negative observation, with a 75% probability
    they will be ordered in the correct way, that is, the random positive example
    is to the right of the random negative example.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子可能会更容易理解。假设我们有一些来自模型的预测结果，这些预测结果按得分/概率升序排列。*图 13.21* 展示了这一点。AUC 为 75% 意味着，如果我们随机选择一个正类样本和一个负类样本，那么它们以
    75% 的概率会被正确排序，也就是说，随机的正类样本位于随机负类样本的右侧。
- en: '![](../Images/B18112_13_21.png)'
  id: totrans-546
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_21.png)'
- en: 'Figure 13.21: Model’s output ordered by predicted score/probability'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.21：按预测得分/概率排序的模型输出
- en: In practice, we may use the ROC curve to select a threshold that results in
    an appropriate balance between false positives and false negatives. Furthermore,
    AUC is a good metric to compare the difference in performance between various
    models.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可能使用 ROC 曲线来选择一个阈值，从而在假正例和假负例之间取得适当的平衡。此外，AUC 是比较不同模型性能差异的一个好指标。
- en: In the last step, we visualized the decision tree using the `plot_tree` function.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一步中，我们使用`plot_tree`函数可视化了决策树。
- en: There’s more...
  id: totrans-550
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We have already covered the basics of using an ML model (in our case, a decision
    tree) to solve a binary classification task. We have also gone through the most
    popular classification evaluation metrics. However, there are still a few interesting
    topics to at least mention.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了使用机器学习模型（在我们的案例中是决策树）解决二分类任务的基础内容，并且讲解了最常用的分类评估指标。然而，仍然有一些有趣的主题值得至少提及。
- en: Diving deeper into classification evaluation metrics
  id: totrans-552
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更深入地探讨分类评估指标
- en: One of the metrics we have covered quite extensively was the ROC curve. One
    issue with it is that it loses its credibility when it comes to evaluating the
    performance of the model when we are dealing with (severe) class imbalance. In
    such cases, we should use another curve—the **Precision-Recall curve**. That is
    because, for calculating both precision and recall, we do not use the true negatives,
    and only consider the correct prediction of the minority class (the positive one).
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经深入探讨了一个常见的评估指标——ROC 曲线。它的一个问题是，在处理（严重）类别不平衡时，它在评估模型表现时失去了可信度。在这种情况下，我们应该使用另一种曲线——**精确率-召回率曲线**。这是因为，在计算精确率和召回率时，我们不使用真正负类，而只考虑少数类（即正类）的正确预测。
- en: 'We start by extracting the predicted scores/probabilities and calculating precision
    and recall for different thresholds:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先提取预测得分/概率，并计算不同阈值下的精确率和召回率：
- en: '[PRE66]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: As we do not actually need the thresholds, we substitute that output of the
    function with an underscore.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们实际上并不需要阈值，我们将该函数的输出替换为下划线。
- en: 'Having calculated the required elements, we can plot the curve:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 计算完所需元素后，我们可以绘制该曲线：
- en: '[PRE67]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码片段后会生成以下图表：
- en: '![](../Images/B18112_13_22.png)'
  id: totrans-560
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_22.png)'
- en: 'Figure 13.22: Precision-recall curve of the fitted decision tree classifier'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.22：拟合的决策树分类器的精确率-召回率曲线
- en: 'Similar to the ROC curve, we can analyze the Precision-Recall curve as follows:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 ROC 曲线，我们可以如下分析精确率-召回率曲线：
- en: Each point in the curve corresponds to the values of precision and recall for
    a different decision threshold.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线中的每个点都对应一个不同决策阈值下的精确率和召回率值。
- en: A decision threshold of 0 results in precision = 0 and recall = 1.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当决策阈值为 0 时，精确率 = 0，召回率 = 1。
- en: A decision threshold of 1 results in precision = 1 and recall = 0.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当决策阈值为 1 时，精确率 = 1，召回率 = 0。
- en: As a summary metric, we can approximate the area under the Precision-Recall
    curve.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为总结性指标，我们可以通过近似计算精确率-召回率曲线下的面积。
- en: The PR-AUC ranges from 0 to 1, where 1 indicates the perfect model.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PR-AUC 的范围从 0 到 1，其中 1 表示完美的模型。
- en: A model with a PR-AUC of 1 can identify all the positive observations (perfect
    recall), while not wrongly labeling a single negative observation as a positive
    one (perfect precision). The perfect point is located in (1, 1), that is, the
    top-right corner of the plot.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 PR-AUC 为 1 的模型可以识别所有正类样本（完美召回率），同时不会错误地将任何负类样本标记为正类（完美精确率）。完美的点位于（1, 1），即图表的右上角。
- en: We can consider models that bow toward the (1, 1) point as skillful.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以认为那些弯向（1, 1）点的模型是有技巧的。
- en: 'One potential issue with the PR-Curve in *Figure 13.22* is that it might be
    overly optimistic due to the undertaken interpolations when plotting the values
    of precision and recall for each threshold. A more realistic representation can
    be obtained using the following snippet:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.22* 中的 PR 曲线可能存在一个潜在问题，那就是由于在绘制每个阈值的精确率和召回率时进行的插值，它可能会显得过于乐观。通过以下代码片段，可以获得更为现实的表示：'
- en: '[PRE68]'
  id: totrans-571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码片段后会生成以下图表：
- en: '![](../Images/B18112_13_23.png)'
  id: totrans-573
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_23.png)'
- en: 'Figure 13.23: More realistic precision-recall curve of the fitted decision
    tree classifier'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.23：拟合的决策树分类器的更为现实的精确率-召回率曲线
- en: First, we can see that even though the shape is different, we can easily recognize
    the pattern and what the interpolation actually does. We can imagine connecting
    the extreme points of the plot with the single point (values of ~0.4 for both
    metrics), which would result in the shape obtained using interpolation.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以看到，尽管形状不同，但我们可以轻松识别出图形的模式以及插值实际的作用。我们可以想象将图表的极端点与单个点（两个指标的值大约为 0.4）连接起来，这样就能得到通过插值得到的形状。
- en: Second, we can also see that the score decreased quite substantially (from 0.45
    to 0.28). In the first case, we obtained the score using trapezoidal interpolation
    of the PR curve (`auc(precision, recall)` in `scikit-learn`). In the second case,
    the score is actually another metric—average precision. **Average precision**
    summarizes a precision-recall curve as the weighted mean of precisions achieved
    at each threshold, where the weights are calculated as the increase in recall
    from the previous threshold.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们还可以看到得分大幅下降（从0.45降至0.28）。在第一个案例中，我们通过PR曲线的梯形插值法计算得分（`auc(precision, recall)`在`scikit-learn`中）。在第二个案例中，得分实际上是另一种指标——平均精度。**平均精度**将精确度-召回曲线总结为每个阈值下精确度的加权平均，其中权重是通过从前一个阈值到当前阈值的召回率增加量来计算的。
- en: Even though these two metrics produce very similar estimates in many cases,
    they are fundamentally different. The first approach uses an overly optimistic
    linear interpolation and its effect might be more pronounced when the data is
    highly skewed/imbalanced.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两种指标在许多情况下产生非常相似的估计值，但它们在本质上是不同的。第一种方法使用了过于乐观的线性插值，并且其影响可能在数据高度偏斜/不平衡时更为明显。
- en: 'We have already covered the F1-Score, which was the harmonic mean of precision
    and recall. Actually, it is a specific case of a more general metric called the
    ![](../Images/B18112_13_001.png)-Score, where the ![](../Images/B18112_13_002.png)
    factor defines how much weight is put on recall, while precision has a weight
    of 1\. To make sure that the weights sum up to one, both are normalized by dividing
    them by ![](../Images/B18112_13_003.png). Such a definition of the score implies
    the following:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍过F1得分，它是精确度和召回率的调和平均数。实际上，它是一个更一般的指标的特例，这个指标被称为![](../Images/B18112_13_001.png)-得分，其中![](../Images/B18112_13_002.png)因子定义了召回率的权重，而精确度的权重为1。为了确保权重的和为1，两者都通过![](../Images/B18112_13_003.png)进行归一化。这样的得分定义意味着以下几点：
- en: '![](../Images/B18112_13_004.png)—more weight is put on recall'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](../Images/B18112_13_004.png)——将更多权重放在召回率上'
- en: '![](../Images/B18112_13_005.png)—the same as the F1-Score, so recall and precision
    are treated equally'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](../Images/B18112_13_005.png)——与F1得分相同，因此召回率和精确度被视为同等重要'
- en: '![](../Images/B18112_13_006.png)—more weight is put on precision'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](../Images/B18112_13_006.png)——将更多权重放在精确度上'
- en: 'Some potential pitfalls of using precision, recall, or F1-Score include the
    fact that those metrics are asymmetric, that is, they focus on the positive class.
    When looking at their formulas, we can clearly see that they never account for
    the true negative category. That is exactly what **Matthew’s correlation coefficient**
    (also known as the *phi-coefficient)* is trying to overcome:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 使用精确度、召回率或F1得分的一些潜在陷阱包括这些指标是非对称的，也就是说，它们侧重于正类。通过查看它们的公式，我们可以清楚地看到它们从未考虑真实负类。这正是**Matthew相关系数**（也叫*phi系数*）试图克服的问题：
- en: '![](../Images/B18112_13_007.png)'
  id: totrans-583
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_007.png)'
- en: 'Analyzing the formula reveals the following:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 分析公式揭示了以下几点：
- en: All of the elements of the confusion matrix are taken into account while calculating
    the score
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算得分时，混淆矩阵的所有元素都被考虑在内
- en: The formula looks similar to the one used for calculating Pearson’s correlation
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该公式类似于用于计算皮尔逊相关系数的公式
- en: MCC treats the true class and the predicted class as two binary variables, and
    effectively calculates their correlation coefficient
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MCC将真实类别和预测类别视为两个二元变量，并有效地计算它们的相关系数
- en: MCC has values between -1 (a classifier always misclassifying) and 1 (a perfect
    classifier). A value of 0 indicates that the classifier is no better than random
    guessing. Overall, as MCC is a symmetric metric, in order to achieve a high value
    the classifier must be doing well in predicting both the positive and negative
    classes.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: MCC的值介于-1（分类器总是错误分类）和1（完美分类器）之间。值为0表示分类器不比随机猜测好。总体而言，由于MCC是一个对称指标，要获得高值，分类器必须在预测正类和负类时都表现良好。
- en: As MCC is not as intuitive and easy to interpret as F1-Score, it might be a
    good metric to use when the cost of low precision and low recall is unknown or
    unquantifiable. Then, MCC can be better than F1-Score as it provides a more balanced
    (symmetric) evaluation of a classifier.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MCC不像F1得分那样直观且容易解释，它可能是一个较好的指标，尤其是在低精度和低召回率的代价未知或无法量化时。在这种情况下，MCC可能比F1得分更好，因为它提供了一个更平衡（对称）的分类器评估。
- en: Visualizing decision trees using dtreeviz
  id: totrans-590
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用dtreeviz可视化决策树
- en: The default plotting functionalities in `scikit-learn` can definitely be considered
    good enough for visualizing decision trees. However, we can take it a step further
    using the `dtreeviz` library.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 中的默认绘图功能绝对可以认为足够好，用于可视化决策树。然而，我们可以通过使用 `dtreeviz` 库将其提升到一个新层次。'
- en: 'First, we import the library:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入库：
- en: '[PRE69]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Then, we train a smaller decision tree with a maximum depth of 3\. We do so
    just in order to make the visualization easier to read. Unfortunately, there is
    no option in `dtreeviz` to plot only *x* levels of a tree:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们训练一个最大深度为 3 的较小决策树。这样做只是为了使可视化更易于阅读。不幸的是，`dtreeviz`没有选项仅绘制树的 *x* 层级：
- en: '[PRE70]'
  id: totrans-595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Lastly, we plot the tree:'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们绘制树：
- en: '[PRE71]'
  id: totrans-597
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Running the snippet generates the following plot:'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码片段生成以下图表：
- en: '![](../Images/B18112_13_24.png)'
  id: totrans-599
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_24.png)'
- en: 'Figure 13.24: Decision tree visualized using dtreeviz'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.24：使用 dtreeviz 可视化的决策树
- en: Compared to the previously generated plots, the ones created with `dtreeviz`
    additionally show the distribution of the feature used for splitting (separately
    for each class) together with the split value. What is more, the leaf nodes are
    presented as pie charts.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前生成的图相比，使用`dtreeviz`创建的图表额外展示了用于分割的特征分布（针对每个类别分别显示），并附有分割值。此外，叶节点以饼图的形式呈现。
- en: For more examples of using `dtreeviz`, including adding a path following a particular
    observation through all the splits in the tree, please refer to the notebook in
    the book’s GitHub repository.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多使用 `dtreeviz` 的示例，包括在树的所有分割中添加一个跟踪特定观测值的路径，请参考书籍 GitHub 仓库中的笔记本。
- en: See also
  id: totrans-603
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Information on the dangers of using ROC-AUC as a performance evaluation metric:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ROC-AUC 作为性能评估指标的危险性信息：
- en: 'Lobo, J. M., Jiménez‐Valverde, A., & Real, R. (2008). “AUC: a misleading measure
    of the performance of predictive distribution models.” *Global Ecology and Biogeography*,
    17(2), 145-151.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lobo, J. M., Jiménez‐Valverde, A., & Real, R. (2008). “AUC：一个误导性的预测分布模型性能度量。”
    *全球生态学与生物地理学*，17(2)，145-151。
- en: Sokolova, M. & Lapalme, G. (2009). “A systematic analysis of performance measures
    for classification tasks.” *Information Processing and Management*, 45(4), 427-437.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sokolova, M. & Lapalme, G. (2009). “分类任务性能度量的系统分析。” *信息处理与管理*，45(4)，427-437。
- en: 'More information about the Precision-Recall curve:'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 关于精确度-召回率曲线的更多信息：
- en: Davis, J. & Goadrich, M. (2006, June). “The relationship between Precision-Recall
    and ROC curves.” In *Proceedings of the 23rd international conference on Machine
    learning* (pp. 233-240).
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Davis, J. & Goadrich, M. (2006年6月). “精确度-召回率与 ROC 曲线的关系。”发表于 *第23届国际机器学习会议论文集*（第233-240页）。
- en: 'Additional resources on decision trees:'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 关于决策树的附加资源：
- en: Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984) *Classification and
    Regression Trees*. Chapman & Hall, Wadsworth, New York.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984) *分类与回归树*。Chapman &
    Hall，Wadsworth，New York。
- en: Breiman, L. (2017). *Classification and Regression Trees*. Routledge.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Breiman, L. (2017). *分类与回归树*。Routledge。
- en: Organizing the project with pipelines
  id: totrans-612
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用管道组织项目
- en: In the previous recipes, we showed all the steps required to build a machine
    learning model—starting with loading data, splitting it into training and test
    sets, imputing missing values, encoding categorical features, and ultimately fitting
    a decision tree classifier.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们展示了构建机器学习模型所需的所有步骤——从加载数据、将其划分为训练集和测试集、填补缺失值、编码分类特征，到最终拟合决策树分类器。
- en: The process requires multiple steps to be executed in a certain order, which
    can sometimes be tricky with a lot of modifications to the pipeline mid-work.
    That is why `scikit-learn` introduced pipelines. By using pipelines, we can sequentially
    apply a list of transformations to the data, and then train a given estimator
    (model).
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程需要按照一定顺序执行多个步骤，在进行大量管道修改时有时会变得复杂。这就是 `scikit-learn` 引入管道的原因。通过使用管道，我们可以依次将一系列转换应用于数据，然后训练给定的估算器（模型）。
- en: One important point to be aware of is that the intermediate steps of the pipeline
    must have the `fit` and `transform` methods, while the final estimator only needs
    the `fit` method.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个重要点是，管道的中间步骤必须具有 `fit` 和 `transform` 方法，而最终的估算器只需要 `fit` 方法。
- en: In `scikit-learn`'s terminology, we refer to objects containing the `fit` and
    `transform` methods as **transformers**. We use those to clean and preprocess
    data. An example could be the `OneHotEncoder` we have already covered. Similarly,
    we use the term **estimators** for objects with the `fit` and `predict` methods.
    Those are ML models, such as the `DecisionTreeClassifier`.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 在`scikit-learn`的术语中，我们将包含`fit`和`transform`方法的对象称为**变换器**。我们用它们来清洗和预处理数据。一个例子是我们已经讨论过的`OneHotEncoder`。类似地，我们使用**估计器**一词来指代包含`fit`和`predict`方法的对象。它们是机器学习模型，例如`DecisionTreeClassifier`。
- en: 'Using pipelines has several benefits:'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 使用管道有几个好处：
- en: The flow is much easier to read and understand—the chain of operations to be
    executed on given columns is clear.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流程更加容易阅读和理解——对给定列执行的操作链条清晰可见。
- en: Makes it easier to avoid data leakage, for example, when scaling the training
    set and then using cross-validation.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使避免数据泄漏变得更加容易，例如，在缩放训练集并使用交叉验证时。
- en: The order of steps is enforced by the pipeline.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤的顺序由管道强制执行。
- en: Increased reproducibility.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高了可重复性。
- en: In this recipe, we show how to create the entire project’s pipeline, from loading
    the data to training the classifier.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们展示了如何创建整个项目的管道，从加载数据到训练分类器。
- en: How to do it...
  id: totrans-623
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Execute the following steps to build the project’s pipeline:'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来构建项目的管道：
- en: 'Import the libraries:'
  id: totrans-625
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE72]'
  id: totrans-626
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Load the data, separate the target, and create the stratified train-test split:'
  id: totrans-627
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据，分离目标变量，并创建分层的训练-测试集：
- en: '[PRE73]'
  id: totrans-628
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Prepare lists of numerical/categorical features:'
  id: totrans-629
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备数值/类别特征的列表：
- en: '[PRE74]'
  id: totrans-630
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Define the numerical pipeline:'
  id: totrans-631
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数值管道：
- en: '[PRE75]'
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Define the categorical pipeline:'
  id: totrans-633
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义类别管道：
- en: '[PRE76]'
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Define the `ColumnTransformer` object:'
  id: totrans-635
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`ColumnTransformer`对象：
- en: '[PRE77]'
  id: totrans-636
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Define the full pipeline including the decision tree model:'
  id: totrans-637
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义包括决策树模型的完整管道：
- en: '[PRE78]'
  id: totrans-638
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Fit the pipeline to the data:'
  id: totrans-639
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将管道拟合到数据：
- en: '[PRE79]'
  id: totrans-640
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Executing the snippet generates the following preview of the pipeline:'
  id: totrans-641
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码段会生成管道的以下预览：
- en: '![](../Images/B18112_13_25.png)'
  id: totrans-642
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_25.png)'
- en: 'Figure 13.25: Preview of the pipeline'
  id: totrans-643
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.25：管道的预览
- en: 'Evaluate the performance of the entire pipeline:'
  id: totrans-644
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估整个管道的性能：
- en: '[PRE80]'
  id: totrans-645
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-646
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码段会生成以下图表：
- en: '![](../Images/B18112_13_26.png)'
  id: totrans-647
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_26.png)'
- en: 'Figure 13.26: The performance evaluation report of the fitted pipeline'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.26：拟合管道的性能评估报告
- en: We see that the performance of the model is very similar to what we achieved
    by carrying out all the steps separately. Considering how little has changed,
    this is exactly what we expected to achieve.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，模型的性能与我们通过单独执行所有步骤所取得的结果非常相似。考虑到变化如此之小，这正是我们预期要实现的。
- en: How it works...
  id: totrans-650
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 1*, we imported the required libraries. The list can look a bit daunting,
    but that is due to the fact that we need to combine multiple functions/classes
    used in the previous recipes.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 1*中，我们导入了所需的库。列表可能看起来有些令人畏惧，但这是因为我们需要结合多个在前面的配方中使用的函数/类。
- en: In *Step 2*, we loaded the data from a CSV file, separated the target variable
    from the features, and lastly created a stratified train-test split. Then, we
    also created two lists containing the names of the numerical and categorical features.
    We did so as we will apply different transformations depending on the data type
    of the feature. To select the appropriate columns, we used the `select_dtypes`
    method.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 2*中，我们从CSV文件加载数据，将目标变量与特征分开，最后创建了一个分层的训练-测试集。然后，我们还创建了两个列表，分别包含数值特征和类别特征的名称。这样做是因为我们将根据特征的数据类型应用不同的变换。为了选择适当的列，我们使用了`select_dtypes`方法。
- en: In *Step 4*, we defined the first `Pipeline` containing the transformations
    we wanted to apply to numerical features. As a matter of fact, we only wanted
    to impute the missing values of the features using the median value. While creating
    an instance of the `Pipeline` class, we provided a list of tuples containing the
    steps, each of the tuples consisting of the name of the step (for easier identification
    and accessing) and the class we wanted to use. In this case, it was the `SimpleImputer`
    class we covered in the *Identifying and dealing with missing values* recipe.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们定义了第一个`Pipeline`，其中包含了我们希望应用于数值特征的变换。事实上，我们只想使用中位数值填补特征的缺失值。在创建`Pipeline`类的实例时，我们提供了一个包含步骤的元组列表，每个元组由步骤的名称（便于识别和访问）和我们希望使用的类组成。在这种情况下，我们使用的是在*识别和处理缺失值*配方中涉及的`SimpleImputer`类。
- en: In *Step 5*, we prepared a similar pipeline for categorical features. This time,
    however, we chained two different operations—the imputer (using the most frequent
    value) and the one-hot encoder. For the encoder, we also specified a list of lists
    called `cat_list`, in which we listed all the possible categories. We based that
    information only on `X_train`. We did so as preparation for the next recipe, in
    which we introduce cross-validation, during which it can happen that some of the
    random draws will not contain all of the available categories.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们为类别特征准备了一个类似的流水线。不过，这一次，我们链式操作了两个不同的操作——插补器（使用最频繁的值）和独热编码器。对于编码器，我们还指定了一个名为`cat_list`的列表，其中列出了所有可能的类别。我们仅基于`X_train`提供了这些信息。这样做是为了为下一个步骤做准备，在这个步骤中，我们引入了交叉验证，期间可能会出现某些随机抽样不包含所有可用类别的情况。
- en: In *Step 6*, we defined the `ColumnTransformer` object. In general, we use a
    `ColumnTransformer` when we want to apply separate transformations to different
    groups of columns/features. In our case, we have separate pipelines for numerical
    and categorical features. Again, we passed a list of tuples, where each tuple
    contains a name, one of the pipelines we defined before, and a list of columns
    to which the transformations should be applied. We also specified `remainder="drop"`,
    to drop any extra columns to which no transformations were applied. In this case,
    the transformations were applied to all features, so no columns were dropped.
    One thing to bear in mind is that `ColumnTransformer` returns `numpy` arrays instead
    of `pandas` DataFrames!
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 6*中，我们定义了`ColumnTransformer`对象。通常，当我们希望对不同组的列/特征应用不同的转换时，会使用`ColumnTransformer`。在我们的案例中，我们为数值特征和类别特征分别定义了不同的流水线。同样，我们传递了一个元组列表，每个元组包含一个名称、我们之前定义的流水线之一和一个需要应用转换的列列表。我们还指定了`remainder="drop"`，以删除未应用任何转换的额外列。在这种情况下，所有特征都应用了转换，因此没有列被删除。需要注意的是，`ColumnTransformer`返回的是`numpy`数组，而不是`pandas`数据框！
- en: 'Another useful class available in `scikit-learn` is `FeatureUnion`. We can
    use it when we want to transform the same input data in different ways and then
    use those outputs as features. For example, we could be working with text data
    and want to apply two transformations: **TF-IDF** (term frequency-inverse document
    frequency) vectorization and extracting the text’s length. The outputs of those
    should be appended to the original DataFrame, so we could use them as features
    for our model.'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`中还有一个有用的类是`FeatureUnion`。当我们希望以不同的方式转换相同的输入数据，并将这些输出作为特征使用时，可以使用它。例如，我们可能正在处理文本数据，并希望应用两种变换：**TF-IDF**（词频-逆文档频率）向量化和提取文本的长度。这些输出应该附加到原始数据框中，以便我们将它们作为模型的特征。'
- en: In *Step 7*, we once again used a `Pipeline` to chain the `preprocessor` (the
    previously defined `ColumnTransformer` object) with the decision tree classifier
    (for reproducibility’s sake, we set the random state to 42). The last two steps
    involved fitting the entire pipeline to the data and using the custom function
    to evaluate its performance.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 7*中，我们再次使用了`Pipeline`将`preprocessor`（之前定义的`ColumnTransformer`对象）与决策树分类器链式连接（为了可重复性，我们将随机状态设置为42）。最后两步涉及将整个流水线拟合到数据，并使用自定义函数评估其性能。
- en: The `performance_evaluation_report` function was built in such a way that it
    works with any estimator or `Pipeline` that has the `predict` and `predict_proba`
    methods. Those are used to obtain predictions and their corresponding scores/probabilities.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: '`performance_evaluation_report`函数的构建方式使其可以与任何具有`predict`和`predict_proba`方法的估计器或`Pipeline`一起使用。这些方法用于获取预测值及其对应的分数/概率。'
- en: There’s more...
  id: totrans-659
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Adding custom transformers to a pipeline
  id: totrans-660
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向流水线添加自定义变换器
- en: 'In this recipe, we showed how to create the entire pipeline for a data science
    project. However, there are many other transformations we can apply to data as
    preprocessing steps. Some of them include:'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们展示了如何为数据科学项目创建整个流水线。然而，还有许多其他的变换可以作为预处理步骤应用于数据。其中一些包括：
- en: '**Scaling numerical features**: In other words, changing the range of the features
    due to the fact that different features are measured on different scales, as that
    can introduce bias to the model. We should mostly be concerned with feature scaling
    when dealing with models that calculate some kind of distance between features
    (such as k-Nearest Neighbors) or linear models. Some popular scaling options from
    `scikit-learn` include `StandardScaler` and `MinMaxScaler`.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discretizing continuous variables**: We can transform a continuous variable
    (such as age) into a finite number of bins (for example: <25, 26-50, and >51 years).
    When we want to create specific bins, we can use the `pd.cut` function, while
    `pd.qcut` can be used for splitting based on quantiles.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transforming/removing outliers**: During the EDA, we often see feature values
    that are extreme and can be caused by some kind of error (for example, adding
    an extra digit to the age) or are simply incompatible with the rest (for example,
    a multimillionaire among a sample of middle-class citizens). Such outliers can
    skew the results of the model, and it is good practice to somehow deal with them.
    One solution would be to remove them altogether, but this can have an impact on
    the model’s ability to generalize. We can also bring them closer to regular values.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML models based on decision trees do not require any scaling.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we show how to create a custom transformer to detect and modify
    outliers. We apply a simple rule of thumb—we cap the values above/below the average
    +/- 3 standard deviations. We create a dedicated transformer for this task, so
    we can incorporate the outlier treatment into the previously established pipeline:'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the base estimator and transformer classes from `sklearn`:'
  id: totrans-667
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-668
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: In order for the custom transformer to be compatible with scikit-learn’s pipelines,
    it must have methods such as `fit`, `transform`, `fit_transform`, `get_params`,
    and `set_params`.
  id: totrans-669
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We could define all of those manually, but a definitely more appealing approach
    is to use Python’s **class inheritance** to make the process easier. That is why
    we imported the `BaseEstimator` and `TransformerMixin` classes from `scikit-learn`.
    By inheriting from `TransformerMixin`, we do not need to specify the `fit_transform`
    method, while inheriting from `BaseEstimator` automatically provides the `get_params`
    and `set_params` methods.
  id: totrans-670
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As a learning experience, it definitely makes sense to dive into the code of
    at least some of the more popular transformers/estimators in `scikit-learn`. By
    doing so, we can learn a lot about the best practices of object-oriented programming
    and observe (and appreciate) how all of those classes consistently follow the
    same set of guidelines/principles.
  id: totrans-671
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the `OutlierRemover` class:'
  id: totrans-672
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-673
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The class can be broken down into the following components:'
  id: totrans-674
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the `__init__` method, we stored the number of standard deviations that determines
    whether observations will be treated as outliers (the default is 3)
  id: totrans-675
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`__init__`方法中，我们存储了确定观察值是否会被视为异常值的标准差数量（默认为3）。
- en: In the `fit` method, we stored the upper and lower thresholds for being considered
    an outlier, as well as the number of features in general
  id: totrans-676
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`fit`方法中，我们存储了作为异常值的上下限阈值，以及一般的特征数量。
- en: In the `transform` method, we capped all the values exceeding the thresholds
    determined in the `fit` method
  id: totrans-677
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`transform`方法中，我们对所有超过`fit`方法中确定的阈值的值进行了限制。
- en: Alternatively, we could have used the `clip` method of a `pandas` DataFrame
    to cap the extreme values.
  id: totrans-678
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，我们也可以使用`pandas` DataFrame的`clip`方法来限制极端值。
- en: One known limitation of this class is that it does not handle missing values.
    That is why we raise a `ValueError` when there are any missing values. Also, we
    use the `OutlierRemover` after the imputation in order to avoid that issue. We
    could, of course, account for the missing values in the transformer, however,
    this would make the code longer and less readable. We leave this as an exercise
    for the reader. Please refer to the definition of `SimpleImputer` in `scikit-learn`
    for an example of how to mask missing values while building transformers.
  id: totrans-679
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该类的一个已知限制是它无法处理缺失值。这就是为什么当存在缺失值时我们会抛出`ValueError`的原因。此外，我们在插补后使用`OutlierRemover`来避免这个问题。当然，我们也可以在转换器中处理缺失值，但这会使代码更长且可读性更差。我们将此作为留给读者的练习。请参考`scikit-learn`中`SimpleImputer`的定义，了解如何在构建转换器时掩盖缺失值的示例。
- en: 'Add the `OutlierRemover` to the numerical pipeline:'
  id: totrans-680
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`OutlierRemover`添加到数值管道中：
- en: '[PRE83]'
  id: totrans-681
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Execute the rest of the pipeline to compare the results:'
  id: totrans-682
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行管道的其余部分以比较结果：
- en: '[PRE84]'
  id: totrans-683
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-684
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码片段会生成以下图表：
- en: '![](../Images/B18112_13_27.png)'
  id: totrans-685
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_27.png)'
- en: 'Figure 13.27: The performance evaluation report of the fitted pipeline (including
    treating outliers)'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.27：拟合后的管道性能评估报告（包括处理异常值）
- en: Including the outlier-capping transformation did not result in any significant
    changes in the performance of the entire pipeline.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 包括异常值限制转换并未对整个管道的性能产生任何显著变化。
- en: Accessing the elements of the pipeline
  id: totrans-688
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 访问管道的元素
- en: While pipelines make our project easier to reproduce and less prone to data
    leakage, they come with a small disadvantage. Accessing the elements of a pipeline
    for further inspection or substitution becomes a bit more difficult. Let’s illustrate
    with a few examples.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然管道使我们的项目更易于复现并减少数据泄露的风险，但它也有一个小缺点。访问管道元素以进行进一步检查或替换变得有点困难。让我们通过几个例子来说明。
- en: 'We start by displaying the entire pipeline represented as a dictionary by using
    the following snippet:'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下代码片段开始显示整个管道，以字典形式表示：
- en: '[PRE85]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Using that structure (not printed here for brevity), we can access the ML model
    at the end of the pipeline using the name we assigned to it:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该结构（为简洁起见，此处未打印），我们可以通过我们分配给它的名称访问管道末端的机器学习模型：
- en: '[PRE86]'
  id: totrans-693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'It gets a bit more complicated when we want to dive into the `ColumnTransformer`.
    Let’s assume that we would like to inspect the upper bands (under the `upper_bands_`
    attribute) of the fitted `OutlierRemover`. To do so, we have to use the following
    snippet:'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想深入了解`ColumnTransformer`时，事情变得有点复杂。假设我们想检查拟合后的`OutlierRemover`的上带限（在`upper_bands_`属性下）。为此，我们必须使用以下代码片段：
- en: '[PRE87]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: First, we followed the same approach as we have employed when accessing the
    estimator at the end of the pipeline. This time, we just used the name of the
    step containing the `ColumnTransformer`. Then, we used the `named_transformers_`
    attribute to access the deeper levels of the transformer. We selected the numerical
    pipeline and then the outlier treatment step using their corresponding names.
    Lastly, we accessed the upper bands of the custom transformer.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们遵循了与访问管道末端估计器时相同的方法。这一次，我们只使用包含`ColumnTransformer`的步骤名称。然后，我们使用`named_transformers_`属性访问转换器的更深层次。我们选择了数值管道，然后使用它们相应的名称选择了异常值处理步骤。最后，我们访问了自定义转换器的上带限。
- en: While accessing the steps of the `ColumnTransformer`, we could have used the
    `transformers_` attribute instead of the `named_transformers_`. However, then
    the output would be a list of tuples (the same ones as we have manually provided
    when defining the `ColumnTransformer`) and we have to access their elements using
    integer indices. We show how to access the upper bands using the `transformers_`
    attribute in the notebook available on GitHub.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 在访问`ColumnTransformer`的步骤时，我们本可以使用`transformers_`属性，而不是`named_transformers_`。然而，那样的话，输出将是一个元组列表（与我们在定义`ColumnTransformer`时手动提供的相同），我们必须使用整数索引来访问其元素。我们在GitHub上提供的笔记本中展示了如何使用`transformers_`属性访问上层数据。
- en: Tuning hyperparameters using grid searches and cross-validation
  id: totrans-698
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用网格搜索和交叉验证调整超参数
- en: In the previous recipes, we have used a decision tree model to try to predict
    whether a customer will default on their loan. As we have seen, the tree became
    quite massive with a depth of 44 levels, which prevented us from plotting it.
    However, this can also mean that the model is overfitted to the training data
    and will not perform well on unseen data. Maximum depth is actually one of the
    decision tree’s hyperparameters, which we can tune to achieve better performance
    by finding a balance between underfitting and overfitting (bias-variance trade-off).
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们使用了决策树模型来预测客户是否会违约。正如我们所见，树的深度达到了44级，这使得我们无法将其绘制出来。然而，这也可能意味着模型过拟合了训练数据，在未见过的数据上表现不佳。最大深度实际上是决策树的超参数之一，我们可以通过调整它来在欠拟合和过拟合之间找到平衡（偏差-方差权衡），以提高性能。
- en: 'First, we outline some properties of hyperparameters:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们概述一些超参数的属性：
- en: External characteristics of the model
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的外部特征
- en: Not estimated based on data
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未基于数据进行估算
- en: Can be considered the model’s settings
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以视为模型的设置
- en: Set before the training phase
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练阶段之前设置
- en: Tuning them can result in better performance
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整它们可以提高性能
- en: 'We can also consider some properties of parameters:'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以考虑一些参数的属性：
- en: Internal characteristics of the model
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的内部特征
- en: Estimated based on data, for example, the coefficients of linear regression
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于数据进行估算，例如线性回归的系数
- en: Learned during the training phase
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练阶段学习到的
- en: 'While tuning the model’s hyperparameters, we would like to evaluate its performance
    on data that was not used for training. In the *Splitting data into training and
    test sets* recipe, we mentioned that we can create an additional validation set.
    The validation set is used explicitly to tune the model’s hyperparameters, before
    the ultimate evaluation using the test set. However, creating the validation set
    comes at a price: data used for training (and possibly testing) is sacrificed,
    which can be especially harmful when dealing with small datasets.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整模型的超参数时，我们希望评估其在未用于训练的数据上的表现。在*将数据划分为训练集和测试集*的示例中，我们提到可以创建一个额外的验证集。验证集专门用于调整模型的超参数，在最终使用测试集评估之前。然而，创建验证集是有代价的：用于训练（并可能用于测试）的数据被牺牲掉，这在处理小数据集时尤其有害。
- en: That is the reason why **cross-validation** became so popular. It is a technique
    that allows us to obtain reliable estimates of the model’s generalization error.
    It is easiest to understand how it works with an example. When doing *k*-fold
    cross-validation, we randomly split the training data into *k* folds. Then, we
    train the model using *k*-1 folds and evaluate the performance on the *k*^(th)
    fold. We repeat this process *k* times and average the resulting scores.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是**交叉验证**变得如此流行的原因。它是一种技术，能够帮助我们可靠地估计模型的泛化误差。通过一个例子，最容易理解它是如何工作的。在进行*k*折交叉验证时，我们将训练数据随机拆分为*k*折。然后，我们使用*k*-1折进行训练，并在第*k*折上评估模型的表现。我们重复这个过程*k*次，并对结果得分取平均值。
- en: A potential drawback of cross-validation is the computational cost, especially
    when paired together with a grid search for hyperparameter tuning.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证的一个潜在缺点是计算成本，尤其是在与网格搜索调参一起使用时。
- en: '![A picture containing bar chart  Description automatically generated](../Images/B18112_13_28.png)'
  id: totrans-713
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing bar chart  Description automatically generated](../Images/B18112_13_28.png)'
- en: 'Figure 13.28: Scheme of a 5-fold cross-validation procedure'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.28：5折交叉验证过程示意图
- en: We already mentioned that **grid search** is a technique used for tuning hyperparameters.
    The underlying idea is to create a grid of all possible hyperparameter combinations
    and train the model using each one of them. Due to its exhaustive, brute-force
    search, the approach guarantees to find the optimal parameter within the grid.
    The drawback is that the size of the grid grows exponentially when adding more
    parameters or more considered values. The number of required model fits and predictions
    increases significantly if we additionally use cross-validation!
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到过**网格搜索**是一种用于调整超参数的技术。其基本思路是创建所有可能的超参数组合网格，并使用每一种组合训练模型。由于其详尽的暴力搜索方法，这种方法可以保证在网格中找到最优的参数。缺点是，当增加更多的参数或考虑更多的值时，网格的大小呈指数级增长。如果我们还使用交叉验证，所需的模型拟合和预测数量将显著增加！
- en: 'Let’s illustrate this with an example by assuming that we are training a model
    with two hyperparameters: *a* and *b*. We define a grid that covers the following
    values of the hyperparameters: `{"a": [1, 2, 3], "b": [5, 6]}`. This means that
    there are 6 unique combinations of hyperparameters in our grid and the algorithm
    will fit the model 6 times. If we also use a 5-fold cross-validation procedure,
    it will result in 30 unique models being fitted in the grid search procedure!'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们通过一个例子来说明，假设我们正在训练一个具有两个超参数的模型：*a* 和 *b*。我们定义一个覆盖以下超参数值的网格：`{"a": [1, 2,
    3], "b": [5, 6]}`。这意味着我们的网格中有6种独特的超参数组合，算法将会进行6次模型拟合。如果我们还使用5折交叉验证程序，这将导致在网格搜索过程中拟合30个独特的模型！'
- en: 'As a potential solution to the problems encountered with grid search, we can
    also use the **random search** (also called **randomized grid search**). In this
    approach, we choose a random set of hyperparameters, train the model (also using
    cross-validation), return the scores, and repeat the entire process until we reach
    a predefined number of iterations or the computational time limit. Random search
    is preferred over grid search when dealing with a very large grid. That is because
    the former can explore a wider hyperparameter space and often find a hyperparameter
    set that performs very similarly to the optimal one (obtained from an exhaustive
    grid search) in a much shorter time. The only problematic question is: how many
    iterations are sufficient to find a good solution? Unfortunately, there is no
    simple answer to that. Most of the time, it is indicated by the available resources.'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解决网格搜索中遇到问题的潜在方案，我们还可以使用**随机搜索**（也称为**随机化网格搜索**）。在这种方法中，我们选择一组随机的超参数，训练模型（也使用交叉验证），返回评分，并重复整个过程，直到达到预定义的迭代次数或计算时间限制。对于非常大的网格，随机搜索优于网格搜索。因为前者可以探索更广泛的超参数空间，通常会在更短的时间内找到与最优超参数集（通过详尽的网格搜索获得）非常相似的超参数集。唯一的问题是：多少次迭代足以找到一个好的解决方案？不幸的是，无法简单回答这个问题。大多数情况下，它由可用的资源来决定。
- en: Getting ready
  id: totrans-718
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we use the decision tree pipeline created in the *Organizing
    the project with pipelines* recipe, including the outlier treatment from the *There’s
    more...* section*.*
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了在*使用管道组织项目*食谱中创建的决策树管道，包括*更多内容…*部分中的异常值处理。
- en: How to do it...
  id: totrans-720
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Execute the following steps to run both grid search and randomized search on
    the decision tree pipeline we have created in the *Organizing the project with
    pipelines* recipe:'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以在我们在*使用管道组织项目*食谱中创建的决策树管道上运行网格搜索和随机搜索：
- en: 'Import the libraries:'
  id: totrans-722
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE88]'
  id: totrans-723
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Define a cross-validation scheme:'
  id: totrans-724
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义交叉验证方案：
- en: '[PRE89]'
  id: totrans-725
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Evaluate the pipeline using cross-validation:'
  id: totrans-726
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用交叉验证评估管道：
- en: '[PRE90]'
  id: totrans-727
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Executing the snippet returns an array containing the estimator’s default score
    (accuracy) values:'
  id: totrans-728
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会返回一个包含估计器默认评分（准确度）值的数组：
- en: '[PRE91]'
  id: totrans-729
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Add extra metrics to the cross-validation:'
  id: totrans-730
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为交叉验证添加额外的度量：
- en: '[PRE92]'
  id: totrans-731
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Executing the snippet generates the following table:'
  id: totrans-732
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下表格：
- en: '![](../Images/B18112_13_29.png)'
  id: totrans-733
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_29.png)'
- en: 'Figure 13.29: The results of 5-fold cross-validation'
  id: totrans-734
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.29：5折交叉验证的结果
- en: In *Figure 13.29*, we can see the 4 requested metrics for each of the 5 cross-validation
    folds. The metrics have very similar values in each of the 5 test folds, which
    suggests that the cross-validation with stratified split worked as intended.
  id: totrans-735
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在*图 13.29*中，我们可以看到每个5折交叉验证的4个请求的度量值。这些度量值在每个测试折中非常相似，这表明使用分层拆分的交叉验证如预期般有效。
- en: 'Define the parameter grid:'
  id: totrans-736
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义参数网格：
- en: '[PRE93]'
  id: totrans-737
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Run the exhaustive grid search:'
  id: totrans-738
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行详尽的网格搜索：
- en: '[PRE94]'
  id: totrans-739
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Below we see how many models will be fitted using the exhaustive search:'
  id: totrans-740
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面我们可以看到，通过详尽搜索将拟合多少个模型：
- en: '[PRE95]'
  id: totrans-741
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'The best model from the exhaustive grid search is the following:'
  id: totrans-742
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从详尽的网格搜索中得到的最佳模型如下：
- en: '[PRE96]'
  id: totrans-743
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Evaluate the performance of the tuned pipeline:'
  id: totrans-744
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估调优后的管道性能：
- en: '[PRE97]'
  id: totrans-745
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-746
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码段会生成以下图表：
- en: '![](../Images/B18112_13_30.png)'
  id: totrans-747
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_30.png)'
- en: 'Figure 13.30: The performance evaluation report of the best pipeline identified
    by the exhaustive grid search'
  id: totrans-748
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.30：由详尽网格搜索识别出的最佳管道的性能评估报告
- en: 'Run the randomized grid search:'
  id: totrans-749
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行随机网格搜索：
- en: '[PRE98]'
  id: totrans-750
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Below we can see that the randomized search will train fewer models than the
    exhaustive one:'
  id: totrans-751
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面我们可以看到，随机搜索将训练比详尽搜索更少的模型：
- en: '[PRE99]'
  id: totrans-752
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'The best model from the randomized grid search is the following:'
  id: totrans-753
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从随机网格搜索中得到的最佳模型如下：
- en: '[PRE100]'
  id: totrans-754
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: In the randomized search, we looked at 100 random sets of hyperparameters, which
    correspond to ~1/3 of all possibilities covered by the exhaustive search. Even
    though the randomized search did not identify the same model as the best one,
    the performance of both pipelines is very similar on both training and test sets.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机搜索中，我们查看了100组随机超参数组合，这大约覆盖了详尽搜索中的1/3的可能性。尽管随机搜索没有找到与详尽搜索相同的最佳模型，但两者在训练集和测试集上的性能非常相似。
- en: How it works...
  id: totrans-756
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 2*, we defined the 5-fold cross-validation scheme. As there is no inherent
    order in the data, we used shuffling and specified the random state for reproducibility.
    Stratification ensured that each fold received a similar ratio of classes in the
    target variable. Such a setting is crucial when we are dealing with imbalanced
    classes.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤2*中，我们定义了5折交叉验证方案。由于数据本身没有固有的顺序，我们使用了洗牌并指定了随机种子以确保可重复性。分层抽样确保每一折在目标变量的类别分布上保持相似。这种设置在处理类别不平衡的问题时尤为重要。
- en: In *Step 3*, we evaluated the pipeline created in the *Organizing the project
    with pipelines* recipe using the `cross_val_score` function. We passed the estimator
    (the entire pipeline), the training data, and the cross-validation scheme as arguments
    to the function.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤3*中，我们使用`cross_val_score`函数评估了在*使用管道组织项目*这一食谱中创建的管道。我们将估计器（整个管道）、训练数据和交叉验证方案作为参数传递给该函数。
- en: We could have also provided a number to the `cv` argument (the default is 5)—in
    the case of a classification problem, it would have automatically applied the
    stratified *k*-fold cross-validation. However, by providing a custom scheme, we
    also ensured that the random state was defined and that the results were reproducible.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以为`cv`参数提供一个数字（默认值为5）——在分类问题中，它会自动应用分层*k*折交叉验证。然而，通过提供自定义方案，我们也确保了定义了随机种子并且结果是可重复的。
- en: We can clearly observe another advantage of using pipelines—we are not leaking
    any information while carrying out cross-validation. Without pipelines, we would
    fit our transformers (for example, `StandardScaler`) using the training data and
    then transform training and test sets separately. This way, we are not leaking
    any information from the test set. However, we are leaking a bit of information
    if we carry out cross-validation on such a transformed training set. That is because
    the folds used for validation were transformed using all the information from
    the training set.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以明显观察到使用管道的另一个优势——在执行交叉验证时我们不会泄漏任何信息。如果没有管道，我们会使用训练数据拟合我们的变换器（例如，`StandardScaler`），然后分别对训练集和测试集进行变换。这样，我们就不会泄漏测试集中的任何信息。然而，如果在这种已变换的训练集上进行交叉验证，我们仍然会泄漏一些信息。因为用于验证的折叠是利用整个训练集的信息进行变换的。
- en: In *Step 4*, we extended the cross-validation by using the `cross_validate`
    function. This function is more flexible in the way it allows us to use multiple
    evaluation criteria (we used accuracy, precision, recall, and the ROC AUC). Additionally,
    it records the time spent in both the training and inference steps. We printed
    the results in the form of a `pandas` DataFrame to make them easier to read. By
    default, the output of the function is a dictionary.
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤4*中，我们通过使用`cross_validate`函数扩展了交叉验证。这个函数在多个评估标准上提供了更多的灵活性（我们使用了准确率、精确度、召回率和ROC
    AUC）。此外，它还记录了训练和推断步骤所花费的时间。我们以`pandas`数据框的形式打印了结果，以便于阅读。默认情况下，该函数的输出是一个字典。
- en: 'In *Step 5*, we defined the parameter grid to be used for the grid search.
    An important point to remember here is the naming convention when working with
    `Pipeline` objects. The keys in the grid dictionary are built from the name of
    the step/model concatenated with the hyperparameter name using a double underscore.
    In this example, we searched a space created on top of three hyperparameters of
    the decision tree classifier:'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们定义了用于网格搜索的参数网格。这里需要记住的一个重要点是，使用`Pipeline`对象时的命名规范。网格字典中的键是由步骤/模型的名称与超参数名称通过双下划线连接构成的。在这个例子中，我们在决策树分类器的三个超参数上进行了搜索：
- en: '`criterion`—the metric used for determining a split, either entropy or Gini
    importance.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`criterion`—用于确定分裂的度量，可以是熵或基尼重要性。'
- en: '`max_depth`—the maximum depth of the tree.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`—树的最大深度。'
- en: '`min_samples_leaf`—the minimum number of observations in a leaf. It prevents
    the creation of trees with very few observations in leaves.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`—叶子节点中的最小观察值数。它可以防止在叶子节点中创建样本数量过少的树。'
- en: 'Additionally, we experimented with the outlier transformer, by using either
    three or four standard deviations from the mean to indicate whether an observation
    was an outlier. Please pay attention to the construction of the name, which contains
    the following pieces of information in sequence:'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还通过使用均值的三倍或四倍标准差来进行异常值变换，来指示一个观察值是否为异常值。请注意名称的构造，名称中包含了以下几个信息：
- en: '`preprocessor`—the step of the pipeline.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocessor`—管道中的步骤。'
- en: '`numerical`—which pipeline it was within the `ColumnTransformer`.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numerical`—它在`ColumnTransformer`中的哪个管道内。'
- en: '`outliers`—which step of that inner pipeline we are accessing.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outliers`—我们访问的那个内部管道的步骤。'
- en: '`n_std`—the name of the hyperparameter we wanted to specify.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_std`—我们希望指定的超参数名称。'
- en: When only tuning the estimator (model), we should directly use the names of
    the hyperparameters.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 当仅调整估算器（模型）时，我们应直接使用超参数的名称。
- en: We decided to select the best-performing decision tree model based on recall,
    that is, the percentage of all defaults correctly identified by the model. This
    evaluation metric is definitely useful in cases when we are dealing with imbalanced
    classes, for example, when predicting default or fraud. In real life, there is
    often a different cost of a false negative (predicting no default when the user
    actually defaulted) and a false positive (predicting that a good customer defaults).
    To predict defaults, we decided that we could accept the cost of more false positives,
    in return for reducing the number of false negatives (missed defaults).
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定根据召回率选择表现最佳的决策树模型，即模型正确识别的所有违约事件的百分比。当我们处理不平衡类别时，这一评估指标无疑非常有用，例如在预测违约或欺诈时。在现实生活中，假阴性（预测没有违约时，实际上用户违约了）和假阳性（预测一个好客户违约）通常有不同的成本。为了预测违约，我们决定可以接受更多的假阳性成本，以换取减少假阴性的数量（漏掉的违约）。
- en: In *Step 6*, we created an instance of the `GridSearchCV` class. We provided
    the pipeline and parameter grid as inputs. We also specified recall as the scoring
    metric to be used for selecting the best model (different metrics could have been
    used here). We also used our custom CV scheme and indicated that we wanted to
    use all available cores to speed up the computations (`n_jobs=-1`).
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 6*中，我们创建了`GridSearchCV`类的一个实例。我们将管道和参数网格作为输入提供。我们还指定了召回率作为用于选择最佳模型的评分指标（这里可以使用不同的指标）。我们还使用了自定义的交叉验证方案，并指定我们希望使用所有可用的核心来加速计算（`n_jobs=-1`）。
- en: When working with the grid search classes of `scikit-learn`, we can actually
    provide multiple evaluation metrics (specified as a list or dictionary). That
    is definitely helpful when we want to carry out a more in-depth analysis of the
    fitted models. We need to remember that when using multiple metrics, we must use
    the `refit` argument to specify which metric should be used to determine the best
    combination of hyperparameters.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`scikit-learn`的网格搜索类时，我们实际上可以提供多个评估指标（可以通过列表或字典指定）。当我们希望对拟合的模型进行更深入的分析时，这一点非常有帮助。我们需要记住的是，当使用多个指标时，必须使用`refit`参数来指定应使用哪个指标来确定最佳的超参数组合。
- en: We then used the `fit` method of the `GridSearchCV` object, just like any other
    estimator in `scikit-learn`. From the output, we saw that the grid contained 288
    different combinations of hyperparameters. For each set, we fitted five models
    (5-fold cross-validation).
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用了`GridSearchCV`对象的`fit`方法，就像在`scikit-learn`中使用其他估算器一样。从输出结果中，我们看到网格包含了288种不同的超参数组合。对于每一组，我们都进行了五次模型拟合（5折交叉验证）。
- en: '`GridSearchCV`''s default setting of `refit=True` means that after the entire
    grid search is completed, the best model has automatically been fitted once again,
    this time to the entire training set. We can then directly use that estimator
    (identified by the indicated criterion) for inference by running `classifier_gs.predict(X_test)`.'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV`的默认设置`refit=True`意味着，在整个网格搜索完成后，最佳模型会自动再次进行拟合，这次是使用整个训练集。然后，我们可以直接通过运行`classifier_gs.predict(X_test)`来使用这个估算器（根据指定的标准进行识别）进行推断。'
- en: In *Step 8*, we created an instance of a randomized grid search. It is similar
    to a regular grid search, except that the maximum number of iterations was specified.
    In this case, we tested 100 different combinations from the parameter grid, which
    was roughly 1/3 of all available combinations.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第8步*中，我们创建了一个随机化网格搜索实例。它类似于常规网格搜索，不同之处在于我们指定了最大迭代次数。在这种情况下，我们从参数网格中测试了100种不同的组合，大约是所有可用组合的1/3。
- en: 'There is one additional difference between the exhaustive and randomized approaches
    to grid search. In the latter one, we can provide a hyperparameter distribution
    instead of a list of distinct values. For example, let’s assume that we have a
    hyperparameter that describes a ratio between 0 and 1\. In the exhaustive grid
    search, we might specify the following values: `[0, 0.2, 0.4, 0.6, 0.8, 1]`. In
    the randomized search, we can use the same values and the search will randomly
    (uniformly) pick up a value from the list (there is no guarantee that all of them
    will be tested). Alternatively, we might prefer to draw a random value from the
    uniform distribution (restricted to values between 0 and 1) as the hyperparameter’s
    value.'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 穷举法和随机化法网格搜索之间还有一个额外的区别。在后者中，我们可以提供一个超参数分布，而不是一组离散的值。例如，假设我们有一个描述0到1之间比率的超参数。在穷举法网格搜索中，我们可能会指定以下值：`[0,
    0.2, 0.4, 0.6, 0.8, 1]`。在随机化搜索中，我们可以使用相同的值，搜索会从列表中随机（均匀地）选取一个值（无法保证所有值都会被测试）。或者，我们可能更倾向于从均匀分布（限制在0到1之间的值）中随机抽取一个值作为超参数的值。
- en: Under the hood, `scikit-learn` applies the following logic. If all hyperparameters
    are presented as lists, the algorithm performs sampling without replacement. If
    at least one hyperparameter is represented by a distribution, sampling with replacement
    is used instead.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，`scikit-learn`应用了以下逻辑。如果所有超参数都以列表形式呈现，算法会执行不放回的抽样。如果至少有一个超参数是通过分布表示的，则会改为使用有放回的抽样。
- en: There’s more...
  id: totrans-780
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Faster search with successive halving
  id: totrans-781
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用逐步减半实现更快的搜索
- en: For each candidate set of hyperparameters, both exhaustive and random approaches
    to grid search train a model/pipeline using all available data. `scikit-learn`
    offers an additional approach to grid search called halving grid search, which
    is based on the idea of **successive halving**.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一组候选超参数，穷举法和随机法网格搜索都会使用所有可用数据来训练一个模型/管道。`scikit-learn`还提供了一种叫做“逐步减半网格搜索”的方法，它基于**逐步减半**的思想。
- en: The algorithm works as follows. First, all candidate models are fitted using
    a small subset of the available training data (in general, using a limited amount
    of resources). Then, the best-performing candidates are picked out. In the next
    step, those best-performing candidates are retrained with a bigger subset of the
    training data. Those steps are repeated until the best set of hyperparameters
    is identified. In this approach, after each iteration, the number of available
    hyperparameter candidates is decreasing while the size of the training data (resources)
    is increasing.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的工作原理如下。首先，使用可用训练数据的小子集拟合所有候选模型（通常使用有限的资源）。然后，挑选出表现最好的候选模型。在接下来的步骤中，这些表现最好的候选模型将使用更大的训练数据子集进行重新训练。这些步骤会不断重复，直到找到最佳的超参数组合。在这种方法中，每次迭代后，候选超参数的数量会减少，而训练数据的大小（资源）会增加。
- en: The default behavior of the halving grid search is to use training data as a
    resource. However, we could just as well use another hyperparameter of the estimator
    we are trying to tune, as long as it accepts positive integer values. For example,
    we could use the number of trees (`n_estimators`) of the Random Forest model as
    the resource to be increased with each iteration.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 逐次减半网格搜索的默认行为是将训练数据作为资源。然而，我们也可以使用我们尝试调整的估计器的另一个超参数，只要它接受正整数值。例如，我们可以使用随机森林模型的树木数量（`n_estimators`）作为每次迭代中增加的资源。
- en: 'The speed of the algorithm depends on two hyperparameters:'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的速度取决于两个超参数：
- en: '`min_resources`—the minimum amount of resources that any candidate is allowed
    to use. In practice, this corresponds to the number of resources used in the first
    iteration.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_resources`—任何候选者允许使用的最小资源量。实际上，这对应于第一次迭代中使用的资源数量。'
- en: '`factor`—the halving parameter. The reciprocal of the `factor` (1 / `factor`)
    determines the proportion of candidates to be selected as the best models in each
    iteration. The product of the `factor` and the previous iteration’s number of
    resources determines the current iteration’s number of resources.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`factor`—缩减参数。`factor`的倒数（1 / `factor`）决定了每次迭代中作为最佳模型被选择的候选者比例。`factor`与上一迭代的资源数量的乘积决定了当前迭代的资源数量。'
- en: While picking those two might seem a bit daunting with all the calculations
    to be carried out manually to make use of most of the resources, `scikit-learn`
    makes it easier for us with the `"exhaust"` value of the `min_resources` argument.
    Then, the algorithm will determine for us the number of the resources in the first
    iteration such that the last iteration uses as many resources as possible. In
    the default case, it will result in the last iteration using as much of the training
    data as possible.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然手动进行这些计算以充分利用大部分资源可能看起来有些令人望而生畏，但`scikit-learn`通过`min_resources`参数的`"exhaust"`值使得这一过程变得更加简单。这样，算法将为我们确定第一次迭代中使用的资源数量，以便最后一次迭代使用尽可能多的资源。在默认情况下，它将导致最后一次迭代使用尽可能多的训练数据。
- en: Similar to the randomized grid search, `scikit-learn` also offers a randomized
    halving grid search. The only difference compared to what we have already described
    is that at the very beginning, a fixed number of candidates is sampled at random
    from the parameter space. This number is determined by the `n_candidates` argument.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机网格搜索类似，`scikit-learn`还提供了随机化的逐次减半网格搜索。与我们之前描述的唯一区别是，在一开始，会从参数空间中随机抽取固定数量的候选者。这个数量由`n_candidates`参数决定。
- en: 'Below we demonstrate how to use the `HalvingGridSearchCV`. First, we need to
    explicitly allow using the experimental feature before importing it (in the future,
    this step might be redundant when the feature is no longer experimental):'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们展示如何使用`HalvingGridSearchCV`。首先，在导入之前，我们需要明确允许使用实验特性（未来，当该特性不再是实验性的时，这一步可能会变得多余）：
- en: '[PRE101]'
  id: totrans-791
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Then, we find the best hyperparameters for our decision tree pipeline:'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为我们的决策树管道找到最佳的超参数：
- en: '[PRE102]'
  id: totrans-793
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'We can see how the successive halving algorithm works in practice in the following
    log:'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下日志中看到逐次减半算法在实践中的表现：
- en: '[PRE103]'
  id: totrans-795
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: As we have mentioned before, `max_resources` is determined by the size of the
    training data, that is, 24,000 observations. Then, the algorithm figured out that
    it needs to start with a sample size of 98 in order to end the procedure with
    as big a sample as possible. In this case, in the last iteration, the algorithm
    used 23,814 training observations.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`max_resources`是由训练数据的大小决定的，也就是24,000个观测值。然后，算法计算出它需要从98的样本大小开始，以便在结束时获得尽可能大的样本。在这种情况下，在最后一次迭代中，算法使用了23,814个训练观测值。
- en: In the following table, we can see which values of the hyperparameters were
    picked by each of the 3 approaches to grid search we have covered in this recipe.
    They are very similar, and so is their performance on the test set (the exact
    comparison is available in the notebook on GitHub). We leave the comparison of
    fitting times of all those algorithms as an exercise for the reader.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们可以看到每个我们在本节中介绍的3种网格搜索方法选择的超参数值。它们非常相似，测试集上的性能也是如此（具体的比较可以在GitHub上的笔记本中查看）。我们将所有这些算法的拟合时间比较留给读者作为练习。
- en: '![](../Images/B18112_13_31.png)'
  id: totrans-798
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_13_31.png)'
- en: 'Figure 13.31: The best values of hyperparameters identified by exhaustive,
    randomized, and halving grid search'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.31：通过详尽、随机化和二分网格搜索识别的最佳超参数值
- en: Grid search with multiple classifiers
  id: totrans-800
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用多个分类器进行网格搜索
- en: 'We can also create a grid containing multiple classifiers. This way, we can
    see which model performs best with our data. To do so, we first import another
    classifier from `scikit-learn`. We will use the famous Random Forest:'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以创建一个包含多个分类器的网格。这样，我们可以看到哪个模型在我们的数据上表现最好。为此，我们首先从`scikit-learn`导入另一个分类器。我们将使用著名的随机森林：
- en: '[PRE104]'
  id: totrans-802
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: We selected this model as it is an ensemble of decision trees and thus also
    does not require any further preprocessing of the data. For example, if we wanted
    to use a simple logistic regression classifier (with regularization), we should
    also scale the features (standardize/normalize) by adding an additional step to
    the numerical part of the preprocessing pipeline. We cover the Random Forest model
    in more detail in the next chapter.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了这个模型，因为它是一个决策树集成，因此也不需要对数据进行进一步的预处理。例如，如果我们想使用一个简单的逻辑回归分类器（带正则化），我们还应该通过在数值预处理管道中添加一个额外步骤来对特征进行缩放（标准化/归一化）。我们将在下一章中更详细地介绍随机森林模型。
- en: Again, we need to define the parameter grid. This time, it is a list containing
    multiple dictionaries—one dictionary per classifier. The hyperparameters for the
    decision tree are the same as before, and we chose the simplest hyperparameters
    of the Random Forest, as those do not require additional explanations.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们需要定义参数网格。这一次，它是一个包含多个字典的列表——每个分类器一个字典。决策树的超参数与之前相同，我们选择了随机森林的最简单超参数，因为这些超参数不需要额外的解释。
- en: 'It is worth mentioning that if we want to tune some other hyperparameters in
    the pipeline, we need to specify them in each of the dictionaries in the list.
    That is why `preprocessor__numerical__outliers__n_std` is included twice in the
    following snippet:'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，如果我们想调整管道中的其他超参数，我们需要在列表中的每个字典中指定它们。这就是为什么`preprocessor__numerical__outliers__n_std`在下面的代码片段中出现了两次：
- en: '[PRE105]'
  id: totrans-806
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'The rest of the process is exactly the same as before:'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的过程和之前完全相同：
- en: '[PRE106]'
  id: totrans-808
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Running the snippet generates the following output:'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码片段会生成以下输出：
- en: '[PRE107]'
  id: totrans-810
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: Turns out that the tuned decision tree managed to outperform an ensemble of
    trees. As we will see in the next chapter, we can easily change the outcome with
    a bit more tuning of the Random Forest classifier. After all, we have only tuned
    two of the many hyperparameters available.
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，经过调整的决策树表现优于树的集成。正如我们将在下一章看到的，我们可以通过对随机森林分类器进行更多的调整来轻松改变结果。毕竟，我们只调整了可用的多个超参数中的两个。
- en: 'We can use the following snippet to extract and print all the considered hyperparameter/classifier
    combinations, starting with the best one:'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码片段来提取并打印所有考虑的超参数/分类器组合，从最佳的那个开始：
- en: '[PRE108]'
  id: totrans-813
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: See also
  id: totrans-814
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'An additional resource on the randomized search procedure is available here:'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: 关于随机化搜索过程的额外资源可以在这里找到：
- en: Bergstra, J. & Bengio, Y. (2012). “Random search for hyper-parameter optimization.”
    *Journal of Machine Learning Research*, 13(Feb), 281-305\. [http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf).
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bergstra, J. & Bengio, Y. (2012). “随机搜索用于超参数优化。” *机器学习研究期刊*, 13(2月), 281-305\.
    [http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf).
- en: Summary
  id: totrans-817
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we have covered the basics required to approach any machine
    learning project, not just limited to the financial domain. We did the following:'
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经涵盖了处理任何机器学习项目所需的基础知识，这些知识不仅限于金融领域。我们做了以下几件事：
- en: Imported the data and optimized its memory usage
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入数据并优化其内存使用
- en: Thoroughly explored the data (distributions of features, missing values, and
    class imbalance), which should already provide some ideas about potential feature
    engineering
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彻底探索了数据（特征分布、缺失值和类别不平衡），这应该已经提供了一些关于潜在特征工程的思路
- en: Identified the missing values in our dataset and imputed them
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别了数据集中的缺失值并进行了填充
- en: Learned how to encode categorical variables so that they are correctly interpreted
    by machine learning models
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学会了如何编码类别变量，使其能被机器学习模型正确解读
- en: Fitted a decision tree classifier using the most popular and mature ML library—`scikit-learn`
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最流行且最成熟的机器学习库——`scikit-learn`，拟合了一个决策树分类器
- en: Learned how to organize our entire codebase using pipelines
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学会了如何使用管道组织我们的整个代码库
- en: Learned how to tune the hyperparameters of the model to squeeze out some extra
    performance and find a balance between underfitting and overfitting
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学会了如何调整模型的超参数，以挤压出一些额外的性能，并找到欠拟合和过拟合之间的平衡。
- en: It is crucial to understand those steps and their significance, as they can
    be applied to any data science project, not only binary classification. The steps
    would be virtually the same for a regression problem, for example, predicting
    the price of a house. We would use slightly different estimators (though most
    of them work for both classification and regression) and evaluate the performance
    using different metrics (MSE, RMSE, MAE, MAPE, and so on). But the principles
    stay the same.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些步骤及其意义至关重要，因为它们可以应用于任何数据科学项目，而不仅仅是二元分类问题。例如，对于回归问题（如预测房价），步骤几乎是相同的。我们将使用略微不同的估算器（虽然大多数估算器适用于分类和回归），并使用不同的指标评估性能（如MSE、RMSE、MAE、MAPE等）。但基本原则不变。
- en: 'If you are interested in putting the knowledge from this chapter into practice,
    we can recommend the following sources for finding data for your next project:'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣将本章的知识付诸实践，我们推荐以下资源，供你寻找下一个项目的数据：
- en: 'Google Datasets: [https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/)'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google 数据集： [https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/)
- en: 'Kaggle: [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaggle： [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)
- en: 'UCI Machine Learning Repository: [https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UCI 机器学习库： [https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)
- en: In the next chapter, we cover a selection of techniques that might be helpful
    in further improving the initial model. We will cover, among others, more complex
    classifiers, Bayesian hyperparameter tuning, dealing with class imbalance, exploring
    feature importance and selection, and more.
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一些有助于进一步改进初始模型的技术。我们将涵盖包括更复杂的分类器、贝叶斯超参数调优、处理类别不平衡、探索特征重要性和选择等内容。
- en: Join us on Discord!
  id: totrans-832
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 群组！
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入本书的 Discord 社区，在这里你可以分享反馈、向作者提问并了解新版本发布，请扫描下面的二维码：
- en: '![](../Images/QR_Code203602028422735375.png)'
  id: totrans-834
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/QR_Code203602028422735375.png)'
- en: '[https://packt.link/ips2H](https://packt.link/ips2H)'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/ips2H](https://packt.link/ips2H)'
