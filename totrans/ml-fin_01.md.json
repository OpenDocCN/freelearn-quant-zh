["```py\n$ jupyter notebook\n\n```", "```py\n$ sudo pip install TensorFlow\n\n```", "```py\n$ sudo pip install Keras\n\n```", "```py\nimport numpy as np\nnp.random.seed(1)\n```", "```py\nX = np.array([[0,1,0],\n              [1,0,0],\n              [1,1,1],\n              [0,1,1]])\n\ny = np.array([[0,1,1,0]]).T\n```", "```py\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n```", "```py\nW = 2*np.random.random((3,1)) - 1\nb = 0\n```", "```py\nz = X.dot(W) + b\n```", "```py\nA = sigmoid(z)\n```", "```py\nprint(A)\n```", "```py\nout:\n[[ 0.60841366]\n [ 0.45860596]\n [ 0.3262757 ]\n [ 0.36375058]]\n\n```", "```py\ndef bce_loss(y,y_hat):\n  N = y.shape[0]\n  loss = -1/N * (y*np.log(y_hat) + (1 - y)*np.log(1-y_hat))\n  return loss\n```", "```py\nloss = bce_loss(y,A)\nprint(loss)\n```", "```py\nout: \n0.82232258208779863\n\n```", "```py\ndz = (A - y)\n\ndW = 1/N * np.dot(X.T,dz)\n\ndb = 1/N * np.sum(dz,axis=0,keepdims=True)\n```", "```py\nalpha = 1\nW -= alpha * dW\nb -= alpha * db\n```", "```py\nimport numpy as np\nnp.random.seed(1)\n\nX = np.array([[0,1,0],\n              [1,0,0],\n              [1,1,1],\n              [0,1,1]])\n\ny = np.array([[0,1,1,0]]).T\n```", "```py\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\ndef bce_loss(y,y_hat):\n    N = y.shape[0]\n    loss = -1/N * np.sum((y*np.log(y_hat) + (1 - y)*np.log(1-y_hat)))\n    return loss\n```", "```py\nW = 2*np.random.random((3,1)) - 1\nb = 0\n```", "```py\nalpha = 1\nepochs = 20\n```", "```py\nN = y.shape[0]\nlosses = []\n```", "```py\nfor i in range(epochs):\n    # Forward pass\n    z = X.dot(W) + b \n    A = sigmoid(z)\n\n    # Calculate loss\n    loss = bce_loss(y,A)\n    print('Epoch:',i,'Loss:',loss)\n    losses.append(loss)\n\n    # Calculate derivatives\n    dz = (A - y)\n    dW = 1/N * np.dot(X.T,dz)\n    db = 1/N * np.sum(dz,axis=0,keepdims=True)    \n\n    # Parameter updates\n    W -= alpha * dW\n    b -= alpha * db\n```", "```py\nout: \nEpoch: 0 Loss: 0.822322582088\nEpoch: 1 Loss: 0.722897448125\nEpoch: 2 Loss: 0.646837651208\nEpoch: 3 Loss: 0.584116122241\nEpoch: 4 Loss: 0.530908161024\nEpoch: 5 Loss: 0.48523717872\nEpoch: 6 Loss: 0.445747750118\nEpoch: 7 Loss: 0.411391164148\nEpoch: 8 Loss: 0.381326093762\nEpoch: 9 Loss: 0.354869998127\nEpoch: 10 Loss: 0.331466036109\nEpoch: 11 Loss: 0.310657702141\nEpoch: 12 Loss: 0.292068863232\nEpoch: 13 Loss: 0.275387990352\nEpoch: 14 Loss: 0.260355695915\nEpoch: 15 Loss: 0.246754868981\nEpoch: 16 Loss: 0.234402844624\nEpoch: 17 Loss: 0.22314516463\nEpoch: 18 Loss: 0.21285058467\nEpoch: 19 Loss: 0.203407060401\n\n```", "```py\nimport matplotlib.pyplot as plt\nplt.plot(losses)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```", "```py\nz1 = X.dot(W1) + b1\n\na1 = np.tanh(z1)\n\nz2 = a1.dot(W2) + b2\n\na2 = sigmoid(z2)\n```", "```py\n# Calculate loss derivative with respect to the output\ndz2 = bce_derivative(y=y,y_hat=a2)\n\n# Calculate loss derivative with respect to second layer weights\ndW2 = (a1.T).dot(dz2)\n\n# Calculate loss derivative with respect to second layer bias\ndb2 = np.sum(dz2, axis=0, keepdims=True)\n\n# Calculate loss derivative with respect to first layer\ndz1 = dz2.dot(W2.T) * tanh_derivative(a1)\n\n# Calculate loss derivative with respect to first layer weights\ndW1 = np.dot(X.T, dz1)\n\n# Calculate loss derivative with respect to first layer bias\ndb1 = np.sum(dz1, axis=0)\n```", "```py\nfrom keras.layers import Dense, Activation\n```", "```py\nfrom keras.models import Sequential\n```", "```py\nmodel = Sequential()\n```", "```py\nmodel.add(Dense(3,input_dim=2))\n```", "```py\nmodel.add(Activation('tanh'))\n```", "```py\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n```", "```py\nmodel.summary()\n```", "```py\nout: \nLayer (type)                 Output Shape              Param # \n=================================================================\ndense_3 (Dense)              (None, 3)                 9 \n_________________________________________________________________\nactivation_3 (Activation)    (None, 3)                 0 \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 4 \n_________________________________________________________________\nactivation_4 (Activation)    (None, 1)                 0 \n=================================================================\nTotal params: 13\nTrainable params: 13\nNon-trainable params: 0\n\n```", "```py\nmodel.compile(optimizer='sgd',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n```", "```py\nhistory = model.fit(X,y,epochs=900)\n```", "```py\nEpoch 1/900\n200/200 [==============================] - 0s 543us/step - \nloss: 0.6840 - acc: 0.5900\nEpoch 2/900\n200/200 [==============================] - 0s 60us/step - \nloss: 0.6757 - acc: 0.5950\n...\n\nEpoch 899/900\n200/200 [==============================] - 0s 90us/step - \nloss: 0.2900 - acc: 0.8800\nEpoch 900/900\n200/200 [==============================] - 0s 87us/step - \nloss: 0.2901 - acc: 0.8800\n\n```", "```py\nfrom tensorflow.keras.layers import Dense, Activation\n```"]