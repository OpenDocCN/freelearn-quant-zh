- en: '5'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '5'
- en: Quantum Boltzmann Machine
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 量子玻尔兹曼机
- en: 'As we saw in Chapters [3](Chapter_3.xhtml#x1-630003) and [4](Chapter_4.xhtml#x1-820004),
    quantum annealing can be used to solve hard optimisation problems. However, the
    range of possible applications of quantum annealing is much wider than that. In
    this chapter, we will consider two distinct but related use cases that go beyond
    solving optimisation problems: sampling and training deep neural networks. Specifically,
    we will focus on the Quantum Boltzmann Machine (QBM) – a generative model that
    is a direct quantum annealing counterpart of the classical Restricted Boltzmann
    Machine (RBM), and the Deep Boltzmann Machine (DBM) – a class of deep neural networks
    composed of multiple layers of latent variables with connections between the layers
    but not between units within each layer.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第[3](Chapter_3.xhtml#x1-630003)章和第[4](Chapter_4.xhtml#x1-820004)章中看到的，量子退火可以用来解决难度较大的优化问题。然而，量子退火的应用范围远不止于此。在本章中，我们将讨论两个不同但相关的应用案例，这些应用超出了优化问题的解决：采样和训练深度神经网络。具体来说，我们将重点介绍量子玻尔兹曼机（QBM）——一种生成模型，是经典限制玻尔兹曼机（RBM）和深度玻尔兹曼机（DBM）的直接量子退火对应物——以及深度玻尔兹曼机（DBM）——一种由多层潜在变量组成的深度神经网络，层与层之间有连接，但层内单元之间没有连接。
- en: We start by providing detailed descriptions of the classical RBM, including
    the corresponding training algorithm. Due to the fact that an RBM operates on
    stochastic binary activation units, one can establish the correspondence between
    the RBM graph and the QUBO graph embedded onto the quantum chip. This provides
    the main motivation for performing Boltzmann sampling (the key stage in training
    RBMs and DBMs) using quantum annealing.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先提供经典RBM的详细描述，包括相应的训练算法。由于RBM在随机二进制激活单元上操作，因此可以建立RBM图与嵌入到量子芯片上的QUBO图之间的对应关系。这为使用量子退火进行玻尔兹曼采样（RBM和DBM训练过程中的关键阶段）提供了主要动力。
- en: DBMs can be trained as both generative and discriminative models. In both cases,
    since a DBM can be constructed by stacking together layers of RBMs, efficient
    Boltzmann sampling is the key element of the training process. Quantum annealing,
    which can be integrated into the hybrid quantum-classical training routine, has
    the potential to improve speed and accuracy. Quantum speedup is an especially
    appealing element of the envisaged quantum advantage since it can be achieved
    not only during the RBM training stage but also during the process of generating
    new samples.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: DBM可以作为生成模型和判别模型进行训练。在这两种情况下，由于DBM可以通过堆叠RBM层来构建，因此高效的玻尔兹曼采样是训练过程中的关键元素。量子退火可以集成到混合量子经典训练过程中，具有提高速度和准确度的潜力。量子加速是预期量子优势中特别有吸引力的元素，因为它不仅可以在RBM训练阶段实现，还可以在生成新样本的过程中实现。
- en: 5.1 From Graph Theory to Boltzmann Machines
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 从图论到玻尔兹曼机
- en: We provide here a short self-contained review of graph theory in order to introduce
    Boltzmann machines (or energy-based models), which one can view as particular
    types of connected graphs or networks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此提供一个简短的图论自包含回顾，以介绍玻尔兹曼机（或能量模型），它们可以视为特定类型的连接图或网络。
- en: A *graph* is a set of vertices (points or nodes) and edges that connect the
    vertices. A *directed graph* is a type of graph that contains ordered pairs of
    vertices while an *undirected graph* is a type of graph that contains unordered
    pairs of vertices.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*图* 是一组顶点（点或节点）和连接顶点的边。*有向图* 是一种包含有序顶点对的图，而*无向图* 是一种包含无序顶点对的图。'
- en: We consider a graph 𝒢 = (𝒱*,*ℰ) characterised by a finite number of vertices
    𝒱 and undirected edges ℰ. For a given vertex *v* ∈𝒱, its neighbourhood is defined
    as the set of all vertices connected to it by some edge, or
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个图 𝒢 = (𝒱*,*ℰ)，其中𝒱表示有限个顶点，ℰ表示无向边。对于给定的顶点 *v* ∈𝒱，其邻域被定义为与它通过某条边相连的所有顶点的集合，或者
- en: '![𝒩 (v) := {w ∈ 𝒱 : {v,w} ∈ ℰ}. ](img/file388.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![𝒩 (v) := {w ∈ 𝒱 : {v,w} ∈ ℰ}. ](img/file388.jpg)'
- en: Finally, a *clique* 𝒞 is a subset of 𝒱 such that all vertices in 𝒞 are pairwise
    connected by some edge in ℰ.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*团* 𝒞是𝒱的一个子集，满足𝒞中的所有顶点通过ℰ中的某条边成对相连。
- en: To each vertex *v* ∈𝒱, we associate a random variable *X*[v] taking values in
    some space 𝒳. The vector *X* ∈𝒳^(|𝒱|) is called a Markov random field if
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个顶点 *v* ∈𝒱，我们将关联一个随机变量 *X*[v]，它的取值来自某个空间 𝒳。向量 *X* ∈𝒳^(|𝒱|) 被称为马尔可夫随机场，如果
- en: '![Law (Xv |(Xw ){w∈𝒱∖{v}}) = Law (Xv |(Xw ){w∈𝒩 (v)}). ](img/file389.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Law (Xv |(Xw ){w∈𝒱∖{v}}) = Law (Xv |(Xw ){w∈𝒩 (v)}). ](img/file389.jpg)'
- en: The following theorem, originally proved by Hammersley and Clifford  [[125](Biblography.xhtml#XHammersleyClifford)]
    (see also  [[167](Biblography.xhtml#XKollerFriedman), Theorem 4.2]), provides
    a way to express the law of Markov random fields over graphs in a convenient form.
    The Markovian property is fundamental here, as dynamics (for example, the passing
    of a signal from a hidden layer to a visible layer of an RBM network) should only
    depend on the current state and not on the whole path followed by the system.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下定理最初由哈默斯利和克利福德[[125](Biblography.xhtml#XHammersleyClifford)]（另见[[167](Biblography.xhtml#XKollerFriedman)，定理4.2]）证明，提供了一种在图上以方便的形式表达马尔可夫随机场法则的方法。马尔可夫性质在此处至关重要，因为动态（例如，从隐藏层到可见层的信号传递，在RBM网络中）应仅依赖于当前状态，而不依赖于系统所经过的整个路径。
- en: '**Theorem 8** (Hammersley-Clifford Theorem)**.** *A* *strictly positive distribution
    satisfies the Markov property with respect to an* *undirected graph if and only
    if it factorises over it.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理 8**（哈默斯利-克利福德定理）**。** *A* *严格正分布仅当且仅当它在无向图上进行因式分解时，才满足马尔可夫性质。*'
- en: Phrased differently, the theorem says that *X* is Markovian over 𝒢 if its distribution
    can be written as
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，定理表明，如果其分布可以表示为
- en: '| ![ 1 ∏ ℙX (x) := ℙ(X = x) = -- ψC (xC), for all x ∈ 𝒳 &#124;𝒱&#124;, Z C∈𝒞
    ](img/file390.jpg) |  |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| ![ 1 ∏ ℙX (x) := ℙ(X = x) = -- ψC (xC), for all x ∈ 𝒳 &#124;𝒱&#124;, Z C∈𝒞
    ](img/file390.jpg) |  |'
- en: for a set {*ψ*[C]}[C∈𝒞] of functions called the potential over all the cliques
    *C* ∈𝒞 and where *Z* is a normalisation constant such that the probabilities integrate
    to unity. Here x[C] naturally corresponds to the elements of the vector x over
    the clique *C*. The factorisation is often taken over the so-called *maximal cliques*,
    namely the cliques that are no longer cliques if any node is added. If the distribution
    of *X* is strictly positive, then so are the functions {*ψ*[C]}[C∈𝒞] and therefore ([5.1](#x1-97006r1))
    can be written as
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个函数集{*ψ*[C]}[C∈𝒞]，这些函数被称为所有团体*C* ∈𝒞上的潜力函数，其中*Z*是一个归一化常数，确保概率的积分为1。在这里，x[C]自然对应于向量x在团体*C*上的元素。分解通常是在所谓的*最大团体*上进行的，即那些如果添加任何节点就不再是团体的团体。如果*X*的分布严格为正，那么{*ψ*[C]}[C∈𝒞]也是正的，因此([5.1](#x1-97006r1))可以写为
- en: '| ![ ( ) 1 ∑ 1 ℙX (x) = --exp log(ψC (xC)) =: --e−E(x), Z C∈𝒞 Z ](img/file391.jpg)
    |  |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ) 1 ∑ 1 ℙX (x) = --exp log(ψC (xC)) =: --e−E(x), Z C∈𝒞 Z ](img/file391.jpg)
    |  |'
- en: for all x ∈𝒳^(|𝒱|). The function
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有x ∈𝒳^(|𝒱|)。该函数
- en: '![ ∑ E (x) := − log(ψC(xC )) C∈𝒞 ](img/file392.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑ E (x) := − log(ψC(xC )) C∈𝒞 ](img/file392.jpg)'
- en: is called the *energy* function. Because of their uses in statistical physics,
    strictly positive distributions of Markov random fields, taking the form ([5.1](#x1-97006r1)),
    are also called Boltzmann or Gibbs distributions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数称为*能量*函数。由于它们在统计物理学中的应用，马尔可夫随机场的严格正分布，形式为([5.1](#x1-97006r1))，也被称为玻尔兹曼分布或吉布斯分布。
- en: Energy-based models are generative models that discover data dependencies by
    applying a measure of compatibility (scalar energy) to each configuration of the
    observed and latent variables. The inference consists of finding the values of
    latent variables that minimise the energy given the values of the observed variables.
    Energy-based models posses many useful properties (simplicity, stability, flexibility,
    compositionality) – this makes them models of choice for learning complex multivariate
    probability distributions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基于能量的模型是生成模型，通过对观察到的和潜在变量的每种配置应用兼容性度量（标量能量）来发现数据依赖性。推理的目标是寻找潜在变量的值，在给定观察变量值的情况下，使得能量最小化。基于能量的模型具有许多有用的属性（简洁性、稳定性、灵活性、可组合性）——这使得它们成为学习复杂多变量概率分布的首选模型。
- en: 5.2 Restricted Boltzmann Machine
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 限制玻尔兹曼机
- en: 5.2.1 The RBM as an energy-based model
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 RBM作为一种基于能量的模型
- en: The RBM corresponds to a special structure of such a graph, called bipartite,
    where the set 𝒱 of vertices can be split into two groups of visible vertices 𝒱[V]
    and hidden vertices 𝒱[H] such that the set ℰ of edges only consists of elements
    of the form {*v,h*}∈𝒱[V] ×𝒱[H]. Figure [5.1](#5.1) provides a schematic representation
    of the RBM that implements the bipartite graph structure. This in particular implies
    that cliques can only be of size one (all the singleton nodes) or two (all the
    pairs (*v,h*) in 𝒱[V] ×𝒱[H]). For simplicity, we shall denote v an element of
    𝒳^(|𝒱[V] |) and h an element of 𝒳^(|𝒱[H]|), and identify the random variable *X*
    with the vertices. The following lemma gives us the general form of the energy
    function ([5.1](#x1-97006r1)) for RBMs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: RBM 对应于这种图的特殊结构，称为二分图，其中顶点集 𝒱 可以被拆分成两个组，分别是可见顶点 𝒱[V] 和隐藏顶点 𝒱[H]，使得边集 ℰ 仅由形式为
    {*v,h*}∈𝒱[V] ×𝒱[H] 的元素组成。图  [5.1](#5.1) 提供了实现二分图结构的 RBM 的示意图。这特别意味着，团只能是大小为一（所有单点节点）或二（所有
    (*v,h*) 对）在 𝒱[V] ×𝒱[H] 中。为了简化，我们将 v 表示为 𝒳^(|𝒱[V] |) 的一个元素，h 表示为 𝒳^(|𝒱[H]|) 的一个元素，并将随机变量
    *X* 与顶点对应。以下引理给出了 RBM 的能量函数的一般形式（[5.1](#x1-97006r1)）。
- en: '**Lemma 6** (RBM Energy Lemma)**.** *In a Restricted Boltzmann Machine, the*
    *energy function takes the form*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理 6**（RBM 能量引理）**.** *在限制玻尔兹曼机中，能量函数的形式为*'
- en: '| ![ ∑N ∑M ∑N ∑M E (v,h ) = Ev(vi)+ Eh (hj)+ Ev,h(vi,hj), i=1 j=1 i=1j=1 ](img/file393.jpg)
    |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑N ∑M ∑N ∑M E (v,h ) = Ev(vi)+ Eh (hj)+ Ev,h(vi,hj), i=1 j=1 i=1j=1 ](img/file393.jpg)
    |  |'
- en: '*for any* v := (*v*[1]*,…,v*[N]) ∈𝒳^(|𝒱[V] |)*,* h := (*h*[1]*,…,h*[M]) ∈𝒳^(|𝒱[H]|)*.
    Here,* *N* *is the number* *of visible vertices and* *M* *is the number of hidden
    vertices.*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于任何* v := (*v*[1]*,…,v*[N]) ∈𝒳^(|𝒱[V] |)*,* h := (*h*[1]*,…,h*[M]) ∈𝒳^(|𝒱[H]|)*.
    这里，*N* 是可见顶点的数量，*M* 是隐藏顶点的数量。*'
- en: '*Proof.* By the Hammersley-Clifford theorem, for any v ∈𝒳^(|𝒱[V] |), h ∈𝒳^(|𝒱[H]|),
    we have the factorisation'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明。* 根据哈默斯利-克利福德定理，对于任何 v ∈𝒳^(|𝒱[V] |)，h ∈𝒳^(|𝒱[H]|)，我们有以下分解式：'
- en: '| ℙ(v*,*h) | = ![-1 Z](img/file394.jpg)∏ [C∈𝒞]*ψ*[C]((v[C]*,*h[C]) ∈ *C*) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| ℙ(v*,*h) | = ![-1 Z](img/file394.jpg)∏ [C∈𝒞]*ψ*[C]((v[C]*,*h[C]) ∈ *C*) |'
- en: '|  | = ![-1 Z](img/file395.jpg)∏ [{{v}:v∈𝒱[V] }]*ψ*[{v}](*v*)∏ [{{h}:h∈𝒱[H]}]*ψ*[{h}](*h*)∏
    [{{v,h}∈𝒱[V] ×𝒱[H]}]*ψ*[{v,h}](*v,h*) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | = ![-1 Z](img/file395.jpg)∏ [{{v}:v∈𝒱[V] }]*ψ*[{v}](*v*)∏ [{{h}:h∈𝒱[H]}]*ψ*[{h}](*h*)∏
    [{{v,h}∈𝒱[V] ×𝒱[H]}]*ψ*[{v,h}](*v,h*) |'
- en: '|  | = ![-1 Z](img/file396.jpg)exp![{− E (v,h)}](img/file397.jpg)*,* |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | = ![-1 Z](img/file396.jpg)exp![{− E (v,h)}](img/file397.jpg)*,* |'
- en: over all singletons (cliques of size one) and couples (cliques of size two),
    where the term −*E*(v*,*h) reads
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有单点集（大小为一的团）和二点集（大小为二的团）上，其中项 −*E*(v*,*h) 表示
- en: '| − *E*(v*,*h) | = log ![( ) ∏ ∏ ∏ ( ψ {v}(v) ψ{h}(h ) ψ {v,h}(v,h)) {{v}:v∈𝒱V}
    {{h}:h∈𝒱H} {{v,h}∈ 𝒱V×𝒱H}](img/file398.jpg) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| − *E*(v*,*h) | = log ![( ) ∏ ∏ ∏ ( ψ {v}(v) ψ{h}(h ) ψ {v,h}(v,h)) {{v}:v∈𝒱V}
    {{h}:h∈𝒱H} {{v,h}∈ 𝒱V×𝒱H}](img/file398.jpg) |'
- en: '|  | = log ![( ) ( ∏ ) ψ {v}(v) {{v}:v∈𝒱V}](img/file399.jpg) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | = log ![( ) ( ∏ ) ψ {v}(v) {{v}:v∈𝒱V}](img/file399.jpg) |'
- en: '|  | + log ![( ) ∏ ( ψ {h}(h)) {{h}:h∈𝒱H}](img/file400.jpg) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | + log ![( ) ∏ ( ψ {h}(h)) {{h}:h∈𝒱H}](img/file400.jpg) |'
- en: '|  | + log ![( ) ∏ ( ψ{v,h}(v,h)) {{v,h}∈𝒱V×𝒱H }](img/file401.jpg) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | + log ![( ) ∏ ( ψ{v,h}(v,h)) {{v,h}∈𝒱V×𝒱H }](img/file401.jpg) |'
- en: '|  | = ∑ [{{v}:v∈𝒱[V] }]log ![( ) ψ{v}(v)](img/file402.jpg) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | = ∑ [{{v}:v∈𝒱[V] }]log ![( ) ψ{v}(v)](img/file402.jpg) |'
- en: '|  | + ∑ [{{h}:h∈𝒱[H]}]log ![(ψ{h}(h))](img/file403.jpg) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | + ∑ [{{h}:h∈𝒱[H]}]log ![(ψ{h}(h))](img/file403.jpg) |'
- en: '|  | + ∑ [{{v,h}∈𝒱[V] ×𝒱[H]}]log ![( ) ψ{v,h}(v,h)](img/file404.jpg) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | + ∑ [{{v,h}∈𝒱[V] ×𝒱[H]}]log ![( ) ψ{v,h}(v,h)](img/file404.jpg) |'
- en: '|  | = −∑ [i=1]^N*E* [v](*v*[i]) −∑ [j=1]^M*E* [h](*h*[j]) −∑ [i=1]^N ∑ [j=1]^M*E*
    [v,h](*v*[i]*,h*[j])*,* |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | = −∑ [i=1]^N*E* [v](*v*[i]) −∑ [j=1]^M*E* [h](*h*[j]) −∑ [i=1]^N ∑ [j=1]^M*E*
    [v,h](*v*[i]*,h*[j])*,* |'
- en: which concludes the proof of the lemma. □
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了引理的证明。□
- en: The standard example of an RBM is when the random variables follow Bernoulli
    distribution, i.e., with 𝒳 = {0*,*1}^(|𝒱|). In this case, their energies read
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: RBM 的标准示例是当随机变量服从伯努利分布时，即 𝒳 = {0*,*1}^(|𝒱|)。在这种情况下，它们的能量为
- en: '| ![Ev (vi) = − aivi, Eh(hj) = − bjhj, Ev,h(vi,hj) = − wijvihj, ](img/file405.jpg)
    |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| ![Ev (vi) = − aivi, Eh(hj) = − bjhj, Ev,h(vi,hj) = − wijvihj, ](img/file405.jpg)
    |  |'
- en: for some parameters *a*[i], *b*[j], *w*[ij], *i* = 1*,…,N*, *j* = 1*,…,M*. In
    particular, for a given *v*[i], we can write, using Bayes’ formula,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些参数 *a*[i]，*b*[j]，*w*[ij]，*i* = 1*,…,N*，*j* = 1*,…,M*。特别地，对于给定的 *v*[i]，我们可以使用贝叶斯公式表示：
- en: '| ℙ(*v*[i] = 1&#124;v[v[i]]*,*h) | = ![---------ℙ(vi =-1,vvi,h)------- ℙ (vi
    = 1,vvi,h )+ ℙ(vi = 0,vvi,h)](img/file406.jpg) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ℙ(*v*[i] = 1&#124;v[v[i]]*,*h) | = ![---------ℙ(vi =-1,vvi,h)------- ℙ (vi
    = 1,vvi,h )+ ℙ(vi = 0,vvi,h)](img/file406.jpg) |'
- en: '|  | = ![ exp (− E (v = 1,v ,h)) ----------------------i-----vi---------------
    exp (− E (vi = 1,vvi,h))+ exp (− E (vi = 0,vvi,h ))](img/file407.jpg)*.* | (5.2.1)
    |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | = ![ exp (− E (v = 1,v ,h)) ----------------------i-----vi---------------
    exp (− E (vi = 1,vvi,h))+ exp (− E (vi = 0,vvi,h ))](img/file407.jpg)*.* | (5.2.1)
    |  |'
- en: where we denote v[v[i]] the states of all the nodes in 𝒱∖{*v*[i]}. Now, using
    the RBM energy lemma, we can single out the energy arising from the particular
    node *v* using ([5.2.1](#x1-99002r1)) as
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们表示 v[v[i]] 为 𝒱∖{*v*[i]} 中所有节点的状态。现在，使用 RBM 能量引理，我们可以利用([5.2.1](#x1-99002r1))将来自特定节点
    *v* 的能量单独提取出来，如下所示：
- en: '![E(vi,vvi,h) = − Φv(vi)− Ψv (vvi,h), ](img/file408.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![E(vi,vvi,h) = − Φv(vi)− Ψv (vvi,h), ](img/file408.jpg)'
- en: where
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '| ![ ⌊ ⌋ ∑M M∑ Φv (vi) := aivi + wijvihj = ⌈ai + wijhj⌉ vi, j=1 j=1 ∑N ∑M ∑N
    ∑M Ψv (vvi,h) := akvk + bjhj + wkjvkhj. k=1(k⁄=i) j=1 k=1(k⁄=i)j=1 ](img/file409.jpg)
    |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| ![ ⌊ ⌋ ∑M M∑ Φv (vi) := aivi + wijvihj = ⌈ai + wijhj⌉ vi, j=1 j=1 ∑N ∑M ∑N
    ∑M Ψv (vvi,h) := akvk + bjhj + wkjvkhj. k=1(k⁄=i) j=1 k=1(k⁄=i)j=1 ](img/file409.jpg)
    |  |'
- en: Plugging this into ([5.2.1](#x1-99003r1)) then yields
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将此代入([5.2.1](#x1-99003r1))后得到
- en: '| ![ exp (Φv (vi = 1)+ Ψv (vvi,h)) ℙ(vi = 1&#124;vvi,h ) =-------------------------------------------------------
    exp (Φv(vi = 1)+ Ψv (vvi,h))+ exp (Φv(vi = 0)+ Ψv (vvi,h)) = --exp-(Φv(vi =-1))-
    exp (Φv(vi = 1)) + 1 = σ (Φv(vi = 1)), ](img/file410.jpg) |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| ![ exp (Φv (vi = 1)+ Ψv (vvi,h)) ℙ(vi = 1&#124;vvi,h ) =-------------------------------------------------------
    exp (Φv(vi = 1)+ Ψv (vvi,h))+ exp (Φv(vi = 0)+ Ψv (vvi,h)) = --exp-(Φv(vi =-1))-
    exp (Φv(vi = 1)) + 1 = σ (Φv(vi = 1)), ](img/file410.jpg) |  |'
- en: since Φ[v](*v*[i] = 0) = 0, where
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 Φ[v](*v*[i] = 0) = 0，其中
- en: '| ![σ(x) :=---1--- 1 + e−x ](img/file411.jpg) |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ![σ(x) :=---1--- 1 + e−x ](img/file411.jpg) |  |'
- en: is the sigmoid function.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 是 Sigmoid 函数。
- en: 'Similarly, we can single out the contribution of the energy on a given hidden
    node *h*[j], using the RBM energy lemma:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以使用 RBM 能量引理将给定隐藏节点 *h*[j] 上的能量贡献单独提取出来：
- en: '![E (v,hj,hhj) = − Φh(hj)− Ψh (v,hhj), ](img/file412.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![E (v,hj,hhj) = − Φh(hj)− Ψh (v,hhj), ](img/file412.jpg)'
- en: where
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '| ![ ∑N [ ∑N ] Φh (hj) := bjhj + wijvihj = bj + wijvi hj, i=1 i=1 N M N M Ψ
    (v,h ) := ∑ a v + ∑ b h + ∑ ∑ w vh . h hj i i k k ik i k i=1 k=1(k⁄=j) i=1k=1(k⁄=j)
    ](img/file413.jpg) |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑N [ ∑N ] Φh (hj) := bjhj + wijvihj = bj + wijvi hj, i=1 i=1 N M N M Ψ
    (v,h ) := ∑ a v + ∑ b h + ∑ ∑ w vh . h hj i i k k ik i k i=1 k=1(k⁄=j) i=1k=1(k⁄=j)
    ](img/file413.jpg) |  |'
- en: Plugging this into ([5.2.1](#x1-99003r1)) then yields
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将此代入([5.2.1](#x1-99003r1))后得到
- en: '| ![ ( ( )) ℙ(h = 1&#124;v,h ) = ----(----------exp--Φ(h(hj-=))1)+-Ψh(-v,hhj----------(----))-
    j hj exp Φh(hj = 1)+ Ψh v,hhj + exp Φh (hj = 0) + Ψh v,hhj = --exp(Φh-(hj =-1))-
    exp (Φh(hj = 1)) + 1 = σ (Φ (h = 1 )) , h j ](img/file414.jpg) |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ( )) ℙ(h = 1&#124;v,h ) = ----(----------exp--Φ(h(hj-=))1)+-Ψh(-v,hhj----------(----))-
    j hj exp Φh(hj = 1)+ Ψh v,hhj + exp Φh (hj = 0) + Ψh v,hhj = --exp(Φh-(hj =-1))-
    exp (Φh(hj = 1)) + 1 = σ (Φ (h = 1 )) , h j ](img/file414.jpg) |  |'
- en: since again Φ[h](*h*[j] = 0) = 0\.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因为再次有 Φ[h](*h*[j] = 0) = 0\。
- en: 5.2.2 RBM network architecture
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 RBM 网络架构
- en: As shown above, an RBM is thus a shallow two-layer neural network that operates
    on stochastic binary activation units. The network forms a bipartite graph connecting
    stochastic binary inputs (visible units) to stochastic binary feature detectors
    (hidden units) with no connections between the units within the same layer, as
    shown in Figure [5.1](#5.1)  [[102](Biblography.xhtml#XFischer2012)].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，RBM 是一个浅层的两层神经网络，操作在随机二进制激活单元上。该网络形成一个二分图，将随机二进制输入（可见单元）与随机二进制特征检测器（隐藏单元）连接，且同一层内的单元之间没有连接，如图[5.1](#5.1)所示
    [[102](Biblography.xhtml#XFischer2012)]。
- en: '![Figurex1-100001r1: Schematic representation of an RBM with the visible layer
    units (white) and hidden layer units (dark) forming a bipartite graph. ](img/file415.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![Figurex1-100001r1: Schematic representation of an RBM with the visible layer
    units (white) and hidden layer units (dark) forming a bipartite graph. ](img/file415.jpg)'
- en: 'Figure 5.1: Schematic representation of an RBM with the visible layer units
    (white) and hidden layer units (dark) forming a bipartite graph.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：具有可见层单元（白色）和隐藏层单元（深色）形成二分图的RBM的示意图。
- en: 'Only the visible layer of the network is exposed to the training dataset and
    its inputs v := (*v*[1]*,…,v*[N]) flow through the network (forward pass) to the
    hidden layer, where they are aggregated and added to the hidden layer biases b
    := (*b*[1]*,…,b*[M]). The hidden layer sigmoid activation function ([5.2.1](#x1-99003r1))
    converts aggregated inputs into probabilities. Each hidden unit then "fires" randomly
    and outputs a {0*,*1} Bernoulli random variable with the associated probabilities:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 只有网络的可见层暴露给训练数据集，输入 v := (*v*[1]*,…,v*[N]) 流经网络（前向传播）到达隐藏层，在此它们被聚合并添加到隐藏层偏置
    b := (*b*[1]*,…,b*[M])。隐藏层的 Sigmoid 激活函数([5.2.1](#x1-99003r1))将聚合的输入转换为概率。然后每个隐藏单元以随机方式“触发”，并输出一个
    {0*,*1} 的伯努利随机变量，其相关概率为：
- en: '| ![ ( N∑ ) ( ∑N ) ℙ(hj = 1&#124;v) = σ bj + wijvi and ℙ(hj = 0&#124;v) = 1−
    σ bj + wijvi . i=1 i=1 ](img/file416.jpg) |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( N∑ ) ( ∑N ) ℙ(hj = 1&#124;v) = σ bj + wijvi and ℙ(hj = 0&#124;v) = 1−
    σ bj + wijvi . i=1 i=1 ](img/file416.jpg) |  |'
- en: 'The outputs from the hidden layer h := (*h*[1]*,…,h*[M]) then flow back (backward
    pass) to the visible layer, where they are aggregated and added to the visible
    layer biases a := (*a*[1]*,…,a*[N]). Similar to the hidden layer, the visible
    layer sigmoid activation function first translates aggregated inputs into probabilities
    and then into Bernoulli random variables:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 来自隐藏层的输出 h := (*h*[1]*,…,h*[M]) 随后反向传递（反向传播）到可见层，在那里它们被聚合并加到可见层的偏置 a := (*a*[1]*,…,a*[N])。与隐藏层类似，可见层的sigmoid激活函数首先将聚合的输入转换为概率，然后转化为伯努利随机变量：
- en: '| ![ ( M ) ( M ) ℙ(v = 1&#124;h) = σ(a + ∑ w h ) and ℙ (v = 0&#124;h) = 1−
    σ (a + ∑ w h ) . i i ij j i i ij j j=1 j=1 ](img/file417.jpg) |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( M ) ( M ) ℙ(v = 1&#124;h) = σ(a + ∑ w h ) and ℙ (v = 0&#124;h) = 1−
    σ (a + ∑ w h ) . i i ij j i i ij j j=1 j=1 ](img/file417.jpg) |  |'
- en: Therefore, every unit communicates at most one bit of information. This is especially
    important for the hidden units since this feature implements the information bottleneck
    structure, which acts as a strong regulariser  [[134](Biblography.xhtml#XHinton2010)].
    The hidden layer of the network can learn the low-dimensional probabilistic representation
    of the dataset if the network is organised and trained as an autoencoder.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个单元最多传递一个比特的信息。这对于隐藏单元尤为重要，因为这个特性实现了信息瓶颈结构，作为一种强正则化器[[134](Biblography.xhtml#XHinton2010)]。如果网络被组织并训练成自编码器，网络的隐藏层可以学习数据集的低维概率表示。
- en: 5.2.3 Sample encoding
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 样本编码
- en: Figure [5.2](#5.2) illustrates the binary representation of an input signal
    that enters the network through the visible layer. The number of activation units
    in the visible layer is determined by the number of features we have to encode
    and the desired precision of their binary representation. For example, if our
    sample consists of *m* continuous features and each feature is encoded as an *n*-digit
    binary number, the total number of activation units in the visible layer is *m*
    × *n*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5.2](#5.2)展示了输入信号的二进制表示，信号通过可见层进入网络。可见层中激活单元的数量由我们需要编码的特征数和其二进制表示的所需精度决定。例如，如果我们的样本由
    *m* 个连续特征组成，并且每个特征被编码为 *n* 位二进制数，则可见层中的激活单元总数为 *m* × *n*。
- en: '![Figurex1-101001r2: Schematic binary encoding of continuous variables. ](img/file418.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Figurex1-101001r2: Schematic binary encoding of continuous variables. ](img/file418.jpg)'
- en: 'Figure 5.2: Schematic binary encoding of continuous variables.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：连续变量的示意二进制编码。
- en: 5.2.4 Boltzmann distribution
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4 博尔兹曼分布
- en: 'The network learns the probability distribution ℙ(v*,*h) of the configurations
    of visible and hidden activation units – the Boltzmann distribution – by trying
    to reconstruct the inputs from the training dataset (visible unit values) through
    finding an optimal set of the network weights and biases:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 网络通过尝试从训练数据集（可见单元值）重建输入，来学习可见和隐藏激活单元配置的概率分布 ℙ(v*,*h) —— 即博尔兹曼分布 —— 通过找到网络权重和偏置的最佳集：
- en: '| ![ 1 ℙ(v,h) = --e−E(v,h), Z ](img/file419.jpg) |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| ![ 1 ℙ(v,h) = --e−E(v,h), Z ](img/file419.jpg) |  |'
- en: where the energy function reads
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中能量函数为
- en: '| ![ N M N M ∑ ∑ ∑ ∑ E (v,h) = − aivi − bjhj − wijvihj. i=1 j=1 i=1 j=1 ](img/file420.jpg)
    |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ![ N M N M ∑ ∑ ∑ ∑ E (v,h) = − aivi − bjhj − wijvihj. i=1 j=1 i=1 j=1 ](img/file420.jpg)
    |  |'
- en: 'Here, *Z* is the partition function:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*Z* 是配分函数：
- en: '| ![ ∑ Z = e−E (v,h). v,h ](img/file421.jpg) |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑ Z = e−E (v,h). v,h ](img/file421.jpg) |  |'
- en: 'However, we are usually interested either in learning the probability distribution
    of the visible layer configurations if we want to generate new samples that would
    have the same statistical properties as the original training dataset, or in learning
    the probability distribution of the hidden layer configurations if we want to
    build a deep neural network where the RBM layer performs the feature extraction
    and dimensionality reduction function. The probabilities of the visible (hidden)
    states are given by summing over all possible hidden (visible) vectors:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们通常感兴趣的是，如果我们希望生成与原始训练数据集具有相同统计性质的新样本，则学习可见层配置的概率分布，或者如果我们希望构建一个深度神经网络，其中RBM层执行特征提取和降维功能，则学习隐藏层配置的概率分布。可见（隐藏）状态的概率是通过对所有可能的隐藏（可见）向量求和得到的：
- en: '| ![ 1 ∑ 1 ∑ ℙ (v) = -- e−E(v,h) and ℙ(h) = -- e−E (v,h). Z h Z v ](img/file422.jpg)
    |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ![ 1 ∑ 1 ∑ ℙ (v) = -- e−E(v,h) and ℙ(h) = -- e−E (v,h). Z h Z v ](img/file422.jpg)
    |  |'
- en: The most popular training algorithm for RBM, *k-step Contrastive Divergence*,
    was proposed by Hinton  [[134](Biblography.xhtml#XHinton2010), [133](Biblography.xhtml#XHinton2002)].
    The algorithm aims to maximise the log probability of a training vector, i.e.,
    to find such network weights and biases that the "energy" function *E* is minimised
    for the samples from the training dataset (smaller value of energy corresponds
    to larger probability of a configuration). The *k*-step CD algorithm is fully
    specified in Section [5.3.2](#x1-1060002) and the interested reader can also find
    an excellent introduction to the training of RBMs in the work by Fischer and Igel  [[103](Biblography.xhtml#XFischer2014)].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的 RBM 训练算法 *k-step Contrastive Divergence* 是由 Hinton 提出的 [[134](Biblography.xhtml#XHinton2010)，[133](Biblography.xhtml#XHinton2002)]。该算法旨在最大化训练向量的对数概率，即找到一组网络权重和偏置，使得“能量”函数
    *E* 对训练数据集中的样本最小化（能量值越小，配置的概率越大）。*k*-步 CD 算法在第 [5.3.2](#x1-1060002) 节中有完整说明，感兴趣的读者还可以通过
    Fischer 和 Igel 的工作 [[103](Biblography.xhtml#XFischer2014)] 获得有关 RBM 训练的精彩介绍。
- en: 5.2.5 Extensions of the Bernoulli RBM
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.5 伯努利 RBM 的扩展
- en: The standard Bernoulli RBM setup we considered above restricts the visible layer v
    to a Bernoulli distribution. In fact, as long as the Hammersley-Clifford theorem
    holds, we can consider any distribution or any form of energy function. It was
    shown in  [[62](Biblography.xhtml#XCho), [178](Biblography.xhtml#XKrizhevsky)]
    for example, that a Bernoulli distribution for the hidden layer combined with
    a Gaussian distribution for the visible layer are compatible with an energy function
    of the form
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上述讨论的标准伯努利 RBM 设置将可见层 v 限制为伯努利分布。事实上，只要哈默斯利-克利福德定理成立，我们可以考虑任何分布或任何形式的能量函数。例如，[[62](Biblography.xhtml#XCho)，[178](Biblography.xhtml#XKrizhevsky)]
    中曾表明，隐藏层的伯努利分布与可见层的高斯分布是与如下形式的能量函数兼容的：
- en: '![ ∑N (vi − ai)2 ∑M ∑N ∑M vihj E (v,h) = ---2σ2---− bjhj − wijσ2--, i=1 i j=1
    i=1j=1 i ](img/file423.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑N (vi − ai)2 ∑M ∑N ∑M vihj E (v,h) = ---2σ2---− bjhj − wijσ2--, i=1 i j=1
    i=1j=1 i ](img/file423.jpg)'
- en: for some parameters *a*[i], *σ*[i], *b*[j], *w*[ij], *i* = 1*,…,N*, *j* = 1*,…,M*.
    In this case, for any *h*[j], the conditional probabilities ℙ(*h*[j] = 1|v) remain
    of sigmoid form and the conditional distribution of the visible layer is Gaussian
    as
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些参数 *a*[i]、*σ*[i]、*b*[j]、*w*[ij]，*i* = 1*,…,N*，*j* = 1*,…,M*。在这种情况下，对于任何
    *h*[j]，条件概率 ℙ(*h*[j] = 1|v) 保持 sigmoid 形式，并且可见层的条件分布为高斯分布，如下所示：
- en: '![ ( M ) Law (v|h) = 𝒩 (a + ∑ w h ,σ2 ) , for each i = 1,...,N. i i ij j i
    j=1 ](img/file424.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![ ( M ) Law (v|h) = 𝒩 (a + ∑ w h ,σ2 ) , for each i = 1,...,N. i i ij j i
    j=1 ](img/file424.jpg)'
- en: The RBMs we have considered do not account for time series, i.e., probability
    structures with temporal dependence. By enlarging the corresponding graph, in
    particular adding a conditional layer with directed connections to the classical
    hidden and visible layers, Taylor  [[280](Biblography.xhtml#XTaylorConditional)]
    showed that such dependence can be accounted for.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑的 RBM 不考虑时间序列，即具有时间依赖性的概率结构。通过扩大相应的图，特别是添加一个具有有向连接的条件层到经典的隐藏层和可见层，Taylor
    [[280](Biblography.xhtml#XTaylorConditional)] 表示可以考虑这种依赖性。
- en: An RBM is a neural network represented by a bipartite graph. Its power is derived
    from operating on stochastic binary activation units. It is a generative model
    that encodes learned probability distribution in its weights and biases and then
    generates new samples that are statistically indistinguishable from the samples
    in the original dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: RBM（受限玻尔兹曼机）是一种由二分图表示的神经网络。其强大之处在于操作随机二进制激活单元。它是一种生成模型，通过其权重和偏置编码学习到的概率分布，然后生成与原始数据集中的样本在统计上无法区分的新样本。
- en: If it is organised as an autoencoder with the bottleneck information structure,
    an RBM is able to learn the low-dimensional representation of the dataset. This
    property suggests that an RBM can be used as a feature extraction layer in a machine
    learning pipeline for certain supervised and unsupervised learning problems.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它被组织为具有瓶颈信息结构的自编码器，RBM 能够学习数据集的低维表示。这一特性表明，RBM 可以作为机器学习管道中的特征提取层，用于某些监督和无监督学习问题。
- en: 5.3 Training and Running RBM
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 训练和运行 RBM
- en: To build a neural network means to specify the network architecture and training
    algorithm. Having described the RBM architecture in the previous section, we now
    outline the training routines.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 构建神经网络意味着指定网络架构和训练算法。在前一节中已经描述了 RBM 的架构，现在我们概述训练流程。
- en: 5.3.1 Training RBM with Boltzmann sampling
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 使用玻尔兹曼采样训练 RBM
- en: The goal of RBM training is to estimate the optimal vector 𝜃 of model parameters
    (weights and biases) so that ℙ[𝜃](v) = ℙ[data](v). For a given training sample
    v := (*v*[1]*,…,v*[N]), the RBM aims at maximising the log-likelihood function,
    namely
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: RBM 训练的目标是估计模型参数（权重和偏置）的最优向量 𝜃，使得 ℙ[𝜃](v) = ℙ[data](v)。对于给定的训练样本 v := (*v*[1]*,…,v*[N])，RBM
    旨在最大化对数似然函数，即
- en: '![ n max ∑ 𝔏 (𝜃|v ), 𝜃 i=1 i ](img/file425.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![ n max ∑ 𝔏 (𝜃|v ), 𝜃 i=1 i ](img/file425.jpg)'
- en: where, for any v,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，对于任何 v，
- en: '![ ( ) ( ∑ ) ( ∑ ) ∑ 𝔏(𝜃|v) = log(ℙ(v)) = log -1 e−E(v,h) = log e− E(v,h) −
    log ( e−E (v,h)) . Z h h v,h ](img/file426.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![ ( ) ( ∑ ) ( ∑ ) ∑ 𝔏(𝜃|v) = log(ℙ(v)) = log -1 e−E(v,h) = log e− E(v,h) −
    log ( e−E (v,h)) . Z h h v,h ](img/file426.jpg)'
- en: The standard optimisation method, as proposed in  [[133](Biblography.xhtml#XHinton2002)],
    is a standard gradient ascent method, i.e., starting from an initial guess 𝜃⁰,
    we update it as
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 标准优化方法，如 [[133](Biblography.xhtml#XHinton2002)] 所提出的，是一种标准的梯度上升方法，即从初始猜测 𝜃⁰
    开始，我们按以下方式更新：
- en: '![ N 𝜃k+1 = 𝜃k + ∂ ∑ 𝔏 (𝜃k|v ) 𝜃 i=1 i ](img/file427.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![ N 𝜃k+1 = 𝜃k + ∂ ∑ 𝔏 (𝜃k|v ) 𝜃 i=1 i ](img/file427.jpg)'
- en: until we reach good enough convergence. In order to compute it, one first needs
    to compute the joint probabilities ℙ(*v*[i]*,h*[j]), which is classically done
    via Boltzmann (Gibbs) sampling  [[3](Biblography.xhtml#XAckley)], which is possible
    since we know exactly the conditional distributions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 直到我们达到足够好的收敛。为了计算它，首先需要计算联合概率 ℙ(*v*[i]*,h*[j])，这通常通过玻尔兹曼（吉布斯）采样完成 [[3](Biblography.xhtml#XAckley)]，因为我们确切知道条件分布。
- en: 5.3.2 The Contrastive Divergence algorithm
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 对比散度算法
- en: While training RBMs can be performed with Boltzmann sampling, this is usually
    prohibitively expensive to run. A more efficient training algorithm, the *k*-step
    Contrastive Divergence (CD) algorithm, was proposed in  [[134](Biblography.xhtml#XHinton2010)].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 RBM 时，可以使用玻尔兹曼采样，但这通常是非常昂贵的。为了提高效率，提出了一种更高效的训练算法——*k*-步对比散度（CD）算法，参考了 [[134](Biblography.xhtml#XHinton2010)]。
- en: '![--------------------------------------------------------------------- -Algorithm---2:-k-
    step-Contrastive-Divergence-------------------------- Result: Weights and biases
    updates. Input: • Training minibatch S; • Model parameters ai,bj,wij for i = 1,...,N,j
    = 1,...,M (before update ). Initialisation: for all i,j : Δwij = Δai = Δbj = 0
    for v ∈ S do | v(0) ← v | | for t = 0,...,k − 1 do | | for j = 1,...,M do | |
    | (t) (t) | | | sample Bernoulli random variable hj ∼ ℙ(hj|v ) | | end | | | |
    for i = 1,...,N do | | | sample Bernoulli random variable v(t+1) ∼ ℙ(vi|h(t))
    | | i | end | end | | for i = 1,...,N, j = 1(,...,M do ) | | (0) (0) (k) (k) |
    Δwij ← Δwij + η ℙ(hj = 1|v )vi − ℙ(hj = 1|v )vi | end | | for i = 1,...,N do(
    ) | | Δai ← Δai + η v(0) − v(k) | i i | end | | for j = 1,...,M do( ) | | Δbj
    ← Δbj + η ℙ(hj = 1|v(0)) − ℙ(hj = 1|v(k)) | end end ---------------------------------------------------------------------
    ](img/file428.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- -算法---2:-k-步-对比-散度--------------------------
    结果：权重和偏置更新。输入： • 训练小批量 S； • 模型参数 ai,bj,wij，对于 i = 1,...,N,j = 1,...,M（更新前）。初始化：对所有
    i,j : Δwij = Δai = Δbj = 0 对于 v ∈ S 做 | v(0) ← v | | 对 t = 0,...,k − 1 做 | | 对
    j = 1,...,M 做 | | | (t) (t) | | | 采样伯努利随机变量 hj ∼ ℙ(hj|v ) | | 结束 | | | | 对 i =
    1,...,N 做 | | | 采样伯努利随机变量 v(t+1) ∼ ℙ(vi|h(t)) | | i | 结束 | 结束 | | 对 i = 1,...,N,
    j = 1(,...,M 做 ) | | (0) (0) (k) (k) | Δwij ← Δwij + η ℙ(hj = 1|v )vi − ℙ(hj =
    1|v )vi | 结束 | | 对 i = 1,...,N 做( ) | | Δai ← Δai + η v(0) − v(k) | i i | 结束 |
    | 对 j = 1,...,M 做( ) | | Δbj ← Δbj + η ℙ(hj = 1|v(0)) − ℙ(hj = 1|v(k)) | 结束 结束
    --------------------------------------------------------------------- ](img/file428.jpg)'
- en: 'The choice of *k* balances accuracy and speed. For many practical purposes
    *k* = 1 is an optimal choice, even though the expectations may be biased in this
    case. However, the bias tends to be small  [[53](Biblography.xhtml#XCarreira2005)].
    The network is trained through the updates of weights and biases, which increase
    the log probability of a training vector and are given by the following expressions:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 的选择平衡了准确性和速度。对于许多实际应用，*k* = 1 是一个最佳选择，尽管在这种情况下，期望值可能会有偏差。然而，这种偏差往往是很小的
    [[53](Biblography.xhtml#XCarreira2005)]。网络通过更新权重和偏置来训练，这些更新增加了训练向量的对数概率，更新公式如下：'
- en: '| ![Δw = η∂ℙ(v)-= η (⟨v h ⟩ − ⟨v h ⟩ ), ij ∂wij i j data i j model ](img/file429.jpg)
    |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ![Δw = η∂ℙ(v)-= η (⟨v h ⟩ − ⟨v h ⟩ ), ij ∂wij i j data i j model ](img/file429.jpg)
    |  |'
- en: '| ![ ∂-ℙ(v) Δai = η ∂ai = η(⟨vi⟩data − ⟨vi⟩model), ](img/file430.jpg) |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∂-ℙ(v) Δai = η ∂ai = η(⟨vi⟩data − ⟨vi⟩model), ](img/file430.jpg) |  |'
- en: '| ![ ∂ℙ (v) Δbj = η------= η (⟨hj⟩data − ⟨hj⟩model), ∂bi ](img/file431.jpg)
    |  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∂ℙ (v) Δbj = η------= η (⟨hj⟩data − ⟨hj⟩model), ∂bi ](img/file431.jpg)
    |  |'
- en: where ⟨⋅⟩ denote expectations under the distribution specified by the subscript
    and *η* is the chosen learning rate. Expectations ⟨⋅⟩[data] can be calculated
    directly from the training dataset while getting unbiased samples of ⟨⋅⟩[model]
    requires performing alternating sampling from the model Boltzmann distribution
    for a long time (this is needed to achieve the state of thermal equilibrium),
    starting from some randomly initialised state. However, the *k*-step CD method
    can be used to approximate ⟨⋅⟩[model] with another, easier-to-calculate expectation,
    as shown in Algorithm [2](#x1-106003r2).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ⟨⋅⟩ 表示在下标所指定的分布下的期望，*η* 是选择的学习率。期望 ⟨⋅⟩[data] 可以直接从训练数据集计算，而获得无偏样本 ⟨⋅⟩[model]
    则需要在模型的玻尔兹曼分布中进行交替采样一段较长时间（这是为了实现热平衡状态），并且从某个随机初始化的状态开始。然而，*k* 步骤的CD方法可以用来近似 ⟨⋅⟩[model]，通过另一个更易计算的期望，如算法[2](#x1-106003r2)所示。
- en: 5.3.3 Generation of synthetic samples
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3 合成样本的生成
- en: Once fully trained, the network can be used to generate new samples from the
    learned distribution. For example, the RBM can be used as a market generator that
    produces new market scenarios in the form of the new synthetic samples drawn from
    the multivariate distribution of the market risk factors encoded in the network
    weights and biases.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络完全训练好，就可以用来从学习到的分布中生成新的样本。例如，RBM可以作为一个市场生成器，生成新的市场情景，这些新样本来自于网络权重和偏置中编码的市场风险因素的多元分布。
- en: 'The first step is the generation of a random input: each visible unit is initialised
    with a randomly generated binary variable. The second step is performing a large
    number of forward and backward passes between the visible and the hidden layers,
    until the system reaches a state of *thermal equilibrium*: a state where the initial
    random vector is transformed into a sample from the learned distribution. The
    number of cycles needed to reach the state of thermal equilibrium is problem dependent
    and is a function of network architecture and network parameters (weights and
    biases). In some cases, the generation of independent samples requires 10³ − 10⁴
    forward and backward passes through the network  [[173](Biblography.xhtml#XKS2020)].
    The final step is the readout from the visible layer, which gives us a bitstring,
    encoding the sample from the target distribution.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是生成一个随机输入：每个可见单元都被初始化为一个随机生成的二进制变量。第二步是在可见层和隐藏层之间进行大量的前向和后向传播，直到系统达到*热平衡*状态：即初始的随机向量被转换为从学习到的分布中抽取的样本。达到热平衡状态所需的周期数取决于具体问题，是网络架构和网络参数（权重和偏置）的函数。在某些情况下，生成独立样本需要进行10³
    − 10⁴次前向和后向传播[[173](Biblography.xhtml#XKS2020)]。最后一步是从可见层读取输出，这将给我们一个比特串，编码了来自目标分布的样本。
- en: 'Figure [5.3](#5.3) displays the QQ-plots of the samples drawn from the distributions
    of daily returns for two stock indices: German DAX and Brazilian BOVESPA. Recall
    that a quantile-quantile (or QQ) plot is a scatter plot created by plotting two
    sets of quantiles against one another. If both sets come from the same distribution,
    all points should lie close to the diagonal. The dataset consists of 536 samples – daily
    index returns observed between 5 January 2009 and 22 February 2011 (UCI Machine
    Learning Repository  [[10](Biblography.xhtml#XAkbilgic2013), [9](Biblography.xhtml#XUCI_SP)]).
    The "Normal" distribution models daily returns as Normally distributed with a
    mean and variance that match those from the historical dataset. The "RBM" distribution
    is a dataset of RBM-generated samples that, ideally, should have exactly the same
    statistical properties as the original historical dataset. If the samples drawn
    from two distributions have identical quantiles, the QQ-plots will have all points
    placed on the diagonal and we can conclude that the two distributions are identical.
    Figure [5.3](#5.3) shows that this is indeed the case (with reasonably good accuracy)
    for the samples from the "Data" and "RBM" distributions, while both demonstrate
    much heavier tails in comparison with the fitted Normal distribution.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5.3](#5.3)展示了从两个股票指数的每日回报分布中抽取的样本的QQ图：德国DAX和巴西BOVESPA。回想一下，分位数-分位数（或QQ）图是通过将两个分位数集相互对照，绘制成的散点图。如果这两个集来自同一分布，所有点应该接近对角线。数据集包含536个样本–
    从2009年1月5日到2011年2月22日之间观察到的每日指数回报（UCI机器学习库[[10](Biblography.xhtml#XAkbilgic2013)，[9](Biblography.xhtml#XUCI_SP)]）。"Normal"分布将每日回报建模为正态分布，其均值和方差与历史数据集中的匹配。"RBM"分布是RBM生成的样本数据集，理想情况下应具有与原始历史数据集完全相同的统计特性。如果从两个分布中抽取的样本具有相同的分位数，QQ图中的所有点都将位于对角线处，我们可以得出结论，这两个分布是相同的。图[5.3](#5.3)显示，来自"Data"和"RBM"分布的样本确实符合这一点（具有相当好的准确性），而两者在与拟合的正态分布相比时，显示出更重的尾部。
- en: 'The results shown in Figure [5.3](#5.3) were obtained with an RBM trained on
    a dataset of daily returns. Each return from the training dataset was converted
    into a 12-digit binary number. Every digit of the binary number was treated as
    a separate binary feature (12 features per index; 24 features in total) – this
    required placing 24 activation units in the visible layer of the RBM network.
    The number of hidden units was set to 16\. Thus, the network was trained as a
    strongly regularised autoencoder. The generated returns (in the binary format)
    were then converted back into their continuous representation. The model was Bernoulli
    RBM (sklearn.neural_network.BernoulliRBM) from the open source `scikit-learn`
    package  [[230](Biblography.xhtml#XSL)] with the following set of parameters:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5.3](#5.3)中显示的结果是通过在每日回报数据集上训练的RBM得到的。每个来自训练数据集的回报都被转换为一个12位二进制数。二进制数的每一位被当作一个独立的二进制特征处理（每个索引有12个特征；总共有24个特征）–
    这要求在RBM网络的可见层中放置24个激活单元。隐藏单元的数量设置为16。因此，网络作为一个强正则化的自编码器进行训练。生成的回报（以二进制格式）随后被转换回它们的连续表示。该模型是来自开源`scikit-learn`包的Bernoulli
    RBM（sklearn.neural_network.BernoulliRBM）[[230](Biblography.xhtml#XSL)]，其参数设置如下：
- en: n_components = 16 – number of hidden activation units
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n_components = 16 – 隐藏激活单元的数量
- en: learning_rate = 0.0005
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: learning_rate = 0.0005
- en: batch_size = 10 – size of the training minibatches
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: batch_size = 10 – 训练小批量的大小
- en: n_iter = 40000 – number of iterations
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n_iter = 40000 – 迭代次数
- en: The synthetic data generation approach can be formulated as Algorithm [3](#x1-107014r3).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据生成方法可以表述为算法[3](#x1-107014r3)。
- en: '![--------------------------------------------------------------------- -Algorithm---3:-Synthetic-Data-Generation-----------------------------
    1: The construction of the binary representation of the original dataset: a) A
    continuous feature can be converted into an equivalent binary representation with
    the required precision. b) An integer feature x ∈ {x1,...,xn } can be translated
    into an N -digit binary number through the standard procedure, where N−1 N 2 ≤
    1m≤ajx≤n(xj)− 1m≤ijn≤n(xj) < 2 . c) A categorical feature can be binarised either
    through the one-hot encoding method or following the same procedure as for the
    integer numbers since categorical values can be enumerated. d) The same applies
    to class labels, both integer and categorical. 2: The training of an RBM on the
    binary representation of the original dataset with the help of a 1-step CD algorithm.
    3: The generation of the required number of new synthetic samples in binary format.
    4: For each synthetic data sample: the conversion of the generated binary features
    into the corresponding categorical, integer, and continuous representations. 5:
    The generated synthetic dataset is ready to be used for the training of various
    classifiers and regressors. ---------------------------------------------------------------------
    ](img/file432.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- -算法---3:-合成数据生成-----------------------------
    1: 原始数据集二进制表示的构建：a) 连续特征可以转换为具有所需精度的等效二进制表示。b) 整数特征 x ∈ {x1,...,xn} 可以通过标准程序转换为N位二进制数，其中
    N−1 N 2 ≤ 1m≤ajx≤n(xj)− 1m≤ijn≤n(xj) < 2。c) 类别特征可以通过独热编码方法或与整数特征相同的程序进行二值化，因为类别值可以枚举。d)
    类别标签同样适用，包括整数和类别类型。2: 使用1步CD算法在原始数据集的二进制表示上训练RBM。3: 生成所需数量的新合成样本（以二进制格式）。4: 对于每个合成数据样本：将生成的二进制特征转换为相应的类别、整数和连续表示。5:
    生成的合成数据集已准备好用于各种分类器和回归器的训练。 ---------------------------------------------------------------------](img/file432.jpg)'
- en: '![Figurex1-107016r3: QQ-plots of the generated and historical returns. a)-c) DAX.
    d)-f) BOVESPA. The RBM learns the heavy-tailed empirical distribution of stock
    index returns. ](img/file433.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![Figurex1-107016r3: 生成的和历史回报的QQ图。a)-c) DAX. d)-f) BOVESPA。RBM学习了股票指数回报的重尾经验分布。](img/file433.png)'
- en: 'Figure 5.3: QQ-plots of the generated and historical returns. a)-c) DAX. d)-f) BOVESPA.
    The RBM learns the heavy-tailed empirical distribution of stock index returns.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：生成的和历史回报的QQ图。a)-c) DAX. d)-f) BOVESPA。RBM学习了股票指数回报的重尾经验分布。
- en: Kondratyev and Schwarz  [[173](Biblography.xhtml#XKS2020)] proposed an RBM-based
    market generator and investigated its properties on a dataset of daily spot FX
    log-returns. The time series of four currency pairs’ log-returns covered a 20-year
    time interval (1999-2019), which allowed the RBM to learn the dependence structure
    of the multivariate distribution and successfully reconstruct linear and rank
    correlations as well as joint tail behaviour. Also, it was shown that an RBM can
    be used to perform conditional sampling (e.g., from low-volatility/high-volatility
    regimes) and achieve the desired degree of autocorrelation by varying the thermalisation
    parameter. Other productive applications of RBM-based synthetic data generators
    are data anonymisation, fighting overfitting, and the detection of outliers as
    demonstrated by Kondratyev, Schwarz, and Horvath  [[174](Biblography.xhtml#XKSH2020)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Kondratyev 和 Schwarz [[173](Biblography.xhtml#XKS2020)] 提出了基于RBM的市场生成器，并研究了其在日常外汇对数回报数据集上的属性。四个货币对的对数回报时间序列涵盖了20年的时间间隔（1999-2019），这使得RBM能够学习多变量分布的依赖结构，并成功重建线性和秩相关性以及联合尾部行为。此外，还表明，RBM可以用于执行条件采样（例如，从低波动/高波动状态），并通过调整热化参数来实现所需的自相关程度。RBM基于的合成数据生成器的其他有效应用包括数据匿名化、对抗过拟合和异常值检测，正如Kondratyev、Schwarz和Horvath
    [[174](Biblography.xhtml#XKSH2020)] 所展示的那样。
- en: In addition to operating on stochastic binary activation units, the RBM gains
    extra resistance to overfitting through the autoencoder architecture and being
    trained with stochastic gradient ascent. This allows RBMs to learn complex multivariate
    probability distributions from relatively small datasets while avoiding overfitting.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在随机二进制激活单元上操作外，RBM还通过自编码器架构和使用随机梯度上升法进行训练，从而增强了对过拟合的抵抗力。这使得RBM能够从相对较小的数据集中学习复杂的多变量概率分布，同时避免了过拟合。
- en: 5.4 Quantum Annealing and Boltzmann Sampling
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 量子退火与玻尔兹曼采样
- en: The application of quantum annealing to Boltzmann sampling is based on the direct
    correspondence between the RBM energy function given by ([5.2.4](#x1-1020004))
    and the Hamiltonian in quantum annealing. Recall from Chapter [2](Chapter_2.xhtml#x1-480002)
    that quantum annealing is based on the principles of adiabatic evolution from
    the initial state at *t* = 0 given by a Hamiltonian ℋ[0] to a final state at *t*
    = *T* given by a Hamiltonian ℋ[F] , such that the system Hamiltonian at time *t*
    ∈ [0*,T*] is given by
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 将量子退火应用于玻尔兹曼采样基于RBM能量函数（见[5.2.4](#x1-1020004)）与量子退火中的哈密顿量之间的直接对应关系。回顾第[2](Chapter_2.xhtml#x1-480002)章，量子退火基于绝热演化的原则，从初态
    *t* = 0开始，由哈密顿量ℋ[0]定义，到最终状态 *t* = *T* 结束，由哈密顿量ℋ[F]定义，系统在时间 *t* ∈ [0, T]时的哈密顿量为：
- en: '| ![ℋ (t) = r(t)ℋ0 + (1− r(t))ℋF , ](img/file434.jpg) |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| ![ℋ (t) = r(t)ℋ0 + (1− r(t))ℋF , ](img/file434.jpg) |  |'
- en: 'where *r*(*t*) decreases from 1 to 0 as *t* goes from 0 to *T*. An ideal adiabatic
    evolution scenario envisages the system always staying in the ground state of
    ℋ(*t*): if the system starts in the ground state of ℋ[0] and the evolution proceeds
    slowly enough to satisfy the conditions of the quantum adiabatic theorem (Chapter [2](Chapter_2.xhtml#x1-480002)),
    then the system will end up in the ground state of ℋ[F] .'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *r*(*t*) 从1递减到0，随着 *t* 从0到 *T* 变化。理想的绝热演化场景设想系统始终保持在ℋ(*t*)的基态：如果系统从ℋ[0]的基态开始，且演化过程足够缓慢以满足量子绝热定理的条件（见第[2](Chapter_2.xhtml#x1-480002)章），那么系统最终将达到ℋ[F]的基态。
- en: In practice, existing quantum annealing hardware does not strictly satisfy the
    conditions of the quantum adiabatic theorem. Quantum annealers operate at very
    low temperatures of about 15mK  [[90](Biblography.xhtml#XDW2020)], but some residual
    thermal noise is still present. There is also some amount of cross-talk between
    the qubits and the chains of physical qubits that represent logical qubits can
    be broken. Cross-talk is the effect of a desired action on one or more qubits
    unintentionally affecting one or more other qubits. In some cases, cross-talk
    is the major source of computational errors. This poses serious issues for quantum
    annealers solving optimisation problems where the main objective is to find an
    exact ground state. But some residual amount of thermal and electromagnetic noise
    is desirable if we want to use a quantum annealer as a sampler.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，现有的量子退火硬件并没有严格满足量子绝热定理的条件。量子退火器在约15mK的非常低温下工作[[90](Biblography.xhtml#XDW2020)]，但仍然存在一些残余的热噪声。同时，量子比特之间也会有一些串扰，物理量子比特的链条代表逻辑量子比特时，这些链条可能会被破坏。串扰是指某个或多个量子比特的期望操作不小心影响到其他一个或多个量子比特。在某些情况下，串扰是计算错误的主要来源。这对量子退火器解决优化问题提出了严重的挑战，尤其是当主要目标是找到精确的基态时。但是，如果我们想将量子退火器作为采样器使用，一定量的残余热噪声和电磁噪声是可以接受的。
- en: 5.4.1 Boltzmann sampling
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1 玻尔兹曼采样
- en: 'The quantum annealer as a sampling engine is based on the central proposal
     [[4](Biblography.xhtml#XAdachi2015)] that the distribution of excited states
    can be modelled as a Boltzmann distribution:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 量子退火器作为采样引擎的基础是中心提议[[4](Biblography.xhtml#XAdachi2015)]，即激发态的分布可以被建模为玻尔兹曼分布：
- en: '| ![ 1- ℙ(x) = Z exp (− β ℋF (x )) , ](img/file435.jpg) |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| ![ 1- ℙ(x) = Z exp (− β ℋF (x )) , ](img/file435.jpg) |  |'
- en: 'where *β* is some parameter (which can be seen as an effective inverse temperature)
    and *Z* is the partition function:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *β* 是某个参数（可以视为有效的倒数温度），*Z* 是配分函数：
- en: '| ![ ∑ Z = exp (− βℋF (x)). x ](img/file436.jpg) |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑ Z = exp (− βℋF (x)). x ](img/file436.jpg) |  |'
- en: 'If we define the binary vector x to be the concatenation of the visible node
    vector v and the hidden node vector h:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将二进制向量x定义为可见节点向量v和隐藏节点向量h的连接：
- en: '| ![x := (v ,v ,...,v ,h ,h ,...,h ), 1 2 N 1 2 M ](img/file437.jpg) |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| ![x := (v ,v ,...,v ,h ,h ,...,h ), 1 2 N 1 2 M ](img/file437.jpg) |  |'
- en: 'then, by comparing ([5.2.4](#x1-1020004)) and ([5.4.1](#x1-1090001)), we can
    establish a direct correspondence between the energy function *E* and the Hamiltonian ℋ[F]
    . Therefore, we can suggest an alternative way of calculating the expectations
    ⟨⋅⟩[model] formulated as in the following algorithm  [[4](Biblography.xhtml#XAdachi2015)]:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过比较[5.2.4](#x1-1020004)和[5.4.1](#x1-1090001)，我们可以建立能量函数 *E* 与哈密顿量ℋ[F]之间的直接对应关系。因此，我们可以提出一种替代方法来计算期望值
    ⟨⋅⟩[model]，该方法按照以下算法[[4](Biblography.xhtml#XAdachi2015)]进行：
- en: '![--------------------------------------------------------------------- -Algorithm---4:-Boltzmann--Sampling-----------------------------------
    1: Use the RBM energy function E as the final Hamiltonian ℋF . 2: Run quantum
    annealing K times and collect the readout statistics for vi(k ) and hj(k), i =
    1,...,N , j = 1,...,M , k = 1,...,K. 3: Calculate the unbiased expectations: 1
    ∑K ⟨vihj⟩model :=-- vi(k)hj(k), K k=1 1 ∑K ⟨vi⟩model := K- vi(k), k=1 ∑K ⟨hj⟩model
    := 1- hj(k). K k=1 ---------------------------------------------------------------------
    ](img/file438.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- -算法---4:-玻尔兹曼--采样-----------------------------------
    1: 使用RBM能量函数E作为最终的哈密顿量ℋF。 2: 运行量子退火K次，并收集vi(k)和hj(k)的读取统计量，i = 1,...,N，j = 1,...,M，k
    = 1,...,K。 3: 计算无偏期望值：1 ∑K ⟨vihj⟩model :=-- vi(k)hj(k), K k=1 1 ∑K ⟨vi⟩model :=
    K- vi(k), k=1 ∑K ⟨hj⟩model := 1- hj(k)。 K k=1 ---------------------------------------------------------------------
    ](img/file438.jpg)'
- en: 'There are two main motivations for using quantum annealing to perform Boltzmann
    sampling as described in Algorithm [4](#x1-109006r4). First, it bypasses the need
    for running the Contrastive Divergence algorithm (Algorithm [2](#x1-106003r2)),
    which only provides approximations to the expectations ⟨⋅⟩[model] (even though
    these approximations can be sufficiently accurate). Second, the anneal time needed
    to generate a new sample from the Boltzmann distribution is of the order of ∼1 microsecond
    regardless of the graph size. This is not the case with the classical RBM, where
    it is often necessary to perform thousands of forward and backward passes through
    the network before a new independent sample from the Boltzmann distribution encoded
    in the network weights and biases can be read out  [[173](Biblography.xhtml#XKS2020)].
    For large RBM graphs, it can easily take tens of milliseconds on standard hardware.
    Thus, we have two avenues of exploring the potential quantum advantage offered
    by quantum annealing for Boltzmann sampling: accuracy and speedup.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用量子退火进行玻尔兹曼采样有两个主要的动机，如算法[4](#x1-109006r4)所述。首先，它绕过了运行对比散度算法（算法[2](#x1-106003r2)）的需要，该算法仅提供对期望值⟨⋅⟩[model]的近似值（尽管这些近似可以非常准确）。其次，从玻尔兹曼分布生成新样本所需的退火时间约为∼1微秒，与图的大小无关。在经典的RBM中，情况并非如此，通常需要进行数千次的前向和反向传播，才能读取到网络权重和偏置中编码的玻尔兹曼分布的独立新样本[[173](Biblography.xhtml#XKS2020)]。对于大的RBM图，在标准硬件上可能需要花费数十毫秒。因此，我们可以从准确性和加速两个方面来探索量子退火为玻尔兹曼采样带来的潜在量子优势。
- en: 5.4.2 Mapping
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2 映射
- en: 'The first step in performing Boltzmann sampling on a quantum annealer is the
    mapping of the RBM onto the quantum annealing hardware graph. We start with writing
    an expression for the RBM energy function *E* in the following form:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在量子退火器上执行玻尔兹曼采样的第一步是将RBM映射到量子退火硬件图上。我们从为RBM能量函数*E*写出以下形式的表达式开始：
- en: '| ![E (v,h) = E(x) = βxTQx. ](img/file439.jpg) |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| ![E (v,h) = E(x) = βxTQx. ](img/file439.jpg) |  |'
- en: 'Here, *Q* is the (*N* + *M*) × (*N* + *M*) matrix whose elements are RBM weights
    and biases:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*Q*是一个(*N* + *M*) × (*N* + *M*)矩阵，其元素是RBM的权重和偏置：
- en: '| ![ ⌊ &#124; ⌋ &#124;a1 0 ... 0 &#124;w11 w12 ... w1M &#124; &#124;&#124;
    0 a2 ... 0 &#124;w21 w22 ... w2M &#124;&#124; &#124;&#124; . . . . &#124; . .
    . . &#124;&#124; &#124; .. .. .. .. &#124; .. .. .. .. &#124; &#124;&#124; 0 0
    ... a &#124;w w ... w &#124;&#124; Q = 1-&#124;&#124;-------------N--&#124;--N1---N2--------NM--&#124;&#124;.
    β &#124;&#124; 0 0 ... 0 &#124; b1 0 ... 0 &#124;&#124; &#124;&#124; &#124; &#124;&#124;
    &#124;&#124; 0 0 ... 0 &#124; 0 b2 ... 0 &#124;&#124; &#124; ... ... ... ... &#124;
    ... ... ... ... &#124; ⌈ &#124; ⌉ 0 0 ... 0 &#124; 0 0 ... bM ](img/file440.jpg)
    |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| ![ ⌊ &#124; ⌋ &#124;a1 0 ... 0 &#124;w11 w12 ... w1M &#124; &#124;&#124;
    0 a2 ... 0 &#124;w21 w22 ... w2M &#124;&#124; &#124;&#124; . . . . &#124; . .
    . . &#124;&#124; &#124; .. .. .. .. &#124; .. .. .. .. &#124; &#124;&#124; 0 0
    ... a &#124;w w ... w &#124;&#124; Q = 1-&#124;&#124;-------------N--&#124;--N1---N2--------NM--&#124;&#124;.
    β &#124;&#124; 0 0 ... 0 &#124; b1 0 ... 0 &#124;&#124; &#124;&#124; &#124; &#124;&#124;
    &#124;&#124; 0 0 ... 0 &#124; 0 b2 ... 0 &#124;&#124; &#124; ... ... ... ... &#124;
    ... ... ... ... &#124; ⌈ &#124; ⌉ 0 0 ... 0 &#124; 0 0 ... bM ](img/file440.jpg)
    |  |'
- en: Quantum annealers operate on spin variables {−1*,*+1} instead of binary variables
    {0, 1}. The vector of binary variables x can be transformed into the vector of
    spin variables s using
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 量子退火器操作的是自旋变量{−1*,*+1}，而不是二进制变量{0, 1}。二进制变量向量x可以通过以下方式转换为自旋变量向量s：
- en: '| ![x −→ s = 2x − 1, ](img/file441.jpg) |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| ![x −→ s = 2x − 1, ](img/file441.jpg) |  |'
- en: 'and we obtain the following expression for the RBM energy:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到RBM能量的以下表达式：
- en: '| ![ N∑ N+∑M ∑N N∑+M E = − gisi − gjsj − Jijsisj − const = EIsing − const,
    i=1 j=N+1 i=1j=N+1 ](img/file442.jpg) |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| ![ N∑ N+∑M ∑N N∑+M E = − gisi − gjsj − Jijsisj − const = EIsing − const,
    i=1 j=N+1 i=1j=N+1 ](img/file442.jpg) |  |'
- en: where, for *i* = 1*,…,N* and *j* = *N* + 1*,…,N* + *M*,
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，对于 *i* = 1, …, N 和 *j* = *N* + 1, …, *N* + *M*，
- en: '| ![ N+M N g := ai+ 1- ∑ w , g := bj + 1-∑ w , J := 1w , i 2 4 ij j 2 4 i=1
    ij ij 4 ij j=N+1 ](img/file443.jpg) |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| ![ N+M N g := ai+ 1- ∑ w , g := bj + 1-∑ w , J := 1w , i 2 4 ij j 2 4 i=1
    ij ij 4 ij j=N+1 ](img/file443.jpg) |  |'
- en: and (*s*[i])[i=1,…,N] are spin variables corresponding to the visible nodes
    and (*s*[j])[j=N+1,…,N+M] are spin variables corresponding to the hidden nodes.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 并且 (*s*[i])[i=1,…,N] 是对应可见节点的自旋变量，而 (*s*[j])[j=N+1,…,N+M] 是对应隐藏节点的自旋变量。
- en: We can ignore the constant term in the RBM energy expression ([5.4.2](#x1-1100002))
    since the same factor will appear in the numerator and denominator of ℙ(v*,*h).
    Thus, we have
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以忽略 RBM 能量表达式中的常数项 ([5.4.2](#x1-1100002))，因为相同的因子将在 ℙ(v*,*h) 的分子和分母中出现。因此，我们得到：
- en: '| ![⟨vihj⟩EmIosidngel = ⟨vihj⟩Emodel. ](img/file444.jpg) |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| ![⟨vihj⟩EmIosidngel = ⟨vihj⟩Emodel. ](img/file444.jpg) |  |'
- en: 'To express the Ising Hamiltonian using a quantum mechanical description of
    spins, we replace the spin variables with their respective Pauli operators:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用量子力学描述自旋来表达伊辛哈密顿量，我们用相应的保利算符替换自旋变量：
- en: '| ![ N N+M N N+M ∑ i ∑ j ∑ ∑ i j ℋIsing = − giσz − gjσz − Jijσzσz, i=1 j=N+1
    i=1j=N+1 ](img/file445.jpg) |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| ![ N N+M N N+M ∑ i ∑ j ∑ ∑ i j ℋIsing = − giσz − gjσz − Jijσzσz, i=1 j=N+1
    i=1j=N+1 ](img/file445.jpg) |  |'
- en: with *σ*[z]^i being the usual Pauli matrix representation for an Ising quantum
    spin. With the initial Hamiltonian given by
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *σ*[z]^i 是表示伊辛量子自旋的常用保利矩阵。初始哈密顿量为：
- en: '| ![ N+∑M ℋ0 = σi, i=1 x ](img/file446.jpg) |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| ![ N+∑M ℋ0 = σi, i=1 x ](img/file446.jpg) |  |'
- en: the time-dependent Hamiltonian ([5.4](#x1-1080004)) takes the form
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 随时间变化的哈密顿量 ([5.4](#x1-1080004)) 形式为：
- en: '| ![ℋ (t) = r(t)ℋ0 + (1 − r(t))ℋIsing. ](img/file447.jpg) |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| ![ℋ (t) = r(t)ℋ0 + (1 − r(t))ℋIsing. ](img/file447.jpg) |  |'
- en: 5.4.3 Hardware embedding and parameters optimisation
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3 硬件嵌入和参数优化
- en: In the standard programming practices of existing quantum annealers, each spin
    variable *s*[i] should ideally be assigned to a specific chip element, a superconducting
    flux qubit, modelled by a quantum two-level system that could represent the quantum
    Hamiltonian
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有量子退火器的标准编程实践中，每个自旋变量 *s*[i] 理想情况下应分配给一个特定的芯片元素，即超导量子通量比特，模型为量子二能级系统，可以表示量子哈密顿量。
- en: '| ![ ∑ ℋlocal = giσiz. i ](img/file448.jpg) |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑ ℋlocal = giσiz. i ](img/file448.jpg) |  |'
- en: While each qubit supports the programming of the *g*[i] terms, the *J*[ij] parameters
    can then be implemented energetically through inductive elements, meant to represent
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然每个量子比特支持 *g*[i] 项的编程，但 *J*[ij] 参数可以通过电感元件以能量方式实现，这些元件旨在表示。
- en: '| ![ ∑ ℋcouplers = Jijσizσjz, ij ](img/file449.jpg) |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑ ℋcouplers = Jijσizσjz, ij ](img/file449.jpg) |  |'
- en: if and only if the required circuitry exists between qubits *i* and *j*, which
    cannot be manufactured too far apart in the spatial layout of the processor due
    to engineering considerations  [[296](Biblography.xhtml#XVenturelli2019)]. In
    other words, *J*[ij] = 0 unless (*i,j*) ∈ *G*, where *G* is a particular quantum
    annealing graph (e.g., *Chimera* or *Pegasus* graphs in the case of D-Wave quantum
    annealers).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 仅当量子比特 *i* 和 *j* 之间存在所需的电路时才能实现，否则它们在处理器的空间布局中不能制造得太远，以避免工程上的问题 [[296](Biblography.xhtml#XVenturelli2019)]。换句话说，除非
    (*i,j*) ∈ *G*，否则 *J*[ij] = 0，其中 *G* 是特定的量子退火图（例如，在 D-Wave 量子退火器的情况下，*Chimera*
    或 *Pegasus* 图）。
- en: It would be straightforward to embed the final Hamiltonian ([5.4.2](#x1-1100002))
    on the quantum chip had all the physical qubits been connected to each other.
    Unfortunately, this is not the case. The existing quantum annealers have rather
    limited qubit connectivity. For example, in the case of the *Chimera* (*Pegasus*)
    graph, a physical qubit is connected with a maximum of six (fifteen) other physical
    qubits.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有物理量子比特都已互相连接，则将最终的哈密顿量 ([5.4.2](#x1-1100002)) 嵌入到量子芯片上是很简单的。遗憾的是，实际情况并非如此。现有的量子退火器的量子比特连接性非常有限。例如，在
    *Chimera* (*Pegasus*) 图中，一个物理量子比特最多与六个（十五个）其他物理量子比特连接。
- en: 'To get around this restriction, the standard procedure is to employ the minor-embedding
    compilation technique for fully connected graphs. By means of this procedure,
    we obtain another Ising form, where qubits are arranged in ordered 1D chains (forming
    the *logical* qubits that represent the spin variables) interlaced on the quantum
    annealer graph:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绕过这一限制，标准的做法是采用次要嵌入编译技术来处理完全连接的图。通过这一过程，我们得到另一个伊辛模型形式，其中量子比特按顺序排列成1D链（形成代表自旋变量的*逻辑*量子比特），并交错于量子退火器图上：
- en: '| ![ N∑ [Nc∑−1 ] N+∑M [Nc∑−1 ] ℋIsing = − &#124;JF&#124; σicσi(c+1) − &#124;JF
    &#124; σjcσj(c+1) i=1 c=1 z z j=N+1 c=1 z z ](img/file450.jpg) |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| ![ N∑ [Nc∑−1 ] N+∑M [Nc∑−1 ] ℋIsing = − &#124;JF&#124; σicσi(c+1) − &#124;JF
    &#124; σjcσj(c+1) i=1 c=1 z z j=N+1 c=1 z z ](img/file450.jpg) |  |'
- en: '| ![ N [ Nc ] N+M [ Nc ] − ∑ gi- ∑ σic − ∑ gj- ∑ σjc Nc z Nc z i=1 c=1 j=N+1
    c=1 ](img/file451.jpg) |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| ![ N [ Nc ] N+M [ Nc ] − ∑ gi- ∑ σic − ∑ gj- ∑ σjc Nc z Nc z i=1 c=1 j=N+1
    c=1 ](img/file451.jpg) |  |'
- en: '| ![ ⌊ ⌋ ∑N N+∑M N∑c − Jij⌈ δGij(ci,cj)σizciσjczj⌉ . i=1j=N+1 ci,cj=1 ](img/file452.jpg)
    |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| ![ ⌊ ⌋ ∑N N+∑M N∑c − Jij⌈ δGij(ci,cj)σizciσjczj⌉ . i=1j=N+1 ci,cj=1 ](img/file452.jpg)
    |  |'
- en: 'In ([5.4.3](#x1-1110003)), we explicitly isolate the encoding of the *logical*
    quantum variable: the classical binary variable *s*[i] is associated with *N*[c]
    Ising spins *σ*[z]^(ic), ferromagnetically coupled directly by strength *J*[F]
    , forming an ordered 1D chain subgraph of *G*. The value of *J*[F] should be strong
    enough to correlate the value of the magnetisation of each individual spin if
    measured in the computational basis (⟨*σ*[z]^(ic)⟩ = ⟨*σ*[z]^(i(c+1))⟩).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ([5.4.3](#x1-1110003))中，我们显式地分离出*逻辑*量子变量的编码：经典二进制变量*s*[i]与*N*[c]个伊辛自旋*σ*[z]^(ic)相关联，通过耦合强度*J*[F]
    直接发生铁磁耦合，形成一个有序的1D链子图*G*。如果在计算基下测量，每个自旋的磁化强度值应足够强，以便能够相关联（⟨*σ*[z]^(ic)⟩ = ⟨*σ*[z]^(i(c+1))⟩）。
- en: 'In ([5.4.3](#x1-1110003)) and ([5.4.3](#x1-1110003)), we encode the Ising Hamiltonian ([5.4.2](#x1-1100002))
    through our extended set of variables: the local field *g*[i] is evenly distributed
    across all qubits belonging to the logical chain *i*, and each coupler *J*[ij]
    is active only between one specific pair of qubits (*σ*[z]^(ic[i]^⋆)*,σ*[z]^(jc[j]^⋆)),
    which is specified by the adjacency check function *δ*[ij]^G(*c*[i]*,c*[j]), which
    assumes a unit value only if (*c*[i] = *c*[i]^⋆) and (*c*[j] = *c*[j]^⋆), and
    is zero otherwise.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ([5.4.3](#x1-1110003))和 ([5.4.3](#x1-1110003))中，我们通过扩展的变量集对伊辛哈密顿量 ([5.4.2](#x1-1100002))进行编码：局部场*g*[i]均匀分布在属于逻辑链*i*的所有量子比特上，每个耦合器*J*[ij]仅在一对特定的量子比特之间激活（*σ*[z]^(ic[i]^⋆)*,σ*[z]^(jc[j]^⋆)），该对量子比特由邻接检查函数*δ*[ij]^G(*c*[i]*,c*[j])指定，只有当(*c*[i]
    = *c*[i]^⋆)且(*c*[j] = *c*[j]^⋆)时，函数值为1，否则为0。
- en: Given this particular embedding scheme, we can turn our attention to finding
    an optimal value for the parameter *β* in ([5.4.2](#x1-1100002)), which can only
    be done experimentally. Since the final Hamiltonian is programmed on the quantum
    annealer using dimensionless coefficients, the parameter *β* cannot be expressed
    in the usual form 1*∕kT*, where *k* is the Boltzmann constant and *T* the effective
    temperature. Instead, it should be viewed as an empirical parameter that depends
    on the network architecture, the embedding scheme, and the physical characteristics
    of the quantum annealer (such as the operating temperature, the anneal time, the
    energy scale of the superconducting flux qubit system, etc.).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的嵌入方案下，我们可以将注意力集中在实验性地寻找参数*β*的最优值上（见 [5.4.2](#x1-1100002)），这一点只能通过实验来完成。由于最终的哈密顿量是在量子退火器上使用无量纲系数编程的，因此参数*β*不能用常规形式
    1*∕kT* 来表示，其中*k*是玻尔兹曼常数，*T*是有效温度。相反，它应该被视为一个经验参数，依赖于网络架构、嵌入方案以及量子退火器的物理特性（例如工作温度、退火时间、超导流量量子比特系统的能量尺度等）。
- en: 'The experimental approach of estimating *β* consists of the following five
    steps  [[4](Biblography.xhtml#XAdachi2015)]:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 估算*β*的实验方法包括以下五个步骤 [[4](Biblography.xhtml#XAdachi2015)]：
- en: Construct an RBM.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个RBM。
- en: Map the RBM to a final Hamiltonian assuming a particular value of *β* (Alg. [4](#x1-109006r4)-Step
    1).
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将RBM映射到假设特定*β*值的最终哈密顿量（算法 [4](#x1-109006r4)-第1步）。
- en: Run quantum annealing (Alg. [4](#x1-109006r4)-Step 2).
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行量子退火（算法 [4](#x1-109006r4)-第2步）。
- en: Compute the model expectations using the quantum samples (Alg. [4](#x1-109006r4)-Step
    3).
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用量子样本计算模型期望值（算法 [4](#x1-109006r4)-第3步）。
- en: Compare the resulting expectations with the "correct" benchmark values (e.g.,
    obtained with the classical CD algorithm).
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果期望值与“正确”的基准值进行比较（例如，使用经典CD算法获得的值）。
- en: This process is repeated for different choices of *β*. The value of *β* that
    gives the best fit can then be used for the given RBM architecture. As noted in  [[4](Biblography.xhtml#XAdachi2015)],
    even with the optimal settings for *β*, the estimates of the model expectations
    will still have some error. However, in comparison to the noise associated with
    the Boltzmann sampling in the Contrastive Divergence algorithm, this may be sufficient
    to estimate the gradients in ([5.3.2](#x1-106003r2)), ([5.3.2](#x1-106003r2)),
    and ([5.3.2](#x1-106003r2)).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会针对不同的*β*值进行重复。然后，可以使用给定RBM架构的最佳拟合的*β*值。如在[[4](Biblography.xhtml#XAdachi2015)]中所述，即使对于*β*的最优设置，模型期望值的估计仍然会有一定误差。然而，与对比散度算法中玻尔兹曼采样相关的噪声相比，这可能足以估计[5.3.2](#x1-106003r2)中的梯度，[5.3.2](#x1-106003r2)中的梯度，以及[5.3.2](#x1-106003r2)中的梯度。
- en: 5.4.4 Generative models
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.4 生成模型
- en: The main application of the Boltzmann sampling we’ve considered so far is in
    providing an unbiased estimate of the model expectations as specified in Algorithm [4](#x1-109006r4).
    Once fully trained with the help of quantum annealing, an RBM can be used in a
    conventional classical way to generate new synthetic samples from the learned
    probability distribution. In this case, quantum annealing is only used as a subroutine
    in the hybrid quantum-classical training protocol.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们考虑的玻尔兹曼采样的主要应用是在提供无偏估计模型期望值上，正如算法[4](#x1-109006r4)中所指定的那样。一旦通过量子退火完成充分训练，RBM就可以以传统的经典方式，用于从已学习的概率分布中生成新的合成样本。在这种情况下，量子退火仅作为混合量子-经典训练协议中的一个子程序使用。
- en: 'However, it is possible to use a quantum annealer as a generator in its own
    right. Rather than assisting in training the classical RBM, a quantum annealer
    can output the binary representation of the continuous samples as per the distribution
    encoded in the final Hamiltonian ([5.4.2](#x1-1100002)). The Quantum Variational
    Autoencoder  [[162](Biblography.xhtml#XKhoshaman2018)] is another example of a
    QBM that can be trained end to end by maximising a well-defined cost function:
    a quantum lower bound to a variational approximation of the log-likelihood.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，完全可以将量子退火器本身作为生成器使用。量子退火器不仅仅协助训练经典的RBM，它还可以根据最终哈密顿量([5.4.2](#x1-1100002))编码的分布输出连续样本的二进制表示。量子变分自编码器[[162](Biblography.xhtml#XKhoshaman2018)]是另一个QBM的例子，它可以通过最大化明确定义的代价函数来端到端地训练：量子变分下界近似对数似然。
- en: Botzmann sampling is the key element of RBM training and the generation of new
    samples. Quantum annealing can provide orders of magnitude speedup by replacing
    classical Boltzmann sampling with quantum sampling.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 玻尔兹曼采样是RBM训练和新样本生成的关键元素。通过用量子采样替代经典的玻尔兹曼采样，量子退火可以提供数量级的加速。
- en: 5.5 Deep Boltzmann Machine
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 深度玻尔兹曼机
- en: Deep Boltzmann Machines (DBMs) can be constructed from several RBMs where the
    hidden layer of the first RBM becomes the visible layer of the second, and so
    on, as shown in Figure [5.4](#5.4).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 深度玻尔兹曼机（DBM）可以由多个RBM构成，其中第一个RBM的隐藏层成为第二个RBM的可见层，依此类推，如图[5.4](#5.4)所示。
- en: '![Figurex1-113002r4: Schematic representation of a DBM. ](img/file453.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图x1-113002r4：DBM的示意图。](img/file453.jpg)'
- en: 'Figure 5.4: Schematic representation of a DBM.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：DBM的示意图。
- en: A DBM can be trained layer by layer, one RBM at a time. This will result in
    a powerful generative model capable of learning complex multivariate distributions
    and dependence structures. However, the generative training of the DBM can be
    used as the first step towards building a discriminative model if the training
    dataset samples are labelled. In this case all DBM weights and biases found with
    the help of either CD or quantum Boltzmann sampling algorithms are seen as initial
    values of the weights and biases of the corresponding feedforward neural network.
    The discriminative model will consist of all the layers of the original DBM with
    an extra output layer performing assignment of the class labels. The discriminative
    model can be fine tuned through the standard backpropagation of the error algorithm.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: DBM可以逐层训练，每次训练一个RBM。这将生成一个强大的生成模型，能够学习复杂的多变量分布和依赖结构。然而，DBM的生成训练可以作为构建判别模型的第一步，如果训练数据集的样本有标签。在这种情况下，所有通过CD或量子玻尔兹曼采样算法获得的DBM权重和偏置，都会被视为相应前馈神经网络权重和偏置的初始值。判别模型将包括原始DBM的所有层，并增加一个额外的输出层，用于分类标签的分配。通过标准的误差反向传播算法，可以对判别模型进行微调。
- en: 5.5.1 Training DBMs with quantum annealing
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1 使用量子退火训练DBM
- en: The generative training of DBMs can be seen as a pre-training of the discriminative
    model. Figure [5.5](#5.5) provides a schematic illustration of the hybrid quantum-classical
    training process.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: DBM的生成训练可以看作是判别模型的预训练。图[5.5](#5.5)提供了混合量子-经典训练过程的示意图。
- en: '![Figurex1-114002r5: Generative and discriminative training of a DBM. ](img/file454.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![Figurex1-114002r5：DBM的生成训练与判别训练。](img/file454.jpg)'
- en: 'Figure 5.5: Generative and discriminative training of a DBM.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5：DBM的生成训练与判别训练。
- en: 'In the DBM training scheme shown in Figure [5.5](#5.5), only Step 1 relies
    on quantum annealing. Steps 2 and 3 are completely classical. Step 3 is optional:
    without it we have a standard machine learning "pipeline" where one or several
    RBMs (Step 1) perform "feature extraction" by building a low-dimensional representation
    of the samples in the dataset, thus helping the discriminative model (Step 2)
    to achieve better classification results.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[5.5](#5.5)所示的DBM训练方案中，只有步骤1依赖于量子退火。步骤2和步骤3完全是经典的。步骤3是可选的：没有它，我们就拥有一个标准的机器学习“管道”，其中一个或多个RBM（步骤1）通过构建数据集样本的低维表示来执行“特征提取”，从而帮助判别模型（步骤2）实现更好的分类结果。
- en: 5.5.2 A DBM pipeline example
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2 一个DBM管道示例
- en: The pipeline approach can be illustrated using the popular "King+Rook vs. King+Pawn"
    dataset from the UCI Machine Learning Repository  [[262](Biblography.xhtml#XUCI_KRKP), [263](Biblography.xhtml#XShapiro1987)].
    The task is to classify the end game positions with the black pawn one move from
    queening and the white side (King+Rook) to move. The possible outcomes are "white
    can win" (Class 1) and "white cannot win" (Class 0). The board is described by 36
    categorical attributes that can be encoded as 38 binary variables. The dataset
    consists of 3,196 samples (white can win in 52% of all cases in the dataset).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 管道方法可以使用UCI机器学习库中的流行数据集“国王+车 vs. 国王+兵”来说明[[262](Biblography.xhtml#XUCI_KRKP),
    [263](Biblography.xhtml#XShapiro1987)]。任务是分类残局，其中黑色兵即将升变，而白方（国王+车）即将走子。可能的结果是“白方可以赢”（类1）和“白方不能赢”（类0）。棋盘由36个类别属性描述，可以编码为38个二进制变量。数据集包含3196个样本（在数据集的所有案例中，白方可以赢52%）。
- en: 'The `scikit-learn` package provides all the necessary components for building
    the classical part of a DBM pipeline. The pipeline itself can be constructed with
    the help of `sklearn.pipeline.make_pipeline`. The DBM is constructed from two
    RBMs implemented with the help of `sklearn.neural_network.BernoulliRBM`. RBM #1
    has 38 nodes in the visible layer and 30 nodes in the hidden layer; RBM #2 has 30
    nodes in the visible layer and 20 nodes in the hidden layer. The exact pipeline
    configuration is as follows (all other parameters were set at their default values):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`包提供了构建DBM管道经典部分所需的所有组件。可以借助`sklearn.pipeline.make_pipeline`构建管道本身。DBM是由两个RBM构建的，这些RBM是通过`sklearn.neural_network.BernoulliRBM`实现的。RBM
    #1在可见层有38个节点，在隐藏层有30个节点；RBM #2在可见层有30个节点，在隐藏层有20个节点。确切的管道配置如下（所有其他参数设置为默认值）：'
- en: '| RBM #1 | RBM #2 | MLP Classifier |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| RBM #1 | RBM #2 | MLP 分类器 |'
- en: '| n_components = 30 | n_components = 20 | hidden_layer_sizes = (20) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| n_components = 30 | n_components = 20 | hidden_layer_sizes = (20) |'
- en: '| learning_rate = 0.00025 | learning_rate = 0.00025 | activation = ’tanh’ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| learning_rate = 0.00025 | learning_rate = 0.00025 | activation = ''tanh''
    |'
- en: '| batch_size = 10 | batch_size = 10 | solver = ’adam’ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| batch_size = 10 | batch_size = 10 | solver = ''adam'' |'
- en: '| n_iter = 100000 | n_iter = 100000 | alpha = 0.1 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| n_iter = 100000 | n_iter = 100000 | alpha = 0.1 |'
- en: '|  |  | max_iter = 5000 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  |  | max_iter = 5000 |'
- en: 'Table 5.1: Configuration of the DBM pipeline for the “King+Rook vs. King+Pawn”
    classification problem.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1：用于“国王+车 vs. 国王+兵”分类问题的DBM管道配置。
- en: 'Thus, both RBMs are trained as autoencoders: the DBM translates each 38-feature
    sample into its 20-feature low-dimensional representation. These new "extracted"
    features, ideally, should have higher predicting power in comparison with the
    original features, assuming that both RBMs learned the main characteristics and
    dependence structure of the dataset and stripped away the noise or the less important
    characteristics. The discriminator is `sklearn.neural_network.MLPClassifier` with 20
    tanh activation units in its single hidden layer.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，两个RBM都被训练为自编码器：DBM将每个38特征的样本转换为其20特征的低维表示。这些新的“提取”特征，理想情况下，应该比原始特征具有更高的预测能力，前提是两个RBM都学到了数据集的主要特征和依赖结构，并剔除了噪声或不太重要的特征。判别器是`sklearn.neural_network.MLPClassifier`，其单一隐藏层中有20个tanh激活单元。
- en: 'With this setting, the DBM achieves the following out-of-sample classification
    results (with the dataset split 70:30 into the training and testing datasets using
    `sklearn.model_selection.train_test_split`):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置下，DBM在以下的样本外分类结果（数据集按70:30比例分为训练集和测试集，使用`sklearn.model_selection.train_test_split`）中取得了以下成绩：
- en: 'Classification accuracy: 95.2%'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类准确率：95.2%
- en: 'This compares favourably with, for example, an ensemble learning classifier
    such as random forest (`sklearn.ensemble.RandomForestClassifier`). The random
    forest classifier with the number of estimators set at 1,000 and the maximum depth
    set equal to 5 has the following out-of-sample classification results:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这与例如集成学习分类器如随机森林（`sklearn.ensemble.RandomForestClassifier`）相比具有优势。设置1000个估计器和最大深度为5的随机森林分类器在样本外分类结果上取得了以下成绩：
- en: 'Classification accuracy: 94.9%'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类准确率：94.9%
- en: The architecture of DBMs allows them to be trained as either generative or discriminative
    models. In both cases, Boltzmann sampling can play an important role in improving
    their performance.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: DBM的架构使其可以作为生成模型或判别模型进行训练。在这两种情况下，玻尔兹曼采样都可以在提高其性能方面发挥重要作用。
- en: Summary
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about energy-based models – a special class of powerful
    generative models. We learned how to build, train, and run RBMs in order to generate
    synthetic samples that are statistically indistinguishable from the original training
    dataset.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了基于能量的模型——一种强大的生成模型的特殊类型。我们学习了如何构建、训练和运行RBM，以生成与原始训练数据集统计上无法区分的合成样本。
- en: We familiarised ourselves with the Boltzmann sampling and Contrastive Divergence
    algorithms. Boltzmann sampling can be efficiently performed on NISQ-era quantum
    annealers that may improve the quality of the model and achieve orders of magnitude
    of speedup in generating new samples.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们熟悉了玻尔兹曼采样和对比散度算法。玻尔兹曼采样可以在NISQ时代的量子退火机上高效执行，这可能提高模型的质量，并在生成新样本时实现数量级的加速。
- en: We learned how to combine individual RBMs together to construct a DBM. Quantum
    annealing can be productively applied to the pre-training of a DBM before it is
    fine tuned as a deep feedforward neural network classifier.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何将单个RBM结合起来构建DBM。量子退火可以有效地应用于DBM的预训练，然后将其微调为深度前馈神经网络分类器。
- en: Finally, we explored the possibility of using RBMs and DBMs as the first model
    in the machine learning pipeline for denoising and feature extraction.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探索了使用RBM和DBM作为机器学习管道中第一个模型进行去噪和特征提取的可能性。
- en: 'In the next chapter, we will shift our attention to gate model quantum computing.
    We will start with the concept of a classical binary digit (bit) and classical
    logic gates before introducing their quantum counterparts: the quantum binary
    digit (qubit) and one-qubit/multi-qubit quantum logic gates and quantum circuits.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转向门模型量子计算。我们将从经典二进制数字（bit）和经典逻辑门的概念开始，然后介绍它们的量子对应物：量子二进制数字（qubit）和单量子比特/多量子比特量子逻辑门及量子电路。
- en: Join our book’s Discord space
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/quantum](https://packt.link/quantum)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，结识志同道合的人，并与超过2000名成员一起学习： [https://packt.link/quantum](https://packt.link/quantum)
- en: '![PIC](img/file1.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
