- en: '4'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '4'
- en: Quantum Boosting
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 量子提升
- en: 'In this chapter, we consider a quantum version of the classical boosting meta-algorithm – a
    family of machine learning algorithms that convert weak classifiers into strong
    ones. Classically, boosting consists of two main operations: i) adaptive (iterative)
    training of the weak classifiers, thus improving their individual performance,
    and ii) finding an optimal configuration of weights applied to the individual
    weak learners when combining them into a single strong one.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们考虑经典提升元算法的量子版本——这是一类机器学习算法，能够将弱分类器转化为强分类器。经典的提升包括两个主要操作：i）对弱分类器进行自适应（迭代）训练，从而提高它们的个体性能，ii）在将弱学习器组合成一个强学习器时，找到一组最优的权重配置。
- en: Adaptive learning consists of iterative re-weighting of the samples from the
    training dataset, forcing the model to improve its performance on the difficult-to-classify
    samples by giving them heavier weights. These weights are adjusted at each algorithm
    iteration. Arguably, the best-known and most successful example of such algorithms
    is the popular *adaptive boosting* (AdaBoost) model. It was first formulated in
    1997 by Freund and Schapire   [[107](Biblography.xhtml#XFreund1997)], whose work
    has been recognised by the awarding of the prestigious Gödel Prize in 2003.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应学习包括对训练数据集中的样本进行迭代重新加权，迫使模型通过赋予难以分类的样本更高的权重来提高其性能。这些权重在每次算法迭代时都会进行调整。可以说，最著名和最成功的这种算法示例就是流行的*自适应提升*（AdaBoost）模型。该模型最早由Freund和Schapire于1997年提出[[107](Biblography.xhtml#XFreund1997)]，他们的工作在2003年获得了著名的Gödel奖。
- en: 'The main principle of AdaBoost is that the base classifiers (weak learners)
    are trained in sequence and each base classifier is trained using a weighted form
    of the dataset: the weighting coefficient associated with each sample depends
    on the performance of the previous classifiers. Samples that are misclassified
    by one of the base classifiers are given larger weights when used to train the
    next base classifier in the sequence. Once all base classifiers have been trained,
    their predictions are combined through some kind of weighted majority voting scheme  [[37](Biblography.xhtml#XBishop2006)].
    Therefore, AdaBoost can be seen as a general framework that allows many possible
    realisations with various degrees of sophistication rather than a narrowly defined
    algorithm.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost的主要原理是基分类器（弱学习器）按顺序训练，每个基分类器都使用加权形式的数据集进行训练：与每个样本相关的加权系数取决于前一个分类器的性能。被某个基分类器误分类的样本，在用来训练下一个基分类器时会被赋予更大的权重。一旦所有基分类器都训练完成，它们的预测结果通过某种加权多数投票方案结合起来[[37](Biblography.xhtml#XBishop2006)]。因此，AdaBoost可以被看作是一个通用框架，允许多种实现方式，具有不同程度的复杂性，而不是一个狭义定义的算法。
- en: In contrast to AdaBoost, a boosting approach that consists of finding an optimal
    set of weights for the individual weak learners (with the weak learners being
    trained in the usual way) is straightforward to implement and relies on standard
    optimisation routines. However, this task becomes a hard combinatorial problem
    when an additional set of constraints is introduced. When the weights are only
    allowed to take binary values, the problem naturally lends itself to being formulated
    as a QUBO problem.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 与AdaBoost不同，提升方法通过为个体弱学习器找到一组最优权重（弱学习器按常规方式训练）来实现，这种方法易于实现，并依赖于标准的优化程序。然而，当引入额外约束条件时，这一任务变成了一个复杂的组合问题。当权重只能取二进制值时，该问题自然可以表述为一个QUBO问题。
- en: This is where quantum annealing has a role to play, as we have seen in Chapter [3](Chapter_3.xhtml#x1-630003).
    For a large enough number of weak classifiers, the search space becomes enormous,
    and classical algorithms (such as various evolutionary search heuristics) may
    take a non-trivial amount of time to find an optimal configuration of weights
    (or, at least, a good approximation). This is an ideal scenario for quantum annealing
    to demonstrate its strong points, including the possibility of achieving a material
    quantum speedup.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 量子退火在这里发挥着作用，正如我们在第[3](Chapter_3.xhtml#x1-630003)章中所看到的那样。对于足够数量的弱分类器，搜索空间会变得巨大，经典算法（如各种进化搜索启发式方法）可能需要相当长的时间才能找到最优的权重配置（或者至少找到一个好的近似解）。这是量子退火展示其优势的理想场景，包括实现物质量子加速的可能性。
- en: Quantum boosting is a QUBO-based technique combining individual weak learners
    into a single strong classifier by constructing an optimal linear combination
    of binary classifiers. It is transparent, easy to interpret, and resistant to
    overfitting.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 量子提升是一种基于QUBO的技术，通过构造二进制分类器的最优线性组合，将单个弱学习器组合成一个强分类器。它是透明的、易于解释的，并且抗过拟合。
- en: 4.1 Quantum Annealing for Machine Learning
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 量子退火与机器学习
- en: Quantum boosting is the first QML algorithm we will consider in this book. This
    is also the algorithm that plays to the natural strengths of quantum annealing.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 量子提升是我们在本书中将要考虑的第一个QML算法。这也是最能发挥量子退火自然优势的算法。
- en: 4.1.1 General principles of the QBoost algorithm
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 QBoost算法的一般原理
- en: 'We start with the general principles of the Quantum Boosting (QBoost) algorithm
    before exploring a specific finance-related application. In the formulation of
    QBoost, we will be using the following definitions and notations:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先从量子提升（QBoost）算法的一般原理开始，再探讨一个具体的与金融相关的应用。在QBoost的公式中，我们将使用以下定义和符号：
- en: '| Object | Definition |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 对象 | 定义 |'
- en: '| x[τ] = (*x*[1](*τ*)*,x*[2](*τ*)*,…,x*[N](*τ*)) | Vector of *N* variables
    (features) |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| x[τ] = (*x*[1](*τ*)*,x*[2](*τ*)*,…,x*[N](*τ*)) | *N*个变量（特征）的向量 |'
- en: '| *y*[τ] = ±1 | Binary label indicating whether x[τ] corresponds to Class 0
    (−1) or Class 1 (+1) |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| *y*[τ] = ±1 | 二进制标签，表示x[τ]是否对应类别0（−1）或类别1（+1） |'
- en: '| {x[τ]*,y*[τ]}[τ=1,…,M] | Set of training events |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| {x[τ]*,y*[τ]}[τ=1,…,M] | 训练事件集 |'
- en: '| *c*[i](x[τ]) = ±![Table 4.1: QBoost algorithm notations. ](img/file357.jpg)
    | Value of the weak classifier *i* on the event *τ* |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| *c*[i](x[τ]) = ±![表4.1：QBoost算法符号](img/file357.jpg) | 弱分类器*i*在事件*τ*上的值 |'
- en: '| q := (*q*[1]*,q*[2]*,…,q*[N]) | Vector of binary (0 or 1) weights associated
    with each weak classifier |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| q := (*q*[1]*,q*[2]*,…,q*[N]) | 与每个弱分类器相关联的二进制（0或1）权重的向量 |'
- en: 'Table 4.1: QBoost algorithm notations.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1：QBoost算法符号
- en: We first specify the classification error for sample *τ*, which is given by
    the squared error
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先指定样本*τ*的分类误差，该误差由平方误差给出：
- en: '| ![( N )2 ∑ ci(xτ)qi − yτ . i=1 ](img/file358.jpg) |  |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| ![( N )2 ∑ ci(xτ)qi − yτ . i=1 ](img/file358.jpg) |  |'
- en: 'The total cost function to minimise is then the sum of squared errors across
    all samples:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化的总成本函数是所有样本的平方误差之和：
- en: '| ![ M ( N )2 L(q) = ∑ ∑ c (x )q − y i τ i τ τ=1( i=1 ) M∑ ∑N ∑N ∑N 2 = ( ci(xτ)qi
    cj(xτ)qj − 2yτ ci(xτ)qi + yτ) . τ=1 i=1 j=1 i=1 ](img/file359.jpg) |  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| ![ M ( N )2 L(q) = ∑ ∑ c (x )q − y i τ i τ τ=1( i=1 ) M∑ ∑N ∑N ∑N 2 = ( ci(xτ)qi
    cj(xτ)qj − 2yτ ci(xτ)qi + yτ) . τ=1 i=1 j=1 i=1 ](img/file359.jpg) |  |'
- en: Note that *y*[τ]² does not depend on q and therefore has no influence on the
    minimisation of *L*. Adding a penalty *λ >* 0 to prevent overfitting, the objective
    function to minimise is thus
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*y*[τ]²不依赖于q，因此对*L*的最小化没有影响。为了防止过拟合，添加一个惩罚项*λ >* 0，因此最小化的目标函数为：
- en: '| ![ M ( N N N ) N ^L(q) = ∑ ( ∑ c (x )q ∑ c(x )q − 2y ∑ c (x )q ) + λ ∑ q
    i τ i j τ j τ i τ i i τ=1 ( i=1 j=1 i=1 ) i=1 M∑ ∑N ∑N ∑N N∑ = ( qiqjci(xτ)cj(xτ)−
    2 qici(xτ)yτ) + λ qi τ=1 i=1j=1 i=1 i=1 N N ( M ) N ( M ) N = ∑ ∑ ∑ c (x )c(x
    ) q q − 2∑ ∑ c(x )y q + λ ∑ q i τ j τ i j i τ τ i i i=1 j=1 τ=1 i=1 τ=1 i=1 N∑
    ∑N ∑N = Cijqiqj + (λ − 2Ci)qi, i=1 j=1 i=1 ](img/file360.jpg) |  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ![ M ( N N N ) N ^L(q) = ∑ ( ∑ c (x )q ∑ c(x )q − 2y ∑ c (x )q ) + λ ∑ q
    i τ i j τ j τ i τ i i τ=1 ( i=1 j=1 i=1 ) i=1 M∑ ∑N ∑N ∑N N∑ = ( qiqjci(xτ)cj(xτ)−
    2 qici(xτ)yτ) + λ qi τ=1 i=1j=1 i=1 i=1 N N ( M ) N ( M ) N = ∑ ∑ ∑ c (x )c(x
    ) q q − 2∑ ∑ c(x )y q + λ ∑ q i τ j τ i j i τ τ i i i=1 j=1 τ=1 i=1 τ=1 i=1 N∑
    ∑N ∑N = Cijqiqj + (λ − 2Ci)qi, i=1 j=1 i=1 ](img/file360.jpg) |  |'
- en: with
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与
- en: '| ![ ∑M ∑M Cij := ci(xτ)cj(xτ) and Ci := ci(xτ)yτ. τ=1 τ=1 ](img/file361.jpg)
    |  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑M ∑M Cij := ci(xτ)cj(xτ) 和 Ci := ci(xτ)yτ. τ=1 τ=1 ](img/file361.jpg)
    |  |'
- en: '**Remark:** Adding a penalty term controlled by the coefficient *λ* is analogous
    to the LASSO regression method  [[6](Biblography.xhtml#XAgresti2013)] with *L*[1]
    penalty, which is ubiquitous in machine learning. Ridge regression  [[243](Biblography.xhtml#XRaschka2019)]
    with *L*[2] penalty could also be used and would also lead to a QUBO problem.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**备注：** 添加一个由系数*λ*控制的惩罚项类似于LASSO回归方法[[6](Biblography.xhtml#XAgresti2013)]，其使用*L*[1]惩罚项，在机器学习中非常常见。岭回归[[243](Biblography.xhtml#XRaschka2019)]使用*L*[2]惩罚项，也可以使用，并且也会导致一个QUBO问题。'
- en: 4.1.2 QUBO to Ising
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 从QUBO到Ising
- en: As developed in Chapter [3.1.1](Chapter_3.xhtml#x1-650001), we now perform a
    transformation from QUBO to Ising from the binary decision variables q := (*q*[1]*,…,q*[N])
    ∈{0*,*1}^N to spin variables s := (*s*[1]*,…,s*[N]) ∈{−1*,*+1}^N using
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[3.1.1](Chapter_3.xhtml#x1-650001)章中所述，我们现在执行从QUBO到Ising的转换，使用二进制决策变量q :=
    (*q*[1]*,…,q*[N]) ∈{0*,*1}^N 到自旋变量s := (*s*[1]*,…,s*[N]) ∈{−1*,*+1}^N的转换：
- en: '| ![s = 2q − 1 or q = 1(s+ 1 ). 2 ](img/file362.jpg) |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| ![s = 2q − 1 或 q = 1(s+ 1 ). 2 ](img/file362.jpg) |  |'
- en: Therefore, the Ising problem to be solved on the quantum annealer reads
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，量子退火器上待解决的Ising问题可以表示为
- en: '| ![ ∑N ( ) ( ) ∑N ( ) ℋ = L^(s) = 1si + 1- 1-sj + 1 Cij + 1si + 1- (λ− 2Ci
    ) i,j=1 2 2 2 2 i=1 2 2 N N N 1-∑ 1-∑ 1-∑ = 4 sisjCij + 2 siCij + 4 Cij i,j=1
    i,j=1 i,j=1 1 N∑ λN ∑N ∑N + -- siλ+ ---− siCi − Ci. 2 i=1 2 i=1 i=1 ](img/file363.jpg)
    |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑N ( ) ( ) ∑N ( ) ℋ = L^(s) = 1si + 1- 1-sj + 1 Cij + 1si + 1- (λ− 2Ci
    ) i,j=1 2 2 2 2 i=1 2 2 N N N 1-∑ 1-∑ 1-∑ = 4 sisjCij + 2 siCij + 4 Cij i,j=1
    i,j=1 i,j=1 1 N∑ λN ∑N ∑N + -- siλ+ ---− siCi − Ci. 2 i=1 2 i=1 i=1 ](img/file363.jpg)
    |  |'
- en: Since the three terms
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这三个术语
- en: '![1 ∑N λN ∑N 4- Cij, -2--, and Ci i,j=1 i=1 ](img/file364.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![1 ∑N λN ∑N 4- Cij, -2--, 和 Ci i,j=1 i=1 ](img/file364.jpg)'
- en: do not depend on s, they can be removed from the cost function. The substitution
    *λ* = ![1 2](img/file365.jpg)*λ* then yields the final Ising problem
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 不依赖于 s，它们可以从成本函数中移除。替换 *λ* = ![1 2](img/file365.jpg)*λ* 后，得到最终的Ising问题
- en: '| ![ N N N ℋ = 1-∑ s sC + 1-∑ sC + ∑ s (λ− C ). 4 ij ij 2 i ij i i i,j=1 i,j=1
    i=1 ](img/file366.jpg) |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| ![ N N N ℋ = 1-∑ s sC + 1-∑ sC + ∑ s (λ− C ). 4 ij ij 2 i ij i i i,j=1 i,j=1
    i=1 ](img/file366.jpg) |  |'
- en: The problem that quantum annealing attempts to solve is to minimise ℋ and to
    return the minimising, ground-state spin configuration (*s*[i]^g)[i=1,…,N]. The
    strong classifier is then built as
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 量子退火尝试解决的问题是最小化 ℋ 并返回最小化的基态自旋配置（*s*[i]^g)[i=1,…,N]。然后，强分类器被构建为
- en: '| ![ N ∑ g R (x) = sici(x) ∈ [− 1,1], i=1 ](img/file367.jpg) |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| ![ N ∑ g R (x) = sici(x) ∈ [− 1,1], i=1 ](img/file367.jpg) |  |'
- en: for each new event x that we wish to classify  [[218](Biblography.xhtml#XMott2017)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们希望分类的每一个新事件 x [[218](Biblography.xhtml#XMott2017)]。
- en: 4.2 QBoost Applications in Finance
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 QBoost在金融领域的应用
- en: Quantum Annealing for Machine Learning (QAML) has been applied productively
    to a wide range of financial and non-financial use cases. It demonstrated a performance
    advantage in comparison with standard classical machine learning models such as
    the binary decision tree-based Extreme Gradient Boosting (XGBoost) and Deep Neural
    Network (DNN) classifiers, especially on relatively small datasets. The QAML use
    cases come from such diverse fields as high-energy physics (the Higgs boson detection  [[218](Biblography.xhtml#XMott2017)])
    and computational biology (the classification and ranking of transcription factor
    binding  [[186](Biblography.xhtml#XLi2018)]). In finance, the most obvious application
    of QAML is to credit scoring and fraud detection as well as to the construction
    of strong trading signals from large numbers of weak binary (buy/sell) trading
    signals.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 量子退火在机器学习（QAML）领域已经成功应用于广泛的金融和非金融用例。与传统的经典机器学习模型（如基于二叉决策树的极端梯度提升（XGBoost）和深度神经网络（DNN）分类器）相比，QAML在处理相对较小的数据集时表现出明显的性能优势。QAML的应用案例来自多个不同领域，如高能物理（希格斯玻色子的探测[[218](Biblography.xhtml#XMott2017)]）和计算生物学（转录因子结合的分类和排名[[186](Biblography.xhtml#XLi2018)]）。在金融领域，QAML的最明显应用是信用评分和欺诈检测，以及通过大量弱二进制（买/卖）交易信号构建强交易信号。
- en: In this section, we analyse QBoost performance on the more conventional binary
    classification problem – forecasting credit card client defaults. We also provide
    classical benchmarks (gradient boosting and feedforward neural network classifiers)
    and analyse QBoost performance from different angles. The chosen dataset is relatively
    large, with tens of thousands of samples, which should help standard classical
    classifiers avoid overfitting and demonstrate their best qualities.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了QBoost在更常见的二元分类问题上的表现——预测信用卡客户违约。我们还提供了经典基准（梯度提升和前馈神经网络分类器），并从不同角度分析了QBoost的表现。选择的数据集相对较大，包含数万样本，应该能够帮助标准经典分类器避免过拟合，并展示它们的最佳性能。
- en: It has been established in  [[218](Biblography.xhtml#XMott2017)] that the QBoost
    algorithm is resistant to overfitting because it involves an explicit linearisation
    of correlations (hence its better performance on the smaller dataset in comparison
    with classical benchmarks). Another useful aspect of the model is that it is interpretable
    directly, with each weak classifier corresponding to a specific feature or a combination
    of features (or their functions), and the strong classifier being a simple linear
    combination thereof. This compares favourably with the "black box" machine learning
    discriminants, such as when using gradient boosting or DNNs. This is especially
    important for financial products aimed at retail customers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 已有研究表明[[218](Biblography.xhtml#XMott2017)]，QBoost算法具有抗过拟合的能力，因为它通过显式地线性化相关性（因此在较小数据集上的表现优于经典基准）。该模型的另一个有用特点是它具有可解释性，每个弱分类器都对应一个特定的特征或特征组合（或它们的函数），而强分类器则是这些特征的简单线性组合。这与“黑箱”机器学习判别方法（如使用梯度提升或深度神经网络）相比，具有显著优势。对于面向零售客户的金融产品，这一点尤为重要。
- en: 4.2.1 Credit card defaults
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 信用卡违约
- en: 'The default of credit card clients (DCCC) dataset is available from the UCI
    Machine Learning Repository  [[307](Biblography.xhtml#XUCI_DCCC), [308](Biblography.xhtml#XYeh2009)].
    The dataset consists of 30,000 samples with binary classification: a client defaults
    on the credit card payment (Class 1) and a client does not default (Class 0).
    There are 23 features (F1-F23) that have at least some predictive power and can
    be used for the classification decision:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 信用卡客户违约（DCCC）数据集可以从UCI机器学习库获得 [[307](Biblography.xhtml#XUCI_DCCC), [308](Biblography.xhtml#XYeh2009)]。该数据集包含30,000个样本，进行二分类：客户违约（类1）和客户未违约（类0）。共有23个特征（F1-F23），这些特征至少具有一定的预测能力，可以用于分类决策：
- en: 'F1: Amount of the given credit (NT dollar): it includes both the individual
    consumer credit and his/her family (supplementary) credit.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1：给定信用额度（新台币）：包括个人消费者信用和其家庭（附加）信用。
- en: 'F2: Gender (1 = male; 2 = female).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F2：性别（1 = 男；2 = 女）。
- en: 'F3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F3：教育程度（1 = 研究生；2 = 大学；3 = 高中；4 = 其他）。
- en: 'F4: Marital status (1 = married; 2 = single; 3 = others).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F4：婚姻状况（1 = 已婚；2 = 单身；3 = 其他）。
- en: 'F5: Age (years).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F5：年龄（岁）。
- en: 'F6-F11: History of past payments. F6 – the repayment status in the previous
    month, F7 – the repayment status two months ago, and so on. The measurement scale
    for the repayment status is: −1 = pay duly; 1 = payment delay for one month; 2 =
    payment delay for two months; *…*; 8 = payment delay for eight months; 9 = payment
    delay for nine months and above.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F6-F11：过去支付历史。F6 – 上个月的还款状态，F7 – 两个月前的还款状态，依此类推。还款状态的测量标准为：−1 = 按时还款；1 = 延迟一个月还款；2
    = 延迟两个月还款；*…*；8 = 延迟八个月还款；9 = 延迟九个月及以上还款。
- en: 'F12-F17: Amount of bill statement (NT dollar). F12 – amount of bill statement
    previous month, F13 – amount of bill statement two months ago, and so on.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F12-F17：账单金额（新台币）。F12 – 上个月的账单金额，F13 – 两个月前的账单金额，依此类推。
- en: 'F18-F23: Amount of previous payment (NT dollar). F18 – amount paid last month,
    F19 – amount paid two months ago, and so on.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F18-F23：上一期付款金额（新台币）。F18 – 上个月支付的金额，F19 – 两个月前支付的金额，依此类推。
- en: 'The weak classifiers were constructed in the following way: each feature was
    used separately as an input into the logistic regression classifier with the aim
    of making a binary prediction: −1*∕N* for Class 0 (no default) and +1*∕N* for
    Class 1 (default), where *N* = 23 is the total number of weak classifiers (number
    of features in the dataset). Note that this is not the only possible approach.
    It is perfectly feasible to build the weak classifiers through some (possibly
    non-linear) combination of original features. This should be done every time we
    have a clear understanding of which combination of features would produce a more
    meaningful and insightful result. However, in this particular example, our objective
    is to illustrate the general principles of QBoost algorithm and we do not assume
    any subject matter expertise that would allow us to construct better derived features.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 弱分类器的构建方式如下：每个特征单独作为输入，用于逻辑回归分类器，目的是做出二分类预测：−1*∕N*表示类别0（无违约），+1*∕N*表示类别1（有违约），其中*N*
    = 23表示弱分类器的总数（数据集中的特征数量）。需要注意的是，这不是唯一可能的方法。通过某些（可能是非线性的）特征组合来构建弱分类器是完全可行的。每当我们清楚地了解哪些特征组合能产生更有意义和洞察力的结果时，就应该采用这种方法。然而，在这个特定的例子中，我们的目的是阐明QBoost算法的基本原理，因此我们并不假设拥有足够的专业知识来构建更好的衍生特征。
- en: We have used `sklearn.linear_model.LogisticRegression` from the `scikit-learn`
    package  [[230](Biblography.xhtml#XSL)] for the weak classifiers. The dataset
    was split into training and testing datasets at a 70:30 ratio with the help of
    the sklearn.model_selection.train_test_split module. The class labels were encoded
    as −1 for Class 0 and +1 for Class 1, as per the QBoost algorithm requirements.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了来自`scikit-learn`包的`sklearn.linear_model.LogisticRegression` [[230](Biblography.xhtml#XSL)]
    作为弱分类器。数据集按照70:30的比例被划分为训练集和测试集，并借助sklearn.model_selection.train_test_split模块完成。根据QBoost算法的要求，类标签被编码为−1表示类别0（无违约），+1表示类别1（有违约）。
- en: 'The following configuration of the `LogisticRegression` model was used in constructing
    the weak classifiers dataset (all other parameters were set at their default values):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 构建弱分类器数据集时使用了以下`LogisticRegression`模型配置（其他所有参数均设置为默认值）：
- en: penalty = ‘l2’
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: penalty = ‘l2’
- en: C = 1.0
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C = 1.0
- en: solver = ‘lbfgs’
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: solver = ‘lbfgs’
- en: max_iter = 1000
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: max_iter = 1000
- en: Therefore, we have a training dataset (21,000 samples) and a testing dataset
    (9,000 samples), each consisting of the predictions of 23 weak classifiers (taking
    values {−1*∕*23, +1*∕*23}) and class labels (taking values in {−1, +1}). If the
    prediction of the strong classifier is given by the sum of predictions of the
    weak classifiers (a simple majority voting approach), its value will be in the
    [−1*,*1] range, with the values of −1 and +1 achieved if all the weak classifiers
    are in perfect agreement with each other.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有一个训练数据集（21,000个样本）和一个测试数据集（9,000个样本），每个数据集都包含23个弱分类器的预测（取值{−1*∕*23, +1*∕*23}）和类标签（取值{−1,
    +1}）。如果强分类器的预测由弱分类器的预测和（简单多数投票方法）给出，那么其值将位于[−1*,*1]范围内，只有当所有弱分类器完全一致时，才会得到−1或+1的值。
- en: QBoost provides an improvement on this approach by finding an optimal configuration
    of the weak classifiers such that the majority voting is performed on a subset
    of available weak classifiers. In other words, a majority voting performed on
    all weak classifiers is just a special case of the QBoost (one of the possible
    configurations to explore). Therefore, it is necessary to compare QBoost performance
    with more advanced classical machine learning models such as gradient boosting
    and neural networks. We provide this comparison in Section [4.3](#x1-890003).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: QBoost通过找到最优配置的弱分类器来改进这一方法，使得多数投票仅在部分可用的弱分类器上执行。换句话说，对所有弱分类器执行的多数投票只是QBoost的一种特殊情况（即其中一种可能的配置）。因此，有必要将QBoost的性能与更先进的经典机器学习模型（如梯度提升和神经网络）进行比较。我们将在第[4.3](#x1-890003)节提供这种比较。
- en: 4.2.2 QUBO classification results
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 QUBO分类结果
- en: Each feature in the DCCC dataset is uniquely mapped to the corresponding (weak)
    logistic regression classifier and associated binary decision variable (*q*[i])[i=1,…,23].
    These decision variables are represented by the logical qubits/spin variables
    in the QUBO/Ising formulation of the optimisation problem.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: DCCC数据集中的每个特征都唯一地映射到相应的（弱）逻辑回归分类器，并与二进制决策变量(*q*[i])[i=1,…,23]相关联。这些决策变量在QUBO/Ising形式的优化问题中由逻辑量子比特/自旋变量表示。
- en: The number of non-zero decision variables (weights) depends on the degree of
    regularisation we would like to impose. Table [4.2](#x1-88001r2) shows the optimal
    configurations of the weights as a function of the penalty *λ* obtained for the
    training dataset. Given the relatively small number of weak classifiers in our
    example, the optimal configuration can be found by an exhaustive search. As one
    would expect, the larger the value of the penalty *λ*, the smaller the number
    of non-zero weights.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 非零决策变量（权重）的数量取决于我们希望施加的正则化程度。表[4.2](#x1-88001r2)展示了作为训练数据集惩罚*λ*的函数，权重的最优配置。考虑到我们例子中的弱分类器数量相对较少，可以通过穷举搜索找到最优配置。正如预期的那样，惩罚*λ*值越大，非零权重的数量越小。
- en: '| *λ* | non-zero weights |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| *λ* | 非零权重 |'
- en: '| 500 | {*q*[1]*,q*[6]*,q*[7]*,q*[8]*,q*[9]*,q*[10]*,q*[11]} |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 500 | {*q*[1]*,q*[6]*,q*[7]*,q*[8]*,q*[9]*,q*[10]*,q*[11]} |'
- en: '| 600 | {*q*[6]*,q*[7]*,q*[8]*,q*[9]*,q*[10]*,q*[11]} |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 600 | {*q*[6]*,q*[7]*,q*[8]*,q*[9]*,q*[10]*,q*[11]} |'
- en: '| 700 | {*q*[6]*,q*[7]*,q*[10]*,q*[11]} |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 700 | {*q*[6]*,q*[7]*,q*[10]*,q*[11]} |'
- en: '| 800 | {*q*[6]*,q*[10]*,q*[11]} |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 800 | {*q*[6]*,q*[10]*,q*[11]} |'
- en: '| 900 | {*q*[6]*,q*[11]} |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 900 | {*q*[6]*,q*[11]} |'
- en: '| 1000 | {*q*[6]} |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | {*q*[6]} |'
- en: 'Table 4.2: Optimal configurations of QUBO weights q for various values of the
    penalty λ. The optimal configurations list all non-zero weights.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.2：不同惩罚*λ*值下QUBO权重* q*的最优配置。最优配置列出了所有非零权重。
- en: Given a configuration of weights, we can build the strong classifier as per ([4.1.2](#x1-850002)).
    Then, we can compare the performance of the obtained strong classifier on both
    training (in-sample) and testing (out-of-sample) datasets. The performance metrics
    of choice are *accuracy*, *precision*, and *recall*. The classifier performance
    can also be visualised with the help of a *confusion matrix*. Here are their definitions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组权重配置，我们可以按照([4.1.2](#x1-850002))构建强分类器。然后，我们可以比较获得的强分类器在训练（样本内）和测试（样本外）数据集上的表现。选择的性能指标是*准确率*、*精确度*和*召回率*。分类器的性能也可以借助*混淆矩阵*来可视化。以下是它们的定义。
- en: '**Accuracy** is the ratio of correctly predicted observations to the total
    observations. Accuracy is a good metric for classes of roughly the same size and
    equivalent importance. However, it is a poor metric for the dataset in our example:
    the Class 0 samples (no default) are far more numerous but the relative importance
    of Class 1 samples (default) is much higher.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**是正确预测的观察结果与总观察结果的比例。准确率是评估类别大小大致相同且重要性相当时的一个良好指标。然而，在我们的例子中，它是一个不太理想的指标：类别0样本（无违约）远比类别1样本（违约）多，但类别1样本的相对重要性要高得多。'
- en: '**Precision** is the ratio of correctly predicted positive observations to
    the total predicted positive observations. High precision corresponds with a low
    false positive rate. This is a metric we would like to maximise in the context
    of credit card defaults if there is a high cost associated with the incorrect
    default predictions.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度**是正确预测的正类观察结果与总预测正类观察结果的比例。高精确度意味着低假阳性率。在信用卡违约的背景下，如果错误的违约预测代价高昂，这个指标是我们希望最大化的。'
- en: '**Recall** is the ratio of correctly predicted positive observations to all
    observations in the positive class. In the context of credit card defaults, this
    metric shows how many of the actual defaults were predicted by the classifier.
    We would like to maximise this metric from the risk management perspective.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**是正确预测的正类观察结果与所有正类观察结果的比例。在信用卡违约的背景下，这个指标显示了有多少实际违约被分类器预测出来。从风险管理的角度来看，我们希望最大化这一指标。'
- en: '**Confusion matrix** for a binary classifier is a 2 × 2 matrix whose elements
    are the counts of the true positive (TP), true negative (TN), false positive (FP),
    and false negative (FN) predictions of a classifier, as shown in Figure [4.1](#4.1).'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混淆矩阵**是二分类器的一个2 × 2矩阵，其元素是分类器预测的真实正例（TP）、真实负例（TN）、假正例（FP）和假负例（FN）的计数，如图[4.1](#4.1)所示。'
- en: '![Figurex1-88004r1: Confusion matrix for a binary classifier. ](img/file368.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![Figurex1-88004r1: 二分类器的混淆矩阵。 ](img/file368.jpg)'
- en: 'Figure 4.1: Confusion matrix for a binary classifier.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：二分类器的混淆矩阵。
- en: 'Accuracy, precision, and recall are then defined as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率、精确度和召回率的定义如下：
- en: '| Accuracy | := ![ TP + TN TP-+-TN--+-FP-+-FN--](img/file369.jpg)*,* |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | := ![ TP + TN TP-+-TN--+-FP-+-FN--](img/file369.jpg)*,* |'
- en: '| Precision | := ![ TP --------- TP + FP](img/file370.jpg)*,* |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 精确度 | := ![ TP --------- TP + FP](img/file370.jpg)*,* |'
- en: '| Recall | := ![--TP----- FN + TP](img/file371.jpg)*.* |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | := ![--TP----- FN + TP](img/file371.jpg)*.* |'
- en: Figure [4.2](#4.2) displays in-sample and out-of-sample confusion matrices for
    the strong QBoost classifier assuming that Class 1 (default) is the positive class
    and Class 0 (no default) is the negative class. The penalty was set at *λ* = 10³,
    thus enforcing strong regularisation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4.2](#4.2) 显示了强 QBoost 分类器的样本内和样本外混淆矩阵，假设类别 1（违约）为正类，类别 0（无违约）为负类。惩罚参数设置为
    *λ* = 10³，从而强制执行强正则化。
- en: The in-sample and out-of-sample results are quite close, as one would expect
    from a strongly regularised classifier. Table [4.3](#x1-88008r3) summarises the
    results.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 样本内和样本外的结果非常接近，正如从一个强正则化的分类器所期望的那样。表 [4.3](#x1-88008r3) 总结了结果。
- en: '![Figurex1-88006r2: Confusion matrices for the QBoost classifier (DCCC dataset).
    ](img/file372.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![Figurex1-88006r2: QBoost 分类器的混淆矩阵（DCCC 数据集）。](img/file372.jpg)'
- en: 'Figure 4.2: Confusion matrices for the QBoost classifier (DCCC dataset).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：QBoost 分类器的混淆矩阵（DCCC 数据集）。
- en: '|  | Accuracy | Precision | Recall |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | 准确率 | 精确度 | 召回率 |'
- en: '| In-sample | 0.82 | 0.69 | 0.33 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 样本内 | 0.82 | 0.69 | 0.33 |'
- en: '| Out-of-sample | 0.83 | 0.71 | 0.33 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 样本外 | 0.83 | 0.71 | 0.33 |'
- en: 'Table 4.3: Accuracy, precision, and recall for the QBoost classifier trained
    and tested on the DCCC dataset.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.3：QBoost 分类器在 DCCC 数据集上训练和测试后的准确率、精确度和召回率。
- en: 4.3 Classical Benchmarks
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 经典基准
- en: 'Classical benchmarking is an important element of the testing of quantum algorithms.
    Small-scale (or even stylised) problems are ideally suited for this task. Let
    us see how the QBoost model performs in comparison with the standard classical
    ML classifiers: neural networks and gradient boosting.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 经典基准测试是量子算法测试中的一个重要组成部分。小规模（甚至是风格化的）问题非常适合这个任务。让我们看看 QBoost 模型与标准经典机器学习分类器（神经网络和梯度提升）相比的表现。
- en: 4.3.1 Artificial neural network
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 人工神经网络
- en: 'An Artificial Neural Network (ANN) is a network of interconnected *activation*
    *units* (or *artificial neurons*), where each activation unit performs three main
    functions (Figure [4.3](#4.3)):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANN）是由互联的 *激活* *单元*（或 *人工神经元*）组成的网络，其中每个激活单元执行三项主要功能（图 [4.3](#4.3)）：
- en: Summation of the input signals (*x*[i])[i=1,…,N], from all the upstream units
    to which it is connected with multiplication by the corresponding weights (*w*[i])[i=1,…,N];
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入信号的求和（*x*[i]）[i=1,…,N]，来自所有上游单元，并与相应的权重（*w*[i]）[i=1,…,N] 相乘；
- en: Non-linear transformation of the aggregated input;
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合输入的非线性变换；
- en: Sending the result to the downstream units to which it is connected.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将结果发送到其连接的下游单元。
- en: Sometimes the activation unit also performs binarisation (or, more generally,
    digitisation) of the output – typically, this is a task of the activation units
    in the output layer of an ANN trained as a classifier.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，激活单元还执行二值化（或更一般地，数字化）输出任务——通常，这是作为分类器训练的 ANN 输出层中的激活单元的任务。
- en: '![Figurex1-90004r3: Schematic representation of an artificial neuron (perceptron).
    ](img/file373.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![Figurex1-90004r3: 人工神经元（感知器）的示意图。](img/file373.jpg)'
- en: 'Figure 4.3: Schematic representation of an artificial neuron (perceptron).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：人工神经元（感知器）的示意图。
- en: 'In its simplest form, an ANN is organised as layers of activation units: an
    input layer, an output layer, and one or several hidden layers, as schematically
    pictured in Figure [4.4](#4.4).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的形式下，ANN 组织为激活单元的层次结构：输入层、输出层以及一个或多个隐藏层，如图 [4.4](#4.4) 所示。
- en: '![Figurex1-90006r4: Schematic representation of a feedforward ANN. ](img/file374.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![Figurex1-90006r4: 前馈人工神经网络的示意图。](img/file374.jpg)'
- en: 'Figure 4.4: Schematic representation of a feedforward ANN.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：前馈人工神经网络的示意图。
- en: 'The activation unit in Figure [4.3](#4.3) is known as a *perceptron*, and the
    ANNs consisting of layers of perceptrons are known as Multi-Layer Perceptrons
    (MLPs). MLPs are *feedforward* neural networks: the signal travels in one direction
    from the input layer to the output layer. ANNs can be organised differently with
    signal travelling back and forth between the layers, and we will explore one such
    model in the next chapter. However, when it comes to building a classifier, the
    simple feedforward architecture works well in practice.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4.3](#4.3) 中的激活单元被称为 *感知器*，由感知器层组成的人工神经网络（ANN）被称为多层感知器（MLP）。MLP 是 *前馈* 神经网络：信号从输入层沿一个方向传递到输出层。ANN
    也可以以不同方式组织，信号在各层之间来回传递，我们将在下一章探讨其中一个模型。然而，在构建分类器时，简单的前馈架构在实践中效果很好。
- en: The practical approach to the ANN architecture is based on the fundamental result
    obtained by Cybenko  [[75](Biblography.xhtml#XCybenko1989)]. It states that arbitrary
    decision regions can be arbitrarily well approximated by continuous feedforward
    neural networks with only a single hidden layer and any continuous sigmoidal non-linearity.
    This result was further generalised to the wider range of activation functions
    by Hornik, Stinchcombe, and White  [[141](Biblography.xhtml#XHornik1990)]. It
    was established that multilayer feedforward networks with only a single hidden
    layer and an appropriately smooth hidden layer activation function are capable
    of arbitrarily accurate approximating any arbitrary function and its derivatives.
    In fact, these networks can even approximate functions that are not differentiable
    in the classical sense, but possess only generalised derivatives  [[224](Biblography.xhtml#XNikolskii1975)].
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络架构的实际方法基于Cybenko获得的基本结果[[75](Biblography.xhtml#XCybenko1989)]。该结果表明，任意的决策区域可以通过仅包含一个隐藏层和任何连续的sigmoid非线性激活函数的连续前馈神经网络进行任意逼近。Hornik、Stinchcombe和White[[141](Biblography.xhtml#XHornik1990)]进一步将这一结果推广到了更广泛的激活函数范围。研究表明，具有单个隐藏层并且隐藏层激活函数平滑的多层前馈网络能够以任意精度逼近任何任意函数及其导数。事实上，这些网络甚至可以逼近那些在经典意义上不可微的函数，而只具有广义导数[[224](Biblography.xhtml#XNikolskii1975)]。
- en: 4.3.2 Training artificial neural networks
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 训练人工神经网络
- en: The process of training an ANN consists in finding an optimal configuration
    of network parameters (weights and biases) such that the new unseen input is transformed
    in the desired way. The network is trained on what is known as a *training dataset*.
    The samples from the training dataset can be *labelled* (each sample is assigned
    a class label, either numerical or categorical). In this case, we can perform
    *supervised learning*, where the network is tasked with learning the mapping between
    the features and the class labels – an ANN trained in the supervised learning
    mode becomes a classifier. When the samples are not labelled, we can train the
    network as a regressor. Although ANNs trained as classifiers may seem to be the
    most obvious practical decision-making tools, regressors too find numerous applications
    in various fields of quantitative finance, for example in learning the natural
    dynamics and transformations of interest rate curves  [[169](Biblography.xhtml#XKondratyev2018)].
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 训练人工神经网络的过程是找到一组网络参数（权重和偏置）的最佳配置，使得新的未见输入能以期望的方式转化。网络是在所谓的*训练数据集*上进行训练的。训练数据集中的样本可以是*有标签的*（每个样本都被分配一个类标签，可能是数值的，也可能是分类的）。在这种情况下，我们可以执行*监督学习*，其中网络的任务是学习特征与类标签之间的映射——在监督学习模式下训练的人工神经网络变成了分类器。当样本没有标签时，我们可以将网络训练为回归器。虽然作为分类器训练的人工神经网络似乎是最明显的实际决策工具，但回归器在各种定量金融领域也有着广泛的应用，例如用于学习利率曲线的自然动态和变化[[169](Biblography.xhtml#XKondratyev2018)]。
- en: However, we would like to focus here on the labelled datasets since our objective
    is to consider a classical counterpart of the QBoost classifier. The standard
    approach to training a feedforward ANN is the *backpropagation* of error with
    gradient descent  [[113](Biblography.xhtml#XGoodfellow2016)]. We briefly explain
    the main idea of this method.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们希望在这里重点讨论有标签的数据集，因为我们的目标是考虑QBoost分类器的经典对照。训练前馈人工神经网络的标准方法是通过梯度下降的*反向传播*误差[[113](Biblography.xhtml#XGoodfellow2016)]。我们简要解释这种方法的主要思想。
- en: 'The starting point is the specification of some suitable cost function that
    indicates how far we are from the correct classification. Without loss of generality,
    assume that we work with a training dataset consisting of *M* samples, where each
    sample is a pair of an *N*-dimensional vector of features and a binary class label:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 起点是指定一些合适的代价函数，以指示我们与正确分类的距离。为了不失一般性，假设我们使用一个包含*M*个样本的训练数据集，其中每个样本是一个*N*维特征向量和一个二进制类标签的组合：
- en: '| ![{xj,yj} , with xj := (xj,...,xj ) and (yj) ∈ {0,1}. j=1,...,M 1 N j=1,...,M
    ](img/file375.jpg) |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ![{xj,yj} , 其中 xj := (xj,...,xj ) 且 (yj) ∈ {0,1}。j=1,...,M 1 N j=1,...,M
    ](img/file375.jpg) |  |'
- en: Let (*ŷ*^j)[j=1,…,M] be the class labels assigned to the corresponding training
    samples by the ANN for some configuration of the network weights w = (*w*[1]*,…,w*[K]).
    Then, we can define the cost function as
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 令 (*ŷ*^j)[j=1,…,M] 为人工神经网络根据某个网络权重配置 w = (*w*[1]*,…,w*[K]) 分配给相应训练样本的类标签。然后，我们可以定义代价函数为
- en: '| ![ ∑M ( ) L (w) := g yj,ˆyj(w) , j=1 ](img/file376.jpg) |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑M ( ) L (w) := g yj,ˆyj(w) , j=1 ](img/file376.jpg) |  |'
- en: where *g*(*y*^j*,ŷ*^j(w)) is the estimation error for sample *j*. There are
    many possible ways of specifying the error function, the most popular being the
    squared error
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*g*(*y*^j*,ŷ*^j(w))是样本*j*的估计误差。指定误差函数的方法有很多种，最常见的是平方误差。
- en: '| ![g(yj,ˆyj) := (yj − ˆyj)2\. ](img/file377.jpg) |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ![g(yj,ˆyj) := (yj − ˆyj)2\. ](img/file377.jpg) |  |'
- en: 'Given the cost function *L*(⋅), we can calculate its sensitivities (derivatives)
    *∂L*(w)*∕∂w*[k], for each *k* = 1*,…,K*, with respect to the network weights.
    We can then *update* the weights by changing them in the direction that would
    reduce the estimation error, i.e., by moving in the opposite direction of the
    corresponding gradients:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 给定代价函数*L*(⋅)，我们可以计算其敏感度（导数）*∂L*(w)*∕∂w*[k]，对于每个*k* = 1*,…,K*，即对于网络权重的敏感度。然后我们可以通过在与对应梯度相反的方向上更新权重，从而减少估计误差：
- en: '| ![ ∂L (w) wk ← − wk − η--∂w--, k ](img/file378.jpg) |  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∂L (w) wk ← − wk − η--∂w--, k ](img/file378.jpg) |  |'
- en: where the coefficient *η* is called the *learning rate*, which can be either
    constant or dynamic. We then iterate the procedure given by ([4.3.2](#x1-910002)),
    ([4.3.2](#x1-910002)), and ([4.3.2](#x1-910002)) until either the estimation error
    drops below a predefined threshold or a maximum number of iterations is reached.
    Often the learning rate is set initially at some relatively large value and then
    decays exponentially with the number of iterations.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 其中系数*η*称为*学习率*，它可以是常数或动态的。我们接着按照（[4.3.2](#x1-910002)）、（[4.3.2](#x1-910002)）和（[4.3.2](#x1-910002)）给出的步骤进行迭代，直到估计误差降到预定阈值以下或达到最大迭代次数。学习率通常初始设置为较大的值，然后随着迭代次数的增加呈指数衰减。
- en: 'The gradients can be calculated numerically (e.g., using the finite difference
    method) or analytically, the latter being obviously preferable. The most widely
    used non-linear activation functions and their gradients are listed in Table [4.4](#4.4)
    and their plots are shown in Figure [4.5](#4.5):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度可以通过数值方法（例如，使用有限差分法）或解析法计算，后者显然更为可取。最常用的非线性激活函数及其梯度列在表[4.4](#4.4)中，相关图形展示在图[4.5](#4.5)中：
- en: '![Table 4.4: Activation functions. ](img/file379.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![Table 4.4: 激活函数。](img/file379.jpg)'
- en: 'Table 4.4: Activation functions.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '表4.4: 激活函数。'
- en: '![Figure 4.5: Activation functions. ](img/file380.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 4.5: 激活函数。](img/file380.jpg)'
- en: 'Figure 4.5: Activation functions.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '图4.5: 激活函数。'
- en: '**Remark:** The sigmoid activation functions, such as logistic sigmoid and
    hyperbolic tangent, are the activation functions of choice for shallow neural
    networks with only a couple of hidden layers. In this case, it is possible to
    exploit the smoothness of the sigmoid functions in order to achieve the best possible
    approximation of the function we are trying to learn. However, in the case of
    deep neural networks with a large number of hidden layers, we face the problem
    of vanishing gradients – gradients of *σ*(*x*) and tanh(*x*) become null as *x*
    →±∞. At the same time, ReLU always has a non-zero gradient for all *x >* 0, which
    makes it the activation function of choice for deep neural networks whenever it
    makes sense to sacrifice the smoothness of the activation function for non-zero
    gradients.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**备注：** Sigmoid激活函数，如逻辑Sigmoid和双曲正切，是浅层神经网络（只有少数隐藏层）的首选激活函数。在这种情况下，可以利用Sigmoid函数的平滑性来实现对我们试图学习的函数的最佳近似。然而，对于具有大量隐藏层的深层神经网络，我们面临梯度消失的问题——当*x*
    → ±∞时，*σ*(*x*)和tanh(*x*)的梯度变为零。同时，ReLU对于所有*x >* 0总是具有非零梯度，这使得它成为深层神经网络中首选的激活函数，特别是当可以为了非零梯度而牺牲激活函数的平滑性时。'
- en: 'Finally, the problem of overfitting can be addressed by adding a regularisation
    penalty term to ([4.3.2](#x1-910002)), for example the following *L*[2] penalty,
    which discourages large network weights associated with strong non-linearity:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，过拟合问题可以通过向（[4.3.2](#x1-910002)）中添加正则化惩罚项来解决，例如以下的*L*[2]惩罚项，它能抑制与强非线性相关的大权重：
- en: '| ![ ∑M ( ) L (w) := g yj,ˆyj(w) + λ &#124;&#124;w&#124;&#124;2, j=1 ](img/file381.jpg)
    |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑M ( ) L (w) := g yj,ˆyj(w) + λ &#124;&#124;w&#124;&#124;2, j=1 ](img/file381.jpg)
    |  |'
- en: where the parameter *λ* controls the degree of regularisation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 其中参数*λ*控制正则化的程度。
- en: 4.3.3 Decision trees and gradient boosting
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 决策树与梯度提升
- en: The decision tree approach to classification is based on the concept of splitting
    a dataset on the available features in order to maximise the *information gain*,
    defined as
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类方法基于通过可用特征划分数据集的概念，以最大化*信息增益*，定义为
- en: '| ![ M G(D, f) = I(D )− ∑ Nj-I(d ), j=1 N j ](img/file382.jpg) |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| ![ M G(D, f) = I(D )− ∑ Nj-I(d ), j=1 N j ](img/file382.jpg) |  |'
- en: 'where *D* is the dataset of the parent node, (*d*[j])[j=1,…,M] are the datasets
    of the child nodes into which the parent node is split, *N* is the number of samples
    in the parent node, (*N*[j])[j=1,…,M] are the number of samples in the child nodes,
    and *I* is the chosen *impurity measure*. The latter indicates the presence of
    the samples from the different classes in the same node: it is zero if the node
    holds samples from a single class and is maximal if the node holds an equal number
    of samples from the available classes. Therefore, maximisation of the information
    gain is achieved through minimisation of the child node impurities.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*D* 是父节点的数据集，(*d*[j])[j=1,…,M] 是父节点被划分成的子节点的数据集，*N* 是父节点中的样本数，(*N*[j])[j=1,…,M]
    是子节点中的样本数，*I* 是选择的*不纯度度量*。后者表示同一节点中来自不同类别的样本的存在：如果节点只包含某一类的样本，则为零；如果节点包含来自所有类别的样本，则为最大。因此，信息增益的最大化通过最小化子节点的不纯度来实现。
- en: Figure [4.6](#4.6) provides a schematic representation of a decision tree based
    on the binary ("rainy/not rainy") and continuous ("wind speed") features. The
    decision tree algorithm starts at the *root*, which is shown in the figure as
    a shaded box. Splitting the dataset on the root feature results in the largest
    information gain. The splitting leads to the creation of *branches* (shown in
    the figure as arrows going from the parent node to the child nodes) and *leaves*
    (shown in the figure as white boxes). The terminal leaves (classes) are represented
    as dashed boxes. The splitting continues until either no more branches can be
    created or the maximum allowed depth is reached. It is good practice to avoid
    the construction of a too deep tree by imposing *pruning* – a strict limit on
    the maximum depth of the tree, in order to avoid overfitting.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4.6](#4.6)提供了基于二元（"雨天/非雨天"）和连续（"风速"）特征的决策树的示意图。决策树算法从*根节点*开始，在图中显示为阴影框。基于根特征划分数据集会产生最大的
    信息增益。划分导致了*分支*（图中以箭头表示，从父节点指向子节点）和*叶子*（图中以白色框表示）的生成。终端叶子（类）以虚线框表示。划分持续进行，直到无法再创建更多分支或达到最大允许深度为止。通过施加*剪枝*（限制树的最大深度）来避免构建过深的树是一个好习惯，以避免过拟合。
- en: '![Figurex1-92004r6: Schematic representation of a decision tree. ](img/file383.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图x1-92004r6: 决策树的示意图。](img/file383.jpg)'
- en: 'Figure 4.6: Schematic representation of a decision tree.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：决策树的示意图。
- en: The most widely used impurity measures are *Gini impurity* and *entropy*. Let
    (*p*[i]^l)[i=1,…,C] be the proportion of the samples that belong to class *i*
    for node *l*. Then the impurity measures are defined as
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的不纯度度量是*基尼不纯度*和*熵*。设 (*p*[i]^l)[i=1,…,C] 为节点 *l* 中属于类 *i* 的样本比例。那么不纯度度量定义为
- en: '![ C C ∑ l l ∑ l l IGini := pi(1 − pi) and IEntropy := − pilog2(pi). i=1 i=1
    ](img/file384.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![ C C ∑ l l ∑ l l IGini := pi(1 − pi) 和 IEntropy := − pilog2(pi). i=1 i=1
    ](img/file384.jpg)'
- en: Decision trees can be seen as weak learners that can be *boosted* to be strong
    learners. One of the most popular methods of combining weak classifiers into a
    single strong classifier is *gradient boosting*. The main principle of gradient
    boosting is as follows  [[185](Biblography.xhtml#XChengLi)].
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以视为弱学习器，可以通过*提升*变成强学习器。将弱分类器组合成一个强分类器的最流行方法之一是*梯度提升*。梯度提升的主要原理如下[[185](Biblography.xhtml#XChengLi)]。
- en: The objective is to improve the weak classifier through an iterative process
    with the improvement measured as a minimisation of the estimation error (for example,
    the squared error given by ([4.3.2](#x1-910002))). As before, without loss of
    generality, we assume that we deal with the binary classification problem ([4.3.2](#x1-910002)).
    Further, assume that at the *k*-th iteration, the weak learner returns the estimate
    *ŷ*[k](x^j) for sample x^j. In order to improve the classification results, the
    algorithm should add some estimator *h*[k], such that for the given sample x^j
    we have
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是通过迭代过程改进弱分类器，改进的度量是最小化估计误差（例如，由([4.3.2](#x1-910002))给出的平方误差）。如前所述，假设我们处理的是二分类问题（[4.3.2](#x1-910002)）。进一步假设在第*k*次迭代中，弱学习器为样本x^j返回估计值*ŷ*[k](x^j)。为了改进分类结果，算法应该添加一些估计器*h*[k]，使得对于给定的样本x^j，我们有
- en: '![ˆyk+1(xj) := ˆyk(xj)+ hk(xj) = yj, ](img/file385.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![ˆyk+1(xj) := ˆyk(xj)+ hk(xj) = yj, ](img/file385.jpg)'
- en: 'where *y*^j is the correct class label for sample x^j. In other words, the
    task is to fit the new estimator *h*[k] to the residuals *y*^j −*ŷ*[k](x^j), *j*
    = 1*,…,M*. We also notice that the estimator *h*[k] is proportional to the negative
    gradient of the squared error ([4.3.2](#x1-910002)) with respect to *ŷ*[k]:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*y*^j是样本x^j的正确类别标签。换句话说，任务是将新的估计器*h*[k]拟合到残差*y*^j −*ŷ*[k](x^j)，*j* = 1*,…,M*。我们还注意到，估计器*h*[k]与平方误差([4.3.2](#x1-910002))相对于*ŷ*[k]的负梯度成正比：
- en: '![ j j j 1∂g(yj,ˆyk(xj)) hk(x ) := y − yˆk(x ) = − 2 ∂ˆyk . ](img/file386.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![ j j j 1∂g(yj,ˆyk(xj)) hk(x ) := y − yˆk(x ) = − 2 ∂ˆyk . ](img/file386.jpg)'
- en: Therefore, gradient boosting combines boosting with the gradient descent algorithm.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，梯度提升将提升与梯度下降算法结合起来。
- en: 4.3.4 Benchmarking against standard classical classifiers
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 与标准经典分类器的基准比较
- en: 'The classical benchmarks of choice are the MLP classifier (`sklearn.neural_network.MLPClassifier`)
    and the gradient boosting classifier (`sklearn.ensemble.GradientBoostingClassifier`).
    Table [4.5](#x1-93003r5) holds weakly optimised model parameters: we did not search
    for the absolute best set of model parameters but tried just a few configurations.
    We can think of it as a very rough grid search method that produces a viable configuration
    of model parameters but is not necessarily optimal. All other model parameters
    were set at their default values.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的基准选择是MLP分类器（`sklearn.neural_network.MLPClassifier`）和梯度提升分类器（`sklearn.ensemble.GradientBoostingClassifier`）。表[4.5](#x1-93003r5)列出了经过弱优化的模型参数：我们没有寻找绝对最优的模型参数集，而是尝试了少数几种配置。可以将其看作是一种非常粗略的网格搜索方法，产生了一个可行的模型参数配置，但不一定是最优的。所有其他模型参数都设置为默认值。
- en: '| Gradient Boosting Classifier | MLP Classifier |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 梯度提升分类器 | MLP分类器 |'
- en: '| loss = ‘deviance’ | hidden_layer_sizes = (20) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| loss = ‘deviance’ | hidden_layer_sizes = (20) |'
- en: '| learning_rate = 0.1 | activation = ‘tanh’ |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| learning_rate = 0.1 | activation = ‘tanh’ |'
- en: '| n_estimators = 1000 | solver = ‘adam’ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| n_estimators = 1000 | solver = ‘adam’ |'
- en: '| criterion = ‘friedman_mse’ | alpha = 0.1 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| criterion = ‘friedman_mse’ | alpha = 0.1 |'
- en: '| max_depth = 3 | max_iter = 5000 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| max_depth = 3 | max_iter = 5000 |'
- en: '|  | alpha = 0.01 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | alpha = 0.01 |'
- en: 'Table 4.5: Model parameters for classical benchmarks.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.5：经典基准模型参数。
- en: Figure [4.7](#4.7) displays out-of-sample confusion matrices for the classical
    benchmarks and Table [4.6](#x1-93007r6) provides a direct comparison of the out-of-sample
    results for the QBoost and classical classifiers.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4.7](#4.7)显示了经典基准模型的外部样本混淆矩阵，表[4.6](#x1-93007r6)提供了QBoost和经典分类器的外部样本结果的直接对比。
- en: '![Figurex1-93005r7: Confusion matrices for the gradient boosting and MLP classifiers
    (DCCC dataset, out-of-sample results). ](img/file387.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![Figurex1-93005r7: 梯度提升和MLP分类器的混淆矩阵（DCCC数据集，外部样本结果）。 ](img/file387.jpg)'
- en: 'Figure 4.7: Confusion matrices for the gradient boosting and MLP classifiers
    (DCCC dataset, out-of-sample results).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：梯度提升和MLP分类器的混淆矩阵（DCCC数据集，外部样本结果）。
- en: '|  | Accuracy | Precision | Recall |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | 准确度 | 精度 | 召回率 |'
- en: '| Gradient Boosting | 0.83 | 0.69 | 0.35 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 梯度提升 | 0.83 | 0.69 | 0.35 |'
- en: '| MLP | 0.83 | 0.69 | 0.35 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| MLP | 0.83 | 0.69 | 0.35 |'
- en: '| QBoost | 0.83 | 0.71 | 0.33 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| QBoost | 0.83 | 0.71 | 0.33 |'
- en: 'Table 4.6: Out-of-sample accuracy, precision, and recall for the QBoost, gradient
    boosting, and MLP classifiers (DCCC dataset).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.6：QBoost、梯度提升和MLP分类器在DCCC数据集上的外部样本准确度、精度和召回率。
- en: QBoost achieves similar out-of-sample results to those of gradient boosting
    and MLP classifiers. A comparison of in-sample and out-of-sample QBoost performance
    confirms QBoost’s ability to impose strong regularisation and avoid overfitting.
    At the same time, QBoost provides full transparency in terms of which features
    contribute to the strong classifier. We also obtain an explicit optimal configuration
    of features for any given degree of regularisation. This is not the case when
    we deal with the conventional machine learning models, which may require an extensive
    analysis of sensitivities and feature importances to unpack their "black boxes".
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: QBoost 在样本外的表现与梯度提升和多层感知器（MLP）分类器相似。样本内和样本外的 QBoost 性能对比证明了 QBoost 能够施加强大的正则化并避免过拟合。与此同时，QBoost
    提供了对强分类器所依赖的特征的完全透明性。我们还可以为任何给定的正则化程度获得明确的最优特征配置。传统的机器学习模型则不同，可能需要广泛分析敏感性和特征重要性，才能揭示其“黑盒”机制。
- en: Quantum boosting can be applied to financial optimisation problems where the
    emphasis is on transparency, interpretability, and robustness.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 量子提升可以应用于金融优化问题，特别是在强调透明性、可解释性和鲁棒性的场景中。
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to apply quantum annealing to build a strong
    classifier from several weak ones. We started with the general principles of quantum
    boosting and its corresponding QUBO formulation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何应用量子退火将多个弱分类器组合成一个强分类器。我们从量子提升的基本原理及其对应的 QUBO 公式开始。
- en: We then illustrated the application of the QBoost algorithm to solving a practical
    real-world financial problem, namely predicting credit card clients defaulting
    on their payments. The chosen dataset is reasonably large and complex enough to
    provide a meaningful challenge while remaining easy to understand and interpret
    the obtained results.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着演示了如何将 QBoost 算法应用于解决实际的金融问题，即预测信用卡客户违约的情况。所选的数据集既足够大又足够复杂，能够提供有意义的挑战，同时结果易于理解和解释。
- en: It is important to have an objective comparison with the corresponding classical
    counterparts. With this in mind, we introduced several classical classifiers based
    on the concepts of a feedforward neural network and a decision tree. We benchmarked
    QBoost against the MLP and gradient boosting models using such metrics as accuracy,
    precision, and recall.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与对应的经典模型进行客观对比是非常重要的。为此，我们介绍了几种基于前馈神经网络和决策树概念的经典分类器。我们使用准确率、精确度和召回率等指标，将 QBoost
    与 MLP 和梯度提升模型进行了基准测试。
- en: In the next chapter, we will learn how quantum annealing can assist in training
    powerful generative machine learning models.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习量子退火如何帮助训练强大的生成式机器学习模型。
- en: Join our book’s Discord space
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/quantum](https://packt.link/quantum)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人一起学习，和 2000 多名成员共同成长：[https://packt.link/quantum](https://packt.link/quantum)
- en: '![PIC](img/file1.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1.png)'
