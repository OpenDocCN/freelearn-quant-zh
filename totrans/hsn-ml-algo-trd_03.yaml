- en: Alternative Data for Finance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Propelled by the explosive growth of the internet and mobile networks, digital
    data continues to grow exponentially amid advances in the technology to process,
    store, and analyze new data sources. The exponential growth in the availability of
    and ability to manage more diverse digital data, in turn, has been a critical
    force behind the dramatic performance improvements of **machine learning** (**ML**)
    that are driving innovation across industries, including the investment industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scale of the data revolution is extraordinary: the past two years alone
    have witnessed the creation of 90% of all data that exists in the world today,
    and by 2020, each of the 7.7 billion people worldwide is expected to produce 1.7
    MB of new information every second of every day. On the other hand, back in 2012,
    only 0.5% of all data was ever analyzed and used, whereas 33% is deemed to have
    value by 2020\. The gap between data availability and usage is likely to narrow
    quickly as global investments in analytics are set to rise beyond $210 billion
    by 2020, while the value creation potential is a multiple higher.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explains how individuals, business processes, and sensors produce
    alternative data. It also provides a framework to navigate and evaluate the proliferating
    supply of alternative data for investment purposes. It demonstrates the workflow,
    from acquisition to preprocessing and storage using Python for data obtained through
    web scraping to set the stage for the application of ML. It concludes by providing
    examples of sources, providers, and applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How the alternative data revolution has unleashed new sources of information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How individuals, business processes, and sensors generate alternative data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to evaluate the proliferating supply of alternative data used for algorithmic
    trading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to work with alternative data in Python, such as by scraping the internet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important categories and providers of alternative data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The alternative data revolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data deluge driven by digitization, networking, and plummeting storage
    costs has led to profound qualitative changes in the nature of information available
    for predictive analytics, often summarized by the five Vs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume**: The amount of data generated, collected, and stored is orders of
    magnitude larger as the byproduct of online and offline activity, transactions,
    records, and other sources and volumes continue to grow with the capacity for
    analysis and storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity**: Data is generated, transferred, and processed to become available
    near, or at, real-time speed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variety**: Data is organized in formats no longer limited to structured,
    tabular forms, such as CSV files or relational database tables. Instead, new sources
    produce semi-structured formats, such as JSON or HTML, and unstructured content,
    including raw text, image, and audio or video data, adding new challenges to render
    data suitable for ML algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Veracity**: The diversity of sources and formats makes it much more difficult
    to validate the reliability of the data''s information content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value**: Determining the value of new datasets can be much more time—and
    resource-consuming, as well as more uncertain than before.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For algorithmic trading, new data sources offer an informational advantage if
    they provide access to information unavailable from traditional sources, or provide
    access sooner. Following global trends, the investment industry is rapidly expanding
    beyond market and fundamental data to alternative sources to reap alpha through
    an informational edge. Annual spending on data, technological capabilities, and
    related talent are expected to increase from the current $3 billion by 12.8% annually
    through 2020.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, investors can access macro or company-specific data in real-time that
    historically has been available only at a much lower frequency. Use cases for
    new data sources include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Online price data on a representative set of goods and services can be used
    to measure inflation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of store visits or purchases permits real-time estimates of company
    or industry-specific sales or economic activity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Satellite images can reveal agricultural yields, or activity at mines or on
    oil rigs before this information is available elsewhere
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the standardization and adoption of big datasets advances, the information
    contained in conventional data will likely lose most of its predictive value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the capability to process and integrate diverse datasets and apply
    ML allows for complex insights. In the past, quantitative approaches relied on
    simple heuristics to rank companies using historical data for metrics such as
    the price-to-book ratio, whereas ML algorithms synthesize new metrics, and learn
    and adapt such rules taking into account evolving market data. These insights
    create new opportunities to capture classic investment themes such as value, momentum,
    quality, or sentiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Momentum**: ML can identify asset exposures to market price movements, industry
    sentiment, or economic factors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value**: Algorithms can analyze large amounts of economic and industry-specific
    structured and unstructured data, beyond financial statements, to predict the
    intrinsic value of a company'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality**: The sophisticated analysis of integrated data allows for the evaluation
    of customer or employee reviews, e-commerce, or app traffic to identify gains
    in market share or other underlying earnings quality drivers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, however, useful data is often not freely available and alternative datasets
    instead require thorough evaluation, costly acquisition, careful management, and sophisticated
    analysis to extract tradable signals.
  prefs: []
  type: TYPE_NORMAL
- en: Sources of alternative data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alternative datasets are generated by many sources but can be classified at
    a high-level as predominantly produced by:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Individuals** who post on social media, review products, or use search engines'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Businesses** that record commercial transactions, in particular, credit card
    payments, or capture supply-chain activity as intermediaries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensors** that, among many other things, capture economic activity through
    images such as satellites or security cameras, or through movement patterns such
    as cell phone towers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nature of alternative data continues to evolve rapidly as new data sources
    become available while sources previously labeled alternative become part of the
    mainstream. The **Baltic Dry Index** (**BDI**), for instance, assembles data from
    several hundred shipping companies to approximate the supply/demand of dry bulk
    carriers and is now available on the Bloomberg Terminal.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative data includes raw data as well as data that is aggregated or has
    been processed in some form to add value. For instance, some providers aim to
    extract tradeable signals, such as sentiment scores. We will address the various
    types of providers in [Chapter 4](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml),
    *Alpha Factor Research*.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative data sources differ in crucial respects that determine their value
    or signal content for algorithmic trading strategies. We will address these aspects
    in the next section on *Evaluating alternative datasets*.
  prefs: []
  type: TYPE_NORMAL
- en: Individuals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Individuals automatically create electronic data through online activities,
    as well as through their offline activity as the latter is captured electronically
    and often linked to online identities. Data generated by individuals is frequently
    unstructured in text, image, or video formats, disseminated through multiple platforms
    and includes:'
  prefs: []
  type: TYPE_NORMAL
- en: Social media posts, such as opinions or reactions on general-purpose sites such
    as Twitter, Facebook, or LinkedIn, or business-review sites such as Glassdoor
    or Yelp
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: E-commerce activity that reflects an interest in or the perception of products
    on sites such as Amazon or Wayfair
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search engine activity using platforms such as Google or Bing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mobile app usage, downloads, and reviews
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personal data such as messaging traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The analysis of social media sentiment has become very popular because it can
    be applied to individual stocks, industry baskets, or market indices. The most
    common source is Twitter, followed by various news vendors and blog sites. Supply
    is competitive, and prices are lower because it is often obtained through increasingly
    commoditized web scraping. Reliable social media datasets that include blogs,
    tweets, or videos have typically less than five years of history, given how recently
    consumers have adopted these tools at scale. Search history, in contrast, is available
    from 2004.
  prefs: []
  type: TYPE_NORMAL
- en: Business processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Businesses and public entities produce and collect many valuable sources of
    alternative data. Data that results from business processes has often more structure
    than that generated by individuals. It is very effective as a leading indicator
    for activity that is otherwise available at a much lower frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data generated by business processes include:'
  prefs: []
  type: TYPE_NORMAL
- en: Payment card transaction data made available by processors and financial institutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Company exhaust data produced by ordinary digitized activity or record-keeping,
    such as banking records, cashier scanner data, or supply chain orders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trade flow and market microstructure data (such as L-2 and L-3 order book data,
    illustrated in [Chapter 2](e7bd6fc7-7ef7-4c4e-acec-ac1d083f8902.xhtml), *Market
    and Fundamental Data*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Company payments monitored by credit rating agencies or financial institutions
    to assess liquidity and creditworthiness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Credit card transactions and company exhaust data, such as point-of-sale data,
    are among the most reliable and predictive datasets. Credit card data is available
    with around ten years of history and, at different lags, almost up to real time,
    while corporate earnings are reported quarterly with a 2.5-week lag. The time
    horizon and reporting lag for company exhaust data varies widely depending on
    the source. Market microstructure datasets have over 15 years of history compared
    to sell-side flow data, which typically has fewer than five years of consistent
    history.
  prefs: []
  type: TYPE_NORMAL
- en: Sensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data generated by networked sensors embedded in a broad range of devices are
    among the most rapidly growing data sources, driven by the proliferation of smartphones
    and the reduction in the cost of satellite technologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'This category of alternative data is typically very unstructured and often
    significantly larger in volume than data generated by individuals or business
    processes, and poses much higher processing challenges. Key alternative data sources
    in this category include:'
  prefs: []
  type: TYPE_NORMAL
- en: Satellite imaging to monitor economic activity, such as construction, shipping,
    or commodity supply
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geolocation data to track traffic in retail stores, such as using volunteered
    smartphone data, or on transport routes, such as on ships or trucks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cameras positioned at a location of interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weather and pollution sensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Internet of Things** (**IoT**) will further accelerate the large-scale
    collection of this type of alternative data by embedding networked microprocessors
    into personal and commercial electronic devices such as home appliances, public
    spaces, and industrial production processes.
  prefs: []
  type: TYPE_NORMAL
- en: Sensor-based alternative data that contains satellite images, mobile app usage,
    or cellular-location tracking is typically available with a three to four-year
    history.
  prefs: []
  type: TYPE_NORMAL
- en: Satellites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The resources and timelines required to launch a geospatial imaging satellite
    have dropped dramatically; instead of tens of millions of dollars and years of
    preparation, the cost has fallen to around $100,000 to place a small satellite
    as a secondary payload into a low-earth orbit. Hence, companies can obtain much
    higher-frequency coverage (currently about daily) of specific locations using
    entire fleets of satellites.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases include the monitoring of economic and commercial activity that can
    be captured using aerial coverage, such as agricultural and mineral production
    and shipments, construction of real estates or ships, industrial incidents such
    as a fire, or car, and foot traffic at locations of interest. Related sensor data
    is contributed by drones that are used in agriculture to monitor crops using infrared
    light.
  prefs: []
  type: TYPE_NORMAL
- en: Several challenges may need to be addressed before satellite image data can
    be reliably used in ML models. These include accounting for weather conditions
    and in particular, cloud cover and seasonal effects, around holidays, and the
    irregular coverage of specific locations that may affect the quality of the predictive
    signals.
  prefs: []
  type: TYPE_NORMAL
- en: Geolocation data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Geolocation data is another rapidly-growing category of alternative data generated
    by sensors. A familiar source is smartphones with which individuals voluntarily
    share their geographic location through an application or from wireless signals
    such as GPS, CDMA, or WiFi measure foot traffic around places of interest, such
    as stores, restaurants, or event venues.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, an increasing number of airports, shopping malls, and retail stores
    have installed sensors that track the number and movements of customers. While
    the original motivation to deploy these sensors often was to measure the impact
    of marketing activity, the resulting data can also be used to estimate foot traffic
    or sales. Sensors to capture geolocation include 3D stereo video and thermal imaging,
    which lowers privacy concerns but works well with moving objects. There are also
    sensors attached to ceilings as well as pressure-sensitive mats. Some providers
    use multiple sensors in combination, including vision, audio, and cellphone location
    for a comprehensive account of the shopper journey, which includes not only the
    count and duration of visits but extends to conversion and measurement of repeat
    visits.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating alternative datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ultimate objective of alternative data is to provide an informational advantage
    in the competitive search for trading signals that produce alpha, namely positive,
    uncorrelated investment returns. In practice, the signals extracted from alternative
    datasets can be used on a standalone basis or combined with other signals as part
    of a quantitative strategy. Independent usage is viable if the Sharpe ratio generated
    by a strategy based on a single dataset is sufficiently high, but is rare in practice
    (see [Chapter 4](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml), *Alpha Factor Research* for
    details on signal measurement and evaluation).
  prefs: []
  type: TYPE_NORMAL
- en: Quant firms are building libraries of alpha factors that may be weak signals
    individually but can produce attractive returns in combination. As highlighted
    in [Chapter 1](d68f12f8-66fd-4857-b60e-399e5bbd9ea2.xhtml), *Machine Learning
    for Trading*, investment factors should be based on a fundamental and economic
    rationale, otherwise, they are more likely the result of overfitting to historical
    data than to persist and generate alpha on new data.
  prefs: []
  type: TYPE_NORMAL
- en: Signal decay due to competition is a serious concern, and as the alternative
    data ecosystem evolves, it is unlikely that many datasets will retain meaningful
    Sharpe ratio signals. Effective strategies to extend the half-life of the signal
    content of an alternative dataset include exclusivity agreements or a focus on
    datasets that pose processing challenges to raise the barriers to entry.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation criteria
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative dataset can be evaluated based on the quality of its signal content,
    qualitative aspects of the data, and various technical aspects.
  prefs: []
  type: TYPE_NORMAL
- en: Quality of the signal content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The signal content can be evaluated with respect to the target asset class,
    the investment style, the relation to conventional risk premiums, and most importantly,
    its alpha content.
  prefs: []
  type: TYPE_NORMAL
- en: Asset classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most alternative datasets contain information directly relevant to equities
    and commodities. Interesting datasets targeting investments in real estate have
    also multiplied after Zillow successfully pioneered price estimates in 2006.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative data on corporate credit is growing as alternative sources for monitoring
    corporate payments, including for smaller businesses, are being developed. Data
    on fixed income and around interest-rate projections is a more recent phenomenon
    but continues to increase as more product sales and price information are being
    harvested at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Investment style
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The majority of datasets focus on specific sectors and stocks, and as such naturally
    appeal to long-short equity investors. As the scale and scope of alternative data
    collection continue to rise, alternative data will likely also become relevant
    for investors in macro themes, such as consumer credit, activity in emerging markets,
    and commodity trends.
  prefs: []
  type: TYPE_NORMAL
- en: Some alternative datasets can be used as proxies for traditional measures of
    market risk, while other signals are more relevant for high-frequency traders
    that use quantitative strategies over a brief time horizon.
  prefs: []
  type: TYPE_NORMAL
- en: Risk premiums
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some alternative datasets, such as credit card payments or social media sentiment,
    have been shown to produce signals that have a low correlation (lower than 5%)
    with traditional risk premiums in equity markets, such as value, momentum, and quality
    of volatility. As a result, combining signals derived from such alternative data
    with an algorithmic trading strategy based on traditional risk factors can be
    an important building block toward a more diversified risk premiums portfolio.
  prefs: []
  type: TYPE_NORMAL
- en: Alpha content and quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The signal strength required to justify the investment in an alternative dataset
    naturally depends on its costs, and alternative data prices vary widely. Data
    that scores social sentiment can be acquired for a few thousand dollars or less,
    while the cost of a dataset on comprehensive and timely credit card payments can
    cost several million per year.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore in detail how to evaluate trading strategies driven by alternative
    data using historical data, so-called **backtests**, to estimate the amount of
    alpha contained in a dataset. In isolated cases, a dataset may contain sufficient
    alpha signal to drive a strategy on a standalone basis, but more typical is the
    combined use of various alternative and other sources of data. In these cases,
    a dataset permits the extraction of weak signals that produce a small positive
    Sharpe ratio that would not receive a capital allocation on its own but can deliver
    a portfolio-level strategy when integrated with similar other signals. This is
    not guaranteed, however, as there are also many alternative datasets that do not
    contain any alpha content.
  prefs: []
  type: TYPE_NORMAL
- en: Besides evaluating a dataset's alpha content, it is also important to assess
    to which extent a signal is incremental or orthogonal, that is, unique to a dataset, or
    already captured by other data, and in the latter case compare the costs for this
    type of signal.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is essential to evaluate the potential capacity of a strategy that
    relies on a given, that is, the amount of capital that can be allocated without
    undermining its success because a capacity limit will make it more difficult to
    recover the cost of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Quality of the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The quality of a dataset is another important criterion because it impacts the
    effort required to analyze and monetize it, and the reliability of the predictive
    signal it contains. Quality aspects include the data frequency and the length
    of its available history, the reliability or accuracy of the information it contains,
    the extent to which it complies with current or potential future regulations,
    and how exclusive its use is.
  prefs: []
  type: TYPE_NORMAL
- en: Legal and reputational risks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The use of alternative datasets may carry legal or reputational risk, in particular
    when they include the following items:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Material Non-Public Information** (**MNPI**) because it implies infringement
    of insider trading regulations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personally Identifiable Information** (**PII**), primarily since the European
    Union has enacted the **General Data Protection Regulation** (**GDPR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accordingly, legal and compliance requirements require thorough review. There
    could also be conflicts of interest when the provider of the data is also a market
    participant who is actively trading based on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Exclusivity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The likelihood that an alternative dataset contains a signal that is sufficiently
    predictive to drive a strategy on a stand-alone basis with a high Sharpe ratio
    for a meaningful period is inversely related to its availability and ease of processing.
    In other words, the more exclusive, and the harder to process the data, the better
    the chances that a dataset with alpha content can drive a strategy without suffering
    rapid signal decay.
  prefs: []
  type: TYPE_NORMAL
- en: Public fundamental data that provides standard financial ratios contains little
    alpha and is not attractive for a standalone strategy, but may help diversify
    a portfolio of risk factors. Large, complex datasets will take more time to be
    absorbed by the market, and new datasets continue to emerge on a frequent basis.
    Hence, it is essential to assess how familiar other investors already are with
    a dataset, and whether the provider is the best source for this type of information.
  prefs: []
  type: TYPE_NORMAL
- en: Additional benefits to exclusivity or being an early adopter of a new dataset
    may arise when a business just begins to sell exhaust data that it generated for
    other purposes because it may be possible to influence how the data is collected
    or curated, or to negotiate conditions that limit access for competitors at least
    for a certain time period.
  prefs: []
  type: TYPE_NORMAL
- en: Time horizon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More extensive history is highly desirable to test the predictive power of a
    dataset under different scenarios. The availability varies greatly between several
    months and several decades and has important implications for the scope of the
    trading strategy that can be built and tested based on the data. We mentioned
    some ranges for time horizons for different datasets when introducing the main
    types of sources.
  prefs: []
  type: TYPE_NORMAL
- en: Frequency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The frequency of the data determines how often new information becomes available
    and how differentiated a predictive signal can be over a given period. It also
    impacts the time horizon of the investment strategy and ranges from intra-day,
    to daily, weekly, or an even lower frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Reliability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naturally, the degree to which the data accurately reflects what it intends
    to measure or how well this can be verified is of significant concern and should
    be validated by means of a thorough audit. This applies to both raw and processed
    data where the methodology used to extract or aggregate information needs to be
    analyzed, taking into account the cost-benefit ratio for the proposed acquisition.
  prefs: []
  type: TYPE_NORMAL
- en: Technical aspects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Technical aspects concern the latency or delay of reporting, and the format
    in which the data is made available.
  prefs: []
  type: TYPE_NORMAL
- en: Latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data providers often provide resources in batches, and a delay can result from
    how the data is collected, subsequent processing and transmission, as well as
    regulatory or legal constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data is made available in a broad range of formats, depending on the source.
    Processed data will be in user-friendly formats and easily integrated into existing
    systems or queries via a robust API. On the other end of the spectrum are voluminous
    data sources, such as video, audio, or image data, or a proprietary format, that
    require more skills to be prepared for analysis, but also provide higher barriers
    to entry for potential competitors.
  prefs: []
  type: TYPE_NORMAL
- en: The market for alternative data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The investment industry is going to spend an estimated to $2,000,000,000-3,000,000,000
    on data services in 2018, and this number is expected to grow at double digits
    per year in line with other industries. This expenditure includes the acquisition
    of alternative data, investments in related technology, and the hiring of qualified
    talent.
  prefs: []
  type: TYPE_NORMAL
- en: 'A survey by Ernst and Young shows significant adoption of alternative data
    in 2017; 43% of funds are using scraped web data, for instance, and almost 30%
    are experimenting with satellite data. Based on the experience so far, fund managers
    considered scraped web data and credit card data to be most insightful, in contrast
    to geolocation and satellite data, which around 25% considered to be less informative:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5161f34-9221-4f1c-b245-de87e4a00874.png)'
  prefs: []
  type: TYPE_IMG
- en: Reflecting the rapid growth of this new industry, the market for alternative
    data providers is quite fragmented. J.P. Morgan lists over 500 specialized data
    firms, while [AlternativeData.org](https://alternativedata.org/) lists over 300\.
    Providers play numerous roles, including intermediaries such as consultants, aggregators,
    and tech solutions; sell-side supports deliver data in various formats, ranging
    from raw to semi-processed data or some form of a signal extracted from one or
    more sources.
  prefs: []
  type: TYPE_NORMAL
- en: We will highlight the size of the main categories and profile a few prominent
    examples to illustrate their diversity.
  prefs: []
  type: TYPE_NORMAL
- en: Data providers and use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[AlternativeData.org](https://alternativedata.org/) (supported by provider
    Yipit) lists several categories that can serve as a rough proxy for activity in
    various data-provider segments. Social sentiment analysis is by far the largest
    category, while satellite and geolocation data have been growing rapidly in recent
    years:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Product category** | **Number of providers** | **Goals** |'
  prefs: []
  type: TYPE_TB
- en: '| Social sentiment | 48 | Raw or processed social media data; short-term trends
    |'
  prefs: []
  type: TYPE_TB
- en: '| Satellite | 26 | Aerial monitoring of medium-term economic activity |'
  prefs: []
  type: TYPE_TB
- en: '| Geolocation | 22 | Track retail, commercial real estate, or event foot traffic
    |'
  prefs: []
  type: TYPE_TB
- en: '| Web data and traffic | 22 | Monitor search interest, brand popularity, and
    events |'
  prefs: []
  type: TYPE_TB
- en: '| Credit and debit card usage | 14 | Track near-term consumer spend and business
    revenues |'
  prefs: []
  type: TYPE_TB
- en: '| App usage | 7 | Monitor app sales or collect secondary data |'
  prefs: []
  type: TYPE_TB
- en: '| Email and consumer receipts | 6 | Track consumer spend by chain, brand, sector,
    or geography |'
  prefs: []
  type: TYPE_TB
- en: '| Weather | 4 | Crop and commodity-related longer-term trends |'
  prefs: []
  type: TYPE_TB
- en: '| Other | 87 |  |'
  prefs: []
  type: TYPE_TB
- en: The following brief examples aim to illustrate the broad range of service providers
    and potential use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Social sentiment data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Social sentiment analysis is most closely associated with Twitter data. Gnip
    was an early social-media aggregator that provided data from numerous sites using
    an API and was acquired by Twitter in 2014 for $134 million. Search engines are
    another source that became prominent when researchers published in nature that
    investment strategies based on Google Trends for terms such as debt could be used
    for a profitable trading strategy over an extended period (see the GitHub repo [https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading) for
    references).
  prefs: []
  type: TYPE_NORMAL
- en: Dataminr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dataminr was founded in 2009 and provides social-sentiment and news analysis
    based on an exclusive agreement with Twitter. The company is one of the larger
    alternative providers and raised an additional $392 million in funding in June
    2018 led by Fidelity at a $1,6 billion valuation, bringing total funding to $569
    billion. It emphasizes real-time signals extracted from social media feeds using
    machine learning and serves a wide range of clients, including not only buy and
    sell-side investment firms but also news organizations and the public sector.
  prefs: []
  type: TYPE_NORMAL
- en: StockTwits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: StockTwits is a social network and micro-blogging platform where several hundred
    thousand investment professionals share information and trading ideas in the form
    of StockTwits that are viewed by a large audience across the financial web and
    social media platforms. This data can be exploited because it may reflect investor
    sentiment or itself drive trades that, in turn, impact prices. The references
    on GitHub contain a link to a paper that builds a trading strategy on selected
    features.
  prefs: []
  type: TYPE_NORMAL
- en: RavenPack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RavenPack analyzes a large number of diverse, unstructured, text-based data
    to produce structured indicators, including sentiment scores, that aim to contain
    information relevant to investors. The underlying data sources range from premium
    newswires and regulatory information to press releases and over 19,000 web publications.
    J.P. Morgan tested a long-short sovereign bond and equity strategies based on
    sentiment scores and achieved positive results with low correlation to conventional
    risk premiums (see references).
  prefs: []
  type: TYPE_NORMAL
- en: Satellite data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RS Metrics, founded in 2010, triangulates geospatial data from satellites, drones,
    and airplanes with a focus on metals and commodities, as well as real-estate and
    industrial applications. The company offers signals, predictive analytics, alerts,
    and end-user applications based on its own high-resolution satellites. Use cases
    include the estimation of retail traffic targeting certain chains or commercial
    real estate, as well as the production and storage of certain common metals or
    employment at related production locations.
  prefs: []
  type: TYPE_NORMAL
- en: Geolocation data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advan, founded in 2015, serves hedge fund clients with signals derived from
    mobile phone traffic data, targeting 1,600 tickers across various sectors in the
    US and EU. The company collects data using apps that install geolocation codes
    on smartphones with explicit user consent and track location using several channels
    (such as WiFi, Bluetooth, and cellular signal) for enhanced accuracy. The uses
    cases include estimates of customer traffic at physical store locations, which
    in turn can be used as input to models that predict top-line revenues of traded
    companies.
  prefs: []
  type: TYPE_NORMAL
- en: Email receipt data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Eagle Alpha provides, among other services, data on a large set of online transactions
    using email receipts, covering over 5000 retailers, including item—and SKU-level
    transaction data categorized in 53 product groups. J.P. Morgan analyzed a time
    series dataset, starting in 2013, that covered a constant group of users active
    throughout the entire sample period. The dataset contained total aggregate spend,
    number of orders, and the number of unique buyers per period.
  prefs: []
  type: TYPE_NORMAL
- en: Working with alternative data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will illustrate the acquisition of alternative data using web scraping, targeting
    first OpenTable restaurant data, and then move to earnings call transcripts hosted
    by Seeking Alpha.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping OpenTable data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typical sources of alternative data are review websites such as Glassdoor or
    Yelp that convey insider insights using employee comments or guest reviews. This
    data provides valuable input for ML models that aim to predict a business' prospects or
    directly its market value to obtain trading signals.
  prefs: []
  type: TYPE_NORMAL
- en: The data needs to be extracted from the HTML source, barring any legal obstacles.
    To illustrate the web scraping tools that Python offers, we'll retrieve information
    on restaurant bookings from OpenTable. Data of this nature could be used to forecast
    economic activity by geography, real estate prices, or restaurant chain revenues.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data from HTML using requests and BeautifulSoup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will request and parse HTML source code. We will be using
    the `requests` library to make **Hyper Text Transfer Protocol** (**HTTP**) requests
    and retrieve the HTML source code, and `BeautifulSoup` to parse and extract the
    text content.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will, however, encounter a common obstacle: websites may request certain
    information from the server only after initial page-load using JavaScript. As
    a result, a direct HTTP request will not be successful. To sidestep this type
    of protection, we will use a headless browser that retrieves the website content
    as a browser would:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use `BeautifulSoup` to parse the HTML content, and then look for
    all `span` tags with the class associated with the restaurant names that we obtain
    by inspecting the source code, `rest-row-name-text` (see GitHub repo for linked
    instructions to examine website source code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have identified the page elements of interest, `BeautifulSoup` makes
    it easy to retrieve the contained text. If you want to get the price category
    for each restaurant, you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When you try to get the number of bookings, however, you just get an empty
    list because the site uses JavaScript code to request this information after the
    initial loading is complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Introducing Selenium – using browser automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the browser automation tool Selenium to operate a headless FireFox
    browser that will parse the HTML content for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code opens the FireFox browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s close the browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To retrieve the HTML source code using selenium and Firefox, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Building a dataset of restaurant bookings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now you only need to combine all the interesting elements from the website to
    create a feature that you could use in a model to predict economic activity in
    geographic regions or foot traffic in specific neighborhoods.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Selenium, you can follow the links to the next pages and quickly build
    a dataset of over 10,000 restaurants in NYC that you could then update periodically
    to track a time series. First, we set up a function that parses the content of
    the pages that we plan on crawling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we start a headless browser that continues to click on the Next button
    for us and capture the results displayed on each page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Websites continue to change, so this code may stop working at some point and
    will require updating to follow the latest site navigation and bot detection.
  prefs: []
  type: TYPE_NORMAL
- en: One step further – Scrapy and splash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scrapy is a powerful library to build bots that follow links, retrieve the
    content, and store the parsed result in a structured way. In combination with
    the headless browser splash, it can also interpret JavaScript and becomes an efficient
    alternative to Selenium. You can run the spider using the `scrapy crawl opentable` command
    in the `01_opentable` directory where the results are logged to `spider.log`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There are numerous ways to extract information from this data beyond the reviews
    and bookings of individual restaurants or chains.
  prefs: []
  type: TYPE_NORMAL
- en: We could further collect and geo-encode the restaurants' addresses, for instance,
    to link the restaurants' physical location to other areas of interest, such as
    popular retail spots or neighborhoods to gain insights into particular aspects
    of economic activity. As mentioned before, such data will be most valuable in
    combination with other information.
  prefs: []
  type: TYPE_NORMAL
- en: Earnings call transcripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Textual data is an essential alternative data source. One example of textual
    information is transcripts of earnings calls where executives do not only present
    the latest financial results, but also respond to questions by financial analysts.
    Investors utilize transcripts to evaluate changes in sentiment, emphasis on particular
    topics, or style of communication.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will illustrate the scraping and parsing of earnings call transcripts from
    the popular trading website [www.seekingalpha.com](http://seekingalpha.com/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parsing HTML using regular expressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To collect structured data from the unstructured transcripts, we can use regular
    expressions in addition to `BeautifulSoup`.
  prefs: []
  type: TYPE_NORMAL
- en: 'They allows us to collect detailed information not only about the earnings
    call company and timing but also capture who was present and attribute the statements
    to analysts and company representatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We store the result in several `.csv` files for easy access when we use ML
    to process natural language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: See `README` in the GitHub repo for additional details and references for further
    resources to develop web-scraping applications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced new sources of alternative data made available
    as a result of the big data revolution, including individuals, business processes,
    and sensors, such as satellites or GPS location devices. We presented a framework
    to evaluate alternative datasets from an investment perspective and laid out key
    categories and providers to help you navigate this vast and quickly-expanding
    area that provides critical inputs for algorithmic trading strategies that use
    ML.
  prefs: []
  type: TYPE_NORMAL
- en: We explored powerful Python tools to collect your own datasets at scale so that
    you can potentially work on getting your private informational edge as an algorithmic
    trader using web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: We will now proceed, in the following chapter, to the design and evaluation
    of alpha factors that produce trading signals, and look at how to combine them
    in a portfolio context.
  prefs: []
  type: TYPE_NORMAL
