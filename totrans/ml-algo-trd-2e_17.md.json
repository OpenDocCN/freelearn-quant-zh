["```py\nN = 50000\nfactor = 0.1\nnoise = 0.1\nX, y = make_circles(n_samples=N, shuffle=True,\n                   factor=factor, noise=noise) \n```", "```py\nY = np.zeros((N, 2))\nfor c in [0, 1]:\n   Y[y == c, c] = 1\n'Shape of: X: (50000, 2) | Y: (50000, 2) | y: (50000,)' \n```", "```py\ndef logistic(z):\n   \"\"\"Logistic function.\"\"\"\n   return 1 / (1 + np.exp(-z)) \n```", "```py\ndef hidden_layer(input_data, weights, bias):\n   \"\"\"Compute hidden activations\"\"\"\n   return logistic(input_data @ weights + bias) \n```", "```py\ndef softmax(z):\n   \"\"\"Softmax function\"\"\"\n   return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True) \n```", "```py\ndef output_layer(hidden_activations, weights, bias):\n   \"\"\"Compute the output y_hat\"\"\"\n   return softmax(hidden_activations @ weights + bias) \n```", "```py\ndef forward_prop(data, hidden_weights, hidden_bias, output_weights, output_bias):\n   \"\"\"Neural network as function.\"\"\"\n   hidden_activations = hidden_layer(data, hidden_weights, hidden_bias)\n   return output_layer(hidden_activations, output_weights, output_bias) \n```", "```py\ndef predict(data, hidden_weights, hidden_bias, output_weights, output_bias):\n   \"\"\"Predicts class 0 or 1\"\"\"\n   y_pred_proba = forward_prop(data,\n                               hidden_weights,\n                               hidden_bias,\n                               output_weights,\n                               output_bias)\n   return np.around(y_pred_proba) \n```", "```py\ndef loss(y_hat, y_true):\n   \"\"\"Cross-entropy\"\"\"\n   return - (y_true * np.log(y_hat)).sum() \n```", "```py\ndef loss_gradient(y_hat, y_true):\n   \"\"\"output layer gradient\"\"\"\n   return y_hat - y_true \n```", "```py\ndef output_weight_gradient(H, loss_grad):\n   \"\"\"Gradients for the output layer weights\"\"\"\n   return  H.T @ loss_grad\ndef output_bias_gradient(loss_grad):\n   \"\"\"Gradients for the output layer bias\"\"\"\n   return np.sum(loss_grad, axis=0, keepdims=True) \n```", "```py\ndef hidden_layer_gradient(H, out_weights, loss_grad):\n   \"\"\"Error at the hidden layer.\n   H * (1-H) * (E . Wo^T)\"\"\"\n   return H * (1 - H) * (loss_grad @ out_weights.T) \n```", "```py\ndef hidden_weight_gradient(X, hidden_layer_grad):\n   \"\"\"Gradient for the weight parameters at the hidden layer\"\"\"\n   return X.T @ hidden_layer_grad\n\ndef hidden_bias_gradient(hidden_layer_grad):\n   \"\"\"Gradient for the bias parameters at the output layer\"\"\"\n   return np.sum(hidden_layer_grad, axis=0, keepdims=True) \n```", "```py\ndef compute_gradients(X, y_true, w_h, b_h, w_o, b_o):\n   \"\"\"Evaluate gradients for parameter updates\"\"\"\n   # Compute hidden and output layer activations\n   hidden_activations = hidden_layer(X, w_h, b_h)\n   y_hat = output_layer(hidden_activations, w_o, b_o)\n   # Compute the output layer gradients\n   loss_grad = loss_gradient(y_hat, y_true)\n   out_weight_grad = output_weight_gradient(hidden_activations, loss_grad)\n   out_bias_grad = output_bias_gradient(loss_grad)\n   # Compute the hidden layer gradients\n   hidden_layer_grad = hidden_layer_gradient(hidden_activations,\n                                             w_o, loss_grad)\n   hidden_weight_grad = hidden_weight_gradient(X, hidden_layer_grad)\n   hidden_bias_grad = hidden_bias_gradient(hidden_layer_grad)\n   return [hidden_weight_grad, hidden_bias_grad, out_weight_grad, out_bias_grad] \n```", "```py\ndef update_momentum(X, y_true, param_list, Ms, momentum_term, learning_rate):\n   \"\"\"Compute updates with momentum.\"\"\"\n   gradients = compute_gradients(X, y_true, *param_list)\n   return [momentum_term * momentum - learning_rate * grads\n           for momentum, grads in zip(Ms, gradients)] \n```", "```py\ndef update_params(param_list, Ms):\n   \"\"\"Update the parameters.\"\"\"\n   return [P + M for P, M in zip(param_list, Ms)] \n```", "```py\ndef train_network(iterations=1000, lr=.01, mf=.1):\n   # Initialize weights and biases\n   param_list = list(initialize_weights())\n   # Momentum Matrices = [MWh, Mbh, MWo, Mbo]\n   Ms = [np.zeros_like(M) for M in param_list]\n   train_loss = [loss(forward_prop(X, *param_list), Y)]\n   for i in range(iterations):\n       # Update the moments and the parameters\n       Ms = update_momentum(X, Y, param_list, Ms, mf, lr)\n       param_list = update_params(param_list, Ms)\n       train_loss.append(loss(forward_prop(X, *param_list), Y))\n   return param_list, train_loss \n```", "```py\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nmodel = Sequential([\n    Dense(units=3, input_shape=(2,), name='hidden'),\n    Activation('sigmoid', name='logistic'),\n    Dense(2, name='output'),\n    Activation('softmax', name='softmax'),\n]) \n```", "```py\nmodel.summary()\nLayer (type)                 Output Shape              Param #   \n=================================================================\nhidden (Dense)               (None, 3)                 9         \n_________________________________________________________________\nlogistic (Activation)        (None, 3)                 0         \n_________________________________________________________________\noutput (Dense)               (None, 2)                 8         \n_________________________________________________________________\nsoftmax (Activation)         (None, 2)                 0         \n=================================================================\nTotal params: 17\nTrainable params: 17\nNon-trainable params: 0 \n```", "```py\nmodel.compile(optimizer='rmsprop',\n             loss='binary_crossentropy',\n             metrics=['accuracy']) \n```", "```py\ntb_callback = TensorBoard(log_dir='./tensorboard',\n                         histogram_freq=1,\n                         write_graph=True,\n                         write_images=True) \n```", "```py\nmodel.fit(X, Y,\n         epochs=25,\n         validation_split=.2,\n         batch_size=128,\n         verbose=1,\n         callbacks=[tb_callback]) \n```", "```py\ntensorboard --logdir=/full_path_to_your_logs ## e.g. ./tensorboard \n```", "```py\n%load_ext tensorboard\n%tensorboard --logdir tensorboard/ \n```", "```py\nimport torch\nX_tensor = torch.from_numpy(X)\ny_tensor = torch.from_numpy(y)\nX_tensor.shape, y_tensor.shape\n(torch.Size([50000, 2]), torch.Size([50000])) \n```", "```py\nimport torch.utils.data as utils\ndataset = utils.TensorDataset(X_tensor,y_tensor)\ndataloader = utils.DataLoader(dataset,\n                              batch_size=batch_size,\n                              shuffle=True) \n```", "```py\nimport torch.nn as nn\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Net, self).__init__()  # Inherited from nn.Module\n        self.fc1 = nn.Linear(input_size, hidden_size)  \n        self.logistic = nn.LogSigmoid()                          \n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        \"\"\"Forward pass: stacking each layer together\"\"\"\n        out = self.fc1(x)\n        out = self.logistic(out)\n        out = self.fc2(out)\n        out = self.softmax(out)\n        return out \n```", "```py\nnet = Net(input_size, hidden_size, num_classes)\nnet\nNet(\n  (fc1): Linear(in_features=2, out_features=3, bias=True)\n  (logistic): LogSigmoid()\n  (fc2): Linear(in_features=3, out_features=2, bias=True)\n  (softmax): Softmax()\n) \n```", "```py\nlist(net.parameters())[0]\nParameter containing:\ntensor([[ 0.3008, -0.2117],\n        [-0.5846, -0.1690],\n        [-0.6639,  0.1887]], requires_grad=True) \n```", "```py\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate) \n```", "```py\nfor epoch in range(num_epochs):\n    print(epoch)\n    for i, (features, label) in enumerate(dataloader):\n\n        features = Variable(features.float())         \n        label = Variable(label.long())\n        # Initialize the hidden weights\n        optimizer.zero_grad()  \n\n        # Forward pass: compute output given features\n        outputs = net(features)\n\n        # Compute the loss\n        loss = criterion(outputs, label)\n        # Backward pass: compute the gradients\n        loss.backward()\n        # Update the weights\n        optimizer.step() \n```", "```py\ntest_value = Variable(torch.from_numpy(X)).float()\nprediction = net(test_value).data.numpy()\nPrediction.shape\n(50000, 2) \n```", "```py\ndata = pd.read_hdf('../12_gradient_boosting_machines/data/data.h5', \n                   'model_data').dropna()\noutcomes = data.filter(like='fwd').columns.tolist()\nlookahead = 1\noutcome= f'r{lookahead:02}_fwd'\nX = data.loc[idx[:, :'2017'], :].drop(outcomes, axis=1)\ny = data.loc[idx[:, :'2017'], outcome] \n```", "```py\ndef make_model(dense_layers, activation, dropout):\n    '''Creates a multi-layer perceptron model\n\n    dense_layers: List of layer sizes; one number per layer\n    '''\n    model = Sequential()\n    for i, layer_size in enumerate(dense_layers, 1):\n        if i == 1:\n            model.add(Dense(layer_size, input_dim=X_cv.shape[1]))\n            model.add(Activation(activation))\n        else:\n            model.add(Dense(layer_size))\n            model.add(Activation(activation))\n    model.add(Dropout(dropout))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error',\n                  optimizer='Adam')\n    return model \n```", "```py\nn_splits = 12\ntrain_period_length=21 * 12 * 4\ntest_period_length=21 * 3\ncv = MultipleTimeSeriesCV(n_splits=n_splits,\n                          train_period_length=train_period_length,\n                          test_period_length=test_period_length,\n                          lookahead=lookahead) \n```", "```py\ndense_layer_opts = [(16, 8), (32, 16), (32, 32), (64, 32)]\ndropout_opts = [0, .1, .2]\nparam_grid = list(product(dense_layer_opts, activation_opts, dropout_opts))\nnp.random.shuffle(param_grid)\nlen(param_grid)\n12 \n```", "```py\ndef get_train_valid_data(X, y, train_idx, test_idx):\n    x_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]\n    x_val, y_val = X.iloc[test_idx, :], y.iloc[test_idx]\n    return x_train, y_train, x_val, y_val \n```", "```py\nic = []\nscaler = StandardScaler()\nfor params in param_grid:\n    dense_layers, activation, dropout = params\n    for batch_size in [64, 256]:\n        checkpoint_path = checkpoint_dir / str(dense_layers) / activation /\n                          str(dropout) / str(batch_size)\n        for fold, (train_idx, test_idx) in enumerate(cv.split(X_cv)):\n            x_train, y_train, x_val, y_val = get_train_valid_data(X_cv, y_cv,\n                                             train_idx, test_idx)\n            x_train = scaler.fit_transform(x_train)\n            x_val = scaler.transform(x_val)\n            preds = y_val.to_frame('actual')\n            r = pd.DataFrame(index=y_val.groupby(level='date').size().index)\n            model = make_model(dense_layers, activation, dropout)\n            for epoch in range(20):            \n                model.fit(x_train, y_train,\n                          batch_size=batch_size,\n                          epochs=1, validation_data=(x_val, y_val))\n                model.save_weights(\n                    (checkpoint_path / f'ckpt_{fold}_{epoch}').as_posix())\n                preds[epoch] = model.predict(x_val).squeeze()\n                r[epoch] = preds.groupby(level='date').apply(lambda x: spearmanr(x.actual, x[epoch])[0]).to_frame(epoch)\n            ic.append(r.assign(dense_layers=str(dense_layers), \n                               activation=activation, \n                               dropout=dropout,\n                               batch_size=batch_size,\n                               fold=fold)) \n```", "```py\ndates = sorted(ic.index.unique())\ncv_period = 24 * 21\ncv_dates = dates[:cv_period]\nic_cv = ic.loc[cv_dates]\n(ic_cv.drop('fold', axis=1).groupby(params).median().stack()\n .to_frame('ic').reset_index().rename(columns={'level_3': 'epoch'})\n .nlargest(n=5, columns='ic')) \n```", "```py\ndata = pd.melt(ic, id_vars=params, var_name='epoch', value_name='ic')\ndata = pd.get_dummies(data, columns=['epoch'] + params, drop_first=True)\nmodel = sm.OLS(endog=data.ic, exog=sm.add_constant(data.drop('ic', axis=1))) \n```", "```py\ndef generate_predictions(dense_layers, activation, dropout,\n                         batch_size, epoch):\n    checkpoint_dir = Path('logs')\n    checkpoint_path = checkpoint_dir / dense_layers / activation /\n                      str(dropout) / str(batch_size)\n\n    for fold, (train_idx, test_idx) in enumerate(cv.split(X_cv)):\n        x_train, y_train, x_val, y_val = get_train_valid_data(X_cv, y_cv, \n                                                              train_idx, \n                                                              test_idx)\n        x_val = scaler.fit(x_train).transform(x_val)\n        model = make_model(dense_layers, activation, dropout, input_dim)\n        status = model.load_weights(\n            (checkpoint_path / f'ckpt_{fold}_{epoch}').as_posix())\n        status.expect_partial()\n        predictions.append(pd.Series(model.predict(x_val).squeeze(), \n                                     index=y_val.index))\n    return pd.concat(predictions) \n```"]