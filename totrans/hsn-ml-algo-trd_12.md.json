["```py\n# compute covariance matrix:\ncov = np.cov(data, rowvar=False) # expects variables in rows by default\ncov.shape\n(3, 3)\n```", "```py\neigen_values, eigen_vectors = eig(cov)\neigen_vectors\narray([[ 0.71409739, -0.66929454, -0.20520656],\n[-0.70000234, -0.68597301, -0.1985894 ],\n[ 0.00785136, -0.28545725, 0.95835928]])\n```", "```py\npca = PCA()\npca.fit(data)\nC = pca.components_.T # columns = principal components\nC\narray([[ 0.71409739, 0.66929454, 0.20520656],\n[-0.70000234, 0.68597301, 0.1985894 ],\n[ 0.00785136, 0.28545725, -0.95835928]])\nnp.allclose(np.abs(C), np.abs(eigen_vectors))\nTrue\n```", "```py\n# eigenvalue matrix\nev = np.zeros((3, 3))\nnp.fill_diagonal(ev, eigen_values)\nev # diagonal matrix\narray([[1.92923132, 0\\. , 0\\. ],\n[0\\. , 0.55811089, 0\\. ],\n[0\\. , 0\\. , 0.00581353]])\n```", "```py\ndecomposition = eigen_vectors.dot(ev).dot(inv(eigen_vectors))\nnp.allclose(cov, decomposition)\n```", "```py\nn_features = data.shape[1]\ndata_ = data - data.mean(axis=0\nUsing the centered data, we compute the singular value decomposition:\nU, s, Vt = svd(data_)\nU.shape, s.shape, Vt.shape\n((100, 100), (3,), (3, 3))\nWe can convert the vector s that only contains the singular values into an nxm matrix and show that the decomposition works:\nS = np.zeros_like(data_)\nS[:n_features, :n_features] = np.diag(s)\nS.shape\n(100, 3)\n```", "```py\nnp.allclose(data_, U.dot(S).dot(Vt))\nTrue\n```", "```py\nnp.allclose(np.abs(C), np.abs(Vt.T))\nTrue\n```", "```py\npca = PCA(n_components=2)\nprojected_data = pca.fit_transform(data)\nprojected_data.shape\n(100, 2)\n```", "```py\npca2.explained_variance_ratio_\narray([0.77381099, 0.22385721])\n```", "```py\nidx = pd.IndexSlice\nwith pd.HDFStore('../../data/assets.h5') as store:\nstocks = store['us_equities/stocks'].marketcap.nlargest(500)\nreturns = (store['quandl/wiki/prices']\n.loc[idx['2010': '2018', stocks.index], 'adj_close']\n.unstack('ticker')\n.pct_change())\n```", "```py\nreturns.info()\nDatetimeIndex: 2072 entries, 2010-01-04 to 2018-03-27\nColumns: 351 entries, A to ZTS\n```", "```py\nreturns = returns.clip(lower=returns.quantile(q=.025),\nupper=returns.quantile(q=.975),\naxis=1)\n```", "```py\nreturns = returns.dropna(thresh=int(returns.shape[0] * .95), axis=1)\nreturns = returns.dropna(thresh=int(returns.shape[1] * .95))\n```", "```py\nreturns.info()\nDatetimeIndex: 2070 entries, 2010-01-05 to 2018-03-27\nColumns: 314 entries, A to ZBH\n```", "```py\ndaily_avg = returns.mean(1)\nreturns = returns.apply(lambda x: x.fillna(daily_avg))\n```", "```py\npca = PCA()\npca.fit(returns)\nPCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\nsvd_solver='auto', tol=0.0, whiten=False)\n```", "```py\nrisk_factors = pd.DataFrame(pca.transform(returns)[:, :2],\ncolumns=['Principal Component 1', 'Principal Component 2'],\nindex=returns.index)\nrisk_factors['Principal Component 1'].corr(risk_factors['Principal Component 2'])\n7.773256996252084e-15\n```", "```py\nidx = pd.IndexSlice\nwith pd.HDFStore('../../data/assets.h5') as store:\nstocks = store['us_equities/stocks'].marketcap.nlargest(30)\nreturns = (store['quandl/wiki/prices']\n.loc[idx['2010': '2018', stocks.index], 'adj_close']\n.unstack('ticker')\n.pct_change())\n```", "```py\nnormed_returns = scale(returns\n       .clip(lower=returns.quantile(q=.025),\n        upper=returns.quantile(q=.975),\n        axis=1)\n.apply(lambda x: x.sub(x.mean()).div(x.std())))\n```", "```py\npca.fit(cov)\npd.Series(pca.explained_variance_ratio_).head()\n0 55.91%\n1 15.52%\n2 5.36%\n3 4.85%\n4 3.32%\n```", "```py\ntop4 = pd.DataFrame(pca.components_[:4], columns=cov.columns)\neigen_portfolios = top4.div(top4.sum(1), axis=0)\neigen_portfolios.index = [f'Portfolio {i}' for i in range(1, 5)]\n```", "```py\ndef get_distance_matrix(corr):\n\"\"\"Compute distance matrix from correlation;\n0 <= d[i,j] <= 1\"\"\"\nreturn np.sqrt((1 - corr) / 2)\ndistance_matrix = get_distance_matrix(corr)\nlinkage_matrix = linkage(squareform(distance_matrix), 'single')\n```", "```py\nclustergrid = sns.clustermap(distance_matrix,\nmethod='single',\nrow_linkage=linkage_matrix,\ncol_linkage=linkage_matrix,\ncmap=cmap, center=0)\nsorted_idx = clustergrid.dendrogram_row.reordered_ind\nsorted_tickers = corr.index[sorted_idx].tolist()\n```", "```py\ndef get_cluster_var(cov, cluster_items):\n    \"\"\"Compute variance per cluster\"\"\"\n    cov_ = cov.loc[cluster_items, cluster_items]  # matrix slice\n    w_ = get_inverse_var_pf(cov_)\n    return (w_ @ cov_ @ w_).item()\n```", "```py\ndef get_hrp_allocation(cov, tickers):\n    \"\"\"Compute top-down HRP weights\"\"\"\n\n    weights = pd.Series(1, index=tickers)\n    clusters = [tickers]  # initialize one cluster with all assets\n\n    while len(clusters) > 0:\n        # run bisectional search:\n        clusters = [c[start:stop] for c in clusters\n                    for start, stop in ((0, int(len(c) / 2)),\n                                        (int(len(c) / 2), len(c)))\n                    if len(c) > 1]\n        for i in range(0, len(clusters), 2):  # parse in pairs\n            cluster0 = clusters[i]\n            cluster1 = clusters[i + 1]\n\n            cluster0_var = get_cluster_var(cov, cluster0)\n            cluster1_var = get_cluster_var(cov, cluster1)\n\n            weight_scaler = 1 - cluster0_var / (cluster0_var + cluster1_var)\n            weights[cluster0] *= weight_scaler\n            weights[cluster1] *= 1 - weight_scaler\n    return weights\n```"]