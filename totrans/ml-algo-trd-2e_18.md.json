["```py\nfrom tensorflow.keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train.shape, X_test.shape\n((60000, 28, 28), (10000, 28, 28)) \n```", "```py\nX_train = X_train.astype('float32')/255\nX_test = X_test.astype('float32')/255 \n```", "```py\nlenet5 = Sequential([\n    Conv2D(filters=6, kernel_size=5, activation='relu', \n           input_shape=(28, 28, 1), name='CONV1'),\n    AveragePooling2D(pool_size=(2, 2), strides=(1, 1), \n                     padding='valid', name='POOL1'),\n    Conv2D(filters=16, kernel_size=(5, 5), activation='tanh', name='CONV2'),\n    AveragePooling2D(pool_size=(2, 2), strides=(2, 2), name='POOL2'),\n    Conv2D(filters=120, kernel_size=(5, 5), activation='tanh', name='CONV3'),\n    Flatten(name='FLAT'),\n    Dense(units=84, activation='tanh', name='FC6'),\n    Dense(units=10, activation='softmax', name='FC7')\n]) \n```", "```py\nLayer (type)                 Output Shape              Param #   \nCONV1 (Conv2D)               (None, 24, 24, 6)         156       \nPOOL1 (AveragePooling2D)     (None, 23, 23, 6)         0         \nCONV2 (Conv2D)               (None, 19, 19, 16)        2416      \n_________________________________________________________________\nPOOL2 (AveragePooling2D)     (None, 9, 9, 16)          0         \n_________________________________________________________________\nCONV3 (Conv2D)               (None, 5, 5, 120)         48120     \n_________________________________________________________________\nFLAT (Flatten)               (None, 3000)              0         \n_________________________________________________________________\nFC6 (Dense)                  (None, 84)                252084    \n________________________________________________________________\nFC7 (Dense)                  (None, 10)                850       =================================================================\nTotal params: 303,626\nTrainable params: 303,626 \n```", "```py\nlenet5.compile(loss='sparse_categorical_crossentropy',\n               optimizer='SGD',\n               metrics=['accuracy']) \n```", "```py\nlenet_history = lenet5.fit(X_train.reshape(-1, 28, 28, 1),\n                          y_train,\n                          batch_size=32,\n                          epochs=100,\n                          validation_split=0.2, # use 0 to train on all data\n                          callbacks=[checkpointer, early_stopping],\n                          verbose=1,\n                          shuffle=True) \n```", "```py\naccuracy = lenet5.evaluate(X_test.reshape(-1, 28, 28, 1), y_test, verbose=0)[1]\nprint('Test accuracy: {:.2%}'.format(accuracy))\nTest accuracy: 99.09% \n```", "```py\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n    width_shift_range=0.1,   # randomly horizontal shift\n    height_shift_range=0.1,  # randomly vertical shift\n    horizontal_flip=True)    # randomly horizontal flip\ndatagen.fit(X_train) \n```", "```py\nfrom tensorflow.keras.applications.vgg16 import VGG16\nvgg16 = VGG16()\nvgg16.summary()\nLayer (type)                 Output Shape              Param #   \ninput_1 (InputLayer)         (None, 224, 224, 3)       0            \nâ€¦ several layers omitted... \nblock5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \nfc1 (Dense)                  (None, 4096)              102764544 \nfc2 (Dense)                  (None, 4096)              16781312  \npredictions (Dense)          (None, 1000)              4097000   \nTotal params: 138,357,544\nTrainable params: 138,357,544 \n```", "```py\ny_pred = vgg16.predict(img_input)\nY_pred.shape\n(7, 1000) \n```", "```py\nvgg16 = VGG16(include_top=False)\nvgg16.predict(img_input).shape\n(7, 7, 7, 512) \n```", "```py\nvgg16 = VGG16(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\nvgg16.trainable = False\nvgg16.summary()\nLayer (type)                 Output Shape              Param #   \n... omitted layers...\nblock5_conv3 (Conv2D)        (None, 10, 10, 512)         2359808   \nblock5_pool (MaxPooling2D)   (None, 5, 5, 512)         0         \nTotal params: 14,714,688\nTrainable params: 0\nNon-trainable params: 14,714,688 \n```", "```py\nfeature_batch = vgg16(image_batch)\nFeature_batch.shape\nTensorShape([32, 5, 5, 512]) \n```", "```py\nglobal_average_layer = GlobalAveragePooling2D()\ndense_layer = Dense(64, activation='relu')\ndropout = Dropout(0.5)\nprediction_layer = Dense(1, activation='sigmoid')\nseq_model = tf.keras.Sequential([vgg16, \n                                 global_average_layer, \n                                 dense_layer, \n                                 dropout, \n                                 prediction_layer])\nseq_model.compile(loss = tf.keras.losses.BinaryCrossentropy(from logits=True),\n                       optimizer = 'Adam', \n                       metrics=[\"accuracy\"]) \n```", "```py\nseq_model.summary()\nLayer (type)                 Output Shape              Param #   \nvgg16 (Model)                (None, 5, 5, 512)         14714688  \nglobal_average_pooling2d (Gl (None, 512)               0         \ndense_7 (Dense)              (None, 64)                32832     \ndropout_3 (Dropout)          (None, 64)                0         \ndense_8 (Dense)              (None, 1)                 65        \nTotal params: 14,747,585\nTrainable params: 11,831,937\nNon-trainable params: 2,915,648 \n```", "```py\nhistory = transfer_model.fit(train_batches,\n                            epochs=initial_epochs,\n                            validation_data=validation_batches) \n```", "```py\nvgg16.trainable = True\nlen(vgg16.layers)\n19\n# Fine-tune from this layer onward\nstart_fine_tuning_at = 12\n# Freeze all the layers before the 'fine_tune_at' layer\nfor layer in vgg16.layers[:start_fine_tuning_at]:\n    layer.trainable =  False \n```", "```py\nfine_tune_epochs = 50\ntotal_epochs = initial_epochs + fine_tune_epochs\nhistory_fine_tune = transfer_model.fit(train_batches,\n                                     epochs=total_epochs,\n                                     initial_epoch=history.epoch[-1],\n                                     validation_data=validation_batches,\n                                     callbacks=[early_stopping]) \n```", "```py\nLayer (type)                 Output Shape              Param #   \ndensenet201 (Model)          (None, 1920)              18321984  \nbatch_normalization (BatchNo (None, 1920)              7680      \ndense (Dense)                (None, 2048)              3934208   \nbatch_normalization_1 (Batch (None, 2048)              8192      \ndense_1 (Dense)              (None, 2048)              4196352   \nbatch_normalization_2 (Batch (None, 2048)              8192      \ndense_2 (Dense)              (None, 2048)              4196352   \nbatch_normalization_3 (Batch (None, 2048)              8192      \ndense_3 (Dense)              (None, 2048)              4196352   \nbatch_normalization_4 (Batch (None, 2048)              8192      \ndense_4 (Dense)              (None, 10)                20490     \nTotal params: 34,906,186\nTrainable params: 34,656,906\nNon-trainable params: 249,280 \n```", "```py\nvgg16 = VGG16(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\nvgg16.trainable = False\nx = vgg16.output\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(256)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dense(128)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nn_digits = Dense(SEQ_LENGTH, activation='softmax', name='n_digits')(x)\ndigit1 = Dense(N_CLASSES-1, activation='softmax', name='d1')(x)\ndigit2 = Dense(N_CLASSES, activation='softmax', name='d2')(x)\ndigit3 = Dense(N_CLASSES, activation='softmax', name='d3')(x)\ndigit4 = Dense(N_CLASSES, activation='softmax', name='d4')(x)\npredictions = Concatenate()([n_digits, digit1, digit2, digit3, digit4]) \n```", "```py\ndef weighted_entropy(y_true, y_pred):\n    cce = tf.keras.losses.SparseCategoricalCrossentropy()\n    n_digits = y_pred[:, :SEQ_LENGTH]\n    digits = {}\n    for digit, (start, end) in digit_pos.items():\n        digits[digit] = y_pred[:, start:end]\n    return (cce(y_true[:, 0], n_digits) +\n            cce(y_true[:, 1], digits[1]) +\n            cce(y_true[:, 2], digits[2]) +\n            cce(y_true[:, 3], digits[3]) +\n            cce(y_true[:, 4], digits[4])) / 5 \n```", "```py\ndef weighted_accuracy(y_true, y_pred):\n    n_digits_pred = K.argmax(y_pred[:, :SEQ_LENGTH], axis=1)\n    digit_preds = {}\n    for digit, (start, end) in digit_pos.items():\n        digit_preds[digit] = K.argmax(y_pred[:, start:end], axis=1)\n    preds = tf.dtypes.cast(tf.stack((n_digits_pred,\n                                     digit_preds[1],\n                                     digit_preds[2],\n                                     digit_preds[3],\n                                     digit_preds[4]), axis=1), tf.float32)\n    return K.mean(K.sum(tf.dtypes.cast(K.equal(y_true, preds), tf.int64), axis=1) / 5) \n```", "```py\nmodel = Model(inputs=vgg16.input, outputs=predictions)\nmodel.compile(optimizer='adam',\n              loss=weighted_entropy,\n              metrics=[weighted_accuracy]) \n```", "```py\nprices = (pd.read_hdf('../data/assets.h5', 'quandl/wiki/prices')\n          .adj_close\n          .unstack().loc['2000':])\nprices.info()\nDatetimeIndex: 2896 entries, 2007-01-01 to 2018-03-27\nColumns: 3199 entries, A to ZUMZ \n```", "```py\nreturns = (prices\n           .resample('M')\n           .last()\n           .pct_change()\n           .dropna(how='all')\n           .loc['2000': '2017']\n           .dropna(axis=1)\n           .sort_index(ascending=False))\n# remove outliers likely representing data errors\nreturns = returns.where(returns<1).dropna(axis=1)\nreturns.info()\nDatetimeIndex: 215 entries, 2017-12-31 to 2000-02-29\nColumns: 1511 entries, A to ZQK \n```", "```py\nn = len(returns)\nnlags = 12\nlags = list(range(1, nlags + 1))\ncnn_data = []\nfor i in range(n-nlags-1):\n    df = returns.iloc[i:i+nlags+1]        # select outcome and lags\n    date = df.index.max()                 # use outcome date\n    cnn_data.append(df.reset_index(drop=True)  # append transposed series\n                    .transpose()\n                    .assign(date=date)\n                    .set_index('date', append=True)\n                    .sort_index(1, ascending=True))\ncnn_data = (pd.concat(cnn_data)\n            .rename(columns={0: 'label'})\n            .sort_index()) \n```", "```py\ncnn_data.info(null_counts=True)\nMultiIndex: 305222 entries, ('A', Timestamp('2001-03-31 00:00:00')) to \n                            ('ZQK', Timestamp('2017-12-31 00:00:00'))\nData columns (total 13 columns):\n... \n```", "```py\nmodel = Sequential([Conv1D(filters=32,\n                           kernel_size=4,\n                           activation='relu',\n                           padding='causal',\n                           input_shape=(12, 1),\n                           use_bias=True,\n                           kernel_regularizer=regularizers.l1_l2(l1=1e-5,\n                                                                 l2=1e-5)),\n                    MaxPooling1D(pool_size=4),\n                    Flatten(),\n                    BatchNormalization(),\n                    Dense(1, activation='linear')]) \n```", "```py\nLayer (type)                 Output Shape              Param #   \nconv1d (Conv1D)              (None, 12, 32)            160       \nmax_pooling1d (MaxPooling1D) (None, 3, 32)             0         \nflatten (Flatten)            (None, 96)                0         \nbatch_normalization (BatchNo (None, 96)                384       \ndense (Dense)                (None, 1)                 97        \nTotal params: 641\nTrainable params: 449\nNon-trainable params: 192 \n```", "```py\nT = list(range(6, 21))\nfor t in T:\n    universe[f'{t:02}_RSI'] = universe.groupby(level='symbol').close.apply(RSI, timeperiod=t) \n```", "```py\nfor t in T:\n    universe[f'{t:02}_NATR'] = universe.groupby(\n                        level='symbol', group_keys=False).apply(\n                        lambda x: NATR(x.high, x.low, x.close, timeperiod=t)) \n```", "```py\nimport pandas_datareader.data as web\nfactor_data = (web.DataReader('F-F_Research_Data_5_Factors_2x3_daily', \n                              'famafrench', start=START)[0]) \n```", "```py\nfactors = [Mkt-RF, 'SMB', 'HML', 'RMW', 'CMA']\nwindows = list(range(15, 90, 5))\nfor window in windows:\n    betas = []\n    for symbol, data in universe.groupby(level='symbol'):\n        model_data = data[[ret]].merge(factor_data, on='date').dropna()\n        model_data[ret] -= model_data.RF\n        rolling_ols = RollingOLS(endog=model_data[ret], \n                                 exog=sm.add_constant(model_data[factors]), \n                                                      window=window)\n        factor_model = rolling_ols.fit(params_only=True).params.drop('const',  \n                                                                     axis=1)\n        result = factor_model.assign(symbol=symbol).set_index('symbol', \n                                                              append=True)\n        betas.append(result)\n    betas = pd.concat(betas).rename(columns=lambda x: f'{window:02}_{x}')\n    universe = universe.join(betas) \n```", "```py\ndf = features.join(targets[target]).dropna().sample(n=100000)\nX = df.drop(target, axis=1)\ny = df[target]\nmi[t] = pd.Series(mutual_info_regression(X=X, y=y), index=X.columns) \n```", "```py\ndef cluster_features(data, labels, ax, title):\n    data = StandardScaler().fit_transform(data)\n    pairwise_distance = pdist(data)\n    Z = linkage(data, 'ward')\n    dend = dendrogram(Z,\n                      labels=labels,\n                      orientation='top',\n                      leaf_rotation=0.,\n                      leaf_font_size=8.,\n                      ax=ax)\n    return dend['ivl'] \n```", "```py\nlabels = sorted(best_features)\ncol_order = cluster_features(features.dropna().values.reshape(-1, 15).T, \n                             labels)\nlabels = list(range(1, 16))\nrow_order = cluster_features(\n    features.dropna().values.reshape(-1, 15, 15).transpose((0, 2, 1)).reshape(-1, 15).T, labels) \n```", "```py\nLayer (type)                 Output Shape              Param #   \nCONV1 (Conv2D)               (None, 15, 15, 16)        160       \nCONV2 (Conv2D)               (None, 15, 15, 32)        4640      \nPOOL1 (MaxPooling2D)         (None, 7, 7, 32)          0         \nDROP1 (Dropout)              (None, 7, 7, 32)          0         \nFLAT1 (Flatten)              (None, 1568)              0         \nFC1 (Dense)                  (None, 32)                50208     \nDROP2 (Dropout)              (None, 32)                0         \nFC2 (Dense)                  (None, 1)                 33        \nTotal params: 55,041\nTrainable params: 55,041\nNon-trainable params: 0 \n```", "```py\ndef get_train_valid_data(X, y, train_idx, test_idx):\n    x_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]\n    x_val, y_val = X.iloc[test_idx, :], y.iloc[test_idx]\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    x_train = scaler.fit_transform(x_train)\n    x_val = scaler.transform(x_val)\n    return (x_train.reshape(-1, size, size, 1), y_train,\n            x_val.reshape(-1, size, size, 1), y_val) \n```", "```py\ncheckpoint_path = Path('models', 'cnn_ts')\nfor fold, (train_idx, test_idx) in enumerate(cv.split(features)):\n    X_train, y_train, X_val, y_val = get_train_valid_data(features, target, train_idx, test_idx)\n    preds = y_val.to_frame('actual')\n    r = pd.DataFrame(index=y_val.index.unique(level='date')).sort_index()\n    model = make_model(filter1=16, act1='relu', filter2=32, \n                       act2='relu', do1=.25, do2=.5, dense=32)\n    for epoch in range(n_epochs):            \n        model.fit(X_train, y_train,\n                  batch_size=batch_size,\n                  validation_data=(X_val, y_val),\n                  epochs=1, verbose=0, shuffle=True)\n        model.save_weights(\n            (checkpoint_path / f'ckpt_{fold}_{epoch}').as_posix())\n        preds[epoch] = model.predict(X_val).squeeze()\n        r[epoch] = preds.groupby(level='date').apply(\n            lambda x: spearmanr(x.actual, x[epoch])[0]).to_frame(epoch) \n```", "```py\ndef generate_predictions(epoch):\n    predictions = []\n    for fold, (train_idx, test_idx) in enumerate(cv.split(features)):\n        X_train, y_train, X_val, y_val = get_train_valid_data(\n            features, target, train_idx, test_idx)\n        preds = y_val.to_frame('actual')\n        model = make_model(filter1=16, act1='relu', filter2=32, \n                       act2='relu', do1=.25, do2=.5, dense=32)\n        status = model.load_weights(\n            (checkpoint_path / f'ckpt_{fold}_{epoch}').as_posix())\n        status.expect_partial()\n        predictions.append(pd.Series(model.predict(X_val).squeeze(), \n                                     index=y_val.index))\n    return pd.concat(predictions)   \npreds = {}\nfor i, epoch in enumerate(ic.drop('fold', axis=1).mean().nlargest(3).index):\n    preds[i] = generate_predictions(epoch) \n```"]