- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用无监督学习的数据驱动风险因子和资产配置。
- en: '*Chapter 6*, *The Machine Learning Process*, introduced how unsupervised learning
    adds value by uncovering structures in data without the need for an outcome variable
    to guide the search process. This contrasts with supervised learning, which was
    the focus of the last several chapters: instead of predicting future outcomes,
    unsupervised learning aims to learn an informative representation of the data
    that helps explore new data, discover useful insights, or solve some other task
    more effectively.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*第6章*，*机器学习过程*，介绍了无监督学习通过发现数据结构而增加价值，而不需要结果变量来指导搜索过程。这与前几章的监督学习形成了对比：无监督学习不是预测未来结果，而是旨在学习数据的信息表示，帮助探索新数据、发现有用的见解或更有效地解决其他任务。'
- en: 'Dimensionality reduction and clustering are the main tasks for unsupervised
    learning:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 降维和聚类是无监督学习的主要任务：
- en: '**Dimensionality reduction** transforms the existing features into a new, smaller
    set while minimizing the loss of information. Algorithms differ by how they measure
    the loss of information, whether they apply linear or nonlinear transformations
    or which constraints they impose on the new feature set.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**将现有特征转换为新的、较小的集合，同时尽量减小信息损失。算法在如何衡量信息损失、是否应用线性或非线性转换以及对新特征集施加哪些约束方面存在差异。'
- en: '**Clustering algorithms** identify and group similar observations or features
    instead of identifying new features. Algorithms differ in how they define the
    similarity of observations and their assumptions about the resulting groups.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类算法**识别并对相似的观察结果或特征进行分组，而不是识别新特征。算法在如何定义观察结果的相似性以及对结果组的假设方面存在差异。'
- en: These unsupervised algorithms are useful when a **dataset does not contain an
    outcome**. For instance, we may want to extract tradeable information from a large
    body of financial reports or news articles. In *Chapter 14*, *Text Data for Trading
    – Sentiment Analysis*, we'll use topic modeling to discover hidden themes that
    allow us to explore and summarize content more effectively, and identify meaningful
    relationships that can help us to derive signals.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个**数据集不包含结果**时，这些无监督算法非常有用。例如，我们可能希望从大量财务报告或新闻文章中提取可交易的信息。在*第14章*，*用于交易的文本数据
    - 情感分析*中，我们将使用主题建模来发现隐藏的主题，以更有效地探索和总结内容，并且识别有助于我们提取信号的有意义的关系。
- en: The algorithms are also useful when we want to **extract information independently
    from an outcome**. For example, rather than using third-party industry classifications,
    clustering allows us to identify synthetic groupings based on the attributes of
    assets useful for our purposes, such as returns over a certain time horizon, exposure
    to risk factors, or similar fundamentals. In this chapter, we will learn how to
    use clustering to manage portfolio risks by identifying hierarchical relationships
    among asset returns.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望**独立于结果**地提取信息时，这些算法也非常有用。例如，与使用第三方行业分类不同，聚类允许我们根据资产的属性（例如在一定时间范围内的回报、风险因子的暴露或类似的基本面）识别出对我们有用的合成分组。在本章中，我们将学习如何使用聚类来通过识别资产回报之间的分层关系来管理投资组合风险。
- en: 'More specifically, after reading this chapter, you will understand:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在阅读本章后，您将了解：
- en: How **principal component analysis** (**PCA**) and **independent component analysis**
    (**ICA**) perform linear dimensionality reduction
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过主成分分析（**PCA**）和独立成分分析（**ICA**）进行线性降维。
- en: Identifying data-driven risk factors and eigenportfolios from asset returns
    using PCA
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PCA从资产回报中识别基于数据的风险因子和特征组合。
- en: Effectively visualizing nonlinear, high-dimensional data using manifold learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流形学习有效地可视化非线性、高维数据。
- en: Using T-SNE and UMAP to explore high-dimensional image data
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用T-SNE和UMAP来探索高维图像数据。
- en: How k-means, hierarchical, and density-based clustering algorithms work
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means、层次和基于密度的聚类算法的工作原理。
- en: Using agglomerative clustering to build robust portfolios with hierarchical
    risk parity
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用凝聚式聚类构建具有分层风险平衡的强大投资组合。
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库的相应目录中找到本章的代码示例和附加资源链接。笔记本包括图像的彩色版本。
- en: Dimensionality reduction
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: In linear algebra terms, the features of a dataset create a **vector space**
    whose dimensionality corresponds to the number of linearly independent rows or
    columns, whichever is larger. Two columns are linearly dependent when they are
    perfectly correlated so that one can be computed from the other using the linear
    operations of addition and multiplication.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 用线性代数的术语来说，数据集的特征创建了一个**向量空间**，其维度对应于线性独立行或列的数量，取两者中较大的一个。 当两列完全相关时，它们是线性相关的，以至于一个可以使用加法和乘法的线性运算从另一个计算出来。
- en: In other words, they are parallel vectors that represent the same direction
    rather than different ones in the data and thus only constitute a single dimension.
    Similarly, if one variable is a linear combination of several others, then it
    is an element of the vector space created by those columns and does not add a
    new dimension of its own.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它们是代表相同方向而不是不同方向的平行向量，在数据中只构成一个维度。 同样，如果一个变量是几个其他变量的线性组合，那么它是由这些列创建的向量空间的一个元素，并且不会添加自己的新维度。
- en: 'The number of dimensions of a dataset matters because each new dimension can
    add a signal concerning an outcome. However, there is also a downside known as
    the **curse of dimensionality**: as the number of independent features grows while
    the number of observations remains constant, the average distance between data
    points also grows, and the density of the feature space drops exponentially, with
    dramatic implications for **machine learning** (**ML**). **Prediction becomes
    much harder** when observations are more distant, that is, different from each
    other. Alternative data sources, like text and images, typically are of high dimensionality,
    but they generally affect models that rely on a large number of features. The
    next section addresses the resulting challenges.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的维数数量很重要，因为每个新维度都可能添加有关结果的信号。 但是，还存在一个被称为**维数灾难**的负面效应：随着独立特征数量的增加，而观察数量保持不变，数据点之间的平均距离也增加，并且特征空间的密度呈指数级下降，这对**机器学习**（**ML**）有重大影响。
    当观察值之间距离更远时，即彼此不同，**预测变得更加困难**。 替代数据源，如文本和图像，通常具有很高的维度，但它们通常影响依赖大量特征的模型。 下一节将解决由此产生的挑战。
- en: Dimensionality reduction seeks to **represent the data more efficiently** by
    using fewer features. To this end, algorithms project the data to a lower-dimensional
    space while discarding any variation that is not informative, or by identifying
    a lower-dimensional subspace or manifold on or near to where the data lives.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 降维旨在通过使用更少的特征**更有效地表示数据**。 为此，算法将数据投影到低维空间，同时丢弃任何不具信息量的变化，或者通过识别数据所在位置附近的低维子空间或流形。
- en: A **manifold** is a space that locally resembles Euclidean space. One-dimensional
    manifolds include a line or a circle, but not the visual representation of the
    number eight due to the crossing point.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**流形**是一个在局部类似于欧几里得空间的空间。 一维流形包括线或圆，但不包括数字八的可视表示，因为没有交叉点。'
- en: The manifold hypothesis maintains that high-dimensional data often resides in
    a lower-dimensional space, which, if identified, permits a faithful representation
    of the data in this subspace. Refer to Fefferman, Mitter, and Narayanan (2016)
    for background information and the description of an algorithm that tests this
    hypothesis.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 流形假设认为高维数据通常驻留在较低维空间中，如果识别出，就可以在此子空间中忠实地表示数据。 有关背景信息和测试此假设的算法描述，请参阅Fefferman，Mitter和Narayanan（2016）。
- en: Dimensionality reduction, therefore, compresses the data by finding a different,
    smaller set of variables that capture what matters most in the original features
    to minimize the loss of information. Compression helps counter the curse of dimensionality,
    economizes on memory, and permits the visualization of salient aspects of higher-dimensional
    data that is otherwise very difficult to explore.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，降维通过找到一个不同的、更小的变量集合来捕捉原始特征中最重要的内容，以最小化信息损失。 压缩有助于对抗维数灾难，节省内存，并允许可视化原本非常难以探索的高维数据的显着方面。
- en: 'Dimensionality reduction algorithms differ by the constraints they impose on
    the new variables and how they aim to minimize the loss of information (see Burges
    2010 for an excellent overview):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 降维算法的不同之处在于它们对新变量施加的约束以及它们如何最小化信息损失（参见Burges 2010提供的出色概述）：
- en: '**Linear algorithms** like PCA and ICA constrain the new variables to be linear
    combinations of the original features; for example, hyperplanes in a lower-dimensional
    space. Whereas PCA requires the new features to be uncorrelated, ICA goes further
    and imposes statistical independence, implying the absence of both linear and
    nonlinear relationships.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像PCA和ICA这样的**线性算法**将新变量限制为原始特征的线性组合；例如，低维空间中的超平面。而PCA要求新特征无相关性，ICA进一步强调统计独立性，意味着没有线性和非线性关系。
- en: '**Nonlinear algorithms** are not restricted to hyperplanes and can capture
    a more complex structure in the data. However, given the infinite number of options,
    the algorithms still need to make assumptions in order to arrive at a solution.
    Later in this section, we will explain how **t-distributed Stochastic Neighbor
    Embedding** (**t-SNE**) and **Uniform Manifold Approximation and Projection**
    (**UMAP**) are very useful to visualize higher-dimensional data. *Figure 13.1*
    illustrates how manifold learning identifies a two-dimensional subspace in the
    three-dimensional feature space. (The notebook `manifold_learning` illustrates
    the use of additional algorithms, including local linear embedding.)![](img/B15439_13_01.png)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非线性算法**不受超平面限制，可以捕捉数据中更复杂的结构。然而，鉴于无限的选项，算法仍然需要做出假设才能得出解决方案。本节稍后，我们将解释**t-分布随机邻域嵌入**（**t-SNE**）和**均匀流形近似和投影**（**UMAP**）如何用于可视化更高维度的数据。*图
    13.1*说明了流形学习如何在三维特征空间中识别二维子空间。（笔记本`manifold_learning`说明了使用其他算法，包括局部线性嵌入。）![](img/B15439_13_01.png)'
- en: 'Figure 13.1: Nonlinear dimensionality reduction'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.1：非线性降维
- en: The curse of dimensionality
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度诅咒
- en: An increase in the number of dimensions of a dataset means that there are more
    entries in the vector of features that represents each observation in the corresponding
    Euclidean space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集维度的增加意味着在相应的欧几里得空间中，代表每个观察值的特征向量中有更多的条目。
- en: We measure the distance in a vector space using the Euclidean distance, also
    known as the L² norm, which we applied to the vector of linear regression coefficients
    to train a regularized ridge regression.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用欧几里得距离（也称为L²范数）在向量空间中测量距离，我们将其应用于线性回归系数向量以训练正则化的岭回归。
- en: 'The Euclidean distance between two *n*-dimensional vectors with Cartesian coordinates
    *p* = (*p*[1], *p*[2], ..., *p*[n]) and *q* = (*q*[1], *q*[2], ..., *q*[n]) is
    computed using the familiar formula developed by Pythagoras:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 两个具有笛卡尔坐标*p* = (*p*[1], *p*[2], ..., *p*[n])和*q* = (*q*[1], *q*[2], ..., *q*[n])的*n*维向量之间的欧几里得距离是使用毕达哥拉斯开发的熟悉公式计算的：
- en: '![](img/B15439_13_001.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_001.png)'
- en: Therefore, each new dimension adds a non-negative term to the sum so that the
    distance increases with the number of dimensions for distinct vectors. In other
    words, as the number of features grows for a given number of observations, the
    feature space becomes increasingly sparse, that is, less dense or emptier. On
    the flip side, the lower data density requires more observations to keep the average
    distance between the data points the same.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个新维度都会向总和中添加非负项，使得距离随着不同向量的维数增加而增加。换句话说，随着特征数量对于给定观察数的增长，特征空间变得越来越稀疏，即变得更少或更空。另一方面，较低的数据密度需要更多的观察来保持数据点之间的平均距离不变。
- en: '*Figure 13.2* illustrates the exponential growth in the number of data points
    needed to maintain the average distance among observations as the number of dimensions
    increases. 10 points uniformly distributed on a line correspond to 10² points
    in two dimensions and 10³ points in three dimensions in order to keep the density
    constant.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.2*说明了随着维度数量增加，为保持观察之间的平均距离所需的数据点数量呈指数增长。在线上均匀分布的10个点对应于二维中的10²个点和三维中的10³个点，以保持密度不变。'
- en: '![](img/B15439_13_02.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_02.png)'
- en: 'Figure 13.2: The number of features required to keep the average distance constant
    grows exponentially with the number of dimensions'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2：为保持平均距离恒定所需的特征数量随维度数量的指数增长
- en: The notebook `the_curse_of_dimensionality` in the GitHub repository folder for
    this section simulates how the average and minimum distances between data points
    increase as the number of dimensions grows (see *Figure 13.3*).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的GitHub存储库文件夹中的笔记本`the_curse_of_dimensionality`模拟了随着维度数量增长，数据点之间的平均距离和最小距离如何增加（见*图
    13.3*）。
- en: '![](img/B15439_13_03.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_03.png)'
- en: 'Figure 13.3: Average distance of 1,000 data points in a unit hypercube'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3：单位超立方体中1,000个数据点的平均距离
- en: The **simulation** randomly samples up to 2,500 features in the range [0, 1]
    from an uncorrelated uniform or a correlated normal distribution. The average
    distance between data points increases to over 11 times the unitary feature range
    for the normal distribution, and to over 20 times in the (extreme) case of an
    uncorrelated uniform distribution.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**模拟**随机从不相关均匀或相关正态分布中的[0, 1]范围内抽取高达2,500个特征。对于正态分布，数据点之间的平均距离增加到超过单位特征范围的11倍，对于（极端）不相关均匀分布，增加到超过20倍。'
- en: When the **distance between observations** grows, supervised ML becomes more
    difficult because predictions for new samples are less likely to be based on learning
    from similar training features. Put simply, the number of possible unique rows
    grows exponentially as the number of features increases, making it much harder
    to efficiently sample the space. Similarly, the complexity of the functions learned
    by flexible algorithms that make fewer assumptions about the actual relationship
    grows exponentially with the number of dimensions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当**观察之间的距离**增加时，监督式机器学习变得更加困难，因为对新样本的预测不太可能基于从类似训练特征中学习。简而言之，随着特征数量的增加，可能的唯一行数呈指数增长，使得有效地对空间进行抽样变得更加困难。同样，通过对实际关系做出较少假设的灵活算法学习的函数的复杂度随维度数量的增加呈指数增长。
- en: Flexible algorithms include the tree-based models we saw in *Chapter 11*, *Random
    Forests – A Long-Short Strategy for Japanese Stocks*, and *Chapter 12*, *Boosting
    Your Trading Strategy*. They also include the deep neural networks that we will
    cover later in the book, starting with *Chapter 16*, *Word Embeddings for Earnings
    Calls and SEC Filings*. The variance of these algorithms increases as **more dimensions
    add opportunities to overfit** to noise, resulting in poor generalization performance.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 灵活的算法包括我们在*第11章*看到的基于树的模型，*随机森林-一种日本股票的多空策略*，以及*第12章*，*提升您的交易策略*。它们还包括本书后面将介绍的深度神经网络，从*第16章*开始，*用于盈利电话和SEC提交的词嵌入*。随着**更多维度增加了过拟合的机会**，这些算法的方差增加，导致泛化性能不佳。
- en: Dimensionality reduction leverages the fact that, in practice, features are
    often correlated or exhibit little variation. If so, it can compress data without
    losing much of the signal and complements the use of regularization to manage
    prediction error due to variance and model complexity.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 降维利用了实践中特征通常相关或变化很小的事实。如果是这样，它可以在不损失信号太多的情况下压缩数据，并补充使用正则化来管理由方差和模型复杂性导致的预测误差。
- en: 'The critical question that we take on in the following section then becomes:
    what are the best ways to find a lower-dimensional representation of the data?'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随后我们将探讨的关键问题是：找到数据的低维表示的最佳方法是什么？
- en: Linear dimensionality reduction
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性降维
- en: Linear dimensionality reduction algorithms compute linear combinations that
    **translate**, **rotate**, and **rescale the original features** to capture significant
    variations in the data, subject to constraints on the characteristics of the new
    features.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 线性降维算法计算线性组合，**转换**，**旋转**和**重新缩放原始特征**，以捕获数据中的显着变化，同时受制于对新特征特性的约束。
- en: PCA, invented in 1901 by Karl Pearson, finds new features that reflect directions
    of maximal variance in the data while being mutually uncorrelated. ICA, in contrast,
    originated in signal processing in the 1980s with the goal of separating different
    signals while imposing the stronger constraint of statistical independence.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: PCA由Karl Pearson于1901年发明，它找到反映数据中最大方差方向的新特征，同时彼此不相关。相比之下，ICA起源于20世纪80年代的信号处理，其目标是在施加较强的统计独立性约束的同时分离不同的信号。
- en: This section introduces these two algorithms and then illustrates how to apply
    PCA to asset returns in order to learn risk factors from the data, and build so-called
    eigenportfolios for systematic trading strategies.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了这两种算法，然后说明了如何将PCA应用于资产回报，以从数据中学习风险因素，并构建所谓的特征组合以进行系统交易策略。
- en: Principal component analysis
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主成分分析
- en: PCA finds linear combinations of the existing features and uses these principal
    components to represent the original data. The number of components is a hyperparameter
    that determines the target dimensionality and can be, at most, equal to the lesser
    of the number of rows or columns.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 找到现有特征的线性组合，并使用这些主成分来表示原始数据。组件的数量是一个超参数，它决定了目标维度，并且最多可以等于行数或列数中较小的那个。
- en: PCA aims to capture most of the variance in the data to make it easy to recover
    the original features and ensures that each component adds information. It reduces
    dimensionality by projecting the original data into the principal component space.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 的目标是捕获数据中大部分的方差，以便容易地恢复原始特征，并确保每个组件都添加信息。它通过将原始数据投影到主成分空间来降低维度。
- en: The PCA algorithm works by identifying a sequence of components, each of which
    aligns with the direction of maximum variance in the data after accounting for
    variation captured by previously computed components. The sequential optimization
    ensures that new components are not correlated with existing components and produces
    an orthogonal basis for a vector space.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 算法通过识别一系列组件来工作，每个组件都与考虑了先前计算的组件捕捉的方差后数据中的最大方差的方向对齐。顺序优化确保新组件与现有组件不相关，并为向量空间生成正交基。
- en: This new basis is a rotation of the original basis, such that the new axes point
    in the direction of successively decreasing variance. The decline in the amount
    of variance of the original data explained by each principal component reflects
    the extent of correlation among the original features. In other words, the share
    of components that captures, for example, 95 percent of the original variation
    provides insight into the linearly independent information in the original data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新基是原始基的旋转，使得新轴指向逐渐减小的方差的方向。由每个主成分解释的原始数据方差量的下降反映了原始特征之间相关性的程度。换句话说，捕获例如原始变异的95％的组件的份额提供了有关原始数据中的线性独立信息的见解。
- en: Visualizing PCA in 2D
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在二维中可视化PCA
- en: '*Figure 13.4* illustrates several aspects of PCA for a two-dimensional random
    dataset (refer to the notebook `pca_key_ideas`):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.4*说明了用于二维随机数据集的PCA的几个方面（参考笔记本 `pca_key_ideas`）：'
- en: The left panel shows how the first and second principal components align with
    the **directions of maximum variance** while being orthogonal.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左侧面板显示了第一和第二主成分如何与**最大方差的方向**对齐并且正交。
- en: The central panel shows how the first principal component minimizes the **reconstruction
    error**, measured as the sum of the distances between the data points and the
    new axis.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中央面板显示了第一主成分如何将**重构误差**最小化，其衡量方式为数据点与新轴之间的距离之和。
- en: The right panel illustrates **supervised OLS** (refer to *Chapter 7*, *Linear
    Models – From Risk Factors to Return Forecasts* ), which approximates the outcome
    (*x*[2]) by a line computed from the single feature *x*[1]. The vertical lines
    highlight how OLS minimizes the distance along the outcome axis, whereas PCA minimizes
    the distances that are orthogonal to the hyperplane.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右侧面板说明了**有监督OLS**（参考*第7章*，*线性模型 - 从风险因素到回报预测*），它通过从单个特征*x*[1]计算的线来近似结果(*x*[2])。垂直线突出显示OLS最小化沿结果轴的距离，而PCA最小化与超平面正交的距离。
- en: '![](img/B15439_13_04.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_04.png)'
- en: 'Figure 13.4: PCA in 2D from various perspectives'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4：来自各个角度的二维PCA
- en: Key assumptions made by PCA
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PCA 的关键假设
- en: 'PCA makes several assumptions that are important to keep in mind. These include:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 做出了几个重要的假设，需要记住。其中包括：
- en: High variance implies a high signal-to-noise ratio.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高方差意味着高信噪比。
- en: The data is standardized so that the variance is comparable across features.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据经过标准化处理，使得各个特征的方差可比较。
- en: Linear transformations capture the relevant aspects of the data.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性变换捕捉了数据的相关方面。
- en: Higher-order statistics beyond the first and second moments do not matter, which
    implies that the data has a normal distribution.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一和第二阶的统计量之外的高阶统计量并不重要，这意味着数据具有正态分布。
- en: The emphasis on the first and second moments aligns with standard risk/return
    metrics, but the normality assumption may conflict with the characteristics of
    market data. Market data often exhibits skew or kurtosis (fat tails) that differ
    from those of the normal distribution and will not be taken into account by PCA.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对第一和第二时刻的强调与标准的风险/收益度量一致，但正态性假设可能与市场数据的特征相冲突。市场数据经常表现出与正态分布不同的偏斜或峰度（厚尾），PCA
    将不考虑这些特征。
- en: How the PCA algorithm works
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PCA 算法的工作原理
- en: The algorithm finds vectors to create a hyperplane of target dimensionality
    that minimizes the reconstruction error, measured as the sum of the squared distances
    of the data points to the plane. As illustrated previously, this goal corresponds
    to finding a sequence of vectors that align with directions of maximum retained
    variance given the other components, while ensuring all principal components are
    mutually orthogonal.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法找到向量来创建目标维度的超平面，以最小化重构误差，重构误差以数据点到平面的平方距离之和来衡量。如前所述，这个目标对应于找到一系列向量，这些向量与最大保留方差的方向相一致，给定其他分量，同时确保所有主成分互相正交。
- en: In practice, the algorithm solves the problem either by computing the eigenvectors
    of the covariance matrix or by using the **singular value decomposition** (**SVD**).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，该算法通过计算协方差矩阵的特征向量或使用 **奇异值分解**（**SVD**）来解决问题。
- en: We illustrate the computation using a randomly generated three-dimensional ellipse
    with 100 data points, as shown in the left panel of *Figure 13.5*, including the
    two-dimensional hyperplane defined by the first two principal components. (Refer
    to the notebook `the_math_behind_pca` for the code samples in the following three
    sections.)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个随机生成的具有 100 个数据点的三维椭圆来说明计算，如 *图 13.5* 的左面板所示，包括由前两个主成分定义的二维超平面。（参见笔记本
    `the_math_behind_pca`，下面三个部分的代码示例。）
- en: '![](img/B15439_13_05.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_05.png)'
- en: 'Figure 13.5: Visual representation of dimensionality reduction from 3D to 2D'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13.5: 从 3D 到 2D 的维度缩减的可视化表示'
- en: PCA based on the covariance matrix
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于协方差矩阵的 PCA
- en: 'We first compute the principal components using the square covariance matrix
    with the pairwise sample covariances for the features *x*[i], *x*[j], *i*, *j*
    = 1, ..., *n* as entries in row *i* and column *j*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用方阵协方差矩阵计算主成分，其中特征 *x*[i]、*x*[j] 的成对样本协方差作为第 *i* 行和第 *j* 列的条目：
- en: '![](img/B15439_13_002.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_002.png)'
- en: 'For a square matrix *M* of *n* dimension, we define the eigenvectors ![](img/B15439_13_003.png)
    and eigenvalues ![](img/B15439_13_004.png)[i], *i*=1, ..., *n* as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *n* 维度的方阵 *M*，我们将特征向量 ![](img/B15439_13_003.png) 和特征值 ![](img/B15439_13_004.png)[i]，*i*=1,
    ..., *n* 定义如下：
- en: '![](img/B15439_13_005.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_005.png)'
- en: 'Therefore, we can represent the matrix *M* using eigenvectors and eigenvalues,
    where *W* is a matrix that contains the eigenvectors as column vectors, and *L*
    is a matrix that contains ![](img/B15439_13_006.png)[i] as diagonal entries (and
    0s otherwise). We define the **eigendecomposition** as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用特征向量和特征值来表示矩阵 *M*，其中 *W* 是一个包含特征向量作为列向量的矩阵，*L* 是一个包含特征值 ![](img/B15439_13_006.png)[i]
    作为对角线条目（其他情况下为 0）的矩阵。我们将 **特征值分解** 定义为：
- en: '![](img/B15439_13_007.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_007.png)'
- en: 'Using NumPy, we implement this as follows, where the pandas DataFrame data
    contains the 100 data points of the ellipse:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NumPy，我们实现如下，其中 pandas DataFrame 数据包含椭圆的 100 个数据点：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we calculate the eigenvectors and eigenvalues of the covariance matrix.
    The eigenvectors contain the principal components (where the sign is arbitrary):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算协方差矩阵的特征向量和特征值。特征向量包含主成分（符号是任意的）：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can compare the result with the result obtained from sklearn and find that
    they match in absolute terms:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将结果与从 sklearn 获得的结果进行比较，并发现它们在绝对意义上匹配：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also **verify the eigendecomposition**, starting with the diagonal matrix
    *L* that contains the eigenvalues:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以 **验证特征值分解**，从包含特征值的对角矩阵 *L* 开始：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We find that the result does indeed hold:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现结果确实成立：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: PCA using the singular value decomposition
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用奇异值分解的 PCA
- en: Next, we'll take a look at the alternative computation using the SVD. This algorithm
    is slower when the number of observations is greater than the number of features
    (which is the typical case) but yields better **numerical stability**, especially
    when some of the features are strongly correlated (which is often the reason to
    use PCA in the first place).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看使用SVD进行的备用计算。当观测数量大于特征数量时（这是典型情况），此算法较慢，但在一些特征高度相关的情况下（这通常是使用PCA的原因）产生更好的**数值稳定性**。
- en: SVD generalizes the eigendecomposition that we just applied to the square and
    symmetric covariance matrix to the more general case of *m* x *n* rectangular
    matrices. It has the form shown at the center of the following figure. The diagonal
    values of ![](img/B15439_13_008.png) are the singular values, and the transpose
    of *V** contains the principal components as column vectors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: SVD将我们刚刚应用于方阵和对称协方差矩阵的特征分解推广到更一般的*m* x *n*矩形矩阵情况。它的形式如下图中心所示。![](img/B15439_13_008.png)的对角线值是奇异值，*V*的转置包含作为列向量的主成分。
- en: '![](img/B15439_13_06.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_06.png)'
- en: 'Figure 13.6: The SVD decomposed'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6：SVD分解
- en: 'In this case, we need to make sure our data is centered with mean zero (the
    computation of the covariance earlier took care of this):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要确保我们的数据以零均值为中心（之前的协方差计算已经处理了这个）：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Using the centered data, we compute the SVD:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用居中的数据，我们计算SVD：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can convert the vector `s`, which contains only singular values, into an
    *n* x *m* matrix and show that the decomposition works:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将仅包含奇异值的向量`s`转换为一个*n* x *m*矩阵，并展示分解的工作原理：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We find that the decomposition does indeed reproduce the standardized data:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现分解确实复制了标准化数据：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Lastly, we confirm that the columns of the transpose of *V** contain the principal
    components:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们确认*V*的转置的列包含主成分：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the next section, we will demonstrate how sklearn implements PCA.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将演示sklearn如何实现PCA。
- en: PCA with sklearn
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用sklearn进行PCA
- en: The `sklearn.decomposition.PCA` implementation follows the standard API based
    on the `fit()` and `transform()` methods that compute the desired number of principal
    components and project the data into the component space, respectively. The convenience
    method `fit_transform()` accomplishes this in a single step.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.decomposition.PCA`实现遵循基于`fit()`和`transform()`方法的标准API，分别计算所需数量的主成分并将数据投影到组件空间。方便的`fit_transform()`方法在一个步骤中完成此操作。'
- en: 'PCA offers three different algorithms that can be specified using the `svd_solver`
    parameter:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: PCA提供了三种不同的算法，可以使用`svd_solver`参数指定：
- en: '**full** computes the exact SVD using the LAPACK solver provided by scipy.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**full**使用由scipy提供的LAPACK求解器计算精确的SVD。'
- en: '**arpack** runs a truncated version suitable for computing less than the full
    number of components.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**arpack**运行适合计算不到完整数量组件的截断版本。'
- en: '**randomized** uses a sampling-based algorithm that is more efficient when
    the dataset has more than 500 observations and features, and the goal is to compute
    less than 80 percent of the components.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**randomized**使用基于抽样的算法，当数据集具有超过500个观测值和特征，并且目标是计算少于80%的组件时，它更有效率。'
- en: '**auto** also randomizes where it is most efficient; otherwise, it uses the
    full SVD.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**auto**也随机化到最有效的地方；否则，它使用完整的SVD。'
- en: Please view the references on GitHub for algorithmic implementation details.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请在GitHub上查看算法实现细节的参考资料。
- en: 'Other key configuration parameters of the PCA object are:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: PCA对象的其他关键配置参数是：
- en: '**n_components**: Compute all principal components by passing `None` (the default),
    or limit the number to `int`. For `svd_solver=full`, there are two additional
    options: a `float` in the interval [0, 1] computes the number of components required
    to retain the corresponding share of the variance in the data, and the option
    `mle` estimates the number of dimensions using the maximum likelihood.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_components**：通过传递`None`（默认值）来计算所有主成分，或将数量限制为`int`。对于`svd_solver=full`，还有两个额外选项：[0,
    1]区间内的`float`计算保留数据方差相应份额所需的组件数量，选项`mle`使用最大似然估计维度数量。'
- en: '**whiten**: If `True`, it standardizes the component vectors to unit variance,
    which, in some cases, can be useful in a predictive model (the default is `False`).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**whiten**：如果为`True`，则将组件向量标准化为单位方差，在某些情况下，这可能对预测模型有用（默认值为`False`）。'
- en: 'To compute the first two principal components of the three-dimensional ellipsis
    and project the data into the new space, use `fit_transform()`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算三维椭圆的前两个主成分并将数据投影到新空间中，请使用 `fit_transform()`：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The explained variance of the first two components is very close to 100 percent:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个成分的解释方差非常接近 100%：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Figure 13.5* shows the projection of the data into the new two-dimensional
    space.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.5* 显示了数据投影到新的二维空间中。'
- en: Independent component analysis
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独立成分分析
- en: ICA is another linear algorithm that identifies a new basis to represent the
    original data but pursues a different objective than PCA. Refer to Hyvärinen and
    Oja (2000) for a detailed introduction.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ICA 是另一个线性算法，它确定一个新的基来表示原始数据，但追求的目标与 PCA 不同。有关详细介绍，请参阅 Hyvärinen 和 Oja（2000）。
- en: ICA emerged in signal processing, and the problem it aims to solve is called
    **blind source separation**. It is typically framed as the cocktail party problem,
    where a given number of guests are speaking at the same time so that a single
    microphone records overlapping signals. ICA assumes there are as many different
    microphones as there are speakers, each placed at different locations so that
    they record a different mix of signals. ICA then aims to recover the individual
    signals from these different recordings.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ICA 出现在信号处理中，它旨在解决的问题被称为**盲源分离**。通常将其描述为鸡尾酒会问题，其中给定数量的客人同时发言，以至于单个麦克风记录重叠信号。ICA
    假设存在与说话者数量相同的不同麦克风，每个麦克风放置在不同的位置，以便它们记录不同的信号混合。然后，ICA 旨在从这些不同的记录中恢复单个信号。
- en: In other words, there are *n* original signals and an unknown square mixing
    matrix *A* that produces an *n*-dimensional set of *m* observations so that
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，有 *n* 个原始信号和一个未知的方阵混合矩阵 *A*，产生一个 *n* 维 *m* 观测值集合，使得
- en: '![](img/B15439_13_009.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_009.png)'
- en: The goal is to find the matrix *W* = *A*^(-1) that untangles the mixed signals
    to recover the sources.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是找到矩阵 *W* = *A*^(-1)，解开混合信号以恢复源。
- en: The ability to uniquely determine the matrix *W* hinges on the non-Gaussian
    distribution of the data. Otherwise, *W* could be rotated arbitrarily given the
    multivariate normal distribution's symmetry under rotation. Furthermore, ICA assumes
    the mixed signal is the sum of its components and is, therefore, unable to identify
    Gaussian components because their sum is also normally distributed.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一确定矩阵 *W* 的能力取决于数据的非高斯分布。否则，由于多变量正态分布在旋转下的对称性，*W* 可以任意旋转。此外，ICA 假设混合信号是其组成部分的和，因此无法识别高斯分量，因为它们的总和也是正态分布的。
- en: ICA assumptions
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ICA 假设
- en: 'ICA makes the following critical assumptions:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ICA 做出了以下关键假设：
- en: The sources of the signals are statistically independent
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号的源是统计独立的
- en: Linear transformations are sufficient to capture the relevant information
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性变换足以捕获相关信息
- en: The independent components do not have a normal distribution
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立成分不具有正态分布
- en: The mixing matrix *A* can be inverted
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合矩阵 *A* 是可以求逆的。
- en: ICA also requires the data to be centered and whitened, that is, to be mutually
    uncorrelated with unit variance. Preprocessing the data using PCA, as outlined
    earlier, achieves the required transformations.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ICA 还要求数据被居中和白化，即彼此不相关且具有单位方差。使用前述概述的 PCA 对数据进行预处理可以实现所需的转换。
- en: The ICA algorithm
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ICA 算法
- en: '`FastICA`, used by sklearn, is a fixed-point algorithm that uses higher-order
    statistics to recover the independent sources. In particular, it maximizes the
    distance to a normal distribution for each component as a proxy for independence.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`FastICA` 是 sklearn 中使用的一种固定点算法，它使用高阶统计量来恢复独立源。特别是，它将每个组件的距离最大化到正态分布，作为独立性的代理。'
- en: An alternative algorithm called `InfoMax` minimizes the mutual information between
    components as a measure of statistical independence.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一种称为 `InfoMax` 的替代算法将组件之间的互信息最小化，作为统计独立性的度量。
- en: ICA with sklearn
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用 sklearn 进行 ICA
- en: The ICA implementation by sklearn uses the same interface as PCA, so there is
    little to add. Note that there is no measure of explained variance because ICA
    does not compute components successively. Instead, each component aims to capture
    the independent aspects of the data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn 中的 ICA 实现使用与 PCA 相同的接口，因此几乎没有额外添加。请注意，没有解释方差的度量，因为 ICA 不会连续计算组件。相反，每个组件旨在捕获数据的独立方面。
- en: Manifold learning – nonlinear dimensionality reduction
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流形学习 – 非线性降维
- en: Linear dimensionality reduction projects the original data onto a lower-dimensional
    hyperplane that aligns with informative directions in the data. The focus on linear
    transformations simplifies the computation and echoes common financial metrics,
    such as PCA's goal to capture the maximum variance.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 线性降维将原始数据投影到一个与数据中信息方向对齐的较低维度超平面上。专注于线性变换简化了计算，并回应了常见的金融度量，例如PCA旨在捕获最大方差。
- en: However, linear approaches will naturally ignore signals reflected in nonlinear
    relationships in the data. Such relationships are very important in alternative
    datasets containing, for example, image or text data. Detecting such relationships
    during exploratory analysis can provide important clues about the data's potential
    signal content.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，线性方法自然会忽略数据中非线性关系反映的信号。这样的关系在包含图像或文本数据的替代数据集中非常重要。在探索性分析期间检测到这种关系可以提供有关数据潜在信号内容的重要线索。
- en: In contrast, the **manifold hypothesis** emphasizes that high-dimensional data
    often lies on or near a lower-dimensional nonlinear manifold that is embedded
    in the higher-dimensional space. The two-dimensional Swiss roll displayed in *Figure
    13.1* (at the beginning of this chapter) illustrates such a topological structure.
    Manifold learning aims to find the manifold of intrinsic dimensionality and then
    represent the data in this subspace. A simplified example uses a road as a one-dimensional
    manifold in a three-dimensional space and identifies data points using house numbers
    as local coordinates.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，**流形假设**强调高维数据通常位于或接近嵌入在高维空间中的较低维度非线性流形上。在本章开头显示的二维瑞士卷（*图13.1*）阐明了这样的拓扑结构。流形学旨在找到固有维度的流形，然后在该子空间中表示数据。一个简化的例子使用道路作为三维空间中的一维流形，并使用房屋编号作为局部坐标来识别数据点。
- en: Several techniques approximate a lower-dimensional manifold. One example is
    **locally linear embedding** (**LLE**), which was invented by Lawrence Saul and
    Sam Roweis (2000) and used to "unroll" the Swiss roll shown in *Figure 13.1* (view
    the examples in the `manifold_learning_lle` notebook).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 几种技术可以近似一个较低维度的流形。其中一个例子是**局部线性嵌入**（**LLE**），由劳伦斯·索尔和萨姆·罗维斯（2000年）发明，并用于“展开”在*图13.1*中显示的瑞士卷（查看`manifold_learning_lle`笔记本中的示例）。
- en: For each data point, LLE identifies a given number of nearest neighbors and
    computes weights that represent each point as a linear combination of its neighbors.
    It finds a lower-dimensional embedding by linearly projecting each neighborhood
    on global internal coordinates on the lower-dimensional manifold and can be thought
    of as a sequence of PCA applications.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个数据点，LLE识别给定数量的最近邻居，并计算代表每个点的线性组合的权重。它通过在较低维度流形上的全局内部坐标上线性投影每个邻域来找到一个较低维度的嵌入，并可以被看作是一系列PCA应用。
- en: 'Visualization requires that the reduction is at least three dimensions, possibly
    below the intrinsic dimensionality, and poses the **challenge of faithfully representing
    both the local and global structure**. This challenge relates to the curse of
    dimensionality; that is, while the volume of a sphere expands exponentially with
    the number of dimensions, the lower-dimensional space available to represent high-dimensional
    data is much more limited. For instance, in 12 dimensions, there can be 13 equidistant
    points; however, in two dimensions, there can only be 3 that form a triangle with
    sides of equal length. Therefore, accurately reflecting the distance of one point
    to its high-dimensional neighbors in lower dimensions risks distorting the relationships
    among all other points. The result is the **crowding problem**: to maintain global
    distances, local points may need to be placed too closely together.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化要求降维至少三个维度，可能低于固有维度，并提出了**忠实地表示局部和全局结构的挑战**。这个挑战与维度诅咒有关；也就是说，虽然球体的体积随着维度数量的增加呈指数级增长，但用于表示高维数据的低维空间要有限得多。例如，在12个维度中，可能有13个等距点；然而，在二维空间中，只能有3个形成边长相等的三角形。因此，在较低维度准确反映一个点到其高维邻居的距离会有可能扭曲所有其他点之间的关系。结果就是**拥挤问题**：为了保持全局距离，局部点可能需要被放置得太接近。
- en: The next two sections cover techniques that have allowed us to make progress
    in addressing the crowding problem for the visualization of complex datasets.
    We will use the fashion MNIST dataset, which is a more sophisticated alternative
    to the classic handwritten digit MNIST benchmark data used for computer vision.
    It contains 60,000 training and 10,000 test images of fashion objects in 10 classes
    (take a look at the sample images in the notebook `manifold_learning_intro`).
    The goal of a manifold learning algorithm for this data is to detect whether the classes
    lie on distinct manifolds to facilitate their recognition and differentiation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个部分涵盖了使我们在处理复杂数据集的可视化中取得进展的技术。我们将使用Fashion MNIST数据集，这是一个更复杂的选择，用于计算机视觉的经典手写数字MNIST基准数据。它包含60,000个训练图像和10,000个测试图像，分为10个类别（在笔记本
    `manifold_learning_intro` 中查看样本图像）。该数据的流形学习算法的目标是检测类别是否位于不同的流形上，以促进它们的识别和区分。
- en: t-distributed Stochastic Neighbor Embedding
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: t-分布随机近邻嵌入
- en: t-SNE is an award-winning algorithm, developed by Laurens van der Maaten and
    Geoff Hinton in 2008, to detect patterns in high-dimensional data. It takes a probabilistic,
    nonlinear approach to locate data on several different but related low-dimensional
    manifolds. The algorithm emphasizes keeping similar points together in low dimensions
    as opposed to maintaining the distance between points that are apart in high dimensions,
    which results from algorithms like PCA that minimize squared distances.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE是由Laurens van der Maaten和Geoff Hinton于2008年开发的获奖算法，用于检测高维数据中的模式。它采用概率、非线性的方法来定位数据在几个不同但相关的低维流形上。该算法强调将相似的点放在低维中放在一起，而不是像PCA这样的算法那样保持在高维中相距较远的点之间的距离最小化。
- en: The algorithm proceeds by **converting high-dimensional distances into (conditional)
    probabilities**, where high probabilities imply low distance and reflect the likelihood
    of sampling two points based on similarity. It accomplishes this by, first, positioning
    a normal distribution over each point and computing the density for a point and
    each neighbor, where the `perplexity` parameter controls the effective number
    of neighbors. In the second step, it arranges points in low dimensions and uses
    similarly computed low-dimensional probabilities to match the high-dimensional
    distribution. It measures the difference between the distributions using the Kullback-Leibler
    divergence, which puts a high penalty on misplacing similar points in low dimensions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通过 **将高维距离转换为（条件）概率** 来进行，其中高概率意味着低距离，并反映了基于相似性对两个点进行采样的可能性。首先，在每个点上定位一个正态分布，并计算点和每个邻居的密度，其中
    `perplexity` 参数控制有效邻居的数量。在第二步中，它将点排列在低维中，并使用类似计算的低维概率来匹配高维分布。它通过Kullback-Leibler散度来衡量分布之间的差异，这会对低维中的相似点放置高惩罚。
- en: The low-dimensional probabilities use a Student's t-distribution with one degree
    of freedom because it has fatter tails that reduce the penalty of misplacing points
    that are more distant in high dimensions to manage the crowding problem.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 低维概率使用一个自由度为1的学生t分布，因为它有更胖的尾部，减少了放置更远的高维点的惩罚，以管理拥挤问题。
- en: The upper panels in *Figure 13.7* show how t-SNE is able to differentiate between
    the FashionMNIST image classes. A higher perplexity value increases the number
    of neighbors used to compute the local structure and gradually results in more
    emphasis on global relationships. (Refer to the repository for a high-resolution
    color version of this figure.)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.7* 的上半部分显示了t-SNE如何区分FashionMNIST图像类别。更高的困惑度值增加了用于计算局部结构的邻居数，并逐渐强调全局关系。
    （参考存储库以获取此图的高分辨率彩色版本。）'
- en: '![](img/B15439_13_07.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_07.png)'
- en: 'Figure 13.7: t-SNE and UMAP visualization of Fashion MNIST image data for different
    hyperparameters'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7：Fashion MNIST图像数据的t-SNE和UMAP可视化，针对不同的超参数
- en: t-SNE is the current state of the art in high-dimensional data visualization.
    Weaknesses include the computational complexity that scales quadratically in the
    number *n* of points because it evaluates all pairwise distances, but a subsequent
    tree-based implementation has reduced the cost to *n* log *n*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE是目前高维数据可视化的最新技术。其缺点包括计算复杂度随着点数 *n* 呈二次增长，因为它评估所有成对距离，但随后基于树的实现将成本降低到 *n*
    log *n*。
- en: Unfortunately, t-SNE does not facilitate the projection of new data points into
    the low-dimensional space. The compressed output is not a very useful input for
    distance- or density-based cluster algorithms because t-SNE treats small and large
    distances differently.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，t-SNE 不利于将新数据点投影到低维空间。压缩的输出对于基于距离或密度的聚类算法不是非常有用，因为 t-SNE 对待小距离和大距离的方式不同。
- en: Uniform Manifold Approximation and Projection
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 统一流形近似和投影
- en: UMAP is a more recent algorithm for visualization and general dimensionality
    reduction. It assumes the data is uniformly distributed on a locally connected
    manifold and looks for the closest low-dimensional equivalent using fuzzy topology. It
    uses a `neighbors` parameter, which impacts the result in a similar way to `perplexity`
    in the preceding section.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP 是一种用于可视化和通用降维的较新算法。它假设数据在局部连接流形上均匀分布，并寻找最接近的低维等价物，使用模糊拓扑。它使用一个 `neighbors`
    参数，其影响结果与前面一节中的 `perplexity` 类似。
- en: It is faster and hence scales better to large datasets than t-SNE and sometimes
    preserves the global structure better than t-SNE. It can also work with different
    distance functions, including cosine similarity, which is used to measure the
    distance between word count vectors.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 它比 t-SNE 更快，因此更适用于大型数据集，并且有时比 t-SNE 更好地保留全局结构。它还可以使用不同的距离函数，包括用于测量单词计数向量之间距离的余弦相似度。
- en: The preceding figure illustrates how UMAP does indeed move the different clusters
    further apart, whereas t-SNE provides more granular insight into the local structure.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 上图说明了 UMAP 确实将不同的聚类进一步分开，而 t-SNE 则提供了更精细的局部结构洞察。
- en: The notebook also contains interactive Plotly visualizations for each of the
    algorithms that permit the exploration of the labels and identify which objects
    are placed close to each other.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本还包含交互式 Plotly 可视化，用于探索每个算法的标签，并确定哪些对象彼此靠近。
- en: PCA for trading
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于交易的 PCA
- en: 'PCA is useful for algorithmic trading in several respects, including:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 在算法交易中有几个方面的用处，包括：
- en: The data-driven derivation of risk factors by applying PCA to asset returns
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 PCA 应用于资产收益以数据驱动地推导风险因素
- en: The construction of uncorrelated portfolios based on the principal components
    of the correlation matrix of asset returns
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于资产收益相关系数矩阵的主成分构建不相关投资组合
- en: We will illustrate both of these applications in this section.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中说明这两个应用。
- en: Data-driven risk factors
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据驱动的风险因素
- en: In *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*, we
    explored **risk factor models** used in quantitative finance to capture the main
    drivers of returns. These models explain differences in returns on assets based
    on their exposure to systematic risk factors and the rewards associated with these
    factors. In particular, we explored the **Fama-French approach**, which specifies
    factors based on prior knowledge about the empirical behavior of average returns,
    treats these factors as observable, and then estimates risk model coefficients
    using linear regression.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第7章*中，*线性模型 - 从风险因素到收益预测*，我们探讨了量化金融中用于捕捉收益主要驱动因素的**风险因素模型**。这些模型根据资产暴露于系统性风险因素的程度以及与这些因素相关的回报来解释资产收益的差异。特别是，我们探讨了**法玛-法rench方法**，该方法根据关于平均收益的经验行为的先验知识指定因子，将这些因子视为可观察因子，然后使用线性回归估计风险模型系数。
- en: An alternative approach treats risk factors as **latent variables** and uses
    factor analytic techniques like PCA to simultaneously learn the factors from data
    and estimate how they drive returns. In this section, we will demonstrate how
    this method derives factors in a purely statistical or data-driven way with the
    advantage of not requiring ex ante knowledge of the behavior of asset returns
    (see the notebook `pca_and_risk_factor_models` for more details).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法将风险因素视为**潜在变量**，并使用因子分析技术如 PCA 同时从数据中学习因子并估计它们如何影响收益。在本节中，我们将演示这种方法如何以纯粹的统计或数据驱动方式推导因子，并具有不需要事先了解资产收益行为的优点（详见笔记本
    `pca_and_risk_factor_models` 了解更多详情）。
- en: Preparing the data – top 350 US stocks
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据 - 美国前 350 家股票
- en: 'We will use the Quandl stock price data and select the daily adjusted close
    prices of the 500 stocks with the largest market capitalization and data for the
    2010-2018 period. We will then compute the daily returns as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Quandl 股票价格数据，并选择市值最大的 500 支股票的每日调整收盘价和 2010 年至 2018 年期间的数据。然后，我们将计算每日收益如下：
- en: '[PRE12]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We obtain 351 stocks and returns for over 2,000 trading days:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了351只股票和超过2000个交易日的回报：
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'PCA is sensitive to outliers, so we winsorize the data at the 2.5 percent and
    97.5 percent quantiles, respectively:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: PCA对异常值敏感，因此我们分别在2.5％和97.5％的分位数上修剪数据：
- en: 'PCA does not permit missing data, so we will remove any stocks that do not
    have data for at least 95 percent of the time period. Then, in a second step,
    we will remove trading days that do not have observations on at least 95 percent
    of the remaining stocks:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: PCA不允许缺失数据，因此我们将删除任何在至少95％的时间段内没有数据的股票。然后，在第二步中，我们将删除在剩余股票中至少95％的交易日没有观察到的日子：
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We are left with 315 equity return series covering a similar period:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们留下了315个股票回报系列，覆盖了一个类似的时期：
- en: '[PRE15]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We impute any remaining missing values using the average return for any given
    trading day:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用给定交易日的平均回报来填补任何剩余的缺失值：
- en: '[PRE16]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Running PCA to identify the key return drivers
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行PCA以确定主要的回报驱动因素
- en: 'Now we are ready to fit the principal components model to the asset returns
    using default parameters to compute all of the components using the full SVD algorithm:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好使用默认参数将主成分模型拟合到资产收益上，使用全SVD算法来计算所有组件：
- en: '[PRE17]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We find that the most important factor explains around 55 percent of the daily
    return variation. The dominant factor is usually interpreted as "the market,"
    whereas the remaining factors can be interpreted as industry or style factors
    in line with our discussions in *Chapter 5*, *Portfolio Optimization and Performance
    Evaluation*, and *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*,
    depending on the results of a closer inspection (please refer to the next example).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现最重要的因素解释了大约55％的日回报变动。主导因素通常被解释为“市场”，而其余因素可以根据更密切的检查结果（请参阅下一个示例）被解释为行业或风格因素，与我们在*第5章*，*投资组合优化和绩效评估*和*第7章*，*线性模型—从风险因子到回报预测*中的讨论一致。
- en: The plot on the right of *Figure 13.8* shows the cumulative explained variance
    and indicates that around 10 factors explain 60 percent of the returns of this
    cross-section of stocks.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.8*右侧的图显示了累积解释方差，并指出大约有10个因子解释了这个股票横截面收益的60％。'
- en: '![](img/B15439_13_08.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_08.png)'
- en: 'Figure 13.8: (Cumulative) explained return variance by PCA-based risk factors'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8：基于PCA的风险因子解释回报方差（累积）
- en: 'The notebook contains a **simulation** for a broader cross-section of stocks
    and the longer 2000-2018 time period. It finds that, on average, the first three
    components explained 40 percent, 10 percent, and 5 percent of 500 randomly selected
    stocks, as shown in *Figure 13.9*:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本包含了对更广泛的股票横截面和更长的2000-2018年时间段的**模拟**。结果发现，平均而言，前三个组件解释了500只随机选择的股票的40％、10％和5％，如*图13.9*所示：
- en: '![](img/B15439_13_09.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_09.png)'
- en: 'Figure 13.9: Explained variance of the top 10 principal components—100 trials'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9：前10个主要组件的解释方差—100次试验
- en: The cumulative plot shows a typical "elbow" pattern that can help to identify
    a suitable target dimensionality as the number of components beyond which additional
    components add less incremental value.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 累积图显示了一种典型的“肘部”模式，可以帮助确定一个合适的目标维度，即超过该维度的组件所增加的价值较少。
- en: 'We can select the top two principal components to verify that they are indeed
    uncorrelated:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择前两个主成分来验证它们确实是不相关的：
- en: '[PRE18]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Moreover, we can plot the time series to highlight how each factor captures
    different volatility patterns, as shown in the following figure:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以绘制时间序列以突出每个因子捕捉不同波动性模式的情况，如下图所示：
- en: '![](img/B15439_13_10.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_10.png)'
- en: 'Figure 13.10: Return volatility patterns captured by the first two principal
    components'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10：第一个和第二个主成分捕获的回报波动模式
- en: A risk factor model would employ a subset of the principal components as features
    to predict future returns, similar to our approach in *Chapter 7*, *Linear Models
    – From Risk Factors to Return Forecasts*.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 风险因子模型将采用主成分的子集作为特征来预测未来的回报，类似于我们在*第7章*，*线性模型—从风险因子到回报预测*中的方法。
- en: Eigenportfolios
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征组合
- en: Another application of PCA involves the covariance matrix of the normalized
    returns. The principal components of the correlation matrix capture most of the
    covariation among assets in descending order and are mutually uncorrelated. Moreover,
    we can use standardized principal components as portfolio weights. You can find
    the code example for this section in the notebook `pca_and_eigen_portfolios`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 的另一个应用涉及标准化回报的协方差矩阵。相关矩阵的主成分按降序捕捉大部分资产之间的协变化，并且彼此不相关。此外，我们可以将标准化主成分用作投资组合权重。你可以在笔记本
    `pca_and_eigen_portfolios` 中找到此部分的代码示例。
- en: 'Let''s use the 30 largest stocks with data for the 2010-2018 period to facilitate
    the exposition:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 2010-2018 年间有数据的 30 家最大的股票来简化阐述：
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We again winsorize and also normalize the returns:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次对回报进行截尾并进行标准化处理：
- en: '[PRE20]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After dropping assets and trading days like in the previous example, we are
    left with 23 assets and over 2,000 trading days. We compute the return covariance
    and estimate all of the principal components to find that the two largest explain
    55.9 percent and 15.5 percent of the covariation, respectively:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在像上一个示例中一样剔除资产和交易日后，我们剩下了 23 个资产和超过 2000 个交易日。我们计算回报协方差并估计所有主成分，发现前两个分别解释了 55.9%
    和 15.5% 的协变化：
- en: '[PRE21]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we select and normalize the four largest components so that they sum
    to 1, and we can use them as weights for portfolios that we can compare to an
    EW portfolio formed from all of the stocks:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择并标准化四个最大的成分，使它们总和为 1，然后我们可以将它们用作投资组合的权重，以便与由所有股票组成的等权投资组合进行比较：
- en: '[PRE22]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The weights show distinct emphasis, as you can see in *Figure 13.11*. For example,
    Portfolio 3 puts large weights on Mastercard and Visa, the two payment processors
    in the sample, whereas Portfolio 2 has more exposure to technology companies:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 权重显示出明显的强调，如 *图 13.11* 所示。例如，投资组合 3 对样本中的两个支付处理器 Mastercard 和 Visa 有较大的权重，而投资组合
    2 对技术公司有更多的暴露：
- en: '![](img/B15439_13_11.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_11.png)'
- en: 'Figure 13.11: Eigenportfolio weights'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13.11: 主成分投资组合权重'
- en: When comparing the performance of each portfolio over the sample period to "the
    market" consisting of our small sample, we find that Portfolio 1 performs very
    similarly, whereas the other portfolios capture different return patterns (see
    *Figure 13.12*).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较样本期内每个投资组合的表现与由我们的小样本组成的“市场”时，我们发现投资组合 1 的表现非常相似，而其他投资组合捕捉到不同的回报模式（见 *图 13.12*）。
- en: '![](img/B15439_13_12.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_12.png)'
- en: 'Figure 13.12: Cumulative eigenportfolio returns'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13.12: 累积主成分投资组合回报'
- en: Clustering
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: Both clustering and dimensionality reduction summarize the data. As we have
    just discussed, dimensionality reduction compresses the data by representing it
    using new, fewer features that capture the most relevant information. Clustering
    algorithms, in contrast, assign existing observations to subgroups that consist
    of similar data points.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类和降维都对数据进行总结。正如我们刚刚讨论的，降维通过使用新的、更少的特征来表示数据，从而压缩数据，以捕捉最相关的信息。相比之下，聚类算法将现有观察结果分配给由相似数据点组成的子组。
- en: Clustering can serve to better understand the data through the lens of categories
    learned from continuous variables. It also permits you to automatically categorize
    new objects according to the learned criteria. Examples of related applications
    include hierarchical taxonomies, medical diagnostics, and customer segmentation.
    Alternatively, clusters can be used to represent groups as prototypes, using,
    for example, the midpoint of a cluster as the best representatives of learned
    grouping. An example application includes image compression.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可以通过学习连续变量得到的类别视角更好地理解数据。它还允许您根据学习到的标准自动对新对象进行分类。相关应用的示例包括层次分类、医学诊断和客户分割。或者，可以使用聚类来表示群体作为原型，例如使用聚类的中点作为学习群体的最佳代表。一个示例应用是图像压缩。
- en: 'Clustering algorithms differ with respect to their strategy of identifying
    groupings:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法在识别分组的策略方面存在差异：
- en: '**Combinatorial** algorithms select the most coherent of different groupings
    of observations.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组合** 算法选择不同观察结果的最一致的分组。'
- en: '**Probabilistic** modeling estimates distributions that most likely generated
    the clusters.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率** 建模估计最可能生成聚类的分布。'
- en: '**Hierarchical clustering** finds a sequence of nested clusters that optimizes
    coherence at any given stage.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次聚类** 找到一系列嵌套的聚类，优化任何给定阶段的一致性。'
- en: 'Algorithms also differ by the notion of what constitutes a useful collection
    of objects that needs to match the data characteristics, domain, and goal of the
    applications. Types of groupings include:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 算法还通过何为需要匹配数据特征、领域和应用目标的有用对象的概念而有所不同。 分组类型包括：
- en: Clearly separated groups of various shapes
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确分离的各种形状的组
- en: Prototype- or center-based, compact clusters
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原型或基于中心的紧凑聚类
- en: Density-based clusters of arbitrary shape
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任意形状的基于密度的聚类
- en: Connectivity- or graph-based clusters
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连通性或基于图的聚类
- en: 'Important additional aspects of a clustering algorithm include whether it:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法的其他重要方面包括：
- en: Requires exclusive cluster membership
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要独占式聚类成员资格
- en: Makes hard, that is, binary, or soft, probabilistic assignments
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行硬的，即二进制的，或软的，概率的分配
- en: Is complete and assigns all data points to clusters
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是完整的，并将所有数据点分配到聚类中
- en: The following sections introduce key algorithms, including **k-means**, **hierarchical**,
    and **density-based clustering**, as well as **Gaussian mixture models** (**GMMs**).
    The notebook `clustering_algos` compares the performance of these algorithms on
    different, labeled datasets to highlight strengths and weaknesses. It uses mutual
    information (refer to *Chapter 6*, *The Machine Learning Process*) to measure
    the congruence of cluster assignments and labels.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节介绍了关键算法，包括**k-means**、**层次**和**基于密度的聚类**，以及**高斯混合模型**（**GMMs**）。 笔记本 `clustering_algos`
    比较了这些算法在不同的标记数据集上的性能，以突出它们的优缺点。 它使用互信息（参见*第 6 章*，*机器学习过程*）来衡量聚类分配与标签的一致性。
- en: k-means clustering
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means 聚类
- en: k-means is the most well-known clustering algorithm, and it was first proposed
    by Stuart Lloyd at Bell Labs in 1957\. It finds *k* centroids and assigns each
    data point to exactly one cluster with the goal of minimizing the within-cluster
    variance (called *inertia*). It typically uses the Euclidean distance, but other
    metrics can also be used. k-means assumes that clusters are spherical and of equal
    size and ignores the covariance among features.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 是最知名的聚类算法，最早由 1957 年贝尔实验室的 Stuart Lloyd 提出。 它找到 *k* 个质心，并将每个数据点分配到恰好一个聚类中，目标是最小化簇内方差（称为
    *惯性*）。 它通常使用欧几里得距离，但也可以使用其他度量标准。 k-means 假设聚类是球形且大小相等，并忽略特征之间的协方差。
- en: Assigning observations to clusters
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将观察分配到聚类
- en: 'The problem is computationally difficult (NP-hard) because there are *k*^N
    ways to partition the *N* observations into *k* clusters. The standard iterative
    algorithm delivers a local optimum for a given *k* and proceeds as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 该问题在计算上是困难的（NP-hard），因为有 *k*^N 种方法将 *N* 个观测分成 *k* 个聚类。 标准的迭代算法对于给定的 *k* 提供了局部最优解，并按照以下步骤进行：
- en: Randomly define *k* cluster centers and assign points to the nearest centroid
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机定义 *k* 个聚类中心并将点分配给最近的质心
- en: 'Repeat:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复：
- en: For each cluster, compute the centroid as the average of the features
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个聚类，将特征的平均值计算为质心
- en: Assign each observation to the closest centroid
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个观察分配给最近的质心
- en: 'Convergence: assignments (or within-cluster variation) don''t change'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收敛：分配（或簇内变异）不发生变化
- en: The notebook `kmeans_implementation` shows you how to code the algorithm using
    Python. It visualizes the algorithm's iterative optimization and demonstrates
    how the resulting centroids partition the feature space into areas called Voronoi
    that delineate the clusters. The result is optimal for the given initialization,
    but alternative starting positions will produce different results. Therefore,
    we compute multiple clusterings from different initial values and select the solution
    that minimizes within-cluster variance.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本 `kmeans_implementation` 展示了如何使用 Python 编写该算法的代码。 它可视化了算法的迭代优化，并演示了结果质心如何将特征空间划分为称为
    Voronoi 的区域，这些区域勾勒出了簇。 对于给定的初始化，结果是最优的，但是不同的起始位置将产生不同的结果。 因此，我们从不同的初始值计算多个聚类，并选择最小化簇内方差的解决方案。
- en: k-means requires continuous or one-hot encoded categorical variables. Distance
    metrics are typically sensitive to scale, making it necessary to standardize features
    to ensure they have equal weight.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 需要连续或独热编码的分类变量。 距离度量通常对规模敏感，因此需要对特征进行标准化以确保它们具有相同的权重。
- en: The **strengths** of k-means include its wide range of applicability, fast convergence,
    and linear scalability to large data while producing clusters of even size. The
    **weaknesses** include the need to tune the hyperparameter *k*, no guarantee of
    finding a global optimum, the restrictive assumption that clusters are spheres,
    and features not being correlated. It is also sensitive to outliers.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: k-means的**优点**包括其广泛的适用性，快速收敛，对大数据的线性可伸缩性以及生成大小均匀的聚类。**缺点**包括需要调整超参数*k*，不能保证找到全局最优解，限制性假设聚类为球形，特征不相关。它还对离群值敏感。
- en: Evaluating cluster quality
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估聚类质量
- en: Cluster quality metrics help select from among alternative clustering results.
    The notebook `kmeans_evaluation` illustrates the following options.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类质量度量有助于从多个聚类结果中选择。笔记本`kmeans_evaluation`说明了以下选项。
- en: The **k-means objective** function suggests we compare the evolution of the
    inertia or within-cluster variance. Initially, additional centroids decrease the
    inertia sharply because new clusters improve the overall fit. Once an appropriate
    number of clusters has been found (assuming it exists), new centroids reduce the
    **within-cluster variance** by much less, as they tend to split natural groupings.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**k-means目标**函数建议我们比较惯性或聚类内方差的演变。最初，额外的质心会急剧降低惯性，因为新的聚类改善了整体拟合。一旦找到适当数量的聚类（假设存在），新的质心减少了聚类内方差，因为它们倾向于分割自然的分组。'
- en: Therefore, when k-means finds a good cluster representation of the data, the
    **inertia** tends to follow an elbow-shaped path similar to the explained variance
    ratio for PCA (take a look at the notebook for implementation details).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当k-means找到数据的良好聚类表示时，**惯性**往往会呈现类似于PCA的解释方差比的拐点形状（查看笔记本以获取实现细节）。
- en: 'The **silhouette coefficient** provides a more detailed picture of cluster
    quality. It answers the question: how far are the points in the nearest cluster
    relative to the points in the assigned cluster? To this end, it compares the mean
    intra-cluster distance *a* to the mean distance of the nearest cluster *b* and
    computes the following score *s*:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**轮廓系数**提供了聚类质量的更详细的图景。它回答了一个问题：最近聚类中的点相对于分配的聚类中的点有多远？为此，它比较了平均类内距离*a*与最近聚类的平均距离*b*，并计算了以下分数*s*：'
- en: '![](img/B15439_13_010.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_010.png)'
- en: The score can vary between -1 and 1, but negative values are unlikely in practice
    because they imply that the majority of points are assigned to the wrong cluster.
    A useful visualization of the silhouette score compares the values for each data
    point to the global average because it highlights the coherence of each cluster
    relative to the global configuration. The rule of thumb is to avoid clusters with
    mean scores below the average for all samples.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 分数可以在-1和1之间变化，但在实践中不太可能出现负值，因为它们意味着大多数点被分配到错误的聚类中。轮廓分数的一个有用的可视化将每个数据点的值与全局平均值进行比较，因为它突显了每个聚类相对于全局配置的一致性。经验法则是要避免平均分数低于所有样本的平均值的聚类。
- en: '*Figure 13.13* shows an excerpt from the silhouette plot for three and four
    clusters, where the former highlights the poor fit of cluster 1 by subpar contributions
    to the global silhouette score, whereas all of the four clusters have some values
    that exhibit above-average scores.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.13*显示了三个和四个聚类的轮廓图节选，前者突出了通过对全局轮廓分数的不足贡献来强调聚类 1 的拟合不佳，而所有四个聚类都具有一些值，这些值显示出高于平均分数的分数。'
- en: '![](img/B15439_13_13.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_13.png)'
- en: 'Figure 13.13: Silhouette plots for three and four clusters'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13：三个和四个聚类的轮廓图
- en: In sum, given the usually unsupervised nature, it is necessary to vary the hyperparameters
    of the cluster algorithms and evaluate the different results. It is also important
    to calibrate the scale of the features, particularly when some should be given
    a higher weight and thus be measured on a larger scale. Finally, to validate the
    robustness of the results, use subsets of data to identify whether particular
    cluster patterns emerge consistently.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，鉴于通常是无监督的性质，有必要改变聚类算法的超参数并评估不同的结果。还重要的是校准特征的比例，特别是当一些特征应该被赋予更高的权重并因此以较大的比例进行测量时。最后，为了验证结果的稳健性，使用数据子集来确定是否会一致出现特定的聚类模式。
- en: Hierarchical clustering
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分层聚类
- en: Hierarchical clustering avoids the need to specify a target number of clusters
    because it assumes that data can successively be merged into increasingly dissimilar
    clusters. It does not pursue a global objective but decides incrementally how
    to produce a sequence of nested clusters that range from a single cluster to clusters
    consisting of the individual data points.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 分层聚类避免了需要指定目标聚类数的需要，因为它假设数据可以被逐步合并为越来越不相似的聚类。它不追求全局目标，而是逐步决定如何产生一系列从单个聚类到由个别数据点组成的聚类的嵌套聚类。
- en: Different strategies and dissimilarity measures
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不同的策略和不相似度度量标准
- en: 'There are two approaches to hierarchical clustering:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种分层聚类方法：
- en: '**Agglomerative clustering** proceeds bottom-up, sequentially merging two of
    the remaining groups based on similarity.'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**聚合聚类** 自下而上进行，基于相似性顺序合并剩余的两个组。'
- en: '**Divisive clustering** works top-down and sequentially splits the remaining
    clusters to produce the most distinct subgroups.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分裂聚类** 自顶向下工作，顺序地分裂剩余的聚类，以产生最不同的子组。'
- en: Both groups produce *N*-1 hierarchical levels and facilitate the selection of
    clustering at the level that best partitions data into homogenous groups. We will
    focus on the more common agglomerative clustering approach.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 两个组都产生*N*-1个层次聚类，并有助于在最佳将数据分区为同质组的级别上选择聚类。我们将重点放在更常见的聚合聚类方法上。
- en: The agglomerative clustering algorithm departs from the individual data points
    and computes a similarity matrix containing all mutual distances. It then takes
    *N*-1 steps until there are no more distinct clusters and, each time, updates
    the similarity matrix to substitute elements that have been merged by the new
    cluster so that the matrix progressively shrinks.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合聚类算法不从个别数据点出发，而是计算一个包含所有相互距离的相似度矩阵。然后，它进行*N*-1步，直到没有更多的不同聚类，并且每次都更新相似度矩阵以替换被新聚类合并的元素，使矩阵逐渐缩小。
- en: 'While hierarchical clustering does not have hyperparameters like k-means, the
    **measure of dissimilarity** between clusters (as opposed to individual data points)
    has an important impact on the clustering result. The options differ as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分层聚类没有像k-means那样的超参数，但是聚类之间（而不是个别数据点之间）的不相似度度量对聚类结果有重要影响。选项有以下不同：
- en: '**Single-link**: Distance between the nearest neighbors of two clusters'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单连接法**：两个聚类的最近邻之间的距离'
- en: '**Complete link**: Maximum distance between the respective cluster members'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全连接法**：各自聚类成员之间的最大距离'
- en: '**Ward''s method**: Minimize within-cluster variance'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**瓦德法**：最小化簇内方差'
- en: '**Group average**: Uses the cluster midpoint as a reference distance'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组平均法**：使用聚类中点作为参考距离'
- en: Visualization – dendrograms
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化 - 树状图
- en: Hierarchical clustering provides insight into degrees of similarity among observations
    as it continues to merge data. A significant change in the similarity metric from
    one merge to the next suggests that a natural clustering existed prior to this
    point.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 分层聚类提供了关于观察值之间相似程度的见解，因为它继续合并数据。从一次合并到下一次合并的相似度度量的显着变化表明在此点之前存在自然的聚类。
- en: The **dendrogram** visualizes the successive merges as a binary tree, displaying
    the individual data points as leaves and the final merge as the root of the tree.
    It also shows how the similarity monotonically decreases from the bottom to the
    top. Therefore, it is natural to select a clustering by cutting the dendrogram.
    Refer to the notebook `hierarchical_clustering` for implementation details.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**树状图** 将连续的合并可视化为一棵二叉树，将个别数据点显示为叶子，并将最终合并显示为树的根。它还显示了相似度如何从底部向顶部单调递减。因此，通过切割树状图来选择聚类是很自然的。有关实现详细信息，请参阅笔记本`hierarchical_clustering`。'
- en: '*Figure 13.14* illustrates the dendrogram for the classic Iris dataset with
    four classes and three features using the four different distance metrics introduced
    in the preceding section. It evaluates the fit of the hierarchical clustering
    using the **cophenetic correlation** coefficient that compares the pairwise distances
    among points and the cluster similarity metric at which a pairwise merge occurred.
    A coefficient of 1 implies that closer points always merge earlier.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.14* 展示了经典的鸢尾花数据集的树状图，其中有四类和三个特征，使用了前面部分介绍的四种不同的距离度量标准。它评估了分层聚类的拟合程度，使用了**科菲尼系数**，该系数比较了点之间的成对距离和聚类相似度指标，该指标显示了成对合并发生的聚类相似度度量。系数为1意味着更接近的点总是较早合并。'
- en: '![](img/B15439_13_14.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_14.png)'
- en: 'Figure 13.14: Dendrograms and cophenetic correlation for different dissimilarity
    measures'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.14：不同不相似度度量的树状图和共辐相关性
- en: Different linkage methods produce different dendrogram "looks" so that we cannot
    use this visualization to compare results across methods. In addition, the Ward
    method, which minimizes the within-cluster variance, may not properly reflect
    the change in variance from one level to the next. Instead, the dendrogram can
    reflect the total within-cluster variance at different levels, which may be misleading.
    Alternative quality metrics are more appropriate, such as the **cophenetic correlation** or
    measures like **inertia** if aligned with the overall goal.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的链接方法产生不同的树状图“外观”，因此我们无法使用此可视化来跨方法比较结果。此外，最小化簇内方差的Ward方法可能不能正确反映方差从一个级别到下一个级别的变化。相反，树状图可以反映不同级别的总簇内方差，这可能会产生误导。与整体目标一致的替代质量度量更为适当，例如**共辐相关性**或与总体目标一致的度量标准，如**惯性**。
- en: 'The **strengths** of hierarchical clustering include:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 分层聚类的**优势**包括：
- en: The algorithm does not need the specific number of clusters but, instead, provides
    insight about potential clustering by means of an intuitive visualization.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法不需要特定数量的聚类，而是通过直观的可视化提供了关于潜在聚类的见解。
- en: It produces a hierarchy of clusters that can serve as a taxonomy.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它生成一系列可用作分类法的聚类层次结构。
- en: It can be combined with k-means to reduce the number of items at the start of
    the agglomerative process.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以与k均值结合以减少聚合过程开始时的项目数量。
- en: 'On the other hand, its **weaknesses** include:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，它的**缺点**包括：
- en: The high cost in terms of computation and memory due to the numerous similarity
    matrix updates.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于大量相似性矩阵更新而产生的计算和内存成本高昂。
- en: All merges are final so that it does not achieve the global optimum.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有合并都是最终的，因此它无法达到全局最优。
- en: The curse of dimensionality leads to difficulties with noisy, high-dimensional
    data.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度诅咒导致对嘈杂的高维数据困难重重。
- en: Density-based clustering
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于密度的聚类
- en: Density-based clustering algorithms assign cluster membership based on proximity
    to other cluster members. They pursue the goal of identifying dense regions of
    arbitrary shapes and sizes. They do not require the specification of a certain
    number of clusters but instead rely on parameters that define the size of a neighborhood
    and a density threshold.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的聚类算法根据与其他聚类成员的接近程度分配聚类成员资格。它们追求识别任意形状和大小的密集区域的目标。它们不需要指定一定数量的聚类，而是依赖于定义邻域大小和密度阈值的参数。
- en: 'We''ll outline the two popular algorithms: DBSCAN and its newer hierarchical
    refinement. Refer to the notebook `density_based_clustering` for the relevant
    code samples and the link in this chapter''s `README` on GitHub to a Quantopian
    example by Jonathan Larking that uses DBSCAN for a pairs trading strategy.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将概述两种流行的算法：DBSCAN及其较新的分层精化。请参考笔记本`density_based_clustering`中的相关代码示例以及本章的GitHub上的`README`链接，以了解Jonathan
    Larking使用DBSCAN进行配对交易策略的Quantopian示例。
- en: DBSCAN
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBSCAN
- en: '**Density-based spatial clustering of applications with noise** (**DBSCAN**)
    was developed in 1996 and awarded the KDD Test of Time award at the 2014 KDD conference
    because of the attention it has received in theory and practice.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于密度的带噪声空间聚类**（**DBSCAN**）于1996年开发，并因其在理论和实践中所受到的关注，在2014年的KDD会议上获得了KDD时代测试奖。'
- en: It aims to identify core and non-core samples, where the former extend a cluster
    and the latter are part of a cluster but do not have sufficient nearby neighbors
    to further grow the cluster. Other samples are outliers and are not assigned to
    any cluster.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 它旨在识别核心和非核心样本，其中前者扩展了一个聚类，而后者是聚类的一部分，但没有足够的附近邻居来进一步扩展聚类。其他样本是异常值，并且不分配给任何聚类。
- en: It uses a parameter `eps` for the radius of the neighborhood and `min_samples`
    for the number of members required for core samples. It is deterministic and exclusive
    and has difficulties with clusters of different density and high-dimensional data.
    It can be challenging to tune the parameters to the requisite density, especially
    as it is often not constant.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用参数`eps`来表示邻域的半径和`min_samples`来表示需要的核心样本数量。它是确定性的和独占的，并且在具有不同密度和高维数据的情况下存在困难。调整参数以满足必要密度可能是具有挑战性的，特别是因为密度通常不是恒定的。
- en: Hierarchical DBSCAN
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分层DBSCAN
- en: '**Hierarchical DBSCAN** (**HDBSCAN**) is a more recent development that assumes
    clusters are islands of potentially differing density to overcome the DBSCAN challenges
    just mentioned. It also aims to identify the core and non-core samples. It uses
    the parameters `min_cluster_size` and `min_samples` to select a neighborhood and
    extend a cluster. The algorithm iterates over multiple `eps` values and chooses
    the most stable clustering. In addition to identifying clusters of varying density,
    it provides insight into the density and hierarchical structure of the data.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**层次DBSCAN**（**HDBSCAN**）是更近期的发展，它假设聚类是潜在密度不同的岛屿，以克服刚提到的DBSCAN的挑战。它还旨在识别核心和非核心样本。它使用参数`min_cluster_size`和`min_samples`选择邻域并扩展聚类。该算法迭代多个`eps`值并选择最稳定的聚类。除了识别密度不同的聚类外，它还提供了有关数据密度和层次结构的洞察。'
- en: '*Figure 13.15* shows how DBSCAN and HDBSCAN, respectively, are able to identify
    clusters that differ in shape significantly from those discovered by k-means,
    for example. The selection of the clustering algorithm is a function of the structure
    of your data; refer to the pairs trading strategy that was referenced earlier
    in this section for a practical example.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.15*显示了DBSCAN和HDBSCAN分别如何能够识别形状与k均值发现的聚类显著不同的簇。聚类算法的选择取决于数据的结构；请参考本节早期提到的配对交易策略以获得一个实际示例。'
- en: '![](img/B15439_13_15.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_15.png)'
- en: 'Figure 13.15: Comparing the DBSCAN and HDBSCAN clustering algorithms'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.15：比较DBSCAN和HDBSCAN聚类算法
- en: Gaussian mixture models
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: GMMs are generative models that assume the data has been generated by a mix
    of various multivariate normal distributions. The algorithm aims to estimate the
    mean and covariance matrices of these distributions.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: GMM是假设数据由各种多元正态分布的混合生成的生成模型。该算法旨在估计这些分布的均值和协方差矩阵。
- en: 'A GMM generalizes the k-means algorithm: it adds covariance among features
    so that clusters can be ellipsoids rather than spheres, while the centroids are
    represented by the means of each distribution. The GMM algorithm performs soft
    assignments because each point has a probability of being a member of any cluster.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: GMM泛化了k均值算法：它在特征之间添加了协方差，使得聚类可以是椭球而不是球体，而聚类的中心点由每个分布的均值表示。GMM算法执行软分配，因为每个点都有成为任何聚类成员的概率。
- en: The notebook `gaussian_mixture_models` demonstrates the implementation and visualizes
    the resulting cluster. You are likely to prefer GMM over other clustering algorithms
    when the k-means assumption of spherical clusters is too constraining; GMM often
    needs fewer clusters to produce a good fit given its greater flexibility. The
    GMM algorithm is also preferable when you need a generative model; because GMM
    estimates the probability distributions that generated the samples, it is easy
    to generate new samples based on the result.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`gaussian_mixture_models`演示了实现并可视化结果聚类的过程。当k均值假设球形聚类过于约束时，你可能更喜欢GMM而不是其他聚类算法；鉴于其更大的灵活性，GMM通常需要更少的聚类来产生良好的拟合效果。当你需要一个生成模型时，GMM算法也更可取；因为GMM估计生成样本的概率分布，所以基于结果生成新样本很容易。
- en: Hierarchical clustering for optimal portfolios
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于最优组合的层次聚类
- en: In *Chapter 5*, *Portfolio Optimization and Performance Evaluation*, we discussed
    several methods that aim to choose portfolio weights for a given set of assets
    to optimize the risk and return profile of the resulting portfolio. These included
    the mean-variance optimization of Markowitz's modern portfolio theory, the Kelly
    criterion, and risk parity. In this section, we cover **hierarchical risk parity**
    (**HRP**), a more recent innovation (Prado 2016) that leverages hierarchical clustering
    to assign position sizes to assets based on the risk characteristics of subgroups.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5章*，*组合优化和绩效评估*中，我们讨论了几种旨在选择给定资产集的组合权重以优化所得组合的风险和回报特性的方法。这些方法包括马科维茨的现代投资组合理论的均值-方差优化、凯利准则和风险平价。在本节中，我们介绍了更近期的创新（Prado
    2016）——**层次风险平价**（**HRP**），它利用层次聚类根据子组的风险特征来分配资产仓位。
- en: We will first present how HRP works and then compare its performance against
    alternatives using a long-only strategy driven by the gradient boosting models
    we developed in the last chapter.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍HRP的工作原理，然后使用我们在上一章中开发的梯度提升模型，通过长仓策略比较其性能。
- en: How hierarchical risk parity works
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次风险平价的工作原理
- en: 'The key ideas of hierarchical risk parity are to do the following:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 层次风险平价的关键思想包括以下几点：
- en: Use hierarchical clustering of the covariance matrix to group assets with a
    similar correlation structure together
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用协方差矩阵的层次聚类将具有相似相关结构的资产分组在一起
- en: Reduce the number of degrees of freedom by only considering similar assets as
    substitutes when constructing the portfolio
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过仅在构建投资组合时将相似资产视为替代品来减少自由度
- en: Refer to the notebook and Python files in the subfolder `hierarchical_risk_parity`
    for implementation details.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 有关实施详细信息，请参阅子文件夹 `hierarchical_risk_parity` 中的笔记本和 Python 文件。
- en: The first step is to compute a distance matrix that represents proximity for
    correlated assets and meets distance metric requirements. The resulting matrix
    becomes an input to the SciPy hierarchical clustering function that computes the
    successive clusters using one of several available methods, as discussed previously
    in this chapter.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是计算一个距离矩阵，代表相关资产的接近度并满足距离度量的要求。得到的矩阵成为 SciPy 层次聚类函数的输入，该函数使用先前在本章讨论过的几种可用方法之一计算连续的群集。
- en: '[PRE23]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `linkage_matrix` can be used as input to the `sns.clustermap` function to
    visualize the resulting hierarchical clustering. The dendrogram displayed by seaborn
    shows how individual assets and clusters of assets merged based on their relative
    distances (see the left panel of *Figure 13.16*).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`linkage_matrix` 可用作 `sns.clustermap` 函数的输入，以可视化结果的层次聚类。由 seaborn 显示的树状图显示了个别资产和资产集合如何根据它们的相对距离合并（参见*图 13.16*
    的左面板）。'
- en: '[PRE24]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Compared to a `seaborn.heatmap` of the original correlation matrix, there is
    now significantly more structure in the sorted data (the right panel) compared
    to the original correlation matrix displayed in the central panel.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始相关矩阵的 `seaborn.heatmap` 相比，排序数据（右面板）中现在具有显着更多的结构，与中央面板中显示的原始相关矩阵相比。
- en: '![](img/B15439_13_16.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_16.png)'
- en: 'Figure 13.16: Original and clustered correlation matrix'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.16：原始和聚类相关矩阵
- en: Using the tickers sorted according to the hierarchy induced by the clustering
    algorithm, HRP now proceeds to compute a top-down inverse-variance allocation
    that successively adjusts weights depending on the variance of the subclusters
    further down the tree.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 使用根据聚类算法诱导的层次结构排序的标记，HRP 现在开始计算一个自上而下的逆方差分配，根据树下进一步子集的方差连续调整权重。
- en: '[PRE25]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: To this end, the algorithm uses a bisectional search to allocate the variance
    of a cluster to its elements based on their relative riskiness.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，该算法使用二分搜索将群集的方差分配给其元素，这些元素基于它们的相对风险性。
- en: '[PRE26]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The resulting portfolio allocation produces weights that sum to 1 and reflect
    the structure present in the correlation matrix (refer to the notebook for details).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 结果组合配置产生的权重总和为1，并反映在相关矩阵中存在的结构（详见笔记本）。
- en: Backtesting HRP using an ML trading strategy
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 ML 交易策略回测 HRP
- en: Now that we know how HRP works, we would like to test how it performs in practice
    compared to some alternatives, namely a simple equal-weighted portfolio and a
    mean-variance optimized portfolio. You can find the code samples for this section
    and additional details and analyses in the notebook `pf_optimization_with_hrp_zipline_benchmark`.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道 HRP 的工作原理，我们想测试它在实践中的表现如何与一些替代方案相比，即简单的等权重组合和均值-方差优化组合。您可以在笔记本 `pf_optimization_with_hrp_zipline_benchmark`
    中找到此部分的代码示例以及其他详细信息和分析。
- en: To this end, we'll build on the gradient boosting models developed in the last
    chapter. We will backtest a strategy for 2015-2017 with a universe of the 1,000
    most liquid US stocks. The strategy relies on the model predictions to enter long
    positions in the 25 stocks with the highest positive return prediction for the
    next day. On a daily basis, we rebalance our holdings so that the weights for
    our target positions match the values suggested by HRP.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将建立在上一章开发的梯度提升模型的基础上。我们将对2015-2017年的策略进行回测，使用最流动的1000只美国股票的宇宙。该策略依赖于模型预测，以买入次日预测收益最高的25只股票。我们每天重新平衡我们的持仓，以使我们的目标位置的权重与HRP建议的值匹配。
- en: Ensembling the gradient boosting model predictions
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 合并梯度提升模型的预测
- en: 'We begin by averaging the predictions of the 10 models that performed best
    during the 2015-16 cross-validation period (refer to *Chapter 12*, *Boosting Your
    Trading Strategy*, for details), as shown in the following code excerpt:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对 2015-16 交叉验证期间表现最佳的 10 个模型的预测进行平均，如下面的代码摘录所示：
- en: '[PRE27]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'On a daily basis, we obtain the model predictions and select the top 25 tickers.
    If there are at least 20 tickers with positive forecasts, we enter the long positions
    and close all of the other holdings:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们每天都会获得模型预测并选择前 25 个股票代码。如果至少有 20 个股票有正面预测，我们会进入多头仓位并关闭所有其他持仓：
- en: '[PRE28]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Using PyPortfolioOpt to compute HRP weights
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 PyPortfolioOpt 计算 HRP 权重
- en: 'PyPortfolioOpt, which we used in *Chapter 5*, *Portfolio Optimization and Performance
    Evaluation*, to compute mean-variance optimized weights, also implements HRP.
    We''ll run it as part of the scheduled rebalancing that takes place every morning.
    It needs the return history for the target assets and returns a dictionary of
    ticker-weight pairs that we use to place orders:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第五章*《投资组合优化与绩效评估》中使用的 PyPortfolioOpt 来计算均值-方差优化权重，也实现了 HRP。我们将在每天早上进行的定期再平衡的一部分中运行它。它需要目标资产的历史回报，并返回一个我们用于下订单的股票-权重对的字典：
- en: '[PRE29]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Markowitz rebalancing follows a similar process, as outlined in *Chapter 5*,
    *Portfolio Optimization and Performance Evaluation*, and is included in the notebook.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: Markowitz 再平衡遵循类似的过程，如*第五章*《投资组合优化与绩效评估》中所述，并包含在笔记本中。
- en: Performance comparison with pyfolio
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与 pyfolio 的性能比较
- en: The following charts show the cumulative returns for the in- and out-of-sample
    (with respect to the ML model selection process) of the **equal-weighted** (**EW**),
    the HRP, and the **mean-variance** (**MV**) optimized portfolios.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了**等权重**（**EW**）、**HRP**和**均值-方差**（**MV**）优化投资组合的样本内和样本外（相对于 ML 模型选择过程）的累积收益。
- en: '![](img/B15439_13_17.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15439_13_17.png)'
- en: 'Figure 13.17: Cumulative returns for the different portfolios'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.17：不同投资组合的累积收益
- en: The cumulative returns are 207.3 percent for MV, 133 percent for EW, and 75.1
    percent for HRP. The Sharpe ratios are 1.16, 1.01, and 0.83, respectively. Alpha
    returns are 0.28 for MV, 0.16 for EW, and 0.16 for HRP, with betas of 1.77, 1.87,
    and 1.67, respectively.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 累积收益分别为 MV 为 207.3%，EW 为 133%，HRP 为 75.1%。夏普比率分别为 1.16、1.01 和 0.83。Alpha 收益分别为
    MV 为 0.28，EW 为 0.16，HRP 为 0.16，对应的贝塔值分别为 1.77、1.87 和 1.67。
- en: Therefore, it turns out that, in this particular context, the often-criticized
    MV approach does best, while HRP comes up last. However, be aware that the results
    are quite sensitive to the number of stocks traded, the time period, and other
    factors.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种特定情境下，常受批评的**MV方法**效果最好，而**HRP**则排在最后。然而，请注意结果对交易股票数量、时间周期和其他因素非常敏感。
- en: Try it out for yourself, and learn which technique performs best under the circumstances
    most relevant for you!
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 试着自己尝试一下，了解在最适合您的情况下哪种技术表现最好！
- en: Summary
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored unsupervised learning methods that allow us to
    extract valuable signals from our data without relying on the help of outcome
    information provided by labels.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了无监督学习方法，这些方法允许我们从数据中提取有价值的信号，而无需依赖标签提供的结果信息的帮助。
- en: We learned how to use linear dimensionality reduction methods like PCA and ICA
    to extract uncorrelated or independent components from data that can serve as
    risk factors or portfolio weights. We also covered advanced nonlinear manifold
    learning techniques that produce state-of-the-art visualizations of complex, alternative
    datasets. In the second part of the chapter, we covered several clustering methods
    that produce data-driven groupings under various assumptions. These groupings
    can be useful, for example, to construct portfolios that apply risk-parity principles
    to assets that have been clustered hierarchically.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何使用线性降维方法如 PCA 和 ICA 来从数据中提取无关或独立的组件，这些组件可以作为风险因子或投资组合权重。我们还涵盖了先进的非线性流形学习技术，这些技术可以生成复杂、替代数据集的最新可视化效果。在章节的第二部分，我们涵盖了几种根据不同假设产生数据驱动的分组的聚类方法。这些分组可以用来构建将风险平价原则应用于已经按层次聚类的资产的投资组合，例如。
- en: In the next three chapters, we will learn about various machine learning techniques
    for a key source of alternative data, namely natural language processing for text
    documents.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三章中，我们将学习关于一种重要的替代数据来源的各种机器学习技术，即自然语言处理文本文档。
