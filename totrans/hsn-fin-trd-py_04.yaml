- en: '*Chapter 2*: Exploratory Data Analysis in Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter focuses on **exploratory data analysis** (**EDA**), which is the
    first step in processing any dataset. The objective of EDA is to load data into
    data structures most suitable for further analysis to identify and rectify any
    wrong/bad data and get basic insight into the data—the types of fields there are;
    whether they are categorical or not; how many missing values there are; how the
    fields are related; and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the main topics discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to EDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Special Python libraries for EDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Python code used in this chapter is available in the `Chapter02/eda.ipynb`
    notebook in the book's code repository.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to EDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: EDA is the process of procuring, understanding, and deriving meaningful statistical
    insights from structured/unstructured data of interest. It is the first step before
    a more complex analysis, such as predicting future expectations from the data.
    In the case of financial data, EDA helps obtain insights used later for building
    profitable trading signals and strategies.
  prefs: []
  type: TYPE_NORMAL
- en: EDA guides later decisions of which features/signals to use or avoid and which
    predictive models to use or avoid, and invalidates incorrect hypotheses while
    validating and introducing correct hypotheses about the nature of variables and
    the relationships between them.
  prefs: []
  type: TYPE_NORMAL
- en: EDA is also important in understanding how sample (a smaller dataset representative
    of a complete dataset) statistics differ from population (a complete dataset or
    an ultimate truth) statistics and keeping that in mind when drawing conclusions
    about the population, based on observations of samples. Thus, EDA helps cut down
    possible search spaces down the road; otherwise, we would waste a lot more time
    later on building incorrect/insignificant models or strategies.
  prefs: []
  type: TYPE_NORMAL
- en: EDA must be approached with a scientific mindset. Sometimes, we might reach
    inadequately validated conclusions based on anecdotal evidence rather than statistical
    evidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypotheses based on anecdotal evidence suffer from issues stemming from the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Not being statistically significant—too low number of observations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selection bias—the hypothesis is only created because it was first observed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confirmation bias—our inherent belief in the hypothesis biases our results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inaccuracies in observations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's explore the different steps and techniques involved in EDA, using real
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Steps in EDA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is a list of steps involved in EDA (we''ll be going through each of them
    in the subsections that follow):'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the necessary libraries and setting them up
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data wrangling/munging
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data cleaning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Obtaining descriptive statistics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visual inspection of the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data cleaning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advanced visualization techniques
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loading the necessary libraries and setting them up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will be using `numpy`, `pandas`, and `matplotlib`, and these libraries can
    be loaded with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We use the `mpld3` library for enabling zooming within Jupyter's `matplotlib`
    charts. The last line of the preceding code block specifies that only a maximum
    of two rows of `pandas` DataFrames should be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data collection is usually the first step for EDA. Data may come from many different
    sources (**comma-separated values** (**CSV**) files, Excel files, web scrapes,
    binary files, and so on) and will often need to be standardized and first formatted
    together correctly.
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise, we will use data for three different trading instruments
    for a period of 5 years, stored in `.csv` format. The identity of these instruments
    is deliberately not revealed since that might give away their expected behavior/relationships,
    but we will reveal their identity at the end of this exercise to evaluate intuitively
    how well we performed EDA on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by loading up our available datasets into three DataFrames (`A`,
    `B`, and `C`), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'DataFrame `A` has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – DataFrame constructed from the A.csv file](img/Figure_2.1_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – DataFrame constructed from the A.csv file
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, let''s load DataFrame `B`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'DataFrame `B` has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – DataFrame constructed from the B.csv file](img/Figure_2.2_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – DataFrame constructed from the B.csv file
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s load the `C` data into a DataFrame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And we see `C` has the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – DataFrame constructed from the C.csv file](img/Figure_2.3_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – DataFrame constructed from the C.csv file
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe, all three data sources have the same format with `2015-05-15`
    and `2020-05-14`.
  prefs: []
  type: TYPE_NORMAL
- en: Data wrangling/munging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data rarely comes in a ready-to-use format. Data wrangling/munging refers to
    the process of manipulating and transforming data from its initial raw source
    into structured, formatted, and easily usable datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use `pandas.DataFrame.join(...)` to merge the DataFrames and align them
    to have the same `DateTimeIndex` format. Using the `lsuffix=` and `rsuffix=` parameters,
    we assign the `_A`, `_B`, and `_C` suffixes to the columns coming from the three
    DataFrames, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will inspect the `merged_df` DataFrame we just created and make sure it
    has all the fields we expected from all three DataFrames (displaying only the
    first seven columns). The DataFrame can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – DataFrame constructed by joining the DataFrames A, B, and C](img/Figure_2.4_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – DataFrame constructed by joining the DataFrames A, B, and C
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the original three DataFrames (`A`, `B`, and `C`) had 1,211, 1,209
    and 1,206 rows respectively, but the combined DataFrame has 1,259 rows. This is
    because we used an outer join, which uses the union of dates across all three
    DataFrames. When it cannot find values for a specific DataFrame for a specific
    date, it places a `NaN` value there for that DataFrame's fields.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data cleaning refers to the process of addressing data errors coming from missing
    data, incorrect data values, and outliers.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, `merged_df` has missing values for many fields coming from the
    original datasets and coming from merging DataFrames with different sets of dates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first check if there are any rows where all values are missing (`NaN`),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The result shows that we do not have any row with all fields missing, as we
    can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – DataFrame showing that there are no rows with all fields missing](img/Figure_2.5_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – DataFrame showing that there are no rows with all fields missing
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s find out how many rows exist that have at least one field that
    is missing/`NaN`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'So, it turns out 148 rows out of our 1,259 rows have one or more fields with
    missing values, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For our further analysis, we need to have valid `Close` prices. Thus, we can
    drop all rows where the `Close` price for any of the three instruments is missing,
    by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After dropping the missing `Close` prices, we should have no more missing `Close`
    price fields, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The result confirms there are no rows left where any of the `Close_A`, `Close_B`,
    or `Close_C` fields are `NaN` values, as we can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s inspect the new DataFrame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result (displaying only the first seven columns):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Resulting DataFrame with no missing/NaN values for any close
    prices](img/Figure_2.6_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – Resulting DataFrame with no missing/NaN values for any close prices
  prefs: []
  type: TYPE_NORMAL
- en: As expected, we dropped the 148 rows that had missing/`NaN` values for any of
    the close prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s deal with rows that have `NaN` values for any of the other fields,
    starting with getting a sense of how many such rows exist. We can do this by running
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of that query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: So, there exist 165 rows that have at least some fields with a missing value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s quickly inspect a few of the rows with at least some fields with a missing
    value, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Some of the rows with some missing values are displayed (displaying only the
    first seven columns), as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – DataFrame showing there are still some rows with some missing
    values](img/Figure_2.7_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – DataFrame showing there are still some rows with some missing values
  prefs: []
  type: TYPE_NORMAL
- en: So, we can see that the `Low_C` field on `2015-05-18` (not visible in the preceding
    screenshot) and the `Open_B` field on `2020-05-01` have `NaN` values (among 163
    others, of course).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the `pandas.DataFrame.fillna(...)` method with a method called `backfill`—this
    uses the next valid value after the missing value to fill in the missing value.
    The code is illustrated in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the impact of the backfilling, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, this is the output for the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, after the `backfill` operation, there are no more missing/`NaN`
    values left for any field in any row.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining descriptive statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next step is to generate the key basic statistics on data to build familiarity
    with each field, with the `DataFrame.describe(...)` method. The code is illustrated
    in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we have increased the number of rows of a `pandas` DataFrame to
    display.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output of running `pandas.DataFrame.describe(…)`, displaying only
    the first seven columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Descriptive statistics of the valid_close_complete DataFrame](img/Figure_2.8_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Descriptive statistics of the valid_close_complete DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: The preceding output provides quick summary statistics for every field in our
    DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key observations from *Figure 2.8* are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Volume_C` has all statistics values to be `0`, implying every row has the
    `Volume_C` value set to `0`. Therefore, we need to remove this column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Open_C` has a minimum value of `-400`, which is unlikely to be true for the
    following reasons:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) The other price fields—`High_C`, `Low_C`, `Close_C`, and `Adj Close_C`—all
    have minimum values around `9`, so it doesn't make sense for `Open_C` to have
    a minimum value of `-400`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Given that the 25th percentile for `Open_C` is `12.4`, it is unlikely that
    the minimum value would be so much lower than that.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The price of an asset should be non-negative.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Low_C` has a maximum value of `330`, which is again unlikely because of the
    following reasons:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) For the same reasons given previously to those outlined previously, as `Open_C`
    is not correct.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) In addition, considering that `Low_C` should always be lower than `High_C`,
    by definition, the lowest price in a day has to be lower than the highest price
    on a day.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s put back the output of all the `pandas` DataFrames to be just two rows,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s remove the `Volume` fields for all three instruments, with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And the `prices_only` DataFrame has the following data (displaying only the
    first seven columns):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – The prices_only DataFrame](img/Figure_2.9_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – The prices_only DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: As expected, after we removed the three volume columns, we reduced the DataFrame
    dimensions to `1111 × 15`—these were previously `1111 × 18`.
  prefs: []
  type: TYPE_NORMAL
- en: Visual inspection of the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There do not seem to be any obvious errors or discrepancies with the other fields,
    so let's plot a quick visualization of the prices to see if that sits in line
    with what we learned from the descriptive statistics.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will start with the prices of `A`, since we expect those to be correct
    based on the descriptive statistics summary. The code is illustrated in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is consistent with our expectations, and we can conclude that the
    prices of `A` are valid based on the statistics and the plot shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Plot showing Open, Close, High, and Low prices for trading
    instrument A over 5 years](img/Figure_2.10_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Plot showing Open, Close, High, and Low prices for trading instrument
    A over 5 years
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s plot the prices of C to see if the plot provides further evidence
    regarding our suspicions about some prices being incorrect. The code can be seen
    in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that `Open_C` and `Low_C` have some erroneous values extremely
    far away from other values—these are the outliers. The following screenshot shows
    a plot illustrating this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Plot showing large outliers in the prices of C in both positive
    and negative directions](img/Figure_2.11_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – Plot showing large outliers in the prices of C in both positive
    and negative directions
  prefs: []
  type: TYPE_NORMAL
- en: We will need to perform some further data cleaning to eliminate these outlier
    values so that we do not derive incorrect statistical insights from our data.
  prefs: []
  type: TYPE_NORMAL
- en: The two most commonly used methods to detect and remove outliers are the **interquartile
    range** (**IQR**) and the Z-score.
  prefs: []
  type: TYPE_NORMAL
- en: IQR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The IQR method uses a percentile/quantile range of values over the entire dataset
    to identify and remove outliers.
  prefs: []
  type: TYPE_NORMAL
- en: When applying the IQR method, we usually use extreme percentile values, such
    as 5% to 95%, to minimize the risk of removing correct data points.
  prefs: []
  type: TYPE_NORMAL
- en: In our example of `Open_C`, let's use the 25th percentile and 75th percentile
    and remove all data points with values outside that range. The 25th-to-75th percentile
    range is (`12.4, 17.68`), so we would remove the outlier value of `-400`.
  prefs: []
  type: TYPE_NORMAL
- en: Z-score
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Z-score (or standard score) is obtained by subtracting the mean of the dataset
    from each data point and normalizing the result by dividing by the standard deviation
    of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the Z-score of a data point represents the distance in the number
    of standard deviations that the data point is away from the mean of all the data
    points.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a normal distribution (applicable for large enough datasets) there is a
    distribution rule of **68-95-99**, summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 68% of all data will lie in a range of one standard deviation from the mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 95% of all data will lie in a range of two standard deviations from the mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 99% of all data will lie within a range of three standard deviations from the
    mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, after computing Z-scores of all data points in our dataset (which is large
    enough), there is an approximately 1% chance of a data point having a Z-score
    larger than or equal to `3`.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we can use this information to filter out all observations with Z-scores
    of `3` or higher to detect and remove outliers.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we will remove all rows with values whose Z-score is less than
    `-6` or greater than `6`—that is, six standard deviations away from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we use `scipy.stats.zscore(...)` to compute Z-scores of each column
    in the `prices_only` DataFrame, and then we use `numpy.abs(...)` to get the magnitude
    of the Z-scores. Finally, we select rows where all fields have Z-scores lower
    than 6, and save that in a `no_outlier_prices` DataFrame. The code is illustrated
    in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what impact this Z-score outlier removal code had on the price fields
    for instrument `C` by plotting its prices again and comparing to the earlier plot,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Plot showing the prices of C after removing outliers by applying
    data cleaning](img/Figure_2.12_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – Plot showing the prices of C after removing outliers by applying
    data cleaning
  prefs: []
  type: TYPE_NORMAL
- en: The plot clearly shows that the earlier observation of extreme values for `Open_C`
    and `Low_C` has been discarded; there is no longer the dip of `-400`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that while we removed the extreme outliers, we were still able to preserve
    the sharp spikes in prices during 2015, 2018, and 2020, thus not leading to a
    lot of data losses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also check the impact of our outlier removal work by re-inspecting the
    descriptive statistics, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'These statistics look significantly better—as we can see in the following screenshot,
    the `min` and `max` values for all prices now look in line with expectations and
    do not have extreme values, so we succeeded in our data cleaning task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Descriptive statistics for the no_outlier_prices selected columns](img/Figure_2.13_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Descriptive statistics for the no_outlier_prices selected columns
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s reset back the number of rows to display for a `pandas` DataFrame, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Advanced visualization techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will explore univariate and multivariate statistics visualization
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s collect the close prices for the three instruments, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's compute the daily close price changes to evaluate if there is a
    relationship between daily price changes between the three instruments.
  prefs: []
  type: TYPE_NORMAL
- en: Daily close price changes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We will use the `pandas.DataFrame.shift(...)` method to shift the original
    DataFrame one period forward so that we can compute the price changes. The `pandas.DataFrame.fillna(...)`
    method here fixes the one missing value generated in the first row as a result
    of the `shift` operation. Finally, we will rename the columns to `Delta_Close_A`,
    `Delta_Close_B`, and `Delta_Close_C` to reflect the fact that these values are
    price differences and not actual prices. The code is illustrated in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The content of the newly generated `delta_close_prices` DataFrame is shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – The delta_close_prices DataFrame](img/Figure_2.14_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – The delta_close_prices DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: These values look correct, judging from the first few actual prices and the
    calculated price differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s quickly inspect the summary statistics for this new DataFrame to
    get a sense of how the delta price values are distributed, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The descriptive statistics on this DataFrame are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 – Descriptive statistics for the delta_close_prices DataFrame](img/Figure_2.15_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – Descriptive statistics for the delta_close_prices DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: We can observe from these statistics that all three delta values' means are
    close to 0, with instrument `A` experiencing large price swings and instrument
    `C` experiencing significantly smaller price moves (from the `std` field).
  prefs: []
  type: TYPE_NORMAL
- en: Histogram plot
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s observe the distribution of `Delta_Close_A` to get more familiar with
    it, using a histogram plot. The code for this is shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, we can see that the distribution is approximately
    normally distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Histogram of Delta_Close_A values roughly normally distributed
    around the 0 value ](img/Figure_2.16_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 – Histogram of Delta_Close_A values roughly normally distributed
    around the 0 value
  prefs: []
  type: TYPE_NORMAL
- en: Box plot
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s draw a box plot, which also helps in assessing the values'' distribution.
    The code for this is shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17 – Box plot showing mean, median, IQR (25th to 75th percentile),
    and outliers](img/Figure_2.17_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 – Box plot showing mean, median, IQR (25th to 75th percentile),
    and outliers
  prefs: []
  type: TYPE_NORMAL
- en: Correlation charts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first step in multivariate data statistics is to assess the correlations
    between `Delta_Close_A`, `Delta_Close_B`, and `Delta_Close_C`.
  prefs: []
  type: TYPE_NORMAL
- en: The most convenient way to do that is to plot a correlation scatter matrix that
    shows the pairwise relationship between the three variables, as well as the distribution
    of each individual variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we demonstrate the option of using **kernel density estimation**
    (**KDE**), which is closely related to histograms but provides a smoother distribution
    surface across the plots on the diagonals. The code for this is shown in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This plot indicates that there is a strong positive correlation between `Delta_Close_A`
    and `Delta_Close_B` and a strong negative correlation between `Delta_Close_C`
    and the other two variables. The diagonals also display the distribution of each
    individual variable, using KDE.
  prefs: []
  type: TYPE_NORMAL
- en: 'A scatter plot of the fields can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18 – Scatter plot of Delta_Close fields with KDE histogram on the
    diagonals](img/Figure_2.18_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.18 – Scatter plot of Delta_Close fields with KDE histogram on the diagonals
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at some statistics that provide the relationship between
    the variables. `DataFrame.corr(...)` does that for us and also displays linear
    correlations. This can be seen in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The correlation matrix confirms that `Delta_Close_A` and `Delta_Close_B` have
    a strong positive correlation (very close to 1.0, which is the maximum), as we
    expected based on the scatter plot. Also, `Delta_Close_C` is negatively correlated
    (closer to -1.0 than 0.0) to the other two variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the correlation matrix in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19 – Correlation matrix for Delta_Close_A, Delta_Close_B, and Delta_Close_C](img/Figure_2.19_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.19 – Correlation matrix for Delta_Close_A, Delta_Close_B, and Delta_Close_C
  prefs: []
  type: TYPE_NORMAL
- en: Pairwise correlation heatmap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An alternative visualization technique known as a `seaborn.heatmap(...)`, as
    illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the plot shown in the following screenshot, the rightmost scale shows a
    legend where the darkest values represent the strongest negative correlation and
    the lightest values represent the strongest positive correlations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.20 – Seaborn heatmap visualizing pairwise correlations between Delta_Close
    fields](img/Figure_2.20_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.20 – Seaborn heatmap visualizing pairwise correlations between Delta_Close
    fields
  prefs: []
  type: TYPE_NORMAL
- en: The heatmap shows graphically the same message as the table in the previous
    section— there is a very high correlation between `Delta_Close_A` and `Delta_Close_B`
    and a very high negative correlation between `Delta_Close_A` and `Delta_Close_C`.
    There is also a very high negative correlation between `Delta_Close_B` and `Delta_Close_C`.
  prefs: []
  type: TYPE_NORMAL
- en: Revelation of the identity of A, B, and C and EDA's conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `A` instrument is the `B` instrument is the `C` instrument is the **Chicago
    Board Options Exchange** (**CBOE**) **Volatility Index** (**VIX**), which basically
    tracks how volatile markets are at any given time (basically, a function of equity
    index price swings).
  prefs: []
  type: TYPE_NORMAL
- en: 'From our EDA on the mystery instruments, we drew the following conclusions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`C` (VIX) cannot have negative prices or prices above 90, which has historically
    been true.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`A` (DJIA) and `B` (SPY) had huge drops in 2008 and 2020, corresponding to
    the stock market crash and the COVID-19 pandemic, respectively. Also, the price
    of `C` (VIX) spiked at the same time, indicating heightened market turmoil.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`A` (DJIA) has largest daily price swings, followed by `B` (SPY), and finally
    `C` (VIX), with very low daily price swings. These are also correct observations
    considering the underlying instruments that they were hiding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`A` (DJIA) and `B` (SPY) have very strong positive correlations, which makes
    sense since both are large cap equity indices. `C` (VIX) has strong negative correlations
    with both `A` (DJIA) and `B` (SPY), which also makes sense since during periods
    of prosperity, volatility remains low and markets rise, and during periods of
    crisis, volatility spikes and markets drop.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we introduce one special Python library that generates
    the most common EDA charts and tables automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Special Python libraries for EDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are multiple Python libraries that provide EDA in a single line of code.
    One of the most advanced of them is `dtale`, shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command produces a table with all the data (displaying only the
    first seven columns), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21 – The dtale component showing spreadsheet-like control over the
    valid_close_df DataFrame](img/Figure_2.21_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.21 – The dtale component showing spreadsheet-like control over the
    valid_close_df DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on the arrow at the top displays a menu with all the functionality,
    as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.22 – The dtale global menu showing its functionality](img/Figure_2.22_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.22 – The dtale global menu showing its functionality
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on the column header displays each feature''s individual commands,
    as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.23 – The dtale column menu showing column functionality](img/Figure_2.23_B15029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.23 – The dtale column menu showing column functionality
  prefs: []
  type: TYPE_NORMAL
- en: Interactive EDA, rather than command-driven EDA, has its advantages—it is intuitive,
    it promotes visual creativity, and it can be faster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of EDA is to get a feel for the dataset we work with, and to correct
    basic data errors such as unlikely outliers. We have described both an EDA built
    by running individual Python commands and an automated EDA using a special Python
    EDA library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next chapter introduces us to one of the most important Python libraries:
    `numpy`.'
  prefs: []
  type: TYPE_NORMAL
