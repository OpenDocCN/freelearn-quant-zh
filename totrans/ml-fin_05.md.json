["```py\n$ pip install -U spacy\n$ python -m spacy download en\n\n```", "```py\na1 = pd.read_csv('../input/articles1.csv',index_col=0)\na2 = pd.read_csv('../input/articles2.csv',index_col=0)\na3 = pd.read_csv('../input/articles3.csv',index_col=0)\n\ndf = pd.concat([a1,a2,a3])\n\ndel a1, a2, a3\n```", "```py\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,7))\ndf.publication.value_counts().plot(kind='bar')\n```", "```py\nimport spacy\nnlp = spacy.load('en')\n```", "```py\ntext = df.loc[0,'content']\n```", "```py\ndoc = nlp(text)\n```", "```py\nfrom spacy import displacy\ndisplacy.render(doc,              #1style='ent',      #2jupyter=True)     #3\n```", "```py\nnlp = spacy.load('en',disable=['parser','tagger','textcat'])\n```", "```py\nfrom tqdm import tqdm_notebook\n\nframes = []\nfor i in tqdm_notebook(range(1000)):\n    doc = df.loc[i,'content']                              #1\n    text_id = df.loc[i,'id']                               #2\n    doc = nlp(doc)                                         #3\n    ents = [(e.text, e.start_char, e.end_char, e.label_)   #4\n            for e in doc.ents \n            if len(e.text.strip(' -â€”')) > 0]\n    frame = pd.DataFrame(ents)                             #5\n    frame['id'] = text_id                                  #6\n    frames.append(frame)                                   #7\n\nnpf = pd.concat(frames)                                    #8\n\nnpf.columns = ['Text','Start','Stop','Type','id']          #9\n```", "```py\nnpf.Type.value_counts().plot(kind='bar')\n```", "```py\norgs = npf[npf.Type == 'ORG']\norgs.Text.value_counts()[:15].plot(kind='bar')\n```", "```py\nTRAIN_DATA = [\n    ('Who is Shaka Khan?', {\n        'entities': [(7, 17, 'PERSON')]\n    }),\n    ('I like London and Berlin.', {\n        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n    })\n]\n```", "```py\nnlp = spacy.load('en')\n```", "```py\nnlp = spacy.blank('en')\n```", "```py\nif 'ner' not in nlp.pipe_names:\n    ner = nlp.create_pipe('ner')\n    nlp.add_pipe(ner, last=True)\nelse:\n    ner = nlp.get_pipe('ner')\n```", "```py\nfor _, annotations in TRAIN_DATA:\n    for ent in annotations.get('entities'):\n        ner.add_label(ent[2])\nimport random\n\n                                                   #1\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n\nwith nlp.disable_pipes(*other_pipes):\n    optimizer = nlp._optimizer                     #2\n    if not nlp._optimizer:\n        optimizer = nlp.begin_training()\n    for itn in range(5):                           #3\n        random.shuffle(TRAIN_DATA)                 #4\n        losses = {} #5\n        for text, annotations in TRAIN_DATA:       #6\n            nlp.update(                            #7\n                [text],  \n                [annotations],  \n                drop=0.5,                          #8\n                sgd=optimizer,                     #9\n                losses=losses)                     #10\n        print(losses)\n```", "```py\n{'ner': 5.0091189558407585}\n{'ner': 3.9693684224622108}\n{'ner': 3.984836024903589}\n{'ner': 3.457960373417813}\n{'ner': 2.570318400714134}\n\n```", "```py\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en')\n\ndoc = 'Google to buy Apple'\ndoc = nlp(doc)\ndisplacy.render(doc,style='dep',jupyter=True, options={'distance':120})\n```", "```py\nnlp = spacy.load('en')\ndoc = 'Google to buy Apple'\ndoc = nlp(doc)\n\nfor chunk in doc.noun_chunks:\n    print(chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text)\n```", "```py\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load('en')\n```", "```py\npattern = [{'LOWER': 'hello'}, {'IS_PUNCT': True}, {'LOWER': 'world'}]\n```", "```py\nmatcher = Matcher(nlp.vocab)\n```", "```py\nmatcher.add('HelloWorld', None, pattern)\n```", "```py\ndoc = nlp(u'Hello, world! Hello world!')\nmatches = matcher(doc)\n```", "```py\nmatches\n[(15578876784678163569, 0, 3)]\n\n```", "```py\ndoc[0:3]\n```", "```py\nHello, wor\nld\n\n```", "```py\nPRODUCT = nlp.vocab.strings['PRODUCT']\n```", "```py\ndef add_product_ent(matcher, doc, i, matches):\n    match_id, start, end = matches[i]            #1\n    doc.ents += ((PRODUCT, start, end),)         #2\n```", "```py\npattern1 = [{'LOWER': 'iPhone'}]                           #1\npattern2 = [{'ORTH': 'iPhone'}, {'IS_DIGIT': True}]        #2\n\nmatcher = Matcher(nlp.vocab)                               #3\nmatcher.add('iPhone', add_product_ent,pattern1, pattern2)  #4\n```", "```py\ndoc = nlp(df.content.iloc[14])         #1\nmatches = matcher(doc)                 #2\n```", "```py\ndef matcher_component(doc):\n    matches = matcher(doc)\n    return doc\n```", "```py\nnlp.add_pipe(matcher_component,last=True)\n```", "```py\ndisplacy.render(doc,style='ent',jupyter=True)\n```", "```py\npattern = [{'ENT_TYPE':'PERSON'},{'LEMMA':'receive'},{'ENT_TYPE':'MONEY'}]\n```", "```py\nimport re\n```", "```py\npattern = 'NL[0-9]{9}B[0-9]{2}'\n```", "```py\nmy_string = 'ING Bank N.V. BTW:NL003028112B01'\n```", "```py\nre.findall(pattern,my_string)\n```", "```py\n['NL003028112B01']\n\n```", "```py\nre.findall(pattern,my_string, flags=re.IGNORECASE)\n```", "```py\nmatch = re.search(pattern,my_string)\n```", "```py\nmatch.span()\n(18, 32)\n\n```", "```py\ndf[df.content.str.contains(pattern)]\n```", "```py\nimport codecs\ninput_file = codecs.open('../input/socialmedia-disaster-tweets-DFE.csv','r',',encoding='utf-8', errors='replace')\n```", "```py\noutput_file = open('clean_socialmedia-disaster.csv', 'w')\n```", "```py\nfor line in input_file:\n    out = line\n    output_file.write(line)\n```", "```py\ninput_file.close()\noutput_file.close()\n```", "```py\ndf = pd.read_csv('clean_socialmedia-disaster.csv')\n```", "```py\nimport spacy\nnlp = spacy.load('en',disable=['tagger','parser','ner'])\n```", "```py\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas(tqdm_notebook)\n```", "```py\ndf['lemmas'] = df[\"text\"].progress_apply(lambda row: [w.lemma_ for w in nlp(row)])\n```", "```py\ndf['joint_lemmas'] = df['lemmas'].progress_apply(lambda row: ' '.join(row))\n```", "```py\ndf.choose_one.unique()\narray(['Relevant', 'Not Relevant', \"Can't Decide\"], dtype=object)\n```", "```py\ndf = df[df.choose_one != \"Can't Decide\"]\n```", "```py\ndf = df[['text','choose_one']]\n```", "```py\nf['relevant'] = df.choose_one.map({'Relevant':1,'Not Relevant':0})\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df['joint_lemmas'], \n                                                    df['relevant'], \n                                                    test_size=0.2,\n                                                    random_state=42)\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer = CountVectorizer(max_features=10000)\n```", "```py\nX_train_counts = count_vectorizer.fit_transform(X_train)\nX_test_counts = count_vectorizer.transform(X_test)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\n\nclf.fit(X_train_counts, y_train)\n\ny_predicted = clf.predict(X_test_counts)\n```", "```py\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_predicted)\n```", "```py\n0.8011049723756906\n\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer()\n\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n```", "```py\nclf_tfidf = LogisticRegression()\nclf_tfidf.fit(X_train_tfidf, y_train)\n\ny_predicted = clf_tfidf.predict(X_test_tfidf)\n```", "```py\naccuracy_score(y_pred=y_predicted, y_true=y_test)\n```", "```py\n0.79788213627992\n63\n\n```", "```py\nfrom sklearn.decomposition import LatentDirichletAllocation\nlda = LatentDirichletAllocation(n_components=2)\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nvectorizer = CountVectorizer(stop_words='english')\ntf = vectorizer.fit_transform(df['joint_lemmas'])\n```", "```py\nlda.fit(tf)\n```", "```py\nn_top_words = 5\ntf_feature_names = vectorizer.get_feature_names()\n```", "```py\nfor topic_idx, topic in enumerate(lda.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([tf_feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\nTopic #0: http news bomb kill disaster\nTopic #1: pron http like just https\n\n```", "```py\nfrom keras.preprocessing.text import Tokenizer\nimport numpy as np\n\nmax_words = 10000\n```", "```py\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(df['joint_lemmas'])\nsequences = tokenizer.texts_to_sequences(df['joint_lemmas'])\n```", "```py\nword_index = tokenizer.word_index\nprint('Token for \"the\"',word_index['the'])\nprint('Token for \"Movie\"',word_index['movie'])\nToken for \"the\" 4\nToken for \"Movie\" 333\n\n```", "```py\nfrom keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 140\n\ndata = pad_sequences(sequences, maxlen=maxlen)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data, df['relevant'],test_size = 0.2, shuffle=True, random_state = 42)\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n```", "```py\nmodel.summary()\n```", "```py\n_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================embedding_2 (Embedding)      (None, 140, 50)           500000    _________________________________________________________________flatten_2 (Flatten)          (None, 7000)              0         _________________________________________________________________dense_3 (Dense)              (None, 1)                 7001      =================================================================Total params: 507,001Trainable params: 507,001Non-trainable params: 0\n_________________________________________________________________\n\n```", "```py\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n\nhistory = model.fit(X_train, y_train,epochs=10,batch_size=32,validation_data=(X_test, y_test))\n```", "```py\nimport os\nglove_dir = '../input/glove6b50d'\nf = open(os.path.join(glove_dir, 'glove.6B.50d.txt'))\n```", "```py\nembeddings_index = {}\n```", "```py\nfor line in f:                                            #1\n    values = line.split()                                 #2\n    word = values[0]                                      #3\n    embedding = np.asarray(values[1:], dtype='float32')   #4\n    embeddings_index[word] = embedding dictionary         #5\nf.close()                                                 #6\n```", "```py\nprint('Found %s word vectors.' % len(embeddings_index))\n```", "```py\nFound 400000-word vectors.\n\n```", "```py\nall_embs = np.stack(embeddings_index.values())\nemb_mean = all_embs.mean()\nemb_std = all_embs.std()\n```", "```py\nembedding_dim = 50\n```", "```py\nword_index = tokenizer.word_index\nnb_words = min(max_words, len(word_index))\n```", "```py\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n```", "```py\nfor word, i in word_index.items():                    #1\n    if i >= max_words:                                #2\n        continue  \n    embedding_vector = embeddings_index.get(word)     #3\n    if embedding_vector is None:                      #4\n        embedding_matrix[i] = embedding_vector\n```", "```py\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights = [embedding_matrix], trainable = False))\n\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n```", "```py\nfrom keras.layers import CuDNNLSTM\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights = [embedding_matrix], trainable = False))\nmodel.add(CuDNNLSTM(32))\nmodel.add(Dense(1, activation='sigmoid'))\n```", "```py\nfrom keras.layers import Bidirectional\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights = [embedding_matrix], trainable = False))\nmodel.add(Bidirectional(CuDNNLSTM(32)))\nmodel.add(Dense(1, activation='sigmoid'))\n```", "```py\nsup1 = nlp('I would like to open a new checking account')\nsup2 = nlp('How do I open a checking account?')\n```", "```py\nsup1.similarity(sup2)\n```", "```py\n0.7079433112862716\n\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=64))\nmodel.add(Activation('relu'))\nmodel.add(Dense(4))\nmodel.add(Activation('softmax'))\nmodel.summary()\n```", "```py\nLayer (type)                 Output Shape              Param #   =================================================================dense_1 (Dense)              (None, 64)                4160      _________________________________________________________________activation_1 (Activation)    (None, 64)                0         _________________________________________________________________dense_2 (Dense)              (None, 4)                 260       _________________________________________________________________activation_2 (Activation)    (None, 4)                 0         =================================================================Total params: 4,420Trainable params: 4,420Non-trainable params: 0\n_________________________________________________________________\n\n```", "```py\nfrom keras.models import Model                        #1\nfrom keras.layers import Dense, Activation, Input\n\nmodel_input = Input(shape=(64,))                      #2\nx = Dense(64)(model_input)                            #3\nx = Activation('relu')(x)                             #4\nx = Dense(4)(x)\nmodel_output = Activation('softmax')(x)\n\nmodel = Model(model_input, model_output)              #5\nmodel.summary()\n```", "```py\nLayer (type)                 Output Shape              Param #   =================================================================input_2 (InputLayer)         (None, 64)                0         _________________________________________________________________dense_3 (Dense)              (None, 64)                4160      _________________________________________________________________activation_3 (Activation)    (None, 64)                0         _________________________________________________________________dense_4 (Dense)              (None, 4)                 260       _________________________________________________________________activation_4 (Activation)    (None, 4)                 0         =================================================================Total params: 4,420Trainable params: 4,420Non-trainable params: 0\n_________________________________________________________________\n\n```", "```py\nmodel_input = Input(shape=(64,))\n\ndense = Dense(64)\n\nx = dense(model_input)\n\nactivation = Activation('relu')\n\nx = activation(x)\n\ndense_2 = Dense(4)\n\nx = dense_2(x)\n\nmodel_output = Activation('softmax')(x)\n\nmodel = Model(model_input, model_output)\n```", "```py\nDense(24, activation='relu')\n```", "```py\ndef attention_3d_block(inputs,time_steps,single_attention_vector = False):\n    input_dim = int(inputs.shape[2])                             #1\n    a = Permute((2, 1),name='Attent_Permute')(inputs)            #2\n    a = Reshape((input_dim, time_steps),name='Reshape')(a)       #3\n    a = Dense(time_steps, activation='softmax', name='Attent_Dense')(a) # Create attention vector            #4\n    if single_attention_vector:                                  #5\n        a = Lambda(lambda x: K.mean(x, axis=1), name='Dim_reduction')(a)                             #6\n        a = RepeatVector(input_dim, name='Repeat')(a)            #7\n        a_probs = Permute((2, 1), name='Attention_vec')(a)       #8\n    output_attention_mul = Multiply(name='Attention_mul')([inputs, a_probs])                                          #9\n    return output_attention_mul\n```", "```py\ninput_tokens = Input(shape=(maxlen,),name='input')\n\nembedding = Embedding(max_words, embedding_dim, input_length=maxlen, weights = [embedding_matrix], trainable = False, name='embedding')(input_tokens)\n\nattention_mul = attention_3d_block(inputs = embedding,time_steps = maxlen,single_attention_vector = True)\n\nlstm_out = CuDNNLSTM(32, return_sequences=True, name='lstm')(attention_mul)\n\nattention_mul = Flatten(name='flatten')(attention_mul)\noutput = Dense(1, activation='sigmoid',name='output')(attention_mul)\nmodel = Model(input_tokens, output)\n```", "```py\nmodel.summary()\n```", "```py\n__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input (InputLayer)              (None, 140)          0                                            __________________________________________________________________________________________________embedding (Embedding)           (None, 140, 50)      500000      input[0][0]                      __________________________________________________________________________________________________Attent_Permute (Permute)        (None, 50, 140)      0           embedding[0][0]                  __________________________________________________________________________________________________Reshape (Reshape)               (None, 50, 140)      0           Attent_Permute[0][0]             __________________________________________________________________________________________________Attent_Dense (Dense)            (None, 50, 140)      19740       Reshape[0][0]                    __________________________________________________________________________________________________Dim_reduction (Lambda)          (None, 140)          0           Attent_Dense[0][0]               __________________________________________________________________________________________________Repeat (RepeatVector)           (None, 50, 140)      0           Dim_reduction[0][0]              __________________________________________________________________________________________________Attention_vec (Permute)         (None, 140, 50)      0           Repeat[0][0]                     __________________________________________________________________________________________________Attention_mul (Multiply)        (None, 140, 50)      0           embedding[0][0]                  Attention_vec[0][0]              __________________________________________________________________________________________________flatten (Flatten)               (None, 7000)         0           Attention_mul[0][0]              __________________________________________________________________________________________________output (Dense)                  (None, 1)            7001        flatten[0][0]                    ==================================================================================================Total params: 526,741Trainable params: 26,741Non-trainable params: 500,000\n__________________________________________________________________________________________________\n\n```", "```py\nbatch_size = 64                #1\nepochs = 100                   #2\nlatent_dim = 256               #3 \nnum_samples = 10000            #4\ndata_path = 'fra-eng/fra.txt'  #5\n```", "```py\ninput_texts = []\ntarget_texts = []\ninput_characters = set()\ntarget_characters = set()\n```", "```py\nlines = open(data_path).read().split('\\n')\nfor line in lines[: min(num_samples, len(lines) - 1)]:\n\n    input_text, target_text = line.split('\\t')          #1\n\n    target_text = '\\t' + target_text + '\\n'             #2\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n\n    for char in input_text:                             #3\n        if char not in input_characters:\n            input_characters.add(char)\n\n    for char in target_text:                            #4\n        if char not in target_characters:\n            target_characters.add(char)\n```", "```py\ninput_characters = sorted(list(input_characters))\ntarget_characters = sorted(list(target_characters))\n```", "```py\nnum_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\n```", "```py\ninput_token_index = {char: i for i, char in enumerate(input_characters)}\ntarget_token_index = {char: i for i, char in enumerate(target_characters)}\n```", "```py\nfor c in 'the cat sits on the mat':\n    print(input_token_index[c], end = ' ')\n```", "```py\n63 51 48 0 46 44 63 0 62 52 63 62 0 58 57 0 63 51 48 0 56 44 63\n\n```", "```py\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\nprint('Max sequence length for inputs:', max_encoder_seq_length)\nprint('Max sequence length for outputs:', max_decoder_seq_length)\n```", "```py\nMax sequence length for inputs: 16\nMax sequence length for outputs: 59\n\n```", "```py\nencoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens),dtype='float32')\n```", "```py\ndecoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n```", "```py\ndecoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n```", "```py\nencoder_inputs = Input(shape=(None, num_encoder_tokens), name = 'encoder_inputs')               #1\nencoder = CuDNNLSTM(latent_dim, return_state=True, name = 'encoder')                         #2\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)   #3\n\nencoder_states = [state_h, state_c]                           #4\n```", "```py\ndecoder_inputs = Input(shape=(None, num_decoder_tokens), name = 'decoder_inputs')                    #1\ndecoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True, name = 'decoder_lstm')                     #2\n\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states) #3\n\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax', name = 'decoder_dense')\ndecoder_outputs = decoder_dense(decoder_outputs)                   #4\n```", "```py\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n```", "```py\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))\n```", "```py\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\nhistory = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,batch_size=batch_size,epochs=epochs,validation_split=0.2)\n```", "```py\nencoder_model = Model(encoder_inputs, encoder_states)\n```", "```py\n#Inputs from the encoder\ndecoder_state_input_h = Input(shape=(latent_dim,))     #1\ndecoder_state_input_c = Input(shape=(latent_dim,))\n\n#Create a combined memory to input into the decoder\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]                                 #2\n\n#Decoder\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)                   #3\n\ndecoder_states = [state_h, state_c]                    #4\n\n#Predict next char\ndecoder_outputs = decoder_dense(decoder_outputs)       #5\n\ndecoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)                #6\n```", "```py\nreverse_input_char_index = {i: char for char, i in input_token_index.items()}\nreverse_target_char_index = {i: char for char, i in target_token_index.items()}\n```", "```py\ndef decode_sequence(input_seq):\n\n    states_value = encoder_model.predict(input_seq)      #1\n\n    target_seq = np.zeros((1, 1, num_decoder_tokens))    #2\n\n    target_seq[0, 0, target_token_index['\\t']] = 1\\.      #3\n\n    stop_condition = False                               #4\n    decoded_sentence = ''\n\n    while not stop_condition:                            #5\n\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)                           #6\n\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])   #7\n\n        sampled_char = reverse_target_char_index[sampled_token_index]                                               #8\n\n        decoded_sentence += sampled_char                           #9\n\n        if (sampled_char == '\\n' or                               #10\n           len(decoded_sentence) > max_decoder_seq_length):\n            stop_condition = True\n\n        target_seq = np.zeros((1, 1, num_decoder_tokens))         #11\n        target_seq[0, 0, sampled_token_index] = 1.\n\n        states_value = [h, c]                                     #12\n\n    return decoded_sentence\n```", "```py\nmy_text = 'Thanks!'\nplaceholder = np.zeros((1,len(my_text)+10,num_encoder_tokens))\n```", "```py\nfor i, char in enumerate(my_text):\n    print(i,char, input_token_index[char])\n    placeholder[0,i,input_token_index[char]] = 1\n```", "```py\n0 T 38\n1 h 51\n2 a 44\n3 n 57\n4 k 54\n5 s 62\n6 ! 1\n\n```", "```py\ndecode_sequence(placeholder)\n```", "```py\n'Merci !\\n'\n\n```"]