["```py\nfrom gensim.models import Word2vec, KeyedVectors\n from gensim.scripts.glove2Word2vec import glove2Word2vec\nglove2Word2vec(glove_input_file=glove_file, Word2vec_output_file=w2v_file)\n model = KeyedVectors.load_Word2vec_format(w2v_file, binary=False)\n```", "```py\nsentences = LineSentence(f'ngrams_1.txt')\nphrases = Phrases(sentences=sentences,\n                  min_count=25,  # ignore terms with a lower count\n                  threshold=0.5,  # only phrases with higher score\n                  delimiter=b'_',  # how to join ngram tokens\n                  scoring='npmi')  # alternative: default\ngrams = Phraser(phrases)\nsentences = grams[sentences]\n```", "```py\nsentence_path = Path('data', 'ngrams', f'ngrams_2.txt')\nsentences = LineSentence(sentence_path)\n```", "```py\nmodel = Word2vec(sentences,\n                 sg=1,    # 1=skip-gram; otherwise CBOW\n                 hs=0,    # hier. softmax if 1, neg. sampling if 0\n                 size=300,      # Vector dimensionality\n                 window=3,      # Max dist. btw target and context word\n                 min_count=50,  # Ignore words with lower frequency\n                 negative=10,  # noise word count for negative sampling\n                 workers=8,     # no threads \n                 iter=1,        # no epochs = iterations over corpus\n                 alpha=0.025,   # initial learning rate\n                 min_alpha=0.0001 # final learning rate\n                )\n```", "```py\nmodel.wv.most_similar(positive=['iphone'], \n                      restrict_vocab=15000)\n                 term  similarity\n0              android    0.600454\n1           smartphone    0.581685\n2                  app    0.559129\n```", "```py\nmodel.wv.most_similar(positive=['france', 'london'], \n                      negative=['paris'], \n                      restrict_vocab=15000)\n\n             term  similarity\n0  united_kingdom    0.606630\n1         germany    0.585644\n2     netherlands    0.578868\n```", "```py\ndf = (pd.read_parquet('yelp_reviews.parquet', engine='fastparquet')\n          .loc[:, ['stars', 'text']])\nstars = range(1, 6)\nsample = pd.concat([df[df.stars==s].sample(n=100000) for s in stars])\n```", "```py\nimport nltk\nnltk.download('stopwords')\nfrom nltk import RegexpTokenizer\nfrom nltk.corpus import stopwords\ntokenizer = RegexpTokenizer(r'\\w+')\nstopword_set = set(stopwords.words('english'))\n\ndef clean(review):\n    tokens = tokenizer.tokenize(review)\n    return ' '.join([t for t in tokens if t not in stopword_set])\n\nsample.text = sample.text.str.lower().apply(clean)\nsample = sample[sample.text.str.split().str.len()>10]\n```", "```py\nsentences = []\nfor i, (_, text) in enumerate(sample.values):\n    sentences.append(TaggedDocument(words=text.split(), tags=[i]))\n```", "```py\nmodel = Doc2vec(documents=sentences,\n                dm=1,          # algorithm: use distributed memory\n                dm_concat=0,   # 1: concat, not sum/avg context vectors\n                dbow_words=0,  # 1: train word vectors, 0: only doc \n                                    vectors\n                alpha=0.025,   # initial learning rate\n                size=300,\n                window=5,\n                min_count=10,\n                epochs=5,\n                negative=5)\nmodel.save('test.model')\n```", "```py\nfor _ in range(10):\n    alpha *= .9\n    model.train(sentences,\n                total_examples=model.corpus_count,\n                epochs=model.epochs,\n                alpha=alpha)\n```", "```py\nX = np.zeros(shape=(len(sample), size))\ny = sample.stars.sub(1) # model needs [0, 5) labels\nfor i in range(len(sample)):\n X[i] = model[i]\n```", "```py\ntrain_data = lgb.Dataset(data=X_train, label=y_train)\ntest_data = train_data.create_valid(X_test, label=y_test)\n```", "```py\nparams = {'objective'  : 'multiclass',\n          'num_classes': 5}\n```", "```py\nlgb_model = lgb.train(params=params,\n                      train_set=train_data,\n                      num_boost_round=250,\n                      valid_sets=[train_data, test_data],\n                      verbose_eval=25)\n```", "```py\ny_pred = np.argmax(lgb_model.predict(X_test), axis=1)\n```", "```py\naccuracy_score(y_true=y_test, y_pred=y_pred)\n0.44955063467061984\n```", "```py\ncm = confusion_matrix(y_true=y_test, y_pred=y_pred)\ncm = pd.DataFrame(cm / np.sum(cm), index=stars, columns=stars)\n```", "```py\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='.1%')\n```"]