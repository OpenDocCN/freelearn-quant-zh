- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Machine Learning Process
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter starts Part 2 of this book, where we''ll illustrate how you can
    use a range of supervised and unsupervised **machine learning** (**ML**) models
    for trading. We will explain each model''s assumptions and use cases before we
    demonstrate relevant applications using various Python libraries. The categories
    of models that we will cover in Parts 2-4 include:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Linear models for the regression and classification of cross-section, time series,
    and panel data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalized additive models, including nonlinear tree-based models, such as
    decision trees
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble models, including random forest and gradient-boosting machines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised linear and nonlinear methods for dimensionality reduction and clustering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network models, including recurrent and convolutional architectures
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will apply these models to the market, fundamental, and alternative data
    sources introduced in the first part of this book. We will build on the material
    covered so far by demonstrating how to embed these models in a trading strategy
    that translates model signals into trades, how to optimize portfolio, and how
    to evaluate strategy performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: There are several aspects that many of these models and their applications have
    in common. This chapter covers these common aspects so that we can focus on model-specific
    usage in the following chapters. They include the overarching goal of learning
    a functional relationship from data by optimizing an objective or loss function.
    They also include the closely related methods of measuring model performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: We'll distinguish between unsupervised and supervised learning and outline use
    cases for algorithmic trading. We'll contrast supervised regression and classification
    problems and the use of supervised learning for statistical inference of relationships
    between input and output data, along with its use for the prediction of future
    outputs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: We'll also illustrate how prediction errors are due to the model's bias or variance,
    or because of a high noise-to-signal ratio in the data. Most importantly, we'll
    present methods to diagnose sources of errors like overfitting and improve your
    model's performance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics relevant to applying the
    ML workflow in practice:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: How supervised and unsupervised learning from data works
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and evaluating supervised learning models for regression and classification
    tasks
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the bias-variance trade-off impacts predictive performance
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to diagnose and address prediction errors due to overfitting
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using cross-validation to optimize hyperparameters with a focus on time-series
    data
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why financial data requires additional attention when testing out-of-sample
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are already quite familiar with ML, feel free to skip ahead and dive
    right into learning how to use ML models to produce and combine alpha factors
    for an algorithmic trading strategy. This chapter's directory in the GitHub repository
    contains the code examples and lists additional resources.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经对机器学习非常熟悉，可以跳过并直接开始学习如何使用机器学习模型为算法交易策略生成和组合阿尔法因子。本章的GitHub存储库中包含代码示例和额外资源的目录。
- en: How machine learning from data works
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习是如何从数据中进行的
- en: 'Many definitions of ML revolve around the automated detection of meaningful
    patterns in data. Two prominent examples include:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习的定义都围绕着对数据中有意义的模式的自动检测。两个显著的例子包括：
- en: AI pioneer **Arthur Samuelson** defined ML in 1959 as a subfield of computer
    science that gives computers the ability to learn without being explicitly programmed.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能先驱**阿瑟·塞缪尔森**在1959年将机器学习定义为计算机科学的一个子领域，使计算机能够在没有明确编程的情况下学习。
- en: '**Tom Mitchell**, one of the current leaders in the field, pinned down a well-posed
    learning problem more specifically in 1998: a computer program learns from experience
    with respect to a task and a performance measure of whether the performance of
    the task improves with experience (Mitchell 1997).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**汤姆·米切尔**，这个领域的现任领导者之一，更加具体地确定了一个明确定义的学习问题，1998年：一台计算机程序通过与任务和性能度量相关的经验来学习，以确定任务的性能是否随着经验的积累而提高（Mitchell 1997）。'
- en: Experience is presented to an algorithm in the form of training data. The principal
    difference from previous attempts of building machines that solve problems is
    that the rules that an algorithm uses to make decisions are learned from the data,
    as opposed to being programmed by humans as was the case, for example, for expert
    systems prominent in the 1980s.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 经验以训练数据的形式呈现给算法。与以往构建解决问题的机器的尝试的主要区别在于，算法用于做出决策的规则是从数据中学习的，而不是像上世纪80年代突出的专家系统那样由人类编程的。
- en: Recommended textbooks that cover a wide range of algorithms and general applications
    include James et al (2013), Hastie, Tibshirani, and Friedman (2009), Bishop (2006),
    and Mitchell (1997).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的涵盖各种算法和通用应用的教科书包括James等人（2013），Hastie、Tibshirani和Friedman（2009），Bishop（2006）和Mitchell（1997）。
- en: The challenge – matching the algorithm to the task
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 挑战——将算法与任务匹配
- en: The key challenge of automated learning is to identify patterns in the training
    data that are meaningful when generalizing the model's learning to new data. There
    are a large number of potential patterns that a model could identify, while the
    training data only constitutes a sample of the larger set of phenomena that the
    algorithm may encounter when performing the task in the future.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 自动学习的关键挑战是识别训练数据中的模式，这些模式在将模型的学习推广到新数据时具有意义。模型能够识别的潜在模式的数量很大，而训练数据只构成了算法在未来执行任务时可能遇到的更大现象集合的样本。
- en: The infinite number of functions that could have generated the observed outputs
    from the given input makes the search process for the true function impossible,
    without restricting the eligible set of candidates. The types of patterns that
    an algorithm is capable of learning are limited by the size of its **hypothesis
    space** that contains the functions it can possibly represent. It is also limited
    by the amount of information provided by the sample data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 无限数量的函数可能会从给定输入生成观察到的输出，这使得搜索真实函数的过程成为不可能，除非限制符合条件的候选集。算法能够学习的模式类型受其包含可能表示的函数的**假设空间**大小以及样本数据提供的信息量的限制。
- en: The size of the hypothesis space varies significantly between algorithms, as
    we will see in the following chapters. On the one hand, this limitation enables
    a successful search, and on the other hand, it implies an inductive bias that
    may lead to poor performance when the algorithm generalizes from the training
    sample to new data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设空间的大小在各种算法之间变化很大，我们将在接下来的章节中看到。一方面，这种限制使得成功搜索成为可能，另一方面，它意味着一种归纳偏差，可能导致算法从训练样本泛化到新数据时性能不佳。
- en: Hence, the key challenge becomes how to choose a model with a hypothesis space
    large enough to contain a solution to the learning problem, yet small enough to
    ensure reliable learning and generalization given the size of the training data.
    With more informative data, a model with a larger hypothesis space has a better
    chance of being successful.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，关键挑战在于如何选择一个具有足够大的假设空间的模型，以包含对学习问题的解决方案，同时又足够小，以确保给定训练数据的可靠学习和概括性。有了更多信息的数据，具有更大假设空间的模型成功的机会更大。
- en: The **no-free-lunch theorem** states that there is no universal learning algorithm.
    Instead, a learner's hypothesis space has to be tailored to a specific task using
    prior knowledge about the task domain in order for the search for meaningful patterns
    that generalize well to succeed (Gómez and Rojas 2015).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**无免费午餐定理**表明没有通用的学习算法。相反，学习者的假设空间必须根据任务领域的先验知识进行定制，以便搜索出能够成功概括的有意义模式（Gómez
    and Rojas 2015）。'
- en: We will pay close attention to the assumptions that a model makes about data
    relationships for a specific task throughout this chapter and emphasize the importance
    of matching these assumptions with empirical evidence gleaned from data exploration.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中密切关注模型对特定任务的数据关系所作的假设，并强调将这些假设与从数据探索中获得的经验证据相匹配的重要性。
- en: There are several categories of machine learning tasks that differ by purpose,
    available information, and, consequently, the learning process itself. The main
    categories are supervised, unsupervised, and reinforcement learning, and we will
    review their key differences next.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习任务有几个类别，其目的、可用信息以及因此学习过程本身有所不同。主要类别包括监督、无监督和强化学习，接下来我们将审查它们的关键区别。
- en: Supervised learning – teaching by example
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习——通过示例教学
- en: '**Supervised learning** is the most commonly used type of ML. We will dedicate
    most of the chapters in this book to applications in this category. The term *supervised*
    implies the presence of an outcome variable that guides the learning process—that
    is, it teaches the algorithm the correct solution to the task at hand. Supervised
    learning aims to capture a functional input-output relationship from individual
    samples that reflect this relationship and to apply its learning by making valid
    statements about new data.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**是最常用的ML类型。我们将在本书的大部分章节中致力于这一类别的应用。术语*监督*意味着存在一个结果变量来引导学习过程——也就是说，它教会了算法对手头任务的正确解决方案。监督学习旨在从反映这种关系的个体样本中捕获功能输入-输出关系，并通过对新数据做出有效的陈述来应用其学习。'
- en: Depending on the field, the output variable is also interchangeably called the
    label, target, or outcome, as well as the endogenous or left-hand side variable.
    We will use *y*[i] for outcome observations *i* = 1, ..., *N*, or *y* for a (column)
    vector of outcomes. Some tasks come with several outcomes and are called **multilabel
    problems**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据领域的不同，输出变量也可以互换地称为标签、目标或结果，以及内生或左侧变量。我们将使用*y*[i]表示结果观测值*i*=1，...，*N*，或者*y*表示（列）向量的结果。一些任务具有多个结果，并称为**多标签问题**。
- en: The input data for a supervised learning problem is also known as features,
    as well as exogenous or right-hand side variables. We use *x*[i] for a vector
    of features with observations *i* = 1, ..., *N*, or *X* in matrix notation, where
    each column contains a feature and each row an observation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习问题的输入数据也称为特征，以及外生或右侧变量。我们使用*x*[i]表示观测值*i*=1，...，*N*的特征向量，或者在矩阵表示中表示为*X*，其中每列包含一个特征，每行包含一个观测值。
- en: The solution to a supervised learning problem is a function ![](img/B15439_06_001.png)
    that represents what the model learned about the input-output relationship from
    the sample and approximates the true relationship, represented by ![](img/B15439_06_002.png).
    This function can potentially be used to infer statistical associations or even
    causal relationships among variables of interest beyond the sample, or it can
    be used to predict outputs for new input data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习问题的解决方案是一个函数 ![](img/B15439_06_001.png)，表示模型从样本中学到的输入-输出关系，并逼近真实关系，表示为 ![](img/B15439_06_002.png)。这个函数可以潜在地用于推断变量之间的统计关联或甚至因果关系，超出样本范围，或者可以用于预测新输入数据的输出。
- en: The task of learning an input-outcome relationship from data that permits accurate
    predictions of outcomes for new inputs faces important trade-offs. More complex
    models have more moving parts that are capable of representing more nuanced relationships.
    However, they are also more likely to learn random noise particular to the training
    sample, as opposed to a systematic signal that represents a general pattern. When
    this happens, we say the model is **overfitting** to the training data. In addition,
    complex models may also be more difficult to inspect, making it more difficult
    to understand the nature of the learned relationship or the drivers of specific
    predictions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中学习输入-输出关系以便对新输入进行准确预测的任务面临重要的权衡。更复杂的模型具有更多的可移动部分，能够表示更微妙的关系。然而，它们也更有可能学习到特定于训练样本的随机噪声，而不是代表一般模式的系统信号。当这种情况发生时，我们称模型对训练数据**过度拟合**。此外，复杂模型可能也更难以检查，这使得理解学到的关系的性质或特定预测的驱动因素变得更加困难。
- en: Overly simple models, on the other hand, will miss complex signals and deliver
    biased results. This trade-off is known as the **bias-variance trade-off** in
    supervised learning, but conceptually, this also applies to the other forms of
    ML where too simple or too complex models may perform poorly beyond the training
    data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，过于简单的模型将错过复杂的信号并提供有偏见的结果。这种权衡在监督学习中被称为**偏差-方差权衡**，但从概念上讲，这也适用于其他形式的机器学习，即太简单或太复杂的模型可能在训练数据之外表现不佳。
- en: Unsupervised learning – uncovering useful patterns
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习-发现有用的模式
- en: When solving an **unsupervised learning** problem, we only observe the features
    and have no measurements of the outcome. Instead of predicting future outcomes
    or inferring relationships among variables, unsupervised algorithms aim to identify
    structure in the input that permits a new representation of the information contained
    in the data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当解决**无监督学习**问题时，我们只观察特征，并没有对结果进行测量。 无监督算法的目标不是预测未来结果或推断变量之间的关系，而是旨在识别输入中的结构，从而允许对数据中包含的信息进行新的表示。
- en: Frequently, the measure of success is the contribution of the result to the
    solution of some other problem. This includes identifying commonalities, or clusters,
    among observations, or transforming features to obtain a compressed summary that
    captures relevant information.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 经常，成功的衡量标准是结果对解决其他问题的贡献。这包括识别观察之间的共同点或群集，或者转换特征以获得捕获相关信息的压缩摘要。
- en: The key challenge is that unsupervised algorithms have to accomplish their mission
    without the guidance provided by outcome information. As a consequence, we are
    often unable to evaluate the result against a ground truth as in the supervised
    case, and its quality may be in the eye of the beholder. However, sometimes, we
    can evaluate its contribution to a downstream task, for example when dimensionality
    reduction enables better predictions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 关键挑战在于无监督算法必须在没有结果信息提供的情况下完成任务。因此，我们通常无法像在监督情况下那样将结果与基本事实进行评估，其质量可能视人而异。但是，有时，我们可以评估其对下游任务的贡献，例如降维使得更好的预测成为可能。
- en: There are numerous approaches, from well-established cluster algorithms to cutting-edge
    deep learning models, and several relevant use cases for our purposes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法，从成熟的聚类算法到前沿的深度学习模型，以及几个与我们目的相关的使用案例。
- en: Use cases – from risk management to text processing
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用案例-从风险管理到文本处理
- en: 'There are numerous trading use cases for unsupervised learning that we will
    cover in later chapters:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续章节中，我们将涵盖许多无监督学习的交易使用案例：
- en: Grouping securities with similar risk and return characteristics (see **hierarchical
    risk parity** in *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation
    with Unsupervised Learning*)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将具有相似风险和回报特征的证券分组（请参阅*第13章*，*使用无监督学习的数据驱动风险因子和资产配置中的分层风险均等化*）
- en: Finding a small number of risk factors driving the performance of a much larger
    number of securities using **principal component analysis** (*Chapter 13*, *Data-Driven
    Risk Factors and Asset Allocation with Unsupervised Learning*) or **autoencoders**
    (*Chapter 19*, *RNN for Multivariate Time Series and Sentiment Analysis*)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**主成分分析**（*第13章*，*使用无监督学习的数据驱动风险因子和资产配置*）或**自编码器**（*第19章*，*用于多变量时间序列和情感分析的RNN*）找出驱动大量证券表现的少量风险因子。
- en: Identifying latent topics in a body of documents (for example, earnings call
    transcripts) that comprise the most important aspects of those documents (*Chapter
    14*, *Text Data for Trading – Sentiment Analysis*)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别文档集合（例如，收入电话会议记录）中包含的最重要方面的潜在主题（*第14章*，*用于交易的文本数据 - 情感分析*）
- en: At a high level, these applications rely on methods to identify clusters and
    methods to reduce the dimensionality of the data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，这些应用依赖于识别聚类的方法和降低数据维度的方法。
- en: Cluster algorithms – seeking similar observations
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类算法 - 寻找相似观察
- en: Cluster algorithms apply a concept of similarity to identify observations or
    data attributes that contain comparable information. They summarize a dataset
    by assigning a large number of data points to a smaller number of clusters. They
    do this so that the cluster members are more closely related to each other than
    to members of other clusters.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法应用相似性概念来识别包含可比较信息的观察或数据属性。它们通过将大量数据点分配给较少数量的聚类来总结数据集。它们这样做是为了使聚类成员彼此之间的关系比与其他聚类成员的关系更密切。
- en: 'Cluster algorithms differ in what they assume about how the various groupings
    were generated and what makes them alike. As a result, they tend to produce alternative
    types of clusters and should thus be selected based on the characteristics of
    the data. Some prominent examples are:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法在假设关于各种分组是如何生成以及是什么使它们相似方面有所不同。因此，它们倾向于产生不同类型的聚类，因此应根据数据的特性选择。一些著名的例子包括：
- en: '**K-means clustering**: Data points belong to one of the *k* clusters of equal
    size that take an elliptical form.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K均值聚类**：数据点属于等大小的*k*个椭圆形簇之一。'
- en: '**Gaussian mixture models**: Data points have been generated by any of the
    various multivariate normal distributions.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯混合模型**：数据点由各种多元正态分布之一生成。'
- en: '**Density-based clusters**: Clusters are of arbitrary shape and defined only
    by the existence of a minimum number of nearby data points.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于密度的聚类**：聚类可以是任意形状的，并且仅由附近的最小数量的数据点的存在定义。'
- en: '**Hierarchical clusters**: Data points belong to various supersets of groups
    that are formed by successively merging smaller clusters.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分层聚类**：数据点属于由逐渐合并较小簇形成的各种超集。'
- en: Dimensionality reduction – compressing information
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降维 - 压缩信息
- en: '**Dimensionality reduction** produces new data that captures the most important
    information contained in the source data. Rather than grouping data into clusters
    while retaining the original data, these algorithms transform the data with the
    goal of using fewer features to represent the original information.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**降维**生成包含源数据中最重要信息的新数据。这些算法不是将数据分组到保留原始数据的簇中，而是以使用更少特征来表示原始信息的目标来转换数据。'
- en: 'Algorithms differ with respect to how they transform data and, thus, the nature
    of the resulting compressed dataset, as shown in the following list:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在数据转换方式及因此产生的压缩数据集的性质上有所不同，如下列表所示：
- en: '**Principal component analysis (PCA)**: Finds the linear transformation that
    captures most of the variance in the existing dataset'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析（PCA）**：找到线性转换，捕获现有数据集中大部分方差'
- en: '**Manifold learning**: Identifies a nonlinear transformation that yields a
    lower-dimensional representation of the data'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流形学习**：识别出产生数据的低维表示的非线性变换'
- en: '**Autoencoders**: Uses a neural network to compress data nonlinearly with minimal
    loss of information'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动编码器**：使用神经网络对数据进行非线性压缩，最小化信息损失。'
- en: We will dive deeper into these unsupervised learning models in several of the
    following chapters, including important applications to **natural language processing**
    (**NLP**) in the form of topic modeling and Word2vec feature extraction.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下几章中，我们将更深入地研究这些无监督学习模型，包括对**自然语言处理**（**NLP**）的重要应用，例如主题建模和Word2vec特征提取。
- en: Reinforcement learning – learning by trial and error
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习 - 通过试错学习
- en: '**Reinforcement learning** (**RL**) is the third type of ML. It centers on
    an agent that needs to pick an action at each time step, based on information
    provided by the environment. The agent could be a self-driving car, a program
    playing a board game or a video game, or a trading strategy operating in a certain
    security market. You find an excellent introduction in *Sutton and Barto (2018)*.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）是ML的第三种类型。它以代理为中心，代理需要在每个时间步选择一个动作，这是基于环境提供的信息。代理可以是自动驾驶汽车、玩棋盘游戏或视频游戏的程序，或者在某个安全市场上运行的交易策略。您可以在*Sutton和Barto（2018）*中找到一篇优秀的介绍。'
- en: 'The agent aims to choose the action that yields the highest reward over time,
    based on a set of observations that describes the current state of the environment.
    It is both dynamic and interactive: the stream of positive and negative rewards
    impacts the algorithm''s learning, and actions taken now may influence both the
    environment and future rewards.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的目标是选择随着时间推移产生最高回报的行动，这是基于一组描述环境当前状态的观察而进行的。它既是动态的又是交互式的：积极和消极奖励的流动影响着算法的学习，而现在采取的行动可能会影响环境和未来的奖励。
- en: The agent needs to take action right from start and learns in an "online" fashion,
    one example at a time as it goes along. The learning process follows a trial-and-error
    approach. This is because the agent needs to manage the trade-off between exploiting
    a course of action that has yielded a certain reward in the past and exploring
    new actions that may increase the reward in the future. RL algorithms optimize
    the agent's learning using dynamical systems theory and, in particular, the optimal
    control of Markov decision processes with incomplete information.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 代理需要从一开始就采取行动，并以“在线”方式学习，随着时间的推移，一次一个样本。学习过程采用试错方法。这是因为代理需要在利用过去产生某种奖励的行动和探索可能增加未来奖励的新行动之间进行权衡。RL算法使用动态系统理论以及具有不完整信息的马尔可夫决策过程的最优控制来优化代理的学习。
- en: RL differs from supervised learning, where the training data lays out both the
    context and the correct decision for the algorithm. It is tailored to interactive
    settings where the outcomes only become available over time and learning must
    proceed in a continuous fashion as the agent acquires new experience.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: RL与监督学习不同，监督学习的训练数据为算法提供了上下文和正确决策。它专门针对交互式设置，其中结果只在一段时间后变得可用，并且学习必须随着代理获取新经验而持续进行。
- en: However, some of the most notable progress in **artificial intelligence** (**AI**)
    involves RL, which uses deep learning to approximate functional relationships
    between actions, environments, and future rewards. It also differs from unsupervised
    learning because feedback on the actions will be available, albeit with a delay.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能**（**AI**）中一些最显著的进展涉及强化学习（RL），它使用深度学习来逼近动作、环境和未来奖励之间的功能关系。它与无监督学习不同，因为尽管有延迟，但会提供对动作的反馈。'
- en: RL is particularly suitable for algorithmic trading because the model of a return-maximizing
    agent in an uncertain, dynamic environment has much in common with an investor
    or a trading strategy that interacts with financial markets. We will introduce
    RL approaches to building an algorithmic trading strategy in *Chapter 21*, *Generative
    Adversarial Networks for Synthetic Time-Series Data*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: RL特别适用于算法交易，因为在不确定的、动态的环境中，追求回报的代理模型与与金融市场互动的投资者或交易策略有很多共同之处。我们将在*第21章*中介绍RL方法来构建算法交易策略，*生成对抗网络用于合成时间序列数据*。
- en: The machine learning workflow
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工作流程
- en: Developing an ML solution for an algorithmic trading strategy requires a systematic
    approach to maximize the chances of success while economizing on resources. It
    is also very important to make the process transparent and replicable in order
    to facilitate collaboration, maintenance, and later refinements.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大程度地提高成功的机会并节约资源，为算法交易策略开发ML解决方案需要系统的方法。使过程透明且可复制非常重要，以促进协作、维护和后续的改进。
- en: 'The following chart outlines the key steps, from problem definition to the
    deployment of a predictive solution:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表概述了从问题定义到部署预测解决方案的关键步骤：
- en: '![](img/B15439_06_01.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_01.png)'
- en: 'Figure 6.1: Key steps of the machine learning workflow'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：机器学习工作流程的关键步骤
- en: 'The process is iterative throughout, and the effort required at different stages
    will vary according to the project. Generally, however, this process should include
    the following steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Frame the problem, identify a target metric, and define success.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Source, clean, and validate the data.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understand your data and generate informative features.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick one or more machine learning algorithms suitable for your data.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train, test, and tune your models.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your model to solve the original problem.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will walk through these steps in the following sections using a simple example
    to illustrate some of the key points.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Basic walkthrough – k-nearest neighbors
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `machine_learning_workflow.ipynb` notebook in this chapter's folder of this
    book's GitHub repository contains several examples that illustrate the machine
    learning workflow using a dataset of house prices.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: We will use the fairly straightforward **k-nearest neighbors** (**KNN**) algorithm,
    which allows us to tackle both regression and classification problems. In its
    default scikit-learn implementation, it identifies the *k* nearest data points
    (based on the Euclidean distance) to make a prediction. It predicts the most frequent
    class among the neighbors or the average outcome in the classification or regression
    case, respectively.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: The `README` for this chapter on GitHub links to additional resources; see Bhatia
    and Vandana (2010) for a brief survey.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Framing the problem – from goals to metrics
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The starting point for any machine learning project is the use case it ultimately
    aims to address. Sometimes, this goal will be statistical inference in order to
    identify an association or even a causal relationship between variables. Most
    frequently, however, the goal will be the prediction of an outcome to yield a
    trading signal.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Both inference and prediction tasks rely on metrics to evaluate how well a model
    achieves its objective. Due to their prominence in practice, we will focus on
    common objective functions and the corresponding error metrics for predictive
    models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'We distinguish prediction tasks by the nature of the output: a continuous output
    variable poses a **regression** problem, a categorical variable implies **classification**,
    and the special case of ordered categorical variables represents a **ranking**
    problem.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: You can often frame a given problem in different ways. The task at hand may
    be how to efficiently combine several alpha factors. You could frame this task
    as a regression problem that aims to predict returns, a binary classification
    problem that aims to predict the direction of future price movements, or a multiclass
    problem that aims to assign stocks to various performance classes such as return
    quintiles.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will introduce these objectives and look at how
    to measure and interpret related error metrics.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Prediction versus inference
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The functional relationship produced by a supervised learning algorithm can
    be used for inference—that is, to gain insights into how the outcomes are generated.
    Alternatively, you can use it to predict outputs for unknown inputs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法产生的功能关系可用于推断，即了解结果生成的方式。或者，您可以用它来预测未知输入的输出。
- en: For algorithmic trading, we can use inference to estimate the statistical association
    of the returns of an asset with a risk factor. This implies, for instance, assessing
    how likely this observation is due to noise, as opposed to an actual influence
    of the risk factor. Prediction, in turn, can be used to forecast the risk factor,
    which can help predict the asset return and price and be translated into a trading
    signal.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于算法交易，我们可以使用推断来估计资产回报与风险因子的统计关联。这意味着，例如，评估此观察是否可能由噪声引起，而不是风险因子的实际影响。反过来，预测可以用来预测风险因子，这可以帮助预测资产回报和价格，并转化为交易信号。
- en: Statistical inference is about drawing conclusions from sample data about the
    parameters of the underlying probability distribution or the population. Potential
    conclusions include hypothesis tests about the characteristics of the distribution
    of an individual variable, or the existence or strength of numerical relationships
    among variables. They also include the point or interval estimates of metrics.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 统计推断是关于从样本数据中得出关于潜在概率分布或总体参数的结论。可能的结论包括关于个体变量分布特征的假设检验，或者关于变量之间的数值关系的存在或强度的假设检验。它们还包括指标的点估计或区间估计。
- en: Inference depends on the assumptions about the process that originally generated
    the data. We will review these assumptions and the tools that are used for inference
    with linear models where they are well established. More complex models make fewer
    assumptions about the structural relationship between input and output. Instead,
    they approach the task of function approximation with fewer restrictions, while
    treating the data-generating process as a black box.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 推断取决于关于生成数据的过程的假设。我们将回顾这些假设以及在线性模型中用于推断的工具，在那里它们已经得到了很好的确认。更复杂的模型对输入和输出之间的结构关系做出更少的假设。相反，它们以较少的限制处理函数逼近任务，同时将数据生成过程视为黑匣子。
- en: These models, including decision trees, ensemble models, and neural networks,
    have gained in popularity because they often outperform on prediction tasks. However,
    we will see that there have been numerous recent efforts to increase the transparency
    of complex models. Random forests, for example, have recently gained a framework
    for statistical inference (Wager and Athey 2019).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型，包括决策树、集成模型和神经网络，因其在预测任务上的表现通常优于其他模型而备受青睐。然而，我们将看到，近年来已经有大量努力增加复杂模型的透明度。例如，随机森林最近获得了一个用于统计推断的框架（Wager和Athey
    2019）。
- en: Causal inference – correlation does not imply causation
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 因果推断 - 相关性不意味着因果关系
- en: Causal inference aims to identify relationships where certain input values imply
    certain outputs—for example, a certain constellation of macro variables causing
    the price of a given asset to move in a certain way, while assuming all other
    variables remain constant.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因果推断旨在识别输入值导致特定输出的关系 - 例如，一定的宏观变量组合导致给定资产价格以某种方式变动，同时假设所有其他变量保持不变。
- en: Statistical inference about relationships among two or more variables produces
    measures of correlation. Correlation can only be interpreted as a causal relationship
    when several other conditions are met—for example, when alternative explanations
    or reverse causality has been ruled out.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 关于两个或更多变量之间的关系的统计推断会产生相关性的度量。只有在满足其他几个条件时，相关性才能被解释为因果关系，例如当排除了替代解释或逆向因果关系时。
- en: Meeting these conditions requires an experimental setting where all relevant
    variables of interest can be fully controlled to isolate causal relationships.
    Alternatively, quasi-experimental settings expose units of observations to changes
    in inputs in a randomized way. It does this to rule out that other observable
    or unobservable features are responsible for the observed effects of the change
    in the environment.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 满足这些条件需要一个实验设置，在这个设置中，所有感兴趣的变量可以完全控制，以隔离因果关系。或者，准实验设置以随机方式将观察单位暴露于输入变化中，以排除其他可观察或不可观察的特征对环境变化观察效果的影响。
- en: These conditions are rarely met, so inferential conclusions need to be treated
    with care. The same applies to the performance of predictive models that also
    rely on the statistical association between features and outputs, which may change
    with other factors that are not part of the model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The non-parametric nature of the KNN model does not lend itself well to inference,
    so we'll postpone this step in the workflow until we encounter linear models in
    *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Regression – popular loss functions and error metrics
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regression problems aim to predict a continuous variable. The **root-mean-square
    error** (**RMSE**) is the most popular loss function and error metric, not least
    because it is differentiable. The loss is symmetric, but larger errors weigh more
    in the calculation. Using the square root has the advantage that we can measure
    the error in the units of the target variable.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: The **root-mean-square of the log of the error** (**RMSLE**) is appropriate
    when the target is subject to exponential growth. Its asymmetric penalty weighs
    negative errors less than positive errors. You can also log-transform the target
    prior to training the model and then use the RMSE, as we'll do in the example
    later in this section.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: The **mean of the absolute errors** (**MAE**) and **median of the absolute errors**
    (**MedAE**) are symmetric but do not give more weight to larger errors. The MedAE
    is robust to outliers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The **explained variance score** computes the proportion of the target variance
    that the model accounts for and varies between 0 and 1\. The **R2 score** is also
    called the coefficient of determination and yields the same outcome if the mean
    of the residuals is 0, but can differ otherwise. In particular, it can be negative
    when calculated on out-of-sample data (or for a linear regression without intercept).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table defines the formulas used for calculation and the corresponding
    scikit-learn function that can be imported from the metrics module. The `scoring`
    parameter is used in combination with automated train-test functions (such as
    `cross_val_score` and `GridSearchCV`), which we''ll will introduce later in this
    section, and which are illustrated in the accompanying notebook:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Formula | scikit-learn function | Scoring parameter |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| Mean squared error | ![](img/B15439_06_003.png) | `mean_squared_error` |
    `neg_mean_squared_error` |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| Mean squared log error | ![](img/B15439_06_004.png) | `mean_squared_log_error`
    | `neg_mean_squared_log_error` |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| Mean absolute error | ![](img/B15439_06_005.png) | `mean_absolute_error`
    | `neg_mean_absolute_error` |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| Median absolute error | ![](img/B15439_06_006.png) | `median_absolute_error`
    | `neg_median_absolute_error` |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| Explained variance | ![](img/B15439_06_007.png) | `explained_variance_score`
    | `explained_variance` |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: '| R² score | ![](img/B15439_06_008.png) | `r2_score` | `r2` |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '*Figure 6.2* shows the various error metrics for the house price regression
    that we''ll compute in the notebook:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.2* 展示了我们将在笔记本中计算的房价回归的各种错误度量：'
- en: '![](img/B15439_06_02.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_02.png)'
- en: 'Figure 6.2: In-sample regression errors'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：样本内回归误差
- en: The `sklearn` function also supports multilabel evaluation—that is, assigning
    multiple outcome values to a single observation; see the documentation referenced
    on GitHub for more details.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn` 函数还支持多标签评估——即，将多个结果值分配给单个观测值；有关更多详细信息，请参阅 GitHub 上引用的文档。'
- en: Classification – making sense of the confusion matrix
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类 – 理解混淆矩阵的含义
- en: Classification problems have categorical outcome variables. Most predictors
    will output a score to indicate whether an observation belongs to a certain class.
    In the second step, these scores are then translated into actual predictions using
    a threshold value.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题具有分类结果变量。大多数预测器将输出一个分数，以指示观察是否属于某一类别。在第二步中，这些分数然后被转换为实际预测，使用一个阈值值。
- en: In the binary case, with a positive and a negative class label, the score typically
    varies between zero and one or is normalized accordingly. Once the scores are
    converted into predictions of one class or the other, there can be four outcomes,
    since each of the two classes can be either correctly or incorrectly predicted.
    With more than two classes, there can be more cases if you differentiate between
    the several potential mistakes.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元情况下，具有正类标签和负类标签，得分通常在零和一之间变化，或者相应地进行归一化。一旦将得分转换为对其中一类的预测，就会产生四种结果，因为两个类中的每一个都可以被正确或错误地预测。如果区分几种潜在的错误，那么在两个以上的类别中可能会有更多的情况。
- en: All error metrics are computed from the breakdown of predictions across the
    four fields of the 2×2 confusion matrix that associates actual and predicted classes.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 所有错误度量都是从 2×2 混淆矩阵的四个字段的预测分解中计算的，该矩阵将实际类别和预测类别相关联。
- en: 'The metrics listed in the following table, such as accuracy, evaluate a model
    for a given threshold:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 下表中列出的度量，如准确性，评估了给定阈值下的模型：
- en: '![](img/B15439_06_03.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_03.png)'
- en: 'Figure 6.3: Confusion matrix and related error metrics'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3：混淆矩阵和相关的错误度量
- en: The classifier usually doesn't output calibrated probabilities. Instead, the
    threshold used to distinguish positive from negative cases is itself a decision
    variable that should be optimized, taking into account the costs and benefits
    of correct and incorrect predictions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器通常不会输出校准的概率。相反，用于区分正负情况的阈值本身是一个决策变量，应该进行优化，考虑到正确和错误预测的成本和效益。
- en: All things equal, a lower threshold tends to imply more positive predictions,
    with a potentially rising false positive rate, whereas for a higher threshold,
    the opposite is likely to be true.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一切相等的情况下，较低的阈值往往意味着更多的正预测，可能会导致假阳性率上升，而对于较高的阈值，相反的可能是真的。
- en: Receiver operating characteristics the area under the curve
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 接收器操作特性曲线下的面积
- en: The **receiver operating characteristics** (**ROC**) curve allows us to visualize,
    compare, and select classifiers based on their performance. It computes the pairs
    of **true positive rates** (**TPR**) and **false positive rates** (**FPR**) that
    result from using all predicted scores as a threshold to produce class predictions.
    It visualizes these pairs inside a square with unit side length.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收器操作特性** (**ROC**) 曲线允许我们根据它们的性能可视化、比较和选择分类器。它计算了使用所有预测分数作为阈值产生类预测的**真正阳性率**
    (**TPR**) 和**假阳性率** (**FPR**) 的对。它在一个边长为单位的正方形内可视化这些对。'
- en: Random predictions (weighted to take into account class imbalance), on average,
    yield equal TPR and FPR that appear on the diagonal, which becomes the benchmark
    case. Since an underperforming classifier would benefit from relabeling the predictions,
    this benchmark also becomes the minimum.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 随机预测（根据类别不平衡加权），平均而言，产生相等的 TPR 和 FPR，这些都出现在对角线上，这成为基准案例。由于性能不佳的分类器会从重新标记预测中受益，因此该基准也成为最小值。
- en: The **area under the curve** (**AUC**) is defined as the area under the ROC
    plot that varies between 0.5 and the maximum of 1\. It is a summary measure of
    how well the classifier's scores are able to rank data points with respect to
    their class membership. More specifically, the AUC of a classifier has the important
    statistical property of representing the probability that the classifier will
    rank a randomly chosen positive instance higher than a randomly chosen negative
    instance, which is equivalent to the Wilcoxon ranking test (Fawcett 2006). In
    addition, the AUC has the benefit of not being sensitive to class imbalances.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**曲线下面积**（AUC）被定义为 ROC 图下的面积，其值在 0.5 和最大值 1 之间变化。它是分类器分数能够根据其类别成员资格对数据点进行排名的摘要度量。更具体地说，分类器的
    AUC 具有重要的统计属性，表示分类器将随机选择的正实例排在随机选择的负实例之上的概率，这相当于 Wilcoxon 排名检验（Fawcett 2006）。此外，AUC
    具有不对类别不平衡敏感的好处。'
- en: Precision-recall curves – zooming in on one class
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精确率-召回率曲线 - 放大一个类
- en: 'When predictions for one of the classes are of particular interest, precision
    and recall curves visualize the trade-off between these error metrics for different
    thresholds. Both measures evaluate the quality of predictions for a particular
    class. The following list shows how they are applied to the positive class:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当对其中一个类的预测特别感兴趣时，精确率和召回率曲线可视化了这些误差指标在不同阈值下的权衡。这两个指标评估了特定类别的预测质量。以下列表显示了它们如何应用于正类别：
- en: '**Recall** measures the share of actual positive class members that a classifier
    predicts as positive for a given threshold. It originates from information retrieval
    and measures the share of relevant documents successfully identified by a search
    algorithm.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**（Recall）衡量分类器预测为正例的实际正类成员所占比例，针对给定的阈值。它源自信息检索，并且衡量了搜索算法成功识别的相关文档的比例。'
- en: '**Precision**, in contrast, measures the share of positive predictions that
    are correct.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确率**（Precision），相反地，衡量了正确的正预测所占的比例。'
- en: Recall typically increases with a lower threshold, but precision may decrease.
    Precision-recall curves visualize the attainable combinations and allow for the
    optimization of the threshold, given the costs and benefits of missing a lot of
    relevant cases or producing lower-quality predictions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率通常随着较低的阈值而增加，但精确率可能会降低。精确率-召回率曲线可视化了可达到的组合，并允许根据错过大量相关情况或产生质量较低的预测的成本和收益来优化阈值。
- en: The **F1 score** is a harmonic mean of precision and recall for a given threshold,
    and can be used to numerically optimize the threshold, all while taking into account
    the relative weights that these two metrics should assume.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**F1 分数**是给定阈值下精确率和召回率的调和平均数，并且可以用于数值优化阈值，同时考虑到这两个指标应该承担的相对权重。'
- en: '*Figure 6.4* illustrates the ROC curve and corresponding AUC, alongside the
    precision-recall curve and the F1 score, which, using equal weights for precision
    and recall, yields an optimal threshold of 0.37\. The chart has been taken from
    the accompanying notebook, where you can find the code for the KNN classifier
    that operates on binarized housing prices:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.4* 展示了 ROC 曲线及相应的 AUC，以及精确率-召回率曲线和 F1 分数，使用精确率和召回率的相等权重，得出了 0.37 的最佳阈值。该图表摘自附带的笔记本，您可以在其中找到针对二值化房价操作的
    KNN 分类器的代码：'
- en: '![](img/B15439_06_04.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_04.png)'
- en: 'Figure 6.4: Receiver-Operating Characteristics, Precision-Recall Curve, and
    F1 Scores charts'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：接收器操作特征（ROC）、精确率-召回率曲线和 F1 分数图
- en: Collecting and preparing the data
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集和准备数据
- en: We already addressed important aspects of how to source market, fundamental,
    and alternative data in *Chapter 2*, *Market and Fundamental Data – Sources and
    Techniques,* and *Chapter 3*, *Alternative Data for Finance – Categories and Use
    Cases*. We will continue to work with various examples of these sources as we
    illustrate the application of the various models.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在*第 2 章*，*市场和基础数据 - 来源和技术*和*第 3 章*，*金融替代数据 - 类别和用例*中讨论了如何获取市场、基础和替代数据的重要方面。我们将继续使用这些来源的各种示例，以展示各种模型的应用。
- en: In addition to market and fundamental data, we will also acquire and transform
    text data as we explore natural language processing and image data when we look
    at image processing and recognition. Besides obtaining, cleaning, and validating
    the data, we may need to assign labels such as sentiment for news articles or
    timestamps to align it with trading data typically available in a time-series
    format.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 除了市场和基本数据外，我们还将获取和转换文本数据，当我们探索自然语言处理时，获取图像数据并进行图像处理和识别时。除了获取、清洗和验证数据外，我们还可能需要分配标签，如新闻文章的情感或时间戳，以使其与通常以时间序列格式可用的交易数据对齐。
- en: It is also important to store it in a format that enables quick exploration
    and iteration. We recommend the HDF and parquet formats (see *Chapter 2*, *Market
    and Fundamental Data – Sources and Techniques*). For data that does not fit into
    memory and requires distributed processing on several machines, Apache Spark is
    often the best solution for interactive analysis and machine learning.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 将其存储在能够快速探索和迭代的格式中也很重要。我们推荐HDF和parquet格式（参见*第2章*，*市场和基本数据-来源和技术*）。对于不适合内存且需要在多台机器上进行分布式处理的数据，Apache
    Spark通常是交互式分析和机器学习的最佳解决方案。
- en: Exploring, extracting, and engineering features
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索、提取和工程化特征
- en: 'Understanding the distribution of individual variables and the relationships
    among outcomes and features is the basis for picking a suitable algorithm. This
    typically starts with **visualizations** such as scatter plots, as illustrated
    in the accompanying notebook and shown in *Figure 6.5*:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 理解单个变量的分布以及结果和特征之间的关系是选择适当算法的基础。这通常从诸如散点图的**可视化**开始，如附带的笔记本中所示并在*图6.5*中显示：
- en: '![](img/B15439_06_05.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_05.png)'
- en: 'Figure 6.5: Pairwise scatter plots of outcome and features'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：结果和特征的成对散点图
- en: It also includes **numerical evaluations** ranging from linear metrics like
    correlation to nonlinear statistics, such as the Spearman rank correlation coefficient
    that we encountered when we introduced the information coefficient in *Chapter
    4*, *Financial Feature Engineering – How to Research Alpha Factors*. There are
    also information-theoretic measures, such as mutual information, which we'll illustrate
    in the next subsection.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 它还包括从线性指标如相关性到非线性统计量如我们在*第4章*中介绍的信息系数时遇到的Spearman等级相关系数的**数值评估**。还有信息论量度，如互信息，我们将在下一小节中进行说明。
- en: 'A systematic exploratory analysis is also the basis of what is often the single
    most important ingredient of a successful predictive model: the **engineering
    of features** that extract information contained in the data, but which are not
    necessarily accessible to the algorithm in their raw form. Feature engineering
    benefits from domain expertise, the application of statistics and information
    theory, and creativity.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 系统性的探索性分析也是成功预测模型中往往最重要的一个组成部分的基础：**特征工程**，它提取了数据中包含的信息，但这些信息在原始形式下不一定对算法可见。特征工程受益于领域专业知识、统计学和信息论的应用，以及创造力。
- en: It relies on smart data transformations that effectively tease out the systematic
    relationship between input and output data. There are many choices that include
    outlier detection and treatment, functional transformations, and the combination
    of several variables, including unsupervised learning. We will illustrate examples
    throughout, but will emphasize that this central aspect of the ML workflow is
    best learned through experience. Kaggle is a great place to learn from other data
    scientists who share their experiences with the community.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 它依赖于智能数据转换，可以有效地揭示输入和输出数据之间的系统关系。有许多选择，包括异常值检测和处理、功能转换以及多个变量的组合，包括无监督学习。我们将在整个过程中举例说明，但会强调这个ML工作流程的核心方面最好通过经验来学习。Kaggle是一个与社区分享经验的其他数据科学家的好地方。
- en: Using information theory to evaluate features
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用信息论评估特征
- en: The **mutual information** (**MI**) between a feature and the outcome is a measure
    of the mutual dependence between the two variables. It extends the notion of correlation
    to nonlinear relationships. More specifically, it quantifies the information obtained
    about one random variable through the other random variable.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 特征和结果之间的**互信息**（**MI**）是两个变量之间相互依赖的一种度量。它将相关性的概念扩展到非线性关系。更具体地说，它量化了通过另一个随机变量获得的有关某一随机变量的信息。
- en: 'The concept of MI is closely related to the fundamental notion of entropy of
    a random variable. Entropy quantifies the amount of information contained in a
    random variable. Formally, the mutual information—*I*(*X*, *Y*)—of two random
    variables, *X* and *Y*, is defined as the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_06_009.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: The sklearn function implements `feature_selection.mutual_info_regression`,
    which computes the mutual information between all features and a continuous outcome
    to select the features that are most likely to contain predictive information.
    There is also a classification version (see the sklearn documentation for more
    details). The `mutual_information.ipynb` notebook contains an application for
    the financial data we created in *Chapter 4*, *Financial Feature Engineering –
    How to Research Alpha Factors*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Selecting an ML algorithm
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The remainder of this book will introduce several model families, ranging from
    linear models, which make fairly strong assumptions about the nature of the functional
    relationship between input and output variables, to deep neural networks, which
    make very few assumptions. As mentioned in the introductory section, fewer assumptions
    will require more data with significant information about the relationship so
    that the learning process can be successful.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: We will outline the key assumptions and how to test them where applicable as
    we introduce these models.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Design and tune the model
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ML process includes steps to diagnose and manage model complexity based
    on estimates of the model's generalization error. An important goal of the ML
    process is to obtain an unbiased estimate of this error using a statistically
    sound and efficient procedure. Key to managing the model design and tuning process
    is an understanding of how the bias-variance tradeoff relates to under- and overfitting.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The bias-variance trade-off
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The prediction errors of an ML model can be broken down into reducible and irreducible
    parts. The irreducible part is due to random variation (noise) in the data due
    to, for example, the absence of relevant variables, natural variation, or measurement
    errors. The reducible part of the generalization error, in turn, can be broken
    down into errors due to **bias** and **variance**.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Both result from discrepancies between the true functional relationship and
    the assumptions made by the machine learning algorithm, as detailed in the following
    list:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '**Error due to bias**: The hypothesis is too simple to capture the complexity
    of the true functional relationship. As a result, whenever the model attempts
    to learn the true function, it makes systematic mistakes and, on average, the
    predictions will be similarly biased. This is also called *underfitting*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error due to variance**: The algorithm is overly complex in view of the true
    relationship. Instead of capturing the true relationship, it overfits the data
    and extracts patterns from the noise. As a result, it learns different functional
    relationships from each sample, and out-of-sample predictions will vary widely.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差导致的错误**：该算法在真实关系方面过于复杂。它不是捕捉真实关系，而是对数据过拟合，并从噪声中提取模式。因此，它从每个样本中学习到不同的功能关系，并且样本外的预测会变化很大。'
- en: Underfitting versus overfitting – a visual example
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欠拟合与过拟合 – 一个可视化例子
- en: '*Figure 6.6* illustrates overfitting by measuring the in-sample error of approximations
    of a *sine* function by increasingly complex polynomials. More specifically, we
    draw a random sample with some added noise (*n* = 30) to learn a polynomial of
    varying complexity (see the code in the notebook, `bias_variance.ipynb`). The
    model predicts new data points, and we capture the mean-squared error for these
    predictions.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.6* 通过逐渐复杂的多项式逼近 *正弦* 函数来说明过拟合。更具体地说，我们抽取一个带有一些噪声的随机样本（*n* = 30）来学习不同复杂度的多项式（请参见笔记本中的代码
    `bias_variance.ipynb`）。该模型预测新的数据点，我们捕获这些预测的均方误差。'
- en: The left-hand panel of *Figure 6.6* shows a polynomial of degree 1; a straight
    line clearly underfits the true function. However, the estimated line will not
    differ dramatically from one sample drawn from the true function to the next.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.6* 的左侧面板显示了一次多项式；一条直线明显地欠拟合了真实函数。然而，估计线不会在从真实函数绘制的一个样本到下一个样本之间有明显差异。'
- en: 'The middle panel shows that a degree 5 polynomial approximates the true relationship
    reasonably well on the interval from about ![](img/B15439_06_010.png) until ![](img/B15439_06_011.png).
    On the other hand, a polynomial of degree 15 fits the small sample almost perfectly,
    but provides a poor estimate of the true relationship: it overfits to the random
    variation in the sample data points, and the learned function will vary strongly
    as a function of the sample:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 中间面板显示了一个 5 次多项式在大约从 ![](img/B15439_06_010.png) 到 ![](img/B15439_06_011.png)
    的区间上合理地逼近真实关系。另一方面，15 次多项式几乎完美地拟合了小样本，但提供了真实关系的不良估计：它对样本数据点的随机变化进行了过拟合，学到的函数将随样本的函数强烈变化：
- en: '![](img/B15439_06_06.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_06.png)'
- en: 'Figure 6.6: A visual example of overfitting with polynomials'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：多项式过拟合的可视化示例
- en: How to manage the bias-variance trade-off
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何处理偏差-方差折衷
- en: To further illustrate the impact of overfitting versus underfitting, we'll try
    to learn a Taylor series approximation of the *sine* function of the ninth degree
    with some added noise. *Figure 6.7* shows the in- and-out-of-sample errors and
    the out-of-sample predictions for polynomials that underfit, overfit, and provide
    an approximately correct level of flexibility with degrees 1, 15, and 9, respectively,
    to 100 random samples of the true function.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明过拟合与欠拟合的影响，我们将尝试学习带有一定噪声的 *正弦* 函数的第九次泰勒级数逼近。*图 6.7* 显示了对真实函数的 100 个随机样本进行了欠拟合、过拟合和提供大约正确的灵活性的多项式的样本内外误差和样本外预测，分别为
    1、15 和 9 次多项式。
- en: The left-hand panel shows the distribution of the errors that result from subtracting
    the true function values from the predictions. The high bias but low variance
    of an underfit polynomial of degree 1 compares to the low bias but exceedingly
    high variance of the errors for an overfitting polynomial of degree 15\. The underfit
    polynomial produces a straight line with a poor in-sample fit that is significantly
    off-target out of sample. The overfit model shows the best fit in-sample with
    the smallest dispersion of errors, but the price is a large variance out-of-sample.
    The appropriate model that matches the functional form of the true model performs,
    on average, by far the best on out-of-sample data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧面板显示了从预测值中减去真实函数值产生的误差的分布。拟合度低但方差小的一次拟合多项式与拟合度高但方差极高的 15 次过拟合多项式的误差相比。欠拟合多项式产生一条直线，其内部拟合较差，样本外明显偏离目标。过拟合模型在样本内显示出最佳拟合，误差最小，但代价是样本外的大方差。与真实模型功能形式相匹配的适当模型在样本外数据上平均表现最佳。
- en: 'The right-hand panel of *Figure 6.7* shows the actual predictions rather than
    the errors to visualize the different types of fit in practice:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.7* 的右侧面板显示实际预测而不是错误，以可视化实践中的不同拟合类型：'
- en: '![](img/B15439_06_07.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_07.png)'
- en: 'Figure 6.7: Errors and out-of-sample predictions for polynomials of different
    degrees'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：不同次数多项式的误差和样本外预测
- en: Learning curves
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习曲线
- en: A learning curve plots the evolution of train and test errors against the size
    of the dataset used to learn the functional relationship. It helps to diagnose
    the bias-variance trade-off for a given model, and also answer the question of
    whether increasing the sample size might improve predictive performance. A model
    with a high bias will have a high but similar training error, both in-sample and
    out-of-sample. An overfit model will have a very low training but much higher
    test errors.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线绘制了用于学习函数关系的数据集大小演变对训练和测试错误的影响。它有助于诊断给定模型的偏差-方差权衡，并回答增加样本量是否可能提高预测性能的问题。具有高偏差的模型将在样本内和样本外都具有高但相似的训练误差。过拟合模型将具有非常低的训练但较高的测试误差。
- en: '*Figure 6.8* shows how the out-of-sample error for the overfitted model declines
    as the sample size increases, suggesting that it may benefit from additional data
    or tools to limit the model''s complexity, such as regularization. Regularization
    adds data-driven constraints to the model''s complexity; we''ll introduce this
    technique in *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.8*显示了过拟合模型的样本外误差随着样本大小增加而下降，表明它可能受益于额外的数据或限制模型复杂性的工具，例如正则化。正则化向模型的复杂性添加了数据驱动的约束；我们将在*第
    7 章*，*线性模型 - 从风险因素到回报预测*中介绍这个技术。'
- en: 'Underfit models, in contrast, require either more features or need to increase
    their capacity to capture the true relationship:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，拟合不足的模型需要更多特征或需要增加其容量以捕获真实关系：
- en: '![](img/B15439_06_08.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_08.png)'
- en: 'Figure 6.8: Learning curves and bias-variance tradeoff'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8：学习曲线和偏差-方差权衡
- en: How to select a model using cross-validation
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用交叉验证选择模型
- en: There are usually several candidate models for your use case, and the task of
    choosing one of them is known as the **model selection problem**. The goal is
    to identify the model that will produce the lowest prediction error when given
    new data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通常有几个适用于您用例的候选模型，选择其中一个的任务称为**模型选择问题**。目标是识别在给定新数据时产生最低预测误差的模型。
- en: A good choice requires an unbiased estimate of this generalization error, which,
    in turn, requires testing the model on data that was not part of model training.
    Otherwise, the model would have already been able to peek at the "solution" and
    learn something about the prediction task ahead of time that will inflate its
    performance.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的选择需要对这种泛化误差进行无偏估计，而这又需要在不参与模型训练的数据上对模型进行测试。否则，模型将已经能够窥视“解决方案”，并提前了解有关预测任务的信息，这将夸大其性能。
- en: 'To avoid this, we only use part of the available data to train the model and
    set aside another part of the data to validate its performance. The resulting
    estimate of the model''s prediction error on new data will only be unbiased if
    absolutely no information about the validation set leaks into the training set,
    as shown in *Figure 6.9*:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，我们仅使用部分可用数据来训练模型，并将另一部分数据保留以验证其性能。模型在新数据上的预测误差的估计结果只有在绝对没有关于验证集的信息泄漏到训练集时才会无偏，如*图
    6.9*所示：
- en: '![](img/B15439_06_09.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_09.png)'
- en: 'Figure 6.9: Training and test set'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9：训练集和测试集
- en: '**Cross-validation** (**CV**) is a popular strategy for model selection. The
    main idea behind CV is to split the data one or several times. This is done so
    that each split is used once as a validation set and the remainder as a training
    set: part of the data (the training sample) is used to train the algorithm, and
    the remaining part (the validation sample) is used to estimate the algorithm''s
    predictive performance. Then, CV selects the algorithm with the smallest estimated
    error or risk.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证**（**CV**）是一种常用的模型选择策略。CV背后的主要思想是将数据分割一次或多次。这样做是为了每次分割都被用作一次验证集，而剩余部分被用作训练集：一部分数据（训练样本）用于训练算法，剩余部分（验证样本）用于估计算法的预测性能。然后，CV选择具有最小估计误差或风险的算法。'
- en: Several methods can be used to split the available data. They differ in terms
    of the amount of data used for training, the variance of the error estimates,
    the computational intensity, and whether structural aspects of the data are taken
    into account when splitting the data, such as maintaining the ratio between class
    labels.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用几种方法来拆分可用数据。它们在使用用于训练的数据量、误差估计的方差、计算强度以及在拆分数据时是否考虑数据的结构性方面有所不同，例如维持类标签之间的比率。
- en: While the data-splitting heuristic is very general, a key assumption of CV is
    that the data is **independently and identically distributed** (**IID**). In the
    following section and throughout this book, we will emphasize that **time-series
    data** requires a different approach because it usually does not meet this assumption.
    Moreover, we need to ensure that splits respect the temporal order to avoid **lookahead
    bias**. We'll do this by including some information from the future that we aim
    to predict in the historical training set.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据拆分启发式方法非常通用，但 CV 的一个关键假设是数据是**独立同分布的**（**IID**）。在接下来的部分和本书中的整个过程中，我们将强调**时间序列数据**需要不同的方法，因为它通常不符合这一假设。此外，我们需要确保拆分尊重时间顺序，以避免**前瞻性偏差**。我们将通过将一些信息从我们旨在预测的未来纳入历史训练集中来实现这一点。
- en: Model selection often involves hyperparameter tuning, which may result in many
    CV iterations. The resulting validation score of the best-performing model will
    be subject to **multiple testing bias**, which reflects the sampling noise inherent
    in the CV process. As a result, it is no longer a good estimate of the generalization
    error. For an unbiased estimate of the error rate, we have to estimate the score
    from a fresh dataset.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择通常涉及超参数调整，这可能导致许多 CV 迭代。表现最佳模型的结果验证分数将受到**多重检验偏差**的影响，它反映了 CV 过程中固有的抽样噪声。因此，它不再是泛化误差的良好估计。为了得到无偏的误差率估计，我们必须从新的数据集中估计分数。
- en: 'For this reason, we use a three-way split of the data, as shown in *Figure
    6.10*: one part is used in cross-validation and is repeatedly split into a training
    and validation set. The remainder is set aside as a hold-out set that is only
    used once after, cross-validation is complete to generate an unbiased test error
    estimate.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用了数据的三分拆分，如 *图 6.10* 所示：其中一部分用于交叉验证，并被重复拆分为训练集和验证集。其余部分被保留为保留集，仅在交叉验证完成后使用一次，以生成一个无偏的测试误差估计。
- en: 'We will illustrate this method as we start building ML models in the next chapter:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章开始构建 ML 模型时说明这种方法：
- en: '![](img/B15439_06_10.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_10.png)'
- en: 'Figure 6.10: Train, validation, and hold-out test set'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10：训练、验证和保留测试集
- en: How to implement cross-validation in Python
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在 Python 中实现交叉验证
- en: 'We will illustrate various options for splitting data into training and test
    sets. We''ll do this by showing how the indices of a mock dataset with 10 observations
    are assigned to the train and test set (see `cross_validation.py` for details),
    as shown in following code:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将说明将数据拆分为训练集和测试集的各种选项。我们将通过展示如何将具有 10 个观测值的模拟数据集的索引分配给训练集和测试集（有关详细信息，请参见 `cross_validation.py`），如下代码所示：
- en: '[PRE0]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Scikit-learn's CV functionality, which we'll demonstrate in this section, can
    be imported from `sklearn.model_selection`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 的 CV 功能，我们将在本节中演示，可以从 `sklearn.model_selection` 导入。
- en: 'For a single split of your data into a training and a test set, use `train_test_split`,
    where the `shuffle` parameter, by default, ensures the randomized selection of
    observations. You can ensure replicability by seeding the random number generator
    by setting `random_state`. There is also a `stratify` parameter, which ensures
    for a classification problem that the train and test sets will contain approximately
    the same proportion of each class. The result looks as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于将数据拆分为训练集和测试集的单次拆分，请使用 `train_test_split`，其中 `shuffle` 参数默认确保观察结果的随机选择。您可以通过设置
    `random_state` 来对随机数生成器进行种子化以确保可复制性。还有一个 `stratify` 参数，它确保对于分类问题，训练集和测试集将包含大约相同比例的每个类别。结果如下所示：
- en: '[PRE1]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this case, we train a model using all data except row numbers `6` and `9`,
    which will be used to generate predictions and measure the errors given on the
    known labels. This method is useful for quick evaluation but is sensitive to the
    split, and the standard error of the performance measure estimate will be higher.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: KFold iterator
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `KFold` iterator produces several disjunct splits and assigns each of these
    splits once to the validation set, as shown in the following code:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In addition to the number of splits, most CV objects take a `shuffle` argument
    that ensures randomization. To render results reproducible, set the `random_state`
    as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Leave-one-out CV
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The original CV implementation used a **leave-one-out method** that used each
    observation once as the validation set, as shown in the following code:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This maximizes the number of models that are trained, which increases computational
    costs. While the validation sets do not overlap, the overlap of training sets
    is maximized, driving up the correlation of models and their prediction errors.
    As a result, the variance of the prediction error is higher for a model with a
    larger number of folds.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Leave-P-Out CV
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A similar version to leave-one-out CV is **leave-P-out CV**, which generates
    all possible combinations of `p` data rows, as shown in the following code:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ShuffleSplit
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `ShuffleSplit` class creates independent splits with potentially overlapping
    validation sets, as shown in the following code:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Challenges with cross-validation in finance
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A key assumption for the cross-validation methods discussed so far is the IID
    distribution of the samples available for training.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: For financial data, this is often not the case. On the contrary, financial data
    is neither independently nor identically distributed because of serial correlation
    and time-varying standard deviation, also known as **heteroskedasticity** (see
    *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts,* and *Chapter
    9*, *Time Series Models for Volatility Forecasts and Statistical Arbitrage*, for
    more details). `TimeSeriesSplit` in the `sklearn.model_selection` module aims
    to address the linear order of time-series data.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Time series cross-validation with scikit-learn
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The time-series nature of the data implies that cross-validation produces a
    situation where data from the future will be used to predict data from the past.
    This is unrealistic at best and data snooping at worst, to the extent that future
    data reflects past events.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'To address time dependency, the `TimeSeriesSplit` object implements a walk-forward
    test with an expanding training set, where subsequent training sets are supersets
    of past training sets, as shown in the following code:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can use the `max_train_size` parameter to implement walk-forward cross-validation,
    where the size of the training set remains constant over time, similar to how
    Zipline tests a trading algorithm. Scikit-learn facilitates the design of custom
    cross-validation methods using **subclassing**, which we will implement in the
    following chapters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Purging, embargoing, and combinatorial CV
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For financial data, labels are often derived from overlapping data points because
    returns are computed from prices across multiple periods. In the context of trading
    strategies, the result of a model's prediction, which may imply taking a position
    in an asset, can only be known later when this decision is evaluated—for example,
    when a position is closed out.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: The risks include the leakage of information from the test into the training
    set, which would very likely artificially inflate performance. We need to address
    this risk by ensuring that all data is point-in-time—that is, truly available
    and known at the time it is used as the input for a model. For example, financial
    disclosures may refer to a certain time period but only become available later.
    If we include this information too early, our model might do much better in hindsight
    than it would have under realistic circumstances.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Marcos Lopez de Prado, one of the leading practitioners and academics in the
    field, has proposed several methods to address these challenges in his book, *Advances
    in Financial Machine Learning* (2018). Techniques to adapt cross-validation to
    the context of financial data and trading include:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '**Purging**: Eliminate training data points where the evaluation occurs after
    the prediction of a point-in-time data point in the validation set to avoid look-ahead
    bias.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embargoing**: Further eliminate training samples that follow a test period.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combinatorial cross-validation**: Walk-forward CV severely limits the historical
    paths that can be tested. Instead, given *T* observations, compute all possible
    train/test splits for *N*<*T* groups that each maintain their order, and purge
    and embargo potentially overlapping groups. Then, train the model on all combinations
    of *N*-*k* groups while testing the model on the remaining *k* groups. The result
    is a much larger number of possible historical paths.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prado's *Advances in Financial Machine Learning* contains sample code to implement
    these approaches; the code is also available via the new Python library, timeseriescv.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Parameter tuning with scikit-learn and Yellowbrick
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model selection typically involves repeated cross-validation of the out-of-sample
    performance of models using different algorithms (such as linear regression and
    random forest) or different configurations. Different configurations may involve
    changes to hyperparameters or the inclusion or exclusion of different variables.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The Yellowbrick library extends the scikit-learn API to generate diagnostic
    visualization tools to facilitate the model-selection process. These tools can
    be used to investigate relationships among features, analyze classification or
    regression errors, monitor cluster algorithm performance, inspect the characteristics
    of text data, and help with model selection. We will demonstrate validation and
    learning curves that provide valuable information during the parameter-tuning
    phase—see the `machine_learning_workflow.ipynb` notebook for implementation details.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Validation curves – plotting the impact of hyperparameters
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Validation curves (see the left-hand panel in *Figure 6.11*) visualize the impact
    of a single hyperparameter on a model's cross-validation performance. This is
    useful to determine whether the model underfits or overfits the given dataset.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: In our example of `KNeighborsRegressor`, which only has a single hyperparameter,
    the number of neighbors is *k*. Note that model complexity increases as the number
    of neighbors drop because the model can now make predictions for more distinct
    areas in the feature space.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the model underfits for values of *k* above 20\. The validation
    error drops as we reduce the number of neighbors and make our model more complex.
    For values below 20, the model begins to overfit as training and validation errors
    diverge and average out-of-sample performance quickly deteriorates:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_06_11.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: Validation and learning curves'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Learning curves – diagnosing the bias-variance trade-off
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The learning curve (see the right-hand panel of *Figure 6.11* for our house
    price regression example) helps determine whether a model's cross-validation performance
    would benefit from additional data, and whether the prediction errors are more
    driven by bias or by variance.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: More data is unlikely to improve performance if training and cross-validation
    scores converge. At this point, it is important to evaluate whether the model
    performance meets expectations, determined by a human benchmark. If this is not
    the case, then you should modify the model's hyperparameter settings to better
    capture the relationship between the features and the outcome, or choose a different
    algorithm with a higher capacity to capture complexity.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the variation of train and test errors shown by the shaded confidence
    intervals provides clues about the bias and variance sources of the prediction
    error. Variability around the cross-validation error is evidence of variance,
    whereas variability for the training set suggests bias, depending on the size
    of the training error.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: In our example, the cross-validation performance has continued to drop, but
    the incremental improvements have shrunk, and the errors have plateaued, so there
    are unlikely to be many benefits from a larger training set. On the other hand,
    the data is showing substantial variance given the range of validation errors
    compared to that shown for the training errors.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Parameter tuning using GridSearchCV and pipeline
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since hyperparameter tuning is a key ingredient of the machine learning workflow,
    there are tools to automate this process. The scikit-learn library includes a
    `GridSearchCV` interface that cross-validates all combinations of parameters in
    parallel, captures the result, and automatically trains the model using the parameter
    setting that performed best during cross-validation on the full dataset.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the training and validation set often requires some processing
    prior to cross-validation. Scikit-learn offers the `Pipeline` to also automate
    any feature-processing steps while using `GridSearchCV`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: You can look at the implementation examples in the included `machine_learning_workflow.ipynb`
    notebook to see these tools in action.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the challenge of learning from data and looked
    at supervised, unsupervised, and reinforcement models as the principal forms of
    learning that we will study in this book to build algorithmic trading strategies.
    We discussed the need for supervised learning algorithms to make assumptions about
    the functional relationships that they attempt to learn. They do this to limit
    the search space while incurring an inductive bias that may lead to excessive
    generalization errors.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: We presented key aspects of the machine learning workflow, introduced the most
    common error metrics for regression and classification models, explained the bias-variance
    trade-off, and illustrated the various tools for managing the model selection
    process using cross-validation.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will dive into linear models for regression and
    classification to develop our first algorithmic trading strategies that use machine
    learning.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
