- en: Chapter 15. Volatility, Implied Volatility, ARCH, and GARCH
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In finance, we know that risk is defined as uncertainty since we are unable
    to predict the future more accurately. Based on the assumption that prices follow
    a lognormal distribution and returns follow a normal distribution, we could define
    risk as standard deviation or variance of the returns of a security. We call this
    our conventional definition of volatility (uncertainty). Since a normal distribution
    is symmetric, it will treat a positive deviation from a mean in the same manner
    as it would a negative deviation. This is against our conventional wisdom since
    we treat them differently. To overcome this, Sortino (1983) suggests a lower partial
    standard deviation. Most of the time, it is assumed that the volatility of a time
    series is a constant. Obviously this is not true. Another observation is volatility
    clustering, which means that high volatility is usually followed by a high-volatility
    period, and this is true for low volatility, which is usually followed by a low-volatility
    period. To model this pattern, Angel (1982) develops an **Auto Regressive Conditional
    Heteroskedasticity** (**ARCH**) process, and Bollerslev (1986) extends it to a
    **Generalized Auto Regressive Conditional Heteroskedasticity** (**GARCH**) process.
    In this chapter, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Conventional volatility measure—standard deviation—based on a normality assumption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tests of normality and fat tails
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lower partial standard deviation and Sortino ratio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test of equivalency of volatility over two periods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test of heteroskedasticity, Breusch and Pagan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volatility smile and skewness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ARCH model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulation of an ARCH (1) process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GARCH model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulation of a GARCH process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulation of a GARCH (p,q) process using modified `garchSim()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GJR_GARCH process by Glosten, Jagannathan, and Runkle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conventional volatility measure – standard deviation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In most finance textbooks, we use the standard deviation of returns as a risk
    measure. This is based on a critical assumption that log returns follow a normal
    distribution. Both standard deviation and variance could be used to measure uncertainty;
    the former is usually called volatility itself. For example, if we say that the
    volatility of IBM is 20 percent, it means that its annualized standard deviation
    is 20 percent. Using IBM as an example, the following program is used to estimate
    its annualized volatility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Tests of normality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Shapiro-Wilk test is a normality test. The following Python program verifies
    whether IBM''s returns are following a normal distribution. The last five-year
    daily data from Yahoo! Finance is used for the test. The null hypothesis is that
    IBM''s daily returns are drawn from a normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first value of the result is the test statistic, and the second one is its
    corresponding P-value. Since this P-value is so close to zero, we reject the null
    hypothesis. In other words, we conclude that IBM's daily returns do not follow
    a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the normality test, we could also apply the Anderson-Darling test, which
    is a modification of the Kolmogorov-Smirnov test, to verify whether the observations
    follow a particular distribution. The `stats.anderson()` function has tests for
    normal, exponential, logistic, and Gumbel (Extreme Value Type I) distributions.
    The default test is for a normal distribution. After calling the function and
    printing the testing results, we see the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have three sets of values: the Anderson-Darling test statistic, a
    set of critical values, and a set of corresponding confidence levels, such as
    15 percent, 10 percent, 5 percent, 2.5 percent, and 1 percent, as shown in the
    previous output. If we choose a 1 percent confidence level—the last value of the
    third set—the critical value is 1.089, the last value of the second set. Since
    our testing statistic is 14.73, which is much higher than the critical value of
    1.089, we reject the null hypothesis. Thus, our Anderson-Darling test leads to
    the same conclusion as our Shapiro-Wilk test.'
  prefs: []
  type: TYPE_NORMAL
- en: Estimating fat tails
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the important properties of a normal distribution is that we could use
    mean and standard deviation, the first two moments, to fully define the whole
    distribution. For n returns of a security, its first four moments are defined
    in equation (1). The mean or average is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating fat tails](img/B06175_15_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Its (sample) variance is defined by the following equation. The standard deviation,
    that is, σ, is the squared root of the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating fat tails](img/B06175_15_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The skewness defined by the following formula indicates whether the distribution
    is skewed to the left or to the right. For a symmetric distribution, its skewness
    is zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating fat tails](img/B06175_15_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The kurtosis reflects the impact of extreme values because of its power of
    four. There are two types of definitions with and without minus three; refer to
    the following two equations. The reason behind the deduction of three in equation
    (4B), is that for a normal distribution, its kurtosis based on equation (4A) is
    three:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating fat tails](img/B06175_15_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Some books distinguish these two equations by calling equation (4B) excess
    kurtosis. However, many functions based on equation (4B) are still named kurtosis.
    Since we know that a standard normal distribution has a zero mean, unit standard
    deviation, zero skewness, and zero kurtosis (based on equation 4B). The following
    output confirms these facts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The mean, skewness, and kurtosis are all close to zero, while the standard
    deviation is close to one. Next, we estimate the four moments for S&P500 based
    on its daily returns as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the five values mentioned in the previous code, including the
    number of observations, is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This result is very close to the result in the paper titled *Study of Fat-tail
    Risk by Cook Pine Capital*, the PDF version of the paper could be downloaded at
    [http://www.cookpinecapital.com/assets/pdfs/Study_of_Fat-tail_Risk.pdf](http://www.cookpinecapital.com/assets/pdfs/Study_of_Fat-tail_Risk.pdf).
    Alternatively, it is available at [http://www3.canisius.edu/~yany/doc/Study_of_Fat-tail_Risk.pdf](http://www3.canisius.edu/~yany/doc/Study_of_Fat-tail_Risk.pdf).
    Using the same argument, we conclude that the S&P500 daily returns are skewed
    to the left, that is, a negative skewness, and have fat tails (kurtosis is 38.22
    instead of zero).
  prefs: []
  type: TYPE_NORMAL
- en: Lower partial standard deviation and Sortino ratio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We discussed this concept already. However, for completeness, in this chapter
    we mention it again. One issue with using standard deviation of returns as a risk
    measure is that the positive deviation is also viewed as bad. The second issue
    is that the deviation is from the average instead of a fixed benchmark, such as
    a risk-free rate. To overcome these shortcomings, Sortino (1983) suggests the
    lower partial standard deviation, which is defined as the average of squared deviation
    from the risk-free rate conditional on negative excess returns, as shown in the
    following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Lower partial standard deviation and Sortino ratio](img/B06175_15_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Because we need the risk-free rate in this equation, we could generate a Fama-French
    dataset that includes the risk-free rate as one of their time series. First, download
    their daily factors from [http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html](http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html).
    Then, unzip it and delete the non-data part at the end of the text file. Assume
    the final text file is saved under `C:/temp/`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The name of the final dataset is `ffDaily.pkl`. It is a good idea to generate
    this dataset yourself. However, the dataset could be downloaded from [http://canisius.edu/~yany/python/ffDaily.pkl](http://canisius.edu/~yany/python/ffDaily.pkl).
    Using the last five years'' data (January 1, 2009 to December 31, 2013), we could
    estimate IBM''s LPSD as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that IBM''s LPSD is 14.8 percent–quite different
    from the 20.9 percent shown in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Test of equivalency of volatility over two periods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know that the stock market fell dramatically in October, 1987\. We could
    choose a stock to test the volatility before and after October, 1987\. For instance,
    we could use Ford Motor Corp, with a ticker of F, to illustrate how to test the
    equality of variance before and after the market crash in 1987\. In the following
    Python program, we define a function called `ret_f()` to retrieve daily price
    data from Yahoo! Finance and estimate its daily returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The very high T value and close to zero p-value in the following screenshot
    suggest the rejection of the hypothesis that during these two periods, the stock
    has the same volatility. The corresponding output is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Test of heteroskedasticity, Breusch, and Pagan
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Breusch and Pagan (1979) designed a test to confirm or reject the null assumption
    that the residuals from a regression are homogeneous, that is, with a constant
    volatility. The following formula represents their logic. First, we run a linear
    regression of *y* against *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Test of heteroskedasticity, Breusch, and Pagan](img/B06175_15_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *y* is the dependent variable, *x* is the independent variable, *α* is
    the intercept, *β* is the coefficient, and ![Test of heteroskedasticity, Breusch,
    and Pagan](img/B06175_15_23.jpg) is an error term. After we get the error term
    (residual), we run the second regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Test of heteroskedasticity, Breusch, and Pagan](img/B06175_15_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Assume that the fitted values from running the previous regression is *t f*,
    then the Breusch-Pangan (1979) measure is given as follows, and it follows a *χ2*
    distribution with a *k* degree of freedom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Test of heteroskedasticity, Breusch, and Pagan](img/B06175_15_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following example is borrowed from an R package called `lm.test` (test
    linear regression), and its authors are Hothorn et al. (2014). We generate a time
    series of *x*, *y1* and *y2*. The independent variable is *x*, and the dependent
    variables are *y1* and *y2*. By our design, *y1* is homogeneous, that is, with
    a constant variance (standard deviation), and *y2* is non-homogeneous (heterogeneous),
    that is, the variance (standard deviation) is not constant. For a variable *x*,
    we have the following 100 values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Test of heteroskedasticity, Breusch, and Pagan](img/B06175_15_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we generate two error terms with 100 random values each. For the *error1*,
    its 100 values are drawn from the standard normal distribution, that is, with
    zero mean and unit standard deviation. For *error2*, its 100 values are drawn
    from a normal distribution with a zero mean and 2 as the standard deviation. The
    *y1* and *y2* time-series are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Test of heteroskedasticity, Breusch, and Pagan](img/B06175_15_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the odd scripts of *y2*, the error terms are derived from *error1*, while
    for the even scripts, the error terms are derived from *error2*. To find more
    information about the PDF file related to `lm.test`, or an R package, we have
    the following six steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [http://www.r-project.org](http://www.r-project.org).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **CRAN** under **Download**, **Packages**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a close-by server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Packages on the left-hand side of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a list and search `lm.test`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the link and download the PDF file related to `lm.test`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the related Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'From the result of running regression by using `y1` against `x`, we know that
    its residual value would be homogeneous, that is, the variance or standard deviation
    is a constant. Thus, we expect to accept the null hypothesis. The opposite is
    true for `y2` against `x`, since, based on our design, the error terms for `y2`
    are heterogeneous. Thus, we expect to reject the null hypothesis. The corresponding
    output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Volatility smile and skewness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Obviously, each stock should possess just one volatility. However, when estimating
    implied volatility, different strike prices might offer us different implied volatilities.
    More specifically, the implied volatility based on out-of-the-money options, at-the-money
    options, and in-the-money options might be quite different. Volatility smile is
    the shape going down then up with the exercise prices, while the volatility skewness
    is downward or upward sloping. The key is that investors'' sentiments and the
    supply and demand relationship have a fundamental impact on the volatility skewness.
    Thus, such a smile or skewness provides information on whether investors such
    as fund managers prefer to write calls or puts. First, we go to the Yahoo! Finance
    website to download call and put options data:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [http://finance.yahoo.com](http://finance.yahoo.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a ticker, such as `IBM`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Options** in the center.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy and paste the data for call and options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separate them into two files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If readers use the data for a maturity of March 17, 2017, they can download
    it from the author's website at [http://canisius.edu/~yany/data/calls17march.txt](http://canisius.edu/~yany/data/calls17march.txt),
    [http://canisius.edu/~yany/data/puts17march.txt](http://canisius.edu/~yany/data/puts17march.txt).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python program for calls is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding program, the input file is for call options. The graph of
    the volatility smile is shown here. The other screenshot is based on the relationship
    between implied volatility and exercise (strike) prices. The program is exactly
    the same as the preceding program, except the input file. At the end of the chapter,
    one data case is related to the preceding program. The next image is the volatility
    smile based on the call data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Volatility smile and skewness](img/B06175_15_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Volatility smile based on call data
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the next volatility smile image is based on put data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Volatility smile and skewness](img/B06175_15_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Graphical presentation of volatility clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the observations is labeled as volatility clustering, which means that
    high volatility is usually followed by a high-volatility period, while low volatility
    is usually followed by a low-volatility period. The following program shows this
    phenomenon by using S&P500 daily returns from 1988 to 2006\. Note that, in the
    following code, in order to show 1988 on the *x* axis, we add a few months before
    1988:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This program is inspired by the graph drawn by *M.P. Visser*; refer to [https://pure.uva.nl/ws/files/922823/67947_09.pdf](https://pure.uva.nl/ws/files/922823/67947_09.pdf).
    The graph corresponding to the previous code is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical presentation of volatility clustering](img/B06175_15_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The ARCH model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on previous arguments, we know that the volatility or variance of stock
    returns is not constant. According to the ARCH model, we could use the error terms
    from the previous estimation to help us predict the next volatility or variance.
    This model was developed by Robert F. Engle, the winner of the 2003 Nobel Prize
    in Economics. The formula for an ARCH (q) model is presented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ARCH model](img/B06175_15_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![The ARCH model](img/B06175_15_24.jpg) is the variance at time t, is
    the ith coefficient, ![The ARCH model](img/B06175_15_25.jpg) is the squared error
    term for the period of *t-i*, and *q* is the order of error terms. When *q* is
    1, we have the simplest ARCH (1) process as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ARCH model](img/B06175_15_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Simulating an ARCH (1) process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is a good idea that we simulate an ARCH (1) process and have a better understanding
    of the volatility clustering, which means that high volatility is usually followed
    by a high-volatility period while low volatility is usually followed by a low-volatility
    period. The following code reflects this phenomenon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'From the following graph, we see that indeed a higher volatility period is
    usually followed with high volatility while this is also true for a low-volatility
    clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simulating an ARCH (1) process](img/B06175_15_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The GARCH model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generalized AutoRegressive Conditional Heteroskedasticity** (**GARCH**) is
    an important extension of ARCH, by Bollerslev (1986). The GARCH (p,q) process
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The GARCH model](img/B06175_15_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![The GARCH model](img/B06175_15_26.jpg)is the variance at time *t*,
    *q* is the order for the error terms, p is the order for the variance, ![The GARCH
    model](img/B06175_15_27.jpg)is a constant, ![The GARCH model](img/B06175_15_31.jpg)
    is the coefficient for the error term at *t-i*, ![The GARCH model](img/B06175_15_28.jpg)
    is the coefficient for the variance at time *t-i*. Obviously, the simplest GARCH
    process is when both *p* and *q* are set to 1, that is, GARCH (1,1), which has
    following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The GARCH model](img/B06175_15_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Simulating a GARCH process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on the previous program related to ARCH (1), we could simulate a GARCH
    (1,1) process as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Honestly speaking, the following graph is quite similar to the previous one
    under the ARCH (1) process. The graph corresponding to the previous code is shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simulating a GARCH process](img/B06175_15_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Fig15_04_garch.png
  prefs: []
  type: TYPE_NORMAL
- en: Simulating a GARCH (p,q) process using modified garchSim()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code is based on the R function called `garchSim()`, which is
    included in the R package called `fGarch`. The authors for `fGarch` are Diethelm
    Wuertz and Yohan Chalabi. To find the related manual, we perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [http://www.r-project.org](http://www.r-project.org).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **CRAN** under **Download**, **Packages**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a close-by server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Packages on the left-hand side of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a list and search for `fGarch`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the link and download the PDF file related to `fGarch`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The Python program based on the R program is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding program, omega is the constant in equation (10), while alpha
    is associated with error terms and beta is associated with variance. There are
    two items in `alpha[a,b]: a` is for `t-1`, while b is for `t-2`. However, for
    `eps0[t-2:i]`, they stand for *t-2* and *t-1*. The `alpha` and `eps0` terms are
    not consistent with each other. Thus, we have to reverse the order of `a` and
    `b`. This is the reason why we use `alpha[::-1]`. Since several values are zero,
    such as `mu`, `ar`, and `ma`, the time series of GARCH is identical with `eps`.
    Thus, we show just two time series in the following graph. The high volatility
    is for GARCH, while the other one is for standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simulating a GARCH (p,q) process using modified garchSim()](img/B06175_15_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Fig15_05_two.png
  prefs: []
  type: TYPE_NORMAL
- en: GJR_GARCH by Glosten, Jagannanthan, and Runkle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Glosten, Jagannathan, and Runkle (1993) modeled asymmetry in the GARCH process.
    GJR_GARCH (1,1,1) has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![GJR_GARCH by Glosten, Jagannanthan, and Runkle](img/B06175_15_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the condition *It-1=0*, if ![GJR_GARCH by Glosten, Jagannanthan, and
    Runkle](img/B06175_15_29.jpg) and *It-1=1* if ![GJR_GARCH by Glosten, Jagannanthan,
    and Runkle](img/B06175_15_30.jpg) holds true. The following code is based on the
    codes written by Kevin Sheppard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can write a function called `GJR_GARCH()` by including all initial values,
    constraints, and bounds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to replicate our result, we could use the `random.seed()` function
    to fix our returns obtained from generating a set of random numbers from a uniform
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The interpretations of these five outputs are given in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| # | **Meaning** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Message describing the exit mode from the optimizer |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | The final value of the objective function |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | The number of iterations |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Function evaluations |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Gradient evaluations |'
  prefs: []
  type: TYPE_TB
- en: Table 15.1 Definitions of five outputs
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The descriptions of various exit modes are listed in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Exit code | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| -1 | Gradient evaluation required (g and a) |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Optimization terminated successfully |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Function evaluation required (f and c) |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | More equality constraints than independent variables |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | More than 3*n iterations in LSQ sub problem |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Inequality constraints incompatible |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Singular matrix E in LSQ subproblem |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Singular matrix C in LSQ subproblem |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Rank-deficient equality constraint subproblem HFTI |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Positive directional derivative for line search |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Iteration limit exceeded |'
  prefs: []
  type: TYPE_TB
- en: Table 15.2 Exit modes
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To show our final parameter values, we print our results with the help of the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the important properties of a normal distribution is that we could use
    mean and standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: '*Engle, Robert, 2002*, *DYNAMIC CONDITIONAL CORRELATION – A SIMPLE CLASS OF
    MULTIVARIATE GARCH MODELS*, *Forthcoming Journal of Business and Economic Statistics*,
    [http://pages.stern.nyu.edu/~rengle/dccfinal.pdf](http://pages.stern.nyu.edu/~rengle/dccfinal.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A – data case 8 - portfolio hedging using VIX calls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **CBOE Volatility Index** (**VIX**) is based on the **S&P500 Index** (**SPX**),
    the core index for U.S. equities, and estimates expected volatility by averaging
    the weighted prices of SPX puts and calls over a wide range of strike prices.
  prefs: []
  type: TYPE_NORMAL
- en: By supplying a script for replicating volatility exposure with a portfolio of
    SPX options, this new methodology transformed VIX from an abstract concept into
    a practical standard for trading and hedging volatility.
  prefs: []
  type: TYPE_NORMAL
- en: In 2014, CBOE enhanced the VIX Index to include series of SPX Weekly options.
    The inclusion of SPX Weeklies allows the VIX Index to be calculated with S&P500
    Index option series that most precisely match the 30-day target timeframe for
    expected volatility that the VIX Index is intended to represent. Using SPX options
    with more than 23 days and less than 37 days to expiration ensures that the VIX
    Index will always reflect an interpolation of two points along the S&P 500 volatility
    term structure.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[http://www.theoptionsguide.com/portfolio-hedging-using-vix-calls.aspx](http://www.theoptionsguide.com/portfolio-hedging-using-vix-calls.aspx).'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.cboe.com/micro/vix/historical.aspx](http://www.cboe.com/micro/vix/historical.aspx).'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.tickdata.com/tick-data-adds-vix-futures-data/](https://www.tickdata.com/tick-data-adds-vix-futures-data/).'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B – data case 8 - volatility smile and its implications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several objectives of this data case:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand the concept of the implied volatility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand that the implied volatilities are different with different exercise
    (strike) prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learnt how to process data and produce related graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the implication of a volatility smile?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Source of data: Yahoo! Finance:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [http://finance.yahoo.com](http://finance.yahoo.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a ticker, such as `IBM`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Options** in the center.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy and paste the data for call and options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separate them into two files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the following companies:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Company name | Ticker | Dell company | DELL |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| International Business Machine | IBM | General Electric | GE |'
  prefs: []
  type: TYPE_TB
- en: '| Microsoft | MSFT | Google | GOOG |'
  prefs: []
  type: TYPE_TB
- en: '| Family Dollar Stores | FDO | Apple | AAPL |'
  prefs: []
  type: TYPE_TB
- en: '| Wal-Mart Stores | WMT | eBay | EBAY |'
  prefs: []
  type: TYPE_TB
- en: '| McDonald''s | MCD |   |   |'
  prefs: []
  type: TYPE_TB
- en: 'Note that for each stock, there are several maturity dates; see the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Appendix B – data case 8 - volatility smile and its implications](img/B06175_15_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A sample Python program is shown here and the input file can be downloaded
    from the author''s website at [http://canisius.edu/~yany/data/calls17march.txt](http://canisius.edu/~yany/data/calls17march.txt):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the definition of volatility?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you measure risk (volatility)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the issues related to the widely used definition of risk (standard
    deviation)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'How can you test whether stock returns follow a normal distribution? For the
    following given set of stocks, test whether they follow a normal distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| Company name | Ticker | Dell company | DELL |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| International Business Machine | IBM | General Electric | GE |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Microsoft | MSFT | Google | GOOG |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Family Dollar Stores | FDO | Apple | AAPL |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Wal-Mart Stores | WMT | eBay | EBAY |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| McDonald''s | MCD |   |   |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: What is the lower partial standard deviation? What are its applications?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose five stocks, such as DELL, IBM, Microsoft, Citi Group, and Walmart, and
    compare their standard deviation with LPSD based on the last three-years' daily
    data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is a stock's volatility constant over the years? You could choose **International
    Business Machine** (**IBM**) and **Walmart** (**WMT**) to test your hypothesis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an ARCH (1) process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a GARCH (1,1) process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the GARCH (1,1) process to IBM and WMT.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a Python program to show the volatility smile combine both calls and puts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a Python program to put volatility smiles by using different maturity
    dates. In other words, put several smiles together.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the Breusch-Pagan (1979) test to confirm or reject the hypothesis that daily
    returns for IBM is homogeneous.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you test whether a stock's volatility is constant?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does *fat tail* mean? Why should we care about fat tail?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Could you write a Python program to download the option data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you download all maturity dates?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on several issues, especially on volatility measures
    and ARCH/GARCH. For the volatility measures, first we discussed the widely used
    standard deviation, which is based on the normality assumption. To show that such
    an assumption might not hold, we introduced several normality tests, such as the
    Shapiro-Wilk test and the Anderson-Darling test. To show a fat tail of many stocks'
    real distribution benchmarked on a normal distribution, we vividly used various
    graphs to illustrate it. To show that the volatility might not be constant, we
    presented the test to compare the variance over two periods. Then, we showed a
    Python program to conduct the Breusch-Pangan (1979) test for heteroskedasticity.
    ARCH and GARCH are used widely to describe the evolution of volatility over time.
    For these models, we simulate their simple form such as ARCH (1) and GARCH (1,1)
    processes. In addition to their graphical presentations, the Python codes of Kevin
    Sheppard are included to solve the GJR_GARCH (1,1,1) process.
  prefs: []
  type: TYPE_NORMAL
