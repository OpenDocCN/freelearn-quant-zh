- en: Strategy Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alpha factors drive an algorithmic strategy that translates into trades that,
    in turn, produce a portfolio. The returns and risk of the resulting portfolio
    determine the success of the strategy. Testing a strategy requires simulating
    the portfolios generated by an algorithm to verify its performance under market
    conditions. Strategy evaluation includes backtesting against historical data to
    optimize the strategy's parameters, and forward-testing to validate the in-sample
    performance against new, out-of-sample data and avoid false discoveries from tailoring
    a strategy to specific past circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a portfolio context, positive asset returns can offset negative price movements
    in a non-linear way so that the overall variation of portfolio returns is less
    than the weighted average of the variation of the portfolio positions unless their
    returns are perfectly and positively correlated. Harry Markowitz developed the
    theory behind modern portfolio management based on diversification in 1952, which
    gave rise to mean-variance optimization: for a given set of assets, portfolio
    weights can be optimized to reduce risk, measured as the standard deviation of
    returns for a given expected level of returns.'
  prefs: []
  type: TYPE_NORMAL
- en: The **capital asset pricing model** (**CAPM**) introduced a risk premium as
    an equilibrium reward for holding an asset that compensates for the exposure to
    a single risk factor—the market—that cannot be diversified away. Risk management
    has evolved to become much more sophisticated as additional risk factors and more
    granular choices for exposure have emerged. The Kelly Rule is a popular approach
    to dynamic portfolio optimization, which is the choice of a sequence of positions
    over time; it has been famously adapted from its original application in gambling
    to the stock market by Edward Thorp in 1968.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, there are several approaches to optimize portfolios that include
    the application of **machine learning** (**ML**) to learn hierarchical relationships
    among assets and treat their holdings as complements or substitutes with respect
    to the portfolio risk profile.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to build and test a portfolio based on alpha factors using `zipline`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to measure portfolio risk and return
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to evaluate portfolio performance using `pyfolio`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to manage portfolio weights using mean-variance optimization and alternatives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use machine learning to optimize asset allocation in a portfolio context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code examples for this chapter are in the `05_strategy_evaluation_and_portfolio_management`
    directory of the companion GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: How to build and test a portfolio with zipline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we introduced `zipline` to simulate the computation of
    alpha factors from trailing cross-sectional market, fundamental, and alternative
    data. Now we will exploit the alpha factors to derive and act on buy and sell
    signals. We will postpone optimizing the portfolio weights until later in this
    chapter, and for now, just assign positions of equal value to each holding. The
    code for this section is in the `01_trading_zipline` subdirectory.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduled trading and portfolio rebalancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the custom `MeanReversion` factor developed in the last chapter—see
    the implementation in `alpha_factor_zipline_with_trades.py`.
  prefs: []
  type: TYPE_NORMAL
- en: The `Pipeline` created by the `compute_factors()` method returns a table with
    a long and a short column for the 25 stocks with the largest negative and positive
    deviations of their last monthly return from its annual average, normalized by
    the standard deviation. It also limited the universe to the 500 stocks with the
    highest average trading volume over the last 30 trading days. `before_trading_start()` ensures
    the daily execution of the pipeline and the recording of the results, including
    the current prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new `rebalance()` method submits trade orders to the `exec_trades()` method
    for the assets flagged for long and short positions by the pipeline with equal
    positive and negative weights. It also divests any current holdings that are no
    longer included in the factor signals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `rebalance()` method runs according to `date_rules` and `time_rules` set
    by the `schedule_function()` utility at the beginning of the week, right after
    `market_open` as stipulated by the built-in `US_EQUITIES` calendar (see docs for
    details on rules). You can also specify a trade commission both in relative terms
    and as a minimum amount. There is also an option to define slippage, which is
    the cost of an adverse change in price between trade decision and execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The algorithm continues to execute after calling the `run_algorithm()` function
    and returns the same backtest performance `DataFrame`. We will now turn to common
    measures of portfolio return and risk, and how to compute them using the `pyfolio`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: How to measure performance with pyfolio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML is about optimizing objective functions. In algorithmic trading, the objectives
    are the return and the risk of the overall investment portfolio, typically relative
    to a benchmark (which may be cash or the risk-free interest rate).
  prefs: []
  type: TYPE_NORMAL
- en: There are several metrics to evaluate these objectives. We will briefly review
    the most commonly-used metrics and how to compute them using the `pyfolio` library,
    which is also used by `zipline` and Quantopian. We will also review how to apply
    these metrics on Quantopian when testing an algorithmic trading strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use some simple notations: let *R* be the time series of one-period
    simple portfolio returns, *R=(r[1], ..., r[T])*, from dates 1 to *T*, and *R^f =(r^f[1],
    ..., r^f[T]**)* be the matching time series of risk-free rates, so that *R^([e])=R-R^([f]) =(r[1]-r^f[1],...,
    r[T]-r^f[T])* is the excess return.'
  prefs: []
  type: TYPE_NORMAL
- en: The Sharpe ratio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ex-ante **Sharpe ratio** (**SR**) compares the portfolio''s expected excess
    portfolio to the volatility of this excess return, measured by its standard deviation.
    It measures the compensation as the average excess return per unit of risk taken:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fca453c0-1d24-46da-b421-3a70e66f38e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Expected returns and volatilities are not observable, but can be estimated
    as follows using historical data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9ea9cdb-a152-4593-93a3-546fdc9a21e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Unless the risk-free rate is volatile (as in emerging markets), the standard
    deviation of excess and raw returns will be similar. When the SR is used with
    a benchmark other than the risk-free rate, for example, the S&P 500, it is called
    the **i****nformation ratio**. In this case, it measures the excess return of
    the portfolio, also called **alpha**, relative to the tracking error, which is
    the deviation of the portfolio returns from the benchmark returns.
  prefs: []
  type: TYPE_NORMAL
- en: For **independently and identically-distributed** (**iid**) returns, the derivation
    of the distribution of the estimator of the SR for tests of statistical significance follows
    from the application of the Central Limit Theorem, according to large-sample statistical
    theory, to μ̂ and σ̂².
  prefs: []
  type: TYPE_NORMAL
- en: However, financial returns often violate the iid assumptions. Andrew Lo has
    derived the necessary adjustments to the distribution and the time aggregation
    for returns that are stationary but autocorrelated returns. This is important
    because the time-series properties of investment strategies (for example, mean
    reversion, momentum, and other forms of serial correlation) can have a non-trivial
    impact on the SR estimator itself, especially when annualizing the SR from higher-frequency
    data (Lo 2002).
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental law of active management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A high **Information Ratio** (**IR**) implies attractive out-performance relative
    to the additional risk taken. The Fundamental Law of Active Management breaks
    the IR down into the **information coefficient** (**IC**) as a measure of forecasting
    skill, and the ability to apply this skill through independent bets. It summarizes
    the importance to play both often (high breadth) and to play well (high IC):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/117dfc33-30b4-4fad-8295-166fbe1f8d54.png)'
  prefs: []
  type: TYPE_IMG
- en: The IC measures the correlation between an alpha factor and the forward returns
    resulting from its signals and captures the accuracy of a manager's forecasting
    skills. The breadth of the strategy is measured by the independent number of bets
    an investor makes in a given time period, and the product of both values is proportional
    to the IR, also known as **appraisal risk** (Treynor and Black).
  prefs: []
  type: TYPE_NORMAL
- en: This framework has been extended to include the **t****ransfer coefficient**
    (**TC**) to reflect portfolio constraints (for example, on short-selling) that
    may limit the information ratio below a level otherwise achievable given IC or
    strategy breadth. The TC proxies the efficiency with which the manager translates
    insights into portfolio bets (Clarke et al. 2002).
  prefs: []
  type: TYPE_NORMAL
- en: 'The fundamental law is important because it highlights the key drivers of outperformance:
    both accurate predictions and the ability to make independent forecasts and act
    on these forecasts matter. In practice, managers with a broad set of investment
    decisions can achieve significant risk-adjusted excess returns with information
    coefficients between 0.05 and 0.15 (if there is space possibly include simulation
    chart).'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, estimating the breadth of a strategy is difficult given the cross-sectional
    and time-series correlation among forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: In and out-of-sample performance with pyfolio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pyfolio facilitates the analysis of portfolio performance and risk in-sample
    and out-of-sample using many standard metrics. It produces tear sheets covering
    the analysis of returns, positions, and transactions, as well as event risk during
    periods of market stress using several built-in scenarios, and also includes Bayesian
    out-of-sample performance analysis.
  prefs: []
  type: TYPE_NORMAL
- en: It relies on portfolio returns and position data, and can also take into account
    the transaction costs and slippage losses of trading activity. The metrics are
    computed using the `empyrical` library that can also be used on a standalone basis.
  prefs: []
  type: TYPE_NORMAL
- en: The performance `DataFrame` produced by the `zipline` backtesting engine can
    be translated into the requisite `pyfolio` input.
  prefs: []
  type: TYPE_NORMAL
- en: Getting pyfolio input from alphalens
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'However, `pyfolio` also integrates with `alphalens` directly and permits the
    creation of `pyfolio` input data using `create_pyfolio_input`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two options to specify how portfolio weights will be generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '`long_short`: If `False`, weights will correspond to factor values divided
    by their absolute value so that negative factor values generate short positions.
    If `True`, factor values are first demeaned so that long and short positions cancel
    each other out and the portfolio is market neutral.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`equal_weight`: If `True`, and `long_short` is `True`, assets will be split
    into two equal-sized groups with the top/bottom half making up long/short positions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long-short portfolios can also be created for groups if `factor_data` includes,
    for example, sector info for each asset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting pyfolio input from a zipline backtest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The result of a `zipline` backtest can be converted into the required `pyfolio`
    input using `extract_rets_pos_txn_from_zipline`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Walk-forward testing  out-of-sample returns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing a trading strategy involves backtesting against historical data to fine-tune
    alpha factor parameters, as well as forward-testing against new market data to
    validate that the strategy performs well out of sample or if the parameters are
    too closely tailored to specific historical circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: Pyfolio allows for the designation of an out-of-sample period to simulate walk-forward
    testing. There are numerous aspects to take into account when testing a strategy
    to obtain statistically reliable results, which we will address here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `plot_rolling_returns` function displays cumulative in and out-of-sample
    returns against a user-defined benchmark (we are using the S&P 500):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot includes a cone that shows expanding confidence intervals to indicate
    when out-of-sample returns appear unlikely given random-walk assumptions. Here,
    our strategy did not perform well against the benchmark during the simulated 2017
    out-of-sample period:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b28cb6a9-97ae-4190-8387-47b94f84b7f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary performance statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`pyfolio` offers several analytic functions and plots. The `perf_stats` summary
    displays the annual and cumulative returns, volatility, skew, and kurtosis of
    returns and the SR. The following additional metrics (which can also be calculated
    individually) are most important:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Max drawdown**: Highest percentage loss from the previous peak'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calmar ratio**: Annual portfolio return relative to maximal drawdown'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Omega ratio**: The probability-weighted ratio of gains versus losses for
    a return target, zero per default'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sortino ratio**: Excess return relative to downside standard deviation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tail ratio**: Size of the right tail (gains, the absolute value of the 95th
    percentile) relative to the size of the left tail (losses, abs. value of the 5th
    percentile)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Daily value at risk (VaR)**: Loss corresponding to a return two standard
    deviations below the daily mean'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alpha**: Portfolio return unexplained by the benchmark return'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beta**: Exposure to the benchmark'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For the simulated long-short portfolio derived from the `MeanReversion` factor,
    we obtain the following performance statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric | All | In-sample | Out-of-sample | Metric | All | In-sample | Out-of-sample
    |'
  prefs: []
  type: TYPE_TB
- en: '| Annual return | 1.80% | 0.60% | 4.20% | Skew | 0.34 | 0.40 | 0.09 |'
  prefs: []
  type: TYPE_TB
- en: '| Cumulative returns | 5.40% | 1.10% | 4.20% | Kurtosis | 3.70 | 3.37 | 2.59
    |'
  prefs: []
  type: TYPE_TB
- en: '| Annual volatility | 5.80% | 6.30% | 4.60% | Tail ratio | 0.91 | 0.88 | 1.03
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sharpe ratio | 0.33 | 0.12 | 0.92 | Daily value at risk | -0.7% | -0.8% |
    -0.6% |'
  prefs: []
  type: TYPE_TB
- en: '| Calmar ratio | 0.17 | 0.06 | 1.28 | Gross leverage | 0.38 | 0.37 | 0.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Stability | 0.49 | 0.04 | 0.75 | Daily turnover | 4.70% | 4.40% | 5.10% |'
  prefs: []
  type: TYPE_TB
- en: '| Max drawdown | -10.10% | -10.10% | -3.30% | Alpha | 0.01 | 0.00 | 0.04 |'
  prefs: []
  type: TYPE_TB
- en: '| Omega ratio | 1.06 | 1.02 | 1.18 | Beta | 0.15 | 0.16 | 0.03 |'
  prefs: []
  type: TYPE_TB
- en: '| Sortino Ratio | 0.48 | 0.18 | 1.37 |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: See the appendix for details on the calculation and interpretation of portfolio
    risk and return metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Drawdown periods and factor exposure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `plot_drawdown_periods(returns)` function plots the principal drawdown periods
    for the portfolio, and several other plotting functions show the rolling SR and
    rolling factor exposures to the market beta or the Fama French size, growth, and
    momentum factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This plot, which highlights a subset of the visualization contained in the
    various tear sheets, illustrates how pyfolio allows us to drill down into the
    performance characteristics and exposure to fundamental drivers of risk and returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32af78a2-61fa-44ff-ada0-0529f4f89d06.png)'
  prefs: []
  type: TYPE_IMG
- en: Modeling event risk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pyfolio also includes timelines for various events that you can use to compare
    the performance of a portfolio to a benchmark during this period, for example, during
    the fall 2015 selloff following the Brexit vote:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0497ce2a-a1ce-4257-a253-8ab5dee8e432.png)'
  prefs: []
  type: TYPE_IMG
- en: How to avoid the pitfalls of backtesting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Backtesting simulates an algorithmic strategy using historical data with the
    goal of identifying patterns that generalize to new market conditions. In addition
    to the generic challenges of predicting an uncertain future in changing markets,
    numerous factors make mistaking positive in-sample performance for the discovery
    of true patterns very likely. These factors include aspects of the data, the implementation
    of the strategy simulation, and flaws with the statistical tests and their interpretation.
    The risks of false discoveries multiply with the use of more computing power,
    bigger datasets, and more complex algorithms that facilitate the identification
    of apparent patterns in the noise.
  prefs: []
  type: TYPE_NORMAL
- en: We will list the most serious and common methodological mistakes and refer to
    the literature on multiple testing for further detail. We will also introduce
    the deflated SR that illustrates how to adjust metrics that result from repeated
    trials when using the same set of financial data for your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Data challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Challenges to the backtest validity due to data issues include look-ahead bias,
    survivorship bias, and outlier control.
  prefs: []
  type: TYPE_NORMAL
- en: Look-ahead bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tests of trading rules derived from past data will yield biased results when
    the sample data used to develop the rules contains information that was not, in
    fact, available or known at the point in time the data refers to.
  prefs: []
  type: TYPE_NORMAL
- en: A typical source of this bias is the failure to account for the common ex-post
    corrections of reported financials. Stock splits or reverse splits can also generate
    look-ahead bias. When computing the earnings yield, earnings-per-share data comes
    from company financials with low frequency, while market prices are available
    at least daily. Hence, both EPS and price data need to be adjusted for splits
    at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: The solution lies in the careful analysis of the timestamps associated with
    all data that enters a backtest to ensure that only point-in-time data is used.
    High-quality data providers, such as Compustat, ensure that these criteria are
    met. When point-in-time data is not available, assumptions about the lag in reporting
    needs to be made.
  prefs: []
  type: TYPE_NORMAL
- en: Survivorship bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Survivorship bias emerges when a backtest is conducted on data that only contains
    currently active securities and omits assets that have disappeared over time,
    for example, due to bankruptcy, delisting, or acquisition. Securities that are
    no longer part of the investment universe often did not perform well, and including
    these cases can positively skew the backtest result.
  prefs: []
  type: TYPE_NORMAL
- en: The solution, naturally, is to verify that datasets include all securities available over
    time as opposed to only those that are still available when running the test.
  prefs: []
  type: TYPE_NORMAL
- en: Outlier control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data preparation before analysis typically includes treatment of outliers, example,
    by winsorizing, or clipping, extreme values. The challenge is to identify outliers
    that are truly not representative of the period under analysis, as opposed to
    extreme values that are an integral part of the market environment at that time.
    Many market models assume normally-distributed data when extreme values are observed
    more frequently, as suggested by fat-tailed distributions.
  prefs: []
  type: TYPE_NORMAL
- en: The solution involves careful analysis of outliers with respect to the probability
    of extreme values occurring and adjusting the strategy parameters to this reality.
  prefs: []
  type: TYPE_NORMAL
- en: Unrepresentative period
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A backtest will not yield a representative result that generalizes to future
    periods if the time period used does not reflect the current environment well,
    lacks relevant market regime aspects, and does not include enough data points
    or captures extreme historical events that are unlikely to repeat.
  prefs: []
  type: TYPE_NORMAL
- en: The solution involves using sample periods that include important market phenomena,
    or generate synthetic data that reflect relevant market characteristics (see the
    *Resources* section for guidance on implementation).
  prefs: []
  type: TYPE_NORMAL
- en: Implementation issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Practical issues related to the implementation of the historical simulation
    include failure to mark to market, i.e. accurately reflect underlying market prices and
    account for drawdowns, unrealistic assumptions about the availability, cost, or
    market impact of trades, or the timing of signals and trade execution.
  prefs: []
  type: TYPE_NORMAL
- en: Mark-to-market performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This strategy may perform well over the course of the backtest but lead to unacceptable
    losses or volatility over time.
  prefs: []
  type: TYPE_NORMAL
- en: The solution involves plotting performance over time or calculating (rolling)
    risk metrics, such as **value at risk** (**VaR**) or the Sortino Ratio (see appendix
    for details).
  prefs: []
  type: TYPE_NORMAL
- en: Trading costs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This strategy may assume short sales that require a counter-party, hold less
    liquid assets that may move the market when traded or underestimate the costs
    that arise due to broker fees or slippage, which is the difference between the
    market price at the decision to trade and subsequent execution.
  prefs: []
  type: TYPE_NORMAL
- en: The solution includes a limitation to a highly liquid universe and realistic
    parameter assumptions for trading and slippage costs (as illustrated in the preceding `zipline`
    example). This also safeguards against the inclusion of unstable factor signals
    with a high decay and, hence, turnover.
  prefs: []
  type: TYPE_NORMAL
- en: Timing of trades
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simulation could make unrealistic assumptions about the timing of the evaluation
    of the alpha factor signals and the resulting trades. For instance, signals may
    be evaluated at close prices when the next trade is only available at the often-quite-different
    open prices. As a consequence, the backtest will be significantly biased when
    the close price is used to evaluate trading performance.
  prefs: []
  type: TYPE_NORMAL
- en: The solution involves careful orchestration of the sequence of signal arrival,
    trade execution, and performance evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Data-snooping and backtest-overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most prominent challenge to backtest validity, including to published results,
    relates to the discovery of spurious patterns due to multiple testing during the
    strategy-selection process. Selecting a strategy after testing different candidates
    on the same data will likely bias the choice because a positive outcome is more
    likely to be due to the stochastic nature of the performance measure itself. In
    other words, the strategy is overly tailored, or overfit, to the data at hand
    and produces deceptively positive results.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, backtest performance is not informative unless the number of trials is
    reported to allow for an assessment of the risk of selection bias. This is rarely
    the case in practical or academic research, inviting doubts about the validity
    of many published claims.
  prefs: []
  type: TYPE_NORMAL
- en: The risk of overfitting a backtest to a particular dataset does not only arise
    from directly running numerous tests but includes strategies designed based on
    prior knowledge of what works and doesn't, that is, knowledge of different backtests
    run by others on the same data. As a result, backtest-overfitting is hard to avoid in
    practice.
  prefs: []
  type: TYPE_NORMAL
- en: Solutions include selecting tests to undertake based on investment or economic
    theory rather than broad data-mining efforts. It also implies testing in a variety
    of contexts and scenarios, including possibly on synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: The minimum backtest length and the deflated SR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Marcos Lopez de Prado ([http://www.quantresearch.info/](http://www.quantresearch.info/))
    has published extensively on the risks of backtesting, and how to detect or avoid
    it. This includes an online simulator of backtest-overfitting ([http://datagrid.lbl.gov/backtest/](http://datagrid.lbl.gov/backtest/)).
  prefs: []
  type: TYPE_NORMAL
- en: Another result includes an estimate of the minimum length of the backtest that
    an investor should require given the number of trials attempted, to avoid selecting
    a strategy with a given in-sample SR during a given number of trials that has
    an expected out-of-sample SR of zero. This implies that, e.g., if only two years
    of daily backtest data is available no more than seven strategy variations should
    be tried, and if only five years of daily backtest data is available, no more
    than 45 strategy variations should be tried. See references for implementation
    details.
  prefs: []
  type: TYPE_NORMAL
- en: De Lopez Prado and Bailey (2014) also derive a deflated SR to compute the probability
    that the SR is statistically significant while controlling for the inflationary
    effect of multiple testing, non-normal returns, and shorter sample lengths (see
    the `03_multiple_testing` subdirectory for the Python implementation of `deflated_sharpe_ratio.py`
    and references for the derivation of the related formulas).
  prefs: []
  type: TYPE_NORMAL
- en: Optimal stopping for backtests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to limiting backtests to strategies that can be justified on theoretical
    grounds as opposed to as mere data-mining exercises, an important question is
    when to stop running additional tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the solution to the *secretary problem* from optimal stopping theory,
    the recommendation is to decide according to the following rule of thumb: test
    a random sample of 1/e (roughly 37%) of reasonable strategies and record their
    performance. Then, continue tests until a strategy outperforms those tested before.'
  prefs: []
  type: TYPE_NORMAL
- en: This rule applies to tests of several alternatives with the goal to choose a
    near-best as soon as possible while minimizing the risk of a false positive.
  prefs: []
  type: TYPE_NORMAL
- en: How to manage portfolio risk and return
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Portfolio management aims to take positions in financial instruments that achieve
    the desired risk-return trade-off regarding a benchmark. In each period, a manager
    selects positions that optimize diversification to reduce risks while achieving
    a target return. Across periods, the positions will be rebalanced to account for
    changes in weights resulting from price movements to achieve or maintain a target
    risk profile.
  prefs: []
  type: TYPE_NORMAL
- en: Diversification permits us to reduce risks for a given expected return by exploiting
    how price movements interact with each other as one asset's gains can make up
    for another asset's losses. Harry Markowitz invented **Modern Portfolio Theory**
    (**MPT**) in 1952  and provided the mathematical tools to optimize diversification
    by choosing appropriate portfolio weights. Markowitz showed how portfolio risk,
    measured as the standard deviation of portfolio returns, depends on the covariance
    among the returns of all assets and their relative weights. This relationship
    implies the existence of an efficient frontier of portfolios that maximize portfolio
    returns given a maximal level of portfolio risk.
  prefs: []
  type: TYPE_NORMAL
- en: However, mean-variance frontiers are highly sensitive to the estimates of the
    input required for their calculation, such as expected returns, volatilities,
    and correlations. In practice, mean-variance portfolios that constrain these input
    to reduce sampling errors have performed much better. These constrained special
    cases include equal-weighted, minimum-variance, and risk-parity portfolios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Capital Asset Pricing Model (CAPM) is an asset valuation model that builds
    on the MPT risk-return relationship. It introduces the concept of a risk premium
    that an investor can expect in market equilibrium for holding a risky asset; the
    premium compensates for the time value of money and the exposure to overall market
    risk that cannot be eliminated through diversification (as opposed to the idiosyncratic
    risk of specific assets). The economic rationale for non-diversifiable risk is,
    for example, macro drivers of the business risks affecting equity returns or bond
    defaults. Hence, an asset''s expected return, E[r[i]], is the sum of the risk-free
    interest rate, r[f], and a risk premium proportional to the asset''s exposure
    to the expected excess return of the market portfolio, r[m], over the risk-free
    rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dab5d16b-ced4-4d87-8b58-6223883f1aec.png)'
  prefs: []
  type: TYPE_IMG
- en: In theory, the market portfolio contains all investable assets and will be held
    by all rational investors in equilibrium. In practice, a broad value-weighted
    index approximates the market, for example, the S&P 500 for US equity investments. β[i ]measures
    the exposure to the excess returns of the market portfolio. If the CAPM is valid, the
    intercept component, α[i], should be zero. In reality, the CAPM assumptions are
    often not met, and alpha captures the returns left unexplained by exposure to
    the broad market.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, research uncovered non-traditional sources of risk premiums, such
    as the momentum or the equity value effects that explained some of the original
    alpha. Economic rationales, such as behavioral biases of under or overreaction
    by investors to new information justify risk premiums for exposure to these alternative
    risk factors. They evolved into investment styles designed to capture these alternative
    betas that also became tradable in the form of specialized index funds. After
    isolating contributions from these alternative risk premiums, true alpha becomes
    limited to idiosyncratic asset returns and the manager's ability to time risk
    exposures.
  prefs: []
  type: TYPE_NORMAL
- en: The EMH has been refined over the past several decades to rectify many of the
    original shortcomings of the CAPM, including imperfect information and the costs
    associated with transactions, financing, and agency. Many behavioral biases have
    the same effect, and some frictions are modeled as behavioral biases.
  prefs: []
  type: TYPE_NORMAL
- en: ML plays an important role in deriving new alpha factors using supervised and
    unsupervised learning techniques based on the market, fundamental, and alternative
    data sources discussed in the previous chapters. The inputs to a machine learning
    model consist of both raw data and features engineered to capture informative
    signals. ML models are also used to combine individual predictive signals and
    deliver higher-aggregate predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modern portfolio theory and practice have evolved significantly over the last
    several decades. We will introduce:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean-variance optimization, and its shortcomings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatives such as minimum-risk and 1/n allocation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Risk parity approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Risk factor approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean-variance optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MPT solves for the optimal portfolio weights to minimize volatility for a given
    expected return, or maximize returns for a given level of volatility. The key
    requisite input are expected asset returns, standard deviations, and the covariance
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Diversification works because the variance of portfolio returns depends on
    the covariance of the assets and can be reduced below the weighted average of
    the asset variances by including assets with less than perfect correlation. In
    particular, given a vector, ω, of portfolio weights and the covariance matrix,
    Σ, the portfolio variance, σ[PF], is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de5090c0-6172-4dce-8787-62a5fec1268f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Markowitz showed that the problem of maximizing the expected portfolio return
    subject to a target risk has an equivalent dual representation of minimizing portfolio
    risk subject to a target expected return level, μ[PF.] Hence, the optimization
    problem becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17e9a8e3-4c6a-477f-aa99-2d5d8a0a4d45.png)'
  prefs: []
  type: TYPE_IMG
- en: The efficient frontier in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can calculate an efficient frontier using `scipy.optimize.minimize` and
    the historical estimates for asset returns, standard deviations, and the covariance
    matrix. The code can be found in the `efficient_frontier` subfolder of the repo
    for this chapter and implements the following sequence of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The simulation generates random weights using the Dirichlet distribution, and
    computes the mean, standard deviation, and SR for each sample portfolio using
    the historical return data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the quadratic optimization problem to solve for the minimum standard
    deviation for a given return or the maximum SR. To this end, define the functions
    that measure the key metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a target function that represents the negative SR for `scipy`''s `minimize` function
    to optimize given the constraints that the weights are bounded by, [-1, 1], and
    sum to one in absolute terms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the efficient frontier by iterating over a range of target returns
    and solving for the corresponding minimum variance portfolios. The optimization
    problem and the constraints on portfolio risk and return as a function of the
    weights can be formulated as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The solution requires iterating over ranges of acceptable values to identify
    optimal risk-return combinations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The simulation yields a subset of the feasible portfolios, and the efficient
    frontier identifies the optimal in-sample return-risk combinations that were achievable
    given historic data. The below figure shows the result including the minimum variance
    portfolio and the portfolio that maximizes the SR and several portfolios produce
    by alternative optimization strategies that we discuss in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a3d6abc-f1fb-4752-abd5-45decc1a9f6d.png)'
  prefs: []
  type: TYPE_IMG
- en: The portfolio optimization can be run at every evaluation step of the trading
    strategy to optimize the positions.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and shortcomings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding mean-variance frontier example illustrates the in-sample, backward-looking
    optimization. In practice, portfolio optimization requires forward-looking input.
    Expected returns are notoriously difficult to estimate accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 'The covariance matrix can be estimated somewhat more reliably, which has given
    rise to several alternative approaches. However, covariance matrices with correlated
    assets pose computational challenges since the optimization problem requires inverting
    the matrix. The high condition number induces numerical instability, which in
    turn gives rise to Markovitz curse: the more diversification is required (by correlated
    investment opportunities), the more unreliable the weights produced by the algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Many investors prefer to use portfolio-optimization techniques with less onerous
    input requirements. We now introduce several alternatives that aim to address
    these shortcomings, including more recent approaches based on machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatives to mean-variance optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The challenges with accurate input for the mean-variance optimization problem
    have led to the adoption of several practical alternatives that constrain the
    mean, the variance, or both, or omit return estimates that are more challenging,
    such as the risk parity approach.
  prefs: []
  type: TYPE_NORMAL
- en: The 1/n portfolio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simple portfolios provide useful benchmarks to gauge the added value of complex
    models that generate the risk of overfitting. The simplest strategy—an equally-weighted
    portfolio—has been shown to be one of the best performers.
  prefs: []
  type: TYPE_NORMAL
- en: Famously, de Miguel, Garlappi, and Uppal (2009) compared the out-of-sample performance
    of portfolios produced by various mean-variance optimizers, including robust Bayesian
    estimators, portfolio constraints, and optimal combinations of portfolios, to
    the simple 1/N rule. They found that the 1/N portfolio produced a higher Sharpe
    ratio than each asset class position, explained by the high cost of estimation
    errors that often outweighs the benefits of sophisticated optimization out-of-sample.
  prefs: []
  type: TYPE_NORMAL
- en: The 1/n portfolio is also included in the efficient frontier figure above.
  prefs: []
  type: TYPE_NORMAL
- en: The minimum-variance portfolio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another alternative is the **global minimum variance** (**GMV**) portfolio,
    which prioritizes the minimization of risk. It is shown in the efficient frontier figure
     and can be calculated as follows by minimizing the portfolio standard deviation
    using the mean-variance framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The corresponding `min.` volatility portfolio lies on the efficient frontier
    as shown above.
  prefs: []
  type: TYPE_NORMAL
- en: Global Portfolio Optimization - The Black-Litterman approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Global Portfolio Optimization approach of Black and Litterman (1992) combines
    economic models with statistical learning and is popular because it generates
    estimates of expected returns that are plausible in many situations.
  prefs: []
  type: TYPE_NORMAL
- en: The technique departs from the assumption that the market is a mean-variance
    portfolio implied by the CAPM equilibrium model, and builds on the fact that the
    observed market capitalization can be considered as optimal weights assigned by
    the market. Market weights reflect market prices that, in turn, embody the market’s
    expectations of future returns.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the approach can reverse-engineer the unobservable future expected returns
    from the assumption that the market is close enough to equilibrium, as defined
    by the CAPM, and allow investors to adjust these estimates to their own beliefs
    using a shrinkage estimator. The model can be interpreted as a Bayesian approach
    to portfolio optimization. We will introduce Bayesian methods in [Chapter 9](17b367a4-e525-41d4-8cec-0409f29b94c1.xhtml),
    *Bayesian Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: How to size your bets – the Kelly rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kelly rule has a long history in gambling because it provides guidance on
    how much to stake on each of an (infinite) sequence of bets with varying (but
    favorable) odds to maximize terminal wealth. It was published as A New Interpretation
    of the Information Rate in 1956 by John Kelly who was a colleague of Claude Shannon's
    at Bell Labs. He was intrigued by bets placed on candidates at the new quiz show
    The $64,000 Question, where a viewer on the west coast used the three-hour delay
    to obtain insider information about the winners.
  prefs: []
  type: TYPE_NORMAL
- en: Kelly drew a connection to Shannon's information theory to solve for the bet
    that is optimal for long-term capital growth when the odds are favorable, but
    uncertainty remains. His rule maximizes logarithmic wealth as a function of the
    odds of success of each game, and includes implicit bankruptcy protection since
    log(0) is negative infinity so that a Kelly gambler would naturally avoid losing
    everything.
  prefs: []
  type: TYPE_NORMAL
- en: The optimal size of a bet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kelly began by analyzing games with a binary win-lose outcome. The key variables
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**b**: The odds define the amount won for a $1 bet. Odds = 5/1 implies a $5
    gain if the bet wins, plus recovery of the $1 capital.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p**: The probability defines the likelihood of a favorable outcome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**f**: The share of the current capital to bet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**V**: The value of the capital as a result of betting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Kelly rule aims to maximize the value''s growth rate, *G*, of infinitely-repeated
    bets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17ffd2aa-c353-4e40-a6e2-c95e6da97e3b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When *W* and *L* are the numbers of wins and losses, then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94cd25fd-49f3-4e19-8cb6-fa1a8c52dc8e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can maximize the rate of growth *G* by maximizing *G* with respect to *f*,
    as illustrated using `sympy` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We arrive at the optimal share of capital to bet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c72b542-267e-43bd-b9ba-4f49a4bf8021.png)'
  prefs: []
  type: TYPE_IMG
- en: Optimal investment – single asset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a financial market context, both outcomes and alternatives are more complex,
    but the Kelly rule logic does still apply. It was made popular by Ed Thorp, who
    first applied it profitably to gambling (described in Beat the Dealer) and later
    started the successful hedge fund Princeton/Newport Partners.
  prefs: []
  type: TYPE_NORMAL
- en: 'With continuous outcomes, the growth rate of capital is defined by an integrate
    over the probability distribution of the different returns that can be optimized
    numerically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/798661c9-6fc4-4ccb-9e03-bbb7d4e6344e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can solve this expression for the optimal f^* using the `scipy.optimize` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Optimal investment – multiple assets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use an example with various equities. E. Chan (2008) illustrates how
    to arrive at a multi-asset application of the Kelly Rule, and that the result
    is equivalent to the (potentially levered) maximum Sharpe ratio portfolio from
    the mean-variance optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The computation involves the dot product of the precision matrix, which is
    the inverse of the covariance matrix, and the return matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The Kelly Portfolio is also shown in the efficient frontier diagram (after normalization
    so that the absolute weights sum to one). Many investors prefer to reduce the
    Kelly weights to reduce the strategy's volatility, and Half-Kelly has become particularly
    popular.
  prefs: []
  type: TYPE_NORMAL
- en: Risk parity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fact that the previous 15 years have been characterized by two major crises
    in the global equity markets, a consistently upwardly-sloping yield curve, and
    a general decline in interest rates made risk parity look like a particularly
    compelling option. Many institutions carved out strategic allocations to risk
    parity to further diversify their portfolios.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple implementation of risk parity allocates assets according to the inverse
    of their variances, ignoring correlations and, in particular, return forecasts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The risk parity portfolio is also shown in the efficient frontier diagram at
    the beginning of this section.
  prefs: []
  type: TYPE_NORMAL
- en: Risk factor investment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative framework for estimating input is to work down to the underlying
    determinants, or factors, that drive the risk and returns of assets. If we understand
    how the factors influence returns, and we understand the factors, we will be able
    to construct more robust portfolios.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of factor investing looks beyond asset class labels to the underlying
    factor risks to maximize the benefits of diversification. Rather than distinguishing
    investment vehicles by labels such as hedge funds or private equity, factor investing
    aims to identify distinct risk-return profiles based on differences in exposure
    to fundamental risk factors. The naïve approach to mean-variance investing plugs
    (artificial) groupings as distinct asset classes into a mean-variance optimizer.
    Factor investing recognizes that such groupings share many of the same factor
    risks as traditional asset classes. Diversification benefits can be overstated,
    as investors discovered during the last crisis when correlations among risky asset
    classes increased due to exposure to the same underlying factor risks.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical risk parity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mean-variance optimization is very sensitive to the estimates of expected returns
    and the covariance of these returns. The covariance matrix inversion also becomes
    more challenging and less accurate when returns are highly correlated, as is often
    the case in practice. The result has been called the Markowitz curse: when diversification
    is more important because investments are correlated, conventional portfolio optimizers
    will likely produce an unstable solution. The benefits of diversification can
    be more than offset by mistaken estimates. As discussed, even naive, equally-weighted
    portfolios can beat mean-variance and risk-based optimization out of sample.'
  prefs: []
  type: TYPE_NORMAL
- en: More robust approaches have incorporated additional constraints (Clarke et al.,
    2002), Bayesian priors (Black and Litterman, 1992), or used shrinkage estimators
    to make the precision matrix more numerically stable (Ledoit and Wolf [2003],
    available in scikit-learn ( [http://scikit-learn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html](http://scikit-learn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html)). **Hierarchical
    risk parity** (**HRP**), in contrast, leverages unsupervised machine learning
    to achieve superior out-of-sample portfolio allocations.
  prefs: []
  type: TYPE_NORMAL
- en: 'A recent innovation in portfolio optimization leverages graph theory and hierarchical
    clustering to construct a portfolio in three steps (Lopez de Prado, 2015):'
  prefs: []
  type: TYPE_NORMAL
- en: Define a distance metric so that correlated assets are close to each other,
    and apply single-linkage clustering to identify hierarchical relationships
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the hierarchical correlation structure to quasi-diagonalize the covariance
    matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply top-down inverse-variance weighting using a recursive bisectional search
    to treat clustered assets as complements rather than substitutes in portfolio construction
    and to reduce the number of degrees of freedom.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A related method to construct **hierarchical clustering portfolios** (**HCP**)
    was presented by Raffinot (2016). Conceptually, complex systems such as financial
    markets tend to have a structure and are often organized in a hierarchical way,
    while the interaction among elements in the hierarchy shapes the dynamics of the
    system. Correlation matrices also lack the notion of hierarchy, which allows weights
    to vary freely and in potentially unintended ways.
  prefs: []
  type: TYPE_NORMAL
- en: Both HRP and HCP have been tested by JPM on various equity universes. The HRP, in particular,
    produced equal or superior risk-adjusted returns and Sharpe ratios compared to
    naive diversification, the maximum-diversified portfolios, or GMV portfolios.
  prefs: []
  type: TYPE_NORMAL
- en: We will present the Python implementation in [Chapter 12](c187906e-9fde-4f85-b709-df88dd0f7e88.xhtml),
    *Unsupervised Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the important topic of portfolio management, which
    involves the combination of investment positions with the objective of managing
    risk-return trade-offs. We introduced `pyfolio` to compute and visualize key risk
    and return metrics and to compare the performance of various algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how important accurate predictions are to optimize portfolio weights
    and maximize diversification benefits. We also explored how ML can facilitate
    more effective portfolio construction by learning hierarchical relationships from
    the asset-returns covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: We will now move on to the second part of this book, which focuses on the use
    of ML models. These models will produce more accurate predictions by making more
    effective use of more diverse information to capture more complex patterns than
    the simpler alpha factors that were most prominent so far.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin by training, testing, and tuning linear models for regression
    and classification using cross-validation to achieve robust out-of-sample performance.
    We will also embed these models within the framework of defining and backtesting
    algorithmic trading strategies, which we covered in the last two chapters.
  prefs: []
  type: TYPE_NORMAL
