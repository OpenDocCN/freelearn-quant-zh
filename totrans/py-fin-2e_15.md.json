["```py\nimport numpy as np\nfrom matplotlib.finance import quotes_historical_yahoo_ochl as getData\n#\nticker='IBM' \nbegdate=(2009,1,1) \nenddate=(2013,12,31)\np =getData(ticker, begdate, enddate,asobject=True, adjusted=True)\nret = p.aclose[1:]/p.aclose[:-1]-1\nstd_annual=np.std(ret)*np.sqrt(252)\nprint('volatility (std)=',round(std_annual,4))\n('volatility (std)=', 0.2093)\n```", "```py\nimport numpy as np\nfrom scipy import stats\nfrom matplotlib.finance import quotes_historical_yahoo_ochl as getData \n#\nticker='IBM' \nbegdate=(2009,1,1) \nenddate=(2013,12,31)\np =getData(ticker, begdate, enddate,asobject=True, adjusted=True)\nret = p.aclose[1:]/p.aclose[:-1]-1\n#\nprint('ticker=',ticker,'W-test, and P-value') \nprint(stats.shapiro(ret))\n ('ticker=', 'IBM', 'W-test, and P-value')\n(0.9295020699501038, 7.266549629954468e-24)\n```", "```py\nprint(stats.anderson(ret))\nAndersonResult(statistic=inf, critical_values=array([ 0.574,  0.654,  0.785,  0.915,  1.089]), significance_level=array([ 15\\. ,  10\\. ,   5\\. ,   2.5,   1\\. ]))\n```", "```py\nimport numpy as np\nfrom scipy import stats, random\n#\nrandom.seed(12345)\nret=random.normal(0,1,50000)\nprint('mean =',np.mean(ret))\nprint('std =',np.std(ret))\nprint('skewness=',stats.skew(ret))\nprint('kurtosis=',stats.kurtosis(ret))\n('mean =', -0.0018105809899753157)\n('std =', 1.002778144574481)\n('skewness=', -0.014974456637295455)\n('kurtosis=', -0.03657086582842339)\n```", "```py\nimport numpy as np\nfrom scipy import stats\nfrom matplotlib.finance import quotes_historical_yahoo_ochl as getData \n#\nticker='^GSPC' \nbegdate=(1926,1,1)\nenddate=(2013,12,31)\np = getData(ticker, begdate, enddate,asobject=True, adjusted=True)\nret = p.aclose[1:]/p.aclose[:-1]-1\nprint( 'S&P500    n    =',len(ret))\nprint( 'S&P500    mean    =',round(np.mean(ret),8)) \nprint( 'S&P500    std    =',round(np.std(ret),8)) \nprint( 'S&P500    skewness=',round(stats.skew(ret),8))\nprint( 'S&P500    kurtosis=',round(stats.kurtosis(ret),8))\n```", "```py\n('S&P500\\tn\\t=', 16102)\n('S&P500\\tmean\\t=', 0.00033996)\n('S&P500\\tstd\\t=', 0.00971895)\n('S&P500\\tskewness=', -0.65037674)\n('S&P500\\tkurtosis=', 21.24850493)\n```", "```py\nimport datetime\nimport numpy as np\nimport pandas as pd \nfile=open(\"c:/temp/ffDaily.txt\",\"r\") \ndata=file.readlines()\nf=[]\nindex=[]\n#\nfor i in range(5,np.size(data)): \n    t=data[i].split() \n    t0_n=int(t[0]) \n    y=int(t0_n/10000) \n    m=int(t0_n/100)-y*100 \n    d=int(t0_n)-y*10000-m*100\n    index.append(datetime.datetime(y,m,d)) \n    for j in range(1,5):\n         k=float(t[j]) \n         f.append(k/100)\n#\nn=len(f) \nf1=np.reshape(f,[n/4,4])\nff=pd.DataFrame(f1,index=index,columns=['Mkt_Rf','SMB','HML','Rf'])\nff.to_pickle(\"c:/temp/ffDaily.pkl\")\n```", "```py\nimport numpy as np\nimport pandas as pd \nfrom scipy import stats\nfrom matplotlib.finance import quotes_historical_yahoo_ochl as getData \n#\nticker='IBM' \nbegdate=(2009,1,1) \nenddate=(2013,12,31)\np =getData(ticker, begdate, enddate,asobject=True, adjusted=True)\nret = p.aclose[1:]/p.aclose[:-1]-1\ndate_=p.date\nx=pd.DataFrame(data=ret,index=date_[1:],columns=['ret']) \n#\nff=pd.read_pickle('c:/temp/ffDaily.pkl') \nfinal=pd.merge(x,ff,left_index=True,right_index=True) \n#\nk=final.ret-final.RF\nk2=k[k<0] \nLPSD=np.std(k2)*np.sqrt(252)\nprint(\"LPSD=\",LPSD)\nprint(' LPSD (annualized) for ', ticker, 'is ',round(LPSD,3))\n```", "```py\n('LPSD=', 0.14556051947047091)\n(' LPSD (annualized) for ', 'IBM', 'is ', 0.146)\n```", "```py\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nfrom matplotlib.finance import quotes_historical_yahoo_ochl as getData\n#\n# input area\nticker='F'            # stock\nbegdate1=(1982,9,1)   # starting date for period 1 \nenddate1=(1987,9,1)   # ending date for period   1 \nbegdate2=(1987,12,1)  # starting date for period 2 \nenddate2=(1992,12,1)  # ending   date for period 2\n#\n# define a function\ndef ret_f(ticker,begdate,enddate):\n    p =getData(ticker, begdate, enddate,asobject=True, adjusted=True)\n    ret = p.aclose[1:]/p.aclose[:-1]-1 \n    date_=p.date\n    return pd.DataFrame(data=ret,index=date_[1:],columns=['ret'])\n#\n# call the above function twice \nret1=ret_f(ticker,begdate1,enddate1) \nret2=ret_f(ticker,begdate2,enddate2)\n#\n# output\nprint('Std period #1    vs. std period #2') \nprint(round(sp.std(ret1.ret),6),round(sp.std(ret2.ret),6)) \nprint('T value ,    p-value ') \nprint(sp.stats.bartlett(ret1.ret,ret2.ret))\n```", "```py\nStd period #1   vs. std period #2\n(0.01981, 0.017915)\nT value ,       p-value \nBartlettResult(statistic=12.747107745102099, pvalue=0.0003565601014515915)\n```", "```py\nimport numpy as np\nimport scipy as sp\nimport statsmodels.api as sm \n#\ndef breusch_pagan_test(y,x): \n    results=sm.OLS(y,x).fit() \n    resid=results.resid\n    n=len(resid)\n    sigma2 = sum(resid**2)/n \n    f = resid**2/sigma2 - 1\n    results2=sm.OLS(f,x).fit() \n    fv=results2.fittedvalues \n    bp=0.5 * sum(fv**2) \n    df=results2.df_model\n    p_value=1-sp.stats.chi.cdf(bp,df)\n    return round(bp,6), df, round(p_value,7)\n#\nsp.random.seed(12345) \nn=100\nx=[]\nerror1=sp.random.normal(0,1,n) \nerror2=sp.random.normal(0,2,n) \nfor i in range(n):\n    if i%2==1:\n        x.append(1) \n    else:\n        x.append(-1)\n#\ny1=x+np.array(x)+error1 \ny2=sp.zeros(n)\n#\nfor i in range(n): \n    if i%2==1:\n        y2[i]=x[i]+error1[i] \n    else:\n        y2[i]=x[i]+error2[i]\n\nprint ('y1 vs. x (we expect to accept the null hypothesis)') \nbp=breusch_pagan_test(y1,x)\n#\nprint('BP value,    df,    p-value') \nprint 'bp =', bp \nbp=breusch_pagan_test(y2,x)\nprint ('y2 vs. x    (we expect to rject the null hypothesis)') \nprint('BP value,    df,    p-value')\nprint('bp =', bp)\n```", "```py\ny1 vs. x (we expect to accept the null hypothesis)\nBP value,       df,     p-value\nbp = (0.596446, 1.0, 0.5508776)\ny2 vs. x        (we expect to rject the null hypothesis)\nBP value,       df,     p-value\n('bp =', (17.611054, 1.0, 0.0))\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ninfile=\"c:/temp/calls17march.txt\"\ndata=pd.read_table(infile,delimiter='\\t',skiprows=1)\nx=data['Strike']\ny0=list(data['Implied Volatility'])\nn=len(y0)\ny=[]\nfor i in np.arange(n):\n    a=float(y0[i].replace(\"%\",\"\"))/100.\n    y.append(a)\n    print(a)\n#\nplt.title(\"Volatility smile\")\nplt.figtext(0.55,0.80,\"IBM calls\")\nplt.figtext(0.55,0.75,\"maturity: 3/17/2017\")\nplt.ylabel(\"Volatility\")\nplt.xlabel(\"Strike Price\")\nplt.plot(x,y,'o')\nplt.show()\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.finance import quotes_historical_yahoo_ochl as getData\n#\nticker='^GSPC'\nbegdate=(1987,11,1)\nenddate=(2006,12,31)\n#\np = getData(ticker, begdate, enddate,asobject=True, adjusted=True)\nx=p.date[1:] \nret = p.aclose[1:]/p.aclose[:-1]-1\n#\nplt.title('Illustration of volatility clustering (S&P500)') \nplt.ylabel('Daily returns')\nplt.xlabel('Date') \nplt.plot(x,ret)\nplt.show()\n```", "```py\nimport scipy as sp \nimport matplotlib.pyplot as plt\n#\nsp.random.seed(12345)\nn=1000        # n is the number of observations\nn1=100        # we need to drop the first several observations \nn2=n+n1       # sum of two numbers\n#\na=(0.1,0.3)   # ARCH (1) coefficients alpha0 and alpha1, see Equation (3)\nerrors=sp.random.normal(0,1,n2) \nt=sp.zeros(n2)\nt[0]=sp.random.normal(0,sp.sqrt(a[0]/(1-a[1])),1) \nfor i in range(1,n2-1):\n    t[i]=errors[i]*sp.sqrt(a[0]+a[1]*t[i-1]**2) \n    y=t[n1-1:-1] # drop the first n1 observations \n#\nplt.title('ARCH (1) process')\nx=range(n) \nplt.plot(x,y)\nplt.show()\n```", "```py\nimport scipy as sp \nimport matplotlib.pyplot as plt\n#\nsp.random.seed(12345)\nn=1000          # n is the number of observations\nn1=100          # we need to drop the first several observations \nn2=n+n1         # sum of two numbers\n#\na=(0.1,0.3)     # ARCH coefficient\nalpha=(0.1,0.3)    # GARCH (1,1) coefficients alpha0 and alpha1, see Equation (3)\nbeta=0.2 \nerrors=sp.random.normal(0,1,n2) \nt=sp.zeros(n2)\nt[0]=sp.random.normal(0,sp.sqrt(a[0]/(1-a[1])),1)\n#\nfor i in range(1,n2-1): \n    t[i]=errors[i]*sp.sqrt(alpha[0]+alpha[1]*errors[i-1]**2+beta*t[i-1]**2)\n#\ny=t[n1-1:-1]    # drop the first n1 observations \nplt.title('GARCH (1,1) process')\nx=range(n) \nplt.plot(x,y)\nplt.show()\n```", "```py\nimport scipy as sp\nimport numpy as np\nimport matplotlib.pyplot as plt\n#\nsp.random.seed(12345) \nm=2\nn=100              # n is the number of observations\nnDrop=100          # we need to drop the first several observations \ndelta=2\nomega=1e-6 \nalpha=(0.05,0.05)\n#\nbeta=0.8 \nmu,ma,ar=0.0,0.0,0.0\ngamma=(0.0,0.0) \norder_ar=sp.size(ar) \norder_ma=sp.size(ma) \norder_beta=sp.size(beta)\n#\norder_alpha =sp.size(alpha) \nz0=sp.random.standard_normal(n+nDrop) \ndeltainv=1/delta \nspec_1=np.array([2])\nspec_2=np.array([2])\nspec_3=np.array([2])\nz = np.hstack((spec_1,z0)) \nt=np.zeros(n+nDrop)\nh = np.hstack((spec_2,t)) \ny = np.hstack((spec_3,t)) \neps0 = h**deltainv  * z\nfor i in range(m+1,n +nDrop+m-1):\n    t1=sum(alpha[::-1]*abs(eps0[i-2:i]))    # reverse \n    alpha =alpha[::-1] \n    t2=eps0[i-order_alpha-1:i-1]\n    t3=t2*t2 \n    t4=np.dot(gamma,t3.T)\n    t5=sum(beta* h[i-order_beta:i-1]) \n    h[i]=omega+t1-t4+ t5\n    eps0[i] = h[i]**deltainv * z[i] \n    t10=ar * y[i-order_ar:i-1] \n    t11=ma * eps0[i -order_ma:i-1]\n    y[i]=mu+sum(t10)+sum(t11)+eps0[i] \n    garch=y[nDrop+1:] \n    sigma=h[nDrop+1:]**0.5 \n    eps=eps0[nDrop+1:] \n    x=range(1,len(garch)+1) \n#\nplt.plot(x,garch,'r')\nplt.plot(x,sigma,'b')\nplt.title('GARCH(2,1) process')\nplt.figtext(0.2,0.8,'omega='+str(omega)+', alpha='+str(alpha)+',beta='+str(beta))\nplt.figtext(0.2,0.75,'gamma='+str(gamma)) \nplt.figtext(0.2,0.7,'mu='+str(mu)+', ar='+str(ar)+',ma='+str(ma)) \nplt.show()\n```", "```py\nimport numpy as np\nfrom numpy.linalg import inv\nimport matplotlib.pyplot as plt\nfrom matplotlib.mlab import csv2rec\nfrom scipy.optimize import fmin_slsqp \nfrom numpy import size, log, pi, sum, diff, array, zeros, diag, dot, mat, asarray, sqrt\n#\ndef gjr_garch_likelihood(parameters, data, sigma2, out=None): \n    mu = parameters[0]\n    omega = parameters[1] \n    alpha = parameters[2] \n    gamma = parameters[3] \n    beta = parameters[4]\n    T = size(data,0)\n    eps = data-mu\n    for t in xrange(1,T):\n        sigma2[t]=(omega+alpha*eps[t-1]**2+gamma*eps[t-1]**2*(eps[t- 1]<0)+beta*sigma2[t-1])\n        logliks = 0.5*(log(2*pi) + log(sigma2) + eps**2/sigma2) \n    loglik = sum(logliks)\n    if out is None: \n        return loglik\n    else:\n        return loglik, logliks, copy(sigma2)\n#\ndef gjr_constraint(parameters,data, sigma2, out=None):\n    alpha = parameters[2]\n    gamma = parameters[3] \n    beta = parameters[4]\n    return array([1-alpha-gamma/2-beta]) # Constraint alpha+gamma/2+beta<=1\n#\ndef hessian_2sided(fun, theta, args): \n    f = fun(theta, *args)\n    h = 1e-5*np.abs(theta) \n    thetah = theta + h\n    h = thetah-theta \n    K = size(theta,0) \n    h = np.diag(h)\n    fp = zeros(K) \n    fm = zeros(K)\n    for i in xrange(K):\n        fp[i] = fun(theta+h[i], *args) \n        fm[i] = fun(theta-h[i], *args)\n        fpp = zeros((K,K))\n        fmm = zeros((K,K)) \n    for i in xrange(K):\n        for j in xrange(i,K):\n            fpp[i,j] = fun(theta + h[i] + h[j], *args) \n            fpp[j,i] = fpp[i,j]\n            fmm[i,j] = fun(theta-h[i]-h[j], *args) \n            fmm[j,i] = fmm[i,j]\n            hh = (diag(h))\n            hh = hh.reshape((K,1))\n            hh = dot(hh,hh.T)\n            H = zeros((K,K)) \n    for i in xrange(K):\n        for j in xrange(i,K):\n            H[i,j] = (fpp[i,j]-fp[i]-fp[j] + f+ f-fm[i]-fm[j] + fmm[i,j])/hh[i,j]/2\n            H[j,i] = H[i,j]\n    return H\n```", "```py\ndef GJR_GARCH(ret): \n    import numpy as np\n    import scipy.optimize as op \n    startV=np.array([ret.mean(),ret.var()*0.01,0.03,0.09,0.90])\n    finfo=np.finfo(np.float64)\n    t=(0.0,1.0)\n    bounds=[(-10*ret.mean(),10*ret.mean()),(finfo.eps,2*ret.var()),t,t,t] \n    T=np.size(ret,0)\n    sigma2=np.repeat(ret.var(),T) \n    inV=(ret,sigma2)\n    return op.fmin_slsqp(gjr_garch_likelihood,startV,f_ieqcons=gjr_constraint,bounds=bounds,args=inV)\n#\n```", "```py\nsp.random.seed(12345) \nreturns=sp.random.uniform(-0.2,0.3,100) \ntt=GJR_GARCH(returns)\n```", "```py\nprint(tt)\nOptimization terminated successfully.    (Exit mode 0)\n            Current function value: -54.0664733128\n            Iterations: 12\n            Function evaluations: 94\n            Gradient evaluations: 12\n[  7.73958251e-02   6.65706323e-03   0.00000000e+00   2.09662783e-12\n   6.62024107e-01]\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ninfile=\"c:/temp/calls17march.txt\"\ndata=pd.read_table(infile,delimiter='\\t',skiprows=1)\nx=data['Strike']\ny0=list(data['Implied Volatility'])\nn=len(y0)\ny=[]\nfor i in np.arange(n):\n    a=float(y0[i].replace(\"%\",\"\"))/100.\n    y.append(a)\n    print(a)\n#\nplt.title(\"Volatility smile\")\nplt.figtext(0.55,0.80,\"IBM calls\")\nplt.figtext(0.55,0.75,\"maturity: 3/17/2017\")\nplt.ylabel(\"Volatility\")\nplt.xlabel(\"Strike Price\")\nplt.plot(x,y,'o')\nplt.show()\n```"]