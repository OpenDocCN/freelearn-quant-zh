- en: '8'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quantum Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: Quantum neural networks Â [[100](Biblography.xhtml#XFarhi2018)] are parameterised
    quantum circuits that can be trained as either generative or discriminative machine
    learning models in direct analogy with their classical counterparts. In this chapter,
    we will consider parameterised quantum circuits trained as classifiers. In the
    most general case, a classifier is a function that takes an *N*-dimensional input
    and returns one ofÂ *M* possible class values. The classifier can be trained on
    a dataset of samples with known class labels by adjusting the configurable model
    parameters in such a way as to minimise the classification error. Once the classifier
    is fully trained, it can be exposed to new unseen samples for which correct class
    labels are unknown. Therefore, it is critically important to avoid overfitting
    to the training dataset and ensure that the classifier generalises well to the
    new data.
  prefs: []
  type: TYPE_NORMAL
- en: There are many similarities between quantum and classical neural networks. In
    both cases, the key element is the forward propagation of the signal (input),
    which is transformed by the network activation functions. Both quantum and classical
    neural networks can be trained through the backpropagation of error (differentiable
    learning) as well as through various non-differentiable learning techniques. However,
    there are also fundamental differences. For example, classical neural networks
    derive their power from the non-linear transformation of input. In contrast, all
    quantum gates are linear operators and the power of quantum neural networks comes
    from the mapping of the input into the high-dimensional Hilbert space where classification
    can be more easily done.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Quantum Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FigureÂ [8.1](#x1-1630001) provides a schematic representation of a typical Quantum
    Neural Network (QNN) trained as a classifier. Let us have a look at the quantum
    circuit and understand how it operates. The network consists ofÂ *n* quantum registers,
    a number of one-qubit and two-qubit gates, andÂ *m* measurement operators. The
    input is a quantum state ![|Ïˆ âŸ© k](img/file747.jpg) encoding the *k*-th sample
    from the dataset. If our dataset is classical, then every classical sample should
    first be encoded in the input quantum state (as explained in the previous chapter).
    WithÂ *m* measurement operators, the output is a bitstring that can encode up toÂ 2^m
    integer values (class labels). In the case of a binary classifier, it is sufficient
    to perform measurement on a single qubit.
  prefs: []
  type: TYPE_NORMAL
- en: '![FigureÂ 8.1: Schematic representation of a quantum neural network â€“ parameterised
    quantum circuit â€“ consisting of 1-qubit and 2-qubit gates and measurement operators
    on one or more quantum registers. The initial state |ÏˆkâŸ© encodes the k-th sample
    from the dataset. ](img/file749.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 8.1: Schematic representation of a quantum neural network â€“ parameterised
    quantum circuit â€“ consisting of 1-qubit and 2-qubit gates and measurement operators
    on one or more quantum registers. The initial state ![|ÏˆkâŸ©](img/file748.jpg) encodes
    the k-th sample from the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The measurement process produces a single sample from the probability distribution
    encoded in the quantum state. Therefore, we need to run the quantum circuit many
    times for the same input in order to collect sufficient statistics for each qubit
    on which we perform measurement.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if our QNN is organised as a classifier that should be able to
    predict one of the four possible classes ("0", "1", "2", and "3"), then we would
    need to perform measurement on 2 qubits with possible outcomes ![|00âŸ©](img/file750.jpg)
    corresponding to class "0", ![|01âŸ©](img/file751.jpg) corresponding to class "1",
    ![|10âŸ©](img/file752.jpg) corresponding to class "2", and ![|11âŸ©](img/file753.jpg)
    corresponding to class "3". Let us assume that we have run the quantum circuit
    1,000 times and observed the following results as shown in TableÂ [8.1](#x1-163003r1):'
  prefs: []
  type: TYPE_NORMAL
- en: '| Measured bitstring | Class label | Number of observations |'
  prefs: []
  type: TYPE_TB
- en: '| 00 | 0 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 01 | 1 | 550 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 2 | 200 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 3 | 150 |'
  prefs: []
  type: TYPE_TB
- en: 'TableÂ 8.1: 1,000 runs of the quantum circuit.'
  prefs: []
  type: TYPE_NORMAL
- en: Then we can conclude that the most likely class label for the given input is
    class "1" (with probability 55%). At the same time, we also obtain probabilities
    for all other possible class values, which may be useful in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: The network is organised as *l* layers of one-qubit and two-qubit gates. The
    gates can be *adjustable*, meaning that they can be controlled by adjustable parameters,
    such as rotation angles, or they can be *fixed*. The 2-qubit gates in FigureÂ [8.1](#x1-1630001)
    are fixedÂ CX gates but, in principle, they can be adjustable controlled rotation
    gates. Although the network shown schematically in FigureÂ [8.1](#x1-1630001) can
    have up to *n* Ã— *l* adjustable parameters (*ğœƒ*[i]^j)[i=1,â€¦,n; j=1,â€¦,l], it is
    often the case that the two-qubit gates are fixed and we only have one-qubit rotations
    as available degrees of freedom in training the network.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to classical neural networks, QNNs can be trained through either differentiable
    learning (for example, backpropagation of error with gradient descent) or non-differentiable
    learning (e.g., evolutionary search heuristics). Both approaches have their relative
    strengths and weaknesses. In theory, differentiable learning can be faster, but
    convergence is not guaranteed due to the well-known problem of "barren plateaus"
    associated with the gradients becoming vanishingly smallÂ Â [[207](Biblography.xhtml#XMcClean2018)]
    and is problem dependent. Non-differentiable learning is, as a rule, slower but
    avoids being trapped in local minima and works well in situations where the cost
    function is not smooth. SectionsÂ [8.2](#x1-1640002) andÂ [8.3](#x1-1680003) provide
    detailed descriptions of the QNN training procedures.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, the strongest motivation for using quantum classifiers is their ability
    to process quantum data. The input quantum states that must be classified may
    be outputs of some other quantum circuits. As we may not be able to store the
    information encoded in these quantum states classically, a quantum classifier
    becomes an indispensable tool. However, quantum classifiers have a realistic chance
    to demonstrate their advantage on purely classical data too. There are several
    considerations that motivate our interest in trying to apply QNNs to classical
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: First, parameterised quantum circuits possess a larger expressive power than
    equivalent classical neural networks. Second, they are structurally able to efficiently
    fight overfitting. Finally, quantum speedup is achievable on some types of quantum
    hardware for specific use cases even at these very early stages of quantum computing
    development. ChapterÂ [12](Chapter_12.xhtml#x1-22500012) investigates these questions
    in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we focus on using QNNs to efficiently solve specific finance-related
    classification use cases and provide a comparison with a number of standard classical
    classifiers. While experimentally proving quantum speedup and larger expressive
    power of QNNs requires powerful quantum hardware, the way QNNs fight overfitting
    can be verified on relatively small and shallow quantum circuits with the help
    of open-source quantum simulators.
  prefs: []
  type: TYPE_NORMAL
- en: QNNs are PQCs trained as ML models such as classifiers. QNNs have a natural
    advantage over classical neural networks when it comes to classifying quantum
    data. However, classical datasets can also be encoded as quantum states and processed
    by QNNs with their larger expressive power, their ability to efficiently fight
    overfitting, and, ultimately, with their quantum speedup.
  prefs: []
  type: TYPE_NORMAL
- en: As we learned from ChapterÂ [5](Chapter_5.xhtml#x1-960005), to specify the architecture
    of the neural network is not sufficient to build the working ML model â€“ it is
    also necessary to specify the training algorithm. In the following sections, we
    show how a quantum neural network can be trained with the help of differentiable
    and non-differentiable learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Training QNN with Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since we are not only interested in building QNNs as standalone QML tools but
    also in comparing and contrasting them with classical neural networks, we start
    our review of QNN training methods with gradient descent â€“ a ubiquitous classical
    ML algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 The finite difference scheme
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Training QNNs consists of specifying and executing a procedure that finds an
    optimal configuration of the adjustable rotation parametersÂ ğœƒ. Assume that a QNN
    is specified on *n* quantum registers withÂ *l* layers of adjustable quantum gates,
    where each adjustable gate is controlled by a single parameter (*ğœƒ*[i]^j)[i=1,â€¦,n;
    j=1,â€¦,l]. In this case,Â ğœƒ âˆˆâ„³[n,l] is anÂ *n*Ã—*l* matrix of adjustable network parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ âŒŠ 1 lâŒ‹ ğœƒ1 ... ğœƒ1 ğœƒ = &#124;&#124; .. ... .. &#124;&#124; . âŒˆ . . âŒ‰ ğœƒ1n
    ... ğœƒln ](img/file754.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: Without loss of generality, we assume that we work with a binary classifier.
    The latter takes an input (a quantum state that encodes a sample from the dataset),
    applies a sequence of quantum gates (the parameterised quantum circuit controlled
    by at most *n* Ã— *l* adjustable parameters), and performs the measurement of an
    observableÂ *M* on the chosen quantum register. An example of an observable is
    the PauliÂ Z gate and the result of a single measurement is Â±1 for a qubit found
    in the state ![|0âŸ©](img/file755.jpg) or ![|1âŸ©](img/file756.jpg), respectively.
    The value of the measured observable is mapped into a value of a binary variable
    {0,Â 1}. This process is repeatedÂ *N* times for each sample in order to collect
    sufficient statistics for the classification result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in finding an optimal configuration of adjustable parametersÂ ğœƒ
    is to choose an appropriate cost function â€“Â an objective function that represents
    the total error in classifying samples from the training dataset and which can
    be minimised by changing the adjustable network parameters. Let y := (*y*[1]*,â€¦,y*[K])
    be a vector of binary labels and f(ğœƒ) := (*f*[1](ğœƒ)*,â€¦,f*[K](ğœƒ)) a vector of binary
    classifier predictions for the training dataset consisting of *K* samples. The
    cost functionÂ *L*(ğœƒ) can then be defined, for example, as the sum of squared errors
    across all samples in the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ âˆ‘K L (ğœƒ) := 1 (yk âˆ’ fk(ğœƒ))2\. 2 k=1 ](img/file757.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: The next step is an iterative update of the adjustable parameters in the direction
    that reduces the value of the cost function. That direction is given by the cost
    function gradientÂ â€“ hence the name of the method. The parameters are updated towards
    the direction of the steepest descent of the cost function. At step *u* + 1, we
    update the system to
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ âˆ‚L(ğœƒ) u+1 ğœƒijâ† âˆ’ uğœƒji âˆ’ Î·---j-, for each i = 1,...,n, j = 1,...,l, âˆ‚ğœƒi
    ](img/file758.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: 'whereÂ *Î·* is the learning rate, namely a hyperparameter controlling the magnitude
    of the update. For each *i* = 1*,â€¦,n*, *j* = 1*,â€¦,l*, the derivative can be calculated
    numerically using a finite difference scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ j j j j âˆ‚L-(ğœƒ)- L(ğœƒ11,...,ğœƒi-+-Î”ğœƒi,...,ğœƒln)âˆ’-L-(ğœƒ11,...,ğœƒi-âˆ’-Î”ğœƒi,...,ğœƒln)-
    âˆ‚ğœƒj â‰ˆ 2Î” ğœƒj , i i ](img/file759.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: 'with an error of order ğ’ª((Î”*ğœƒ*[i]^j)Â²), whereÂ Î”*ğœƒ*[i]^j is a small rotation
    angle increment. The physical characteristics of the NISQ devices put restrictions
    on how small this increment can be: in most cases Î”*ğœƒ*[i]^j should not be smaller
    thanÂ 0*.*1 radians. The rest of the training routine follows the standard classical
    algorithm of training neural networks through the backpropagation of error with
    gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 The analytic gradient approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An alternative to the finite difference method, which can be unstable and ill-conditioned
    due to truncation and round-off errors (for parameterised quantum circuitsÂ Â [[29](Biblography.xhtml#XBenedetti2019)]
    or, in fact, for classical neural networksÂ Â [[27](Biblography.xhtml#XBaydin)]),
    is the analytic gradient approach. It can be a viable choice for parameterised
    quantum circuits with adjustable one-qubit gates and fixed multi-qubit gates.
    FromÂ ([8.2.1](#x1-1650001)), the cost function gradient with respect to the parameterÂ *ğœƒ*[i]^j
    is given by
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ K âˆ‚L-(ğœƒ)= âˆ’ âˆ‘ (y âˆ’ f (ğœƒ)) âˆ‚fk-(ğœƒ), âˆ‚ ğœƒji k k âˆ‚ğœƒji k=1 ](img/file760.jpg)
    |  |'
  prefs: []
  type: TYPE_TB
- en: so that the task of calculating the gradient of the cost function is reduced
    to the task of calculating the partial derivative of the expected value of the
    measurement operator for each sample quantum state that encodes the classical
    sample from the training dataset. LetÂ ![|ÏˆkâŸ©](img/file761.jpg) be the quantum
    state that encodes the *k*-th sample from the training dataset and letÂ U(ğœƒ) denote
    the unitary operator that represents the sequence of QNN gates transforming the
    initial stateÂ ![|ÏˆkâŸ©](img/file762.jpg). Then the expected value of the measurement
    operatorÂ M is given by
  prefs: []
  type: TYPE_NORMAL
- en: '| ![fk(ğœƒ) = âŸ¨Ïˆk &#124;U â€ (ğœƒ)MU(ğœƒ) &#124;Ïˆk âŸ©. ](img/file763.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: According to the conventions we used in constructing the QNN ansatz, the parameterÂ *ğœƒ*[i]^j
    only affects a single gate, which we will denote as G(*ğœƒ*[i]^j). Therefore, the
    sequence of gatesÂ U(ğœƒ) can be represented as
  prefs: []
  type: TYPE_NORMAL
- en: '| ![U(ğœƒ) = VG(ğœƒji)W, ](img/file764.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: 'whereÂ W andÂ V are gate sequences that precede and follow gate G(*ğœƒ*[i]^j).
    Let us absorb V into the Hermitian observable Q = V^â€ MV and W into the quantum
    state ![|Ï• âŸ© k](img/file765.jpg) = W![|Ïˆ âŸ© k](img/file766.jpg):'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ â€  j j fk(ğœƒ) = âŸ¨Ï•k&#124;G (ğœƒi)QG(ğœƒi) &#124;Ï•kâŸ©. ](img/file767.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: Then the partial derivative of *f*[k](ğœƒ) with respect to parameterÂ *ğœƒ*[i]^j
    is calculated as
  prefs: []
  type: TYPE_NORMAL
- en: '| ![âˆ‚fk(ğœƒ) ----j-- âˆ‚ ğœƒi](img/file768.jpg) | = ![ âˆ‚ ---j âˆ‚ ğœƒi](img/file769.jpg)
    âŸ¨*Ï•*[k]&#124;G^â€ (*ğœƒ* [i]^j)QG(*ğœƒ* [i]^j)![&#124;Ï•kâŸ©](img/file770.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '|  | = âŸ¨*Ï•*[k]&#124;![( ) âˆ‚G (ğœƒj) ----ij- âˆ‚ğœƒi](img/file771.jpg)^â€ QG(*ğœƒ* [i]^j)![&#124;Ï•kâŸ©](img/file772.jpg)
    + âŸ¨*Ï•*[k]&#124;G^â€ (*ğœƒ* [i]^j)Q![( ) âˆ‚G(ğœƒj) ---ji- âˆ‚ğœƒi](img/file773.jpg)![&#124;Ï•kâŸ©](img/file774.jpg)*.*
    | (8.2.1) |  |'
  prefs: []
  type: TYPE_TB
- en: Let us denote
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ âˆ‚G (ğœƒj) B := G(ğœƒji) and C :=----ji, âˆ‚ğœƒi ](img/file775.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: and notice that
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ â€  â€  âŸ¨Ï•k&#124;C( QB &#124;Ï•k âŸ©+ âŸ¨Ï•k&#124;B QC &#124;Ï•k âŸ© ) 1- â€  â€  = 2 âŸ¨Ï•k
    &#124;(B + C) Q(B+ C) &#124;Ï•k âŸ©âˆ’ âŸ¨Ï•k&#124;(Bâˆ’ C) Q(B âˆ’ C) &#124;Ï•kâŸ© . ](img/file776.jpg)
    |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, if we can find the way to implement the operator BÂ±C as part of an
    overall unitary evolution then we can evaluateÂ ([8.2.1](#x1-166003r1)) directly.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3 The parameter shift rule for analytic gradient calculation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: FollowingÂ Â [[257](Biblography.xhtml#XSchuld2018)], we outline the parameter
    shift rule for gates with generators with two distinct eigenvaluesÂ â€“Â this covers
    all one-qubit gates. Being unitary, the gateÂ G(*ğœƒ*[i]^j) above can be represented
    as
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ ( ) G(ğœƒji) = exp âˆ’ iğœƒjiÎ“ , ](img/file777.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: for some Hermitian operatorÂ Î“ (TheoremÂ [6](Chapter_1.xhtml#x1-29009r6)). The
    partial derivative with respect toÂ *ğœƒ*[i]^j reads
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ j ( ) âˆ‚G-(ğœƒi) = âˆ’ iÎ“ exp âˆ’ iğœƒjiÎ“ = âˆ’ iÎ“ G(ğœƒji). âˆ‚ğœƒji ](img/file778.jpg)
    |  |'
  prefs: []
  type: TYPE_TB
- en: SubstitutingÂ ([8.2.3](#x1-1670003)) intoÂ ([8.2.1](#x1-166003r1)) yields
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ âŸ© âŸ© âˆ‚fk(ğœƒ)-= âŸ¨Ï•â€²&#124;iÎ“ Q &#124;Ï•â€² + âŸ¨Ï•â€²&#124;Q(âˆ’ iÎ“ ) &#124;Ï• â€² , âˆ‚ğœƒji
    k k k k ](img/file779.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: where ![ â€²âŸ© |Ï• k](img/file780.jpg) = G(*ğœƒ*[i]^j)![|Ï•kâŸ©](img/file781.jpg). IfÂ Î“
    has just two distinct eigenvalues we can shift the eigenvalues to Â±*r*, since
    the global phase is unobservableÂ Â [[257](Biblography.xhtml#XSchuld2018)]. With
    I denoting the identity operator we can rewriteÂ ([8.2.3](#x1-1670003)) as
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ ( ) âˆ‚fk(ğœƒ)- â€² iÎ“- â€²âŸ© â€² iÎ“- â€²âŸ© j = r âŸ¨Ï• k&#124;r QI &#124;Ï•k âˆ’ âŸ¨Ï•k&#124;IQ
    r &#124;Ï•k . âˆ‚ğœƒi ](img/file782.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: Denoting
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ i B := I and C := âˆ’ rÎ“ , ](img/file783.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: 'and usingÂ ([8.2.2](#x1-166003r2)) we obtain fromÂ ([8.2.3](#x1-1670003)):'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ [ ] âˆ‚f (ğœƒ) r â€² ( i ) â€  ( i ) â€²âŸ© â€² ( i )â€  ( i ) â€²âŸ© --k-j--= -- âŸ¨Ï•k&#124;
    Iâˆ’ - Î“ Q Iâˆ’ -Î“ &#124;Ï•k âˆ’ âŸ¨Ï•k&#124; I + -Î“ Q I + - Î“ &#124;Ï•k . âˆ‚ğœƒi 2 r r r r
    ](img/file784.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: A straightforward computationÂ Â [[257](Biblography.xhtml#XSchuld2018),Â Theorem
    1] shows that if the Hermitian generatorÂ Î“ of the unitary operator G(*ğœƒ*) = exp(âˆ’i*ğœƒ*Î“)
    has at most two unique eigenvaluesÂ Â±*r*, then
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ ( ) ( -Ï€-) -1- i G âˆ“ 4r = âˆš2-- IÂ± rÎ“ . ](img/file785.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: 'In this case the gradient can be estimated using two additional evaluations
    of the quantum circuit. Either the gate G(*Ï€âˆ•*(4*r*)) or the gate G(âˆ’*Ï€âˆ•*(4*r*))
    should be placed in the original circuit next to the gate we are differentiating.
    Since for unitarily generated one-parameter gates G(*a*)G(*b*) = G(*a* + *b*),
    this is equivalent to shifting the gate parameter, and we obtain the â€œparameter
    shift ruleâ€Â Â [[257](Biblography.xhtml#XSchuld2018)] with the shift *s* = *Ï€âˆ•*(4*r*):'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ ( ) âˆ‚fk(ğœƒ)-= r âŸ¨Ï•k &#124;Gâ€ (ğœƒj + s)QG(ğœƒj+ s) &#124;Ï•kâŸ©âˆ’ âŸ¨Ï•k &#124;G â€ (ğœƒj
    âˆ’ s)QG (ğœƒj âˆ’ s) &#124;Ï•kâŸ© . âˆ‚ğœƒji i i i i ](img/file786.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: 'IfÂ Î“ is a one-qubit rotation generator given by PauliÂ X, Y, andÂ Z operators,
    then *r* = 1*âˆ•*2 and *s* = *Ï€âˆ•*2Â Â [[213](Biblography.xhtml#XMitarai2018),Â [257](Biblography.xhtml#XSchuld2018)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![âˆ‚fk(ğœƒ) 1( ( j Ï€) ( j Ï€ ) ---j--= -- âŸ¨Ï•k &#124;Gâ€  ğœƒi + -- QG ğœƒi +-- &#124;Ï•kâŸ©
    âˆ‚ğœƒi 2 ( 2 ) ( 2 ) ) âˆ’ âŸ¨Ï• &#124;Gâ€  ğœƒjâˆ’ Ï€- QG ğœƒj âˆ’ Ï€- &#124;Ï• âŸ© . k i 2 i 2 k ](img/file787.jpg)
    |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, what we need to do in order to estimate the gradient is to execute
    two circuitsÂ *N* times to collect statistics and to calculate the expectations
    on the right-hand side ofÂ ([8.2.3](#x1-1670003)). The first circuit will have
    the gate parameter shifted by *Ï€âˆ•*2 and the second circuit will have the gate
    parameter shifted by âˆ’*Ï€âˆ•*2.
  prefs: []
  type: TYPE_NORMAL
- en: Although this procedure is not necessarily faster than the finite difference
    scheme, it can produce a more accurate estimate of the cost function gradient.
    The main argument here is the fact that the NISQ hardware operates with limited
    precision. The state-of-the-art superconducting qubits have one-qubit gate fidelity
    â‰¤ 99*.*9% and two-qubit gate fidelity â‰¤ 99*.*7% with rotation angle precision
    of order 0.05 radians. Therefore, the finite difference scheme cannot assume infinitesimal
    rotation angles Î”*ğœƒ*Â â€“ they should not be smaller than about 0.1 radians (and,
    probably, materially larger in most cases). This means that gradients obtained
    with the finite difference scheme have some degree of built-in uncertainty that
    can only be fixed with further improvements in the NISQ hardware.
  prefs: []
  type: TYPE_NORMAL
- en: QNNs can be trained with the gradient descent algorithm in full analogy with
    the backpropagation of error in classical neural networks. The gradients can be
    either calculated analytically or estimated numerically.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Training QNN with Particle Swarm Optimisation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having specified the gradient descent scheme for training QNNs in the previous
    section, we now turn our attention to a non-differentiable learning method based
    on the powerful evolutionary search algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 The Particle Swarm Optimisation algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Particle Swarm Optimisation (PSO) algorithm belongs to a wide class of evolutionary
    search heuristics where at each algorithm iteration ("generation" in the language
    of evolutionary algorithms), the population of solutions ("chromosomes" or "particles")
    is evaluated in terms of their fitness with respect to the environment. In the
    standard PSO formulationÂ Â [[236](Biblography.xhtml#XPoli2008)], a number of particles
    are placed in the solution space of some problem and each evaluates the fitness
    at its current location. Each particle then determines its movement through the
    solution space by combining some aspects of the history of its own fitness values
    with those of one or more members of the swarm, and then moves through the solution
    space with a velocity determined by the locations and processed fitness values
    of those other members, along with some random perturbations.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a standard procedure Â [[127](Biblography.xhtml#XHassan2005),Â [172](Biblography.xhtml#XKondratyev2017)]
    to follow three steps in specifying the PSO algorithm. First, we initialise the
    positionsÂ x[k]^i := (*x*[k]^i(1)*,â€¦,x*[k]^i(*n*)) âˆˆâ„^n of each particle *i* at
    time *k* moving through the *n*-dimensional search space and taking values in
    some range [x[min]*,*x[max]]. Next we initialise the velocitiesÂ v[k]^i := (*v*[k]^i(1)*,â€¦,v*[k]^i(*n*))
    âˆˆâ„^n of each particle in the swarm. The initialisation process consists of distributing
    swarm particles randomly across the solution space:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ i i xmin + Ï‰v(xmax âˆ’ xmin) x0 = xmin + Ï‰x (xmax âˆ’ xmin), v0 = ----------Î”t----------,
    ](img/file788.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: whereÂ *Ï‰*[x] andÂ *Ï‰*[v] are uniformly distributed random variables on [0*,*1]
    andÂ Î”*t* is the time step between algorithm iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then update the velocities of all particles at time *k* + 1 according to
    the specified objective function which depends on the particlesâ€™ current positions
    in the solution space at timeÂ *k*. The value of the objective function determines
    which particle has the best positionÂ p[k]^(global) in the current swarm and also
    determines the best positionÂ p^i of each particle over time, i.e., in the current
    and all previous moves. The velocity update formula uses these two pieces of information
    for each particle in the swarm along with the effect of the current motionÂ v[k]^i
    to provide a search directionÂ p[k+1]^i for the next iteration. The velocity update
    formula includes random parameters to ensure good coverage of the solution space
    and to avoid entrapment in local optima. The three values that affect the new
    search direction are the current motion, the particleâ€™s own memory, and the swarm
    influence. They are incorporated via a summation approach with three weight factors:
    inertiaÂ *w*, self-confidenceÂ *c*[1], and swarm confidenceÂ *c*[2]:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ ( ) ( i i) pglobalâˆ’ xi vik+1 = wvik + c1Ï‰1-p-âˆ’-xk-+ c2Ï‰2 --k-------k--,
    Î”t Î”t ](img/file789.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: whereÂ *Ï‰*[1] andÂ *Ï‰*[2] are uniformly distributed random variables on [0*,*1].
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the position of each particle is updated using its velocity vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![xik+1 = xik + vik+1Î”t. ](img/file790.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: 'These steps are repeated until either a desired convergence criterion is met
    or until we reach the maximum number of iterations. Various reflection rules (stopping
    at the boundary, mirror reflection back into the allowed domain, etc.)Â Â [[190](Biblography.xhtml#XLiuYangWang2010)]
    can be designed for the new position x[k+1]^i falling outside the [x[min]*,*x[max]]
    bounds and the dynamics can be normalised with Î”*t* â‰¡ 1\. IfÂ *K* is the last iteration
    of the algorithm, then the best solution found by the PSO is p[K]^(global). FigureÂ [8.2](#8.2)
    provides a schematic illustration of the particle movement through the solution
    space under the influence of three forces: momentum, attraction to the globally
    best solution found by all particles at the previous iteration, and attraction
    to the best solution found by the given particle across all previous iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![FigureÂ 8.2: Schematic illustration of the PSO algorithm. Each particle moves
    through the solution space under the influence of three forces: momentum, own
    memory, and swarm influence. ](img/file791.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 8.2: Schematic illustration of the PSO algorithm. Each particle moves
    through the solution space under the influence of three forces: momentum, own
    memory, and swarm influence.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2 PSO algorithm for training quantum neural networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We are now ready to specify the PSO algorithm to train QNNs. We consider the
    most general case of an *n* Ã— *l* matrix of adjustable parameters (rotations)Â ğœƒ,
    whereÂ *n* is the number of quantum registers andÂ *l* is the number of network
    layers. The solution we look for is the matrixÂ ([8.2.1](#x1-1650001)) of adjustable
    parameters that minimises the chosen cost function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost function can be specified in many different ways depending on what
    particular aspects we want to encourage or penalise. Given the training dataset,
    we would like to find a configuration of adjustable parametersÂ ğœƒ such that as
    many samples as possible are classified correctly. One possible choice of cost
    function, for example, may be the ratio of incorrect to correct classification
    decisions. However, the classification process is probabilistic in nature â€“ we
    decide on the sample label after many runs of the quantum circuit, which generate
    sufficient statistics. Therefore, each classification decision is not just right
    or wrong but can be seen as "more right" or "more wrong". If the correct sample
    label is "1" and we get "0" 51% of the time then the classifier is slightly wrong:
    the chances are that similar samples would be classified correctly or only a small
    change to the adjustable network parameters is required to rectify the classification
    process. But if we get "0", say, 90% of the time, then the classifier is "very
    wrong" and we need to penalise the outcome more aggressively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One possible realisation of the cost function that takes into account the above
    argument is as follows: Without loss of generality, assume that we work with the
    binary class labelsÂ "0" andÂ "1", and let y := (*y*[1]*,â€¦,y*[K]) be a vector of
    sample labels (either "0" or "1") from the training dataset. Further, let â„™(ğœƒ)
    := (â„™[1](ğœƒ)*,â€¦,*â„™[K](ğœƒ)) be a vector of QNN estimated probabilities of predicting
    class "1" for the given sample (i.e., the number of quantum circuit runs that
    returned "1" after measurement divided by the total number of quantum circuit
    runs). Then the cost functionÂ *L*(ğœƒ) is given by the following pseudo code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This cost function penalises large errors in the class probability estimate
    more than small errors and represents the total error across all samples in the
    training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now formulate the QNN training algorithm, which has the following inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| X := (X[1]*,â€¦,*X[K]) âˆˆâ„^(MÃ—K) | training dataset of features encoded as |'
  prefs: []
  type: TYPE_TB
- en: '|  | rotation angles on [0*,Ï€*] |'
  prefs: []
  type: TYPE_TB
- en: '| y := (*y*[1]*,â€¦,y*[K]) âˆˆ{0*,*1}^K | vector of binary labels |'
  prefs: []
  type: TYPE_TB
- en: '| *N*[iter] | number of iterations |'
  prefs: []
  type: TYPE_TB
- en: '| *N*[runs] | number of quantum circuit runs |'
  prefs: []
  type: TYPE_TB
- en: '| *M* | number of particles (solutions) |'
  prefs: []
  type: TYPE_TB
- en: '| *w* | momentum coefficient |'
  prefs: []
  type: TYPE_TB
- en: '| *c* [1] | particle memory coefficient |'
  prefs: []
  type: TYPE_TB
- en: '| *c*[2] | swarm influence coefficient |'
  prefs: []
  type: TYPE_TB
- en: '| *n* | number of quantum registers |'
  prefs: []
  type: TYPE_TB
- en: '| *l* | number of QNN layers |'
  prefs: []
  type: TYPE_TB
- en: 'TableÂ 8.2: Inputs of the QNN training algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm operates on the following objects, where *m* = 1*,â€¦,M* denotes
    the *m*-th particle, and *t* = 0*,â€¦,N*[iter] represents the algorithm iteration
    step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ğœƒ(*t*;*m*) âˆˆâ„³[nl]([âˆ’*Ï€,Ï€*]): position of particleÂ *m* at timeÂ *t*;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'v(*t*;*m*) âˆˆâ„³[nl]([âˆ’*Ï€,Ï€*]): velocity of particleÂ *m* at timeÂ *t*;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Î(*m*) âˆˆ â„³[nl]([âˆ’*Ï€,Ï€*]): best position found by particleÂ *m* across all iterations;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Î¦(*t*) âˆˆ â„³[nl]([âˆ’*Ï€,Ï€*]): the globally best position found by all particles
    at timeÂ *t*;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*L*(ğœƒ): value of the cost function for the solutionÂ ğœƒ.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![--------------------------------------------------------------------- -Algorithm---5:-Particle
    Swarm-Optimisation--------------------------- Result: Optimal con figuration of
    adjustable QNN parameters âˆ— ğœƒ := argmin L(ğœƒ). Initialisation and evaluation of
    the first set of solutions (we set Î”t in (8.3.1) equal to 1): for each particle
    m = 1,...,M do | for i = 1,...,n, j = 1,...,l do | | Randomly draw the rotation
    angle ğœƒj(0;m ) from ğ’° ([âˆ’ Ï€,Ï€]). | | i | | Randomly draw the rotation angle vji(0;m
    ) from ğ’° ([âˆ’ Ï€,Ï€]). | end | | | Initialise the individually best solution: | |
    Î(m ) â† ğœƒ(0;m ) | | for k = 1,...,K do | | Run the quantum circuit Nruns times
    with configuration | | | | ğœƒ(0;m ) on sample Xk to estimate the probability â„™k
    of | | reading out "1" on the target qubit. | | end | | Evaluate the cost function
    L(ğœƒ(0;m )) given the probabilities | â„™ := (â„™ ,...,â„™ ). 1 K end Order solutions
    from best (minimal cost function ) to worst (maximal cost function). Î¦ (0) â† configuration
    corresponding to the minimum of the cost function. Initialise the optimal configuration:
    âˆ— ğœƒ â† Î¦ (0) ---------------------------------------------------------------------
    ](img/file792.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![--------------------------------------------------------------------- Iterations:
    for t = 1,...,Niter do | | for m = 1,...,M do | | for i = 1,...,n, j = 1,...,l
    do | | | Generate independent random numbers Ï‰ âˆ¼ U [0,1 ] and | | | 1 | | | Ï‰2
    âˆ¼ U[0,1]. | | | momentum â† wvj (t âˆ’ 1;m ) | | | i | | | particle â† c1Ï‰1[Îji(m
    )âˆ’ ğœƒji(tâˆ’ 1;m)] | | | swarm â† c Ï‰ [Î¦j (t âˆ’ 1)âˆ’ ğœƒj(tâˆ’ 1;m )] | | | 2 2 i i | |
    | vji(t;m ) â† momentum + particle+ swarm | | | j j j | | | ğœƒi(t;m ) â† ğœƒi(tâˆ’ 1;m
    )+ vi(t;m ) | | end | | | | for k = 1,...,K do | | | Run the quantum circuit Nruns
    times with configuration | | | ğœƒ(t;m ) on sample X to estimate the probability
    â„™ of | | | k k | | reading out "1" on the target qubit. | | end | | | | Evaluate
    the cost function L(ğœƒ(t;m )) given | | | | â„™ := (â„™1,...,â„™K ). | | if L(ğœƒ(t;m))
    < L(Î (m )) then | | | Î(m ) â† ğœƒ(t;m ) | | | | end | | end | Order solutions from
    best (minimum value of the cost function) | | to worst (maximum value of the cost
    function). | | Î¦ (t) â† con figuration corresponding to the minimum of the cost
    | function. | | if L(ğœƒâˆ—) < L (Î¦(t)) then | ğœƒ âˆ— â† Î¦(t) | | end end ---------------------------------------------------------------------
    ](img/file793.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The non-differentiable learning based on the evolutionary search heuristic works
    well for irregular, non-convex objective functions with many local minima.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 QNN Embedding on NISQ QPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ideally, parameterised quantum circuits should be constructed in a hardware-agnostic
    way, only driven by the characteristics of the problem being solved. This, however,
    would require the existence of large and exceptionally well-connected quantum
    computing systems with very high qubit fidelity and coherence time. In other words,
    we would need QPUs with capabilities that significantly exceed those of existing
    NISQ devices. The time for such powerful quantum computing systems may come sooner
    than one may expect but we still have to find a way of running PQCs efficiently
    on NISQ QPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 NISQ QPU connectivity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A typical approach to designing a PQC executable on the NISQ QPU would start
    with observing two main characteristics of the quantum computing systems: the
    graph (qubit connectivity) and the set of native gates. We can illustrate these
    points by looking at Rigettiâ€™s Aspen systemÂ Â [[72](Biblography.xhtml#XCoyle2020)]
    in FigureÂ [8.3](#8.3).'
  prefs: []
  type: TYPE_NORMAL
- en: '![FigureÂ 8.3: Rigettiâ€™s Aspen system. ](img/file794.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 8.3: Rigettiâ€™s Aspen system.'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, most qubits are only connected to their nearest neighbours on
    the linear grid, with only four qubits having three connections. These extra connections
    form a bridge between two 8-qubit islands that, otherwise, would be completely
    independent.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 QNN embedding scheme
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The shaded qubits in FigureÂ [8.3](#8.3) can be used to construct the 8-qubit
    tree network capable of processing a dataset with up toÂ 16 continuous features
    (two features per quantum register) as shown in FigureÂ [8.4](#8.4). The thick
    lines in FigureÂ [8.3](#8.3) represent qubits connectivity used in constructing
    the QNN. The thin lines represent all other available qubit connections that have
    not been utilised in the QNN ansatz.
  prefs: []
  type: TYPE_NORMAL
- en: '![FigureÂ 8.4: QNN for the Aspen system; the gateÂ G is any of the {X,Y,Z} gates.
    ](img/file795.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 8.4: QNN for the Aspen system; the gateÂ G is any of the {X,Y,Z} gates.'
  prefs: []
  type: TYPE_NORMAL
- en: With the limited connectivity of existing QPUs, we need to fully utilise the
    graph structure of the quantum chips to implement the most efficient QNN embedding
    and extract the best possible performance.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 QNN Trained as a Classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now demonstrate how a binary QNN classifier can be trained on a classical
    credit approval dataset using the non-differentiable learning approach.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.1 The ACA dataset and QNN ansatz
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the most fundamental use cases for a binary classifier in finance is
    credit approval. The UCI Machine Learning Database Â [[241](Biblography.xhtml#XUCI_ACA),Â [242](Biblography.xhtml#XQuinlan1987)]
    holds the Australian Credit Approval (ACA) dataset consisting of 690 samples.
    There are 14 features (binary, integer, continuous) representing various attributes
    of potential borrowers and a binary class label (accept/reject credit application).
    The dataset is reasonably hard for classical classifiers due to the limited predictive
    power of the features and its relatively small size. This makes it ideal for testing
    and benchmarking the QNN performance.
  prefs: []
  type: TYPE_NORMAL
- en: We start with the simplest tree network that can be mapped onto Rigettiâ€™s Aspen
    system graph described in the previous section. FigureÂ [8.5](#8.5) shows the full
    quantum circuit consisting of sample encoding and sample processing modulesÂ Â [[171](Biblography.xhtml#XKondratyev2021)].
    The proposed scheme allows us to encode up to two continuous features per quantum
    register with the help of rotations around theÂ *x*- and theÂ *y*-axes.
  prefs: []
  type: TYPE_NORMAL
- en: '![FigureÂ 8.5: PQC for the credit approvals classifier. ](img/file796.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 8.5: PQC for the credit approvals classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: The features are encoded as rotation angles *Ï•* âˆˆ [0*,Ï€*] according to the encoding
    scheme described in SectionÂ [7.2](Chapter_7.xhtml#x1-1520002). With all qubits
    initialised as ![|0âŸ©](img/file797.jpg) in the computational basis, this ensures
    the uniqueness of the encoded samples. The sample processing module consists of
    layers of adjustable one-qubit gates (rotations around theÂ *x*- and theÂ *y*-axes)
    and fixed two-qubit gates (CZ). We split the ACA dataset 50:50 into a training
    and a testing dataset using the train_test_split() function provided by the `sklearn.model_selection`
    module. Our objective is to train the QNN and various classical classifiers (classical
    benchmarks) on the training dataset and compare their out-of-sample performance
    on the testing dataset. The classical classifiers have a number of hyperparameters
    that can be fine-tuned to optimise the classifier performance on the given dataset.
    In contrast, the QNN architecture (location and types of one-qubit and two-qubit
    gates) is fixed.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.2 Training an ACA classifier with the PSO algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We first verify that the QNN can be efficiently trained with the Particle Swarm
    Optimisation algorithmÂ â€“ a non-differentiable learning approach. FigureÂ [8.6](#8.6)
    illustrates PSO convergence for the set of PSO parameters given in TableÂ [8.3](#x1-176001r3).
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Notation | Value |'
  prefs: []
  type: TYPE_TB
- en: '| Inertia coefficient | *w* | 0.25 |'
  prefs: []
  type: TYPE_TB
- en: '| Self-confidence coefficient | *c* [1] | 0.25 |'
  prefs: []
  type: TYPE_TB
- en: '| Swarm confidence coefficient | *c* [2] | 0.25 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of particles | *M* | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of iterations | *N*[iter] | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of quantum circuit runs | *N*[runs] | 1000 |'
  prefs: []
  type: TYPE_TB
- en: 'TableÂ 8.3: PSO parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: The sample algorithm run has reached the minimum of the objective function in
    just four iterations with only ten particles, exploring the search space using
    the `Qiskit` quantum simulator.
  prefs: []
  type: TYPE_NORMAL
- en: '![FigureÂ 8.6: Minimum objective function values found by individual particles.
    ](img/file798.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 8.6: Minimum objective function values found by individual particles.'
  prefs: []
  type: TYPE_NORMAL
- en: The configuration of adjustable parameters (rotations) that corresponds to the
    minimum of the objective function found by the PSO algorithm is given byÂ ([8.5.2](#x1-176003r2)).
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ âŒŠ âŒ‹ 0.16Ï€ &#124; &#124; &#124;&#124;âˆ’ 0.55Ï€ 0.66Ï€ &#124;&#124; &#124;&#124;âˆ’
    0.13Ï€ &#124;&#124; &#124;&#124; &#124;&#124; &#124;&#124; 0.08Ï€ 0.72Ï€ 0.02Ï€ &#124;&#124;
    ğœƒ = &#124;&#124; 0.33Ï€ &#124;&#124; . &#124; &#124; &#124;&#124; 0.06Ï€ 0.95Ï€ &#124;&#124;
    &#124;&#124; 0.48Ï€ &#124;&#124; âŒˆ âŒ‰ 0.19Ï€ âˆ’ 0.91Ï€ âˆ’ 0.83Ï€ 0.59Ï€ ](img/file799.jpg)
    |  |'
  prefs: []
  type: TYPE_TB
- en: FigureÂ [8.7](#8.7) displays the in- and out-of-sample confusion matrices for
    the QNN classifier obtained with the Qiskit quantum simulator assuming that ClassÂ 0
    is the positive class.
  prefs: []
  type: TYPE_NORMAL
- en: '![FigureÂ 8.7: Confusion matrix for the QNN classifier (ACA dataset). ](img/file800.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 8.7: Confusion matrix for the QNN classifier (ACA dataset).'
  prefs: []
  type: TYPE_NORMAL
- en: The results are robust with an in-sample accuracy of 0.86 and an out-of-sample
    accuracy of 0.85\. Interestingly, the in-sample and out-of-sample results are
    very close, indicating that the QNN provides strong regularisation. The question
    of quantum and classical neural networks regularisation will be tackled in ChapterÂ [12](Chapter_12.xhtml#x1-22500012).
  prefs: []
  type: TYPE_NORMAL
- en: 8.6 Classical Benchmarks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In ChapterÂ [4](Chapter_4.xhtml#x1-820004), we introduced two classical classifiers:
    a feedforward artificial neural network (Multi-Layer Perceptron) and a decision
    tree algorithm. We now expand the range of classical benchmark classifiers by
    adding Support Vector Machine (SVM)Â Â [[70](Biblography.xhtml#XCortes1995)], Logistic
    RegressionÂ Â [[31](Biblography.xhtml#XBerkson1944)], and Random ForestÂ Â [[136](Biblography.xhtml#XHo1995)].
    The SVM approach based on the *kernel method* is covered in ChapterÂ [13](Chapter_13.xhtml#x1-23600013).
    Here, we briefly explain the main principles of logistic regression and random
    forest classifiers.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.6.1 Logistic Regression and Random Forest
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Logistic regression can be seen as a special case of a feedforward neural network
    with a single hidden layer consisting of an activation unit with the logistic
    activation function. The model operates as shown in FigureÂ [4.3](Chapter_4.xhtml#4.3)
    with
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) y(s) = 1 + eâˆ’s âˆ’1 . ](img/file801.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The standard logistic regression model is a *linear classifier* because the
    outcome always depends on the sum of the (weighted) inputs. Therefore, logistic
    regression performs well when working with a dataset where the classes are more
    or less linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest is an *ensemble learning* model and, as the name suggests, is
    based on combining the classification results of multiple decision trees. The
    ensemble technique used by random forest is known as *bootstrap aggregation*,
    or *bagging*, by choosing random subsets from the dataset. Hence, each decision
    tree is generated from samples drawn from the original dataset with replacement
    (row sampling). This step of row sampling with replacement is called the *bootstrap*.
    Each decision tree is trained independently. The final output for the given samples
    is based on *majority voting* after combining the results of all individual decision
    trees. This is the *aggregation* step.
  prefs: []
  type: TYPE_NORMAL
- en: 8.6.2 Benchmarking against standard classical classifiers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The classical benchmarking can be done by training several popular `scikit-learn`
    models. TableÂ [8.4](#x1-179001r4) provides classical benchmarking results in terms
    of out-of-sample *F*[1] scores for the following (weakly) optimised scikit-learn
    classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a feedforward neural network (MLP) classifier: `neural_network.MLPClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a support vector machine classifier: `svm.SVC`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'an ensemble learning model: `ensemble.RandomForestClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a logistic regression classifier: `linear_model.LogisticRegression`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *F*[1] score is a harmonic average of two performance metrics, precision
    and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ Precision Ã— Recall F1 := 2 -----------------, Precision + Recall ](img/file802.jpg)
    |  |'
  prefs: []
  type: TYPE_TB
- en: both introduced in ChapterÂ [4](Chapter_4.xhtml#x1-820004). In the context of
    credit approvals, optimising for recall helps with minimising the chance of approving
    a credit application that should be rejected. However, this comes at the cost
    of not approving credit applications for some high-quality borrowers. If we optimise
    for precision, then we improve the overall correctness of our decisions at the
    cost of approving some applicants with bad credits. The *F*[1] score is used to
    balance the positives and negatives in optimising precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: '| Classifier | Average *F*[1] score |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic Regression Classifier | 0.88 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Forest Classifier | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '| MLP Classifier | 0.86 |'
  prefs: []
  type: TYPE_TB
- en: '| QNN Classifier | 0.85 |'
  prefs: []
  type: TYPE_TB
- en: '| Support Vector Classifier | 0.84 |'
  prefs: []
  type: TYPE_TB
- en: 'TableÂ 8.4: Out-of-sample F[1] scores for the classical and QNN classifiers
    trained on the ACA dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The QNN classifier performance, as measured by the average *F*[1] score for
    ClassÂ 0 and ClassÂ 1, falls somewhere in the middle of the range of out-of-sample
    *F*[1] scores for the chosen classical benchmarks. This is encouraging since the
    QNN ansatz was fixed and we did not optimise the QNN hyperparameters â€“ the placement
    and types of the two-qubit gates. The classifier performance can be further improved
    by deploying the standard ensemble learning techniques, as explained in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: QNNs can be productively used for classification tasks on classical finance-related
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 8.7 Improving Performance with Ensemble Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ensemble learning methods combine different weak classifiers into a strong
    classifier that has better generalisation capabilities than each individual standalone
    classifier. In ChapterÂ [4](Chapter_4.xhtml#x1-820004), we saw how the principles
    of ensemble learning can be used in combination with the methods of quantum annealing.
    Here, we look at them from the QNN perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 8.7.1 Majority voting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The popular ensemble learning methods are majority voting (binary classification)
    and plurality voting (multiclass classification). Majority voting means what it
    says: the class label for the given sample is the one that receives more than
    half of the individual votes. Plurality voting chooses the class that receives
    the largest number of votes (the mode).'
  prefs: []
  type: TYPE_NORMAL
- en: The ensemble of the individual classifiers can be built from different classification
    algorithms. For example, by combining neural network classifiers, support vector
    machines, decision trees, etc. On the other hand, the same basic classification
    algorithm can be used to produce multiple classifiers by choosing different configurations
    of hyperparameters and different subsets of the training dataset. The random forest
    classifier, which combines different decision tree classifiers, illustrates the
    latter approach.
  prefs: []
  type: TYPE_NORMAL
- en: With these considerations in mind, we build a strong classifier from several
    individual QNN classifiers by changing the QNN ansatz within the restrictions
    imposed by the QPU qubit connectivity. In order to test the majority voting approach,
    we build two new QNN classifiers by adding a few more two-qubit CZ gates to the
    baseline parameterised quantum circuit, as shown in FiguresÂ [8.8](#8.8) andÂ [8.9](#8.9).
  prefs: []
  type: TYPE_NORMAL
- en: In the case of PQCÂ #2, we add two extraÂ CZ gates, exploiting the "bridge" structure
    of the Aspen system (FigureÂ [8.3](#8.3)). This improves the overall system entaglement
    and allows for a richer set of achievable quantum states. PQCÂ #3 has three extraÂ CZ
    gates in comparison with the baseline circuit. The new classifiers can be trained
    with the same algorithm (PSO) on the same training dataset but will have different
    optimal configurations of the adjustable parameters and will make slightly different
    classification decisions on the testing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: With three QNN classifiers, the majority voting leads to either a unanimous
    or a 2:1 decision. Performance on the ACA dataset improves marginally with all
    three classifiers generally in full agreement with each other. There are only
    a handful of instances where majority voting adds value, but this improves the
    average out-of-sample *F*[1] score from 0.85 to 0.87Â â€“ on par with the random
    forest classifier trained on the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![FigureÂ 8.8: PQCÂ #2 for the credit approvals classifier. New fixed 2-qubit
    gates are shaded grey. ](img/file803.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 8.8: PQCÂ #2 for the credit approvals classifier. New fixed 2-qubit gates
    are shaded grey.'
  prefs: []
  type: TYPE_NORMAL
- en: '![FigureÂ 8.9: PQCÂ #3 for the credit approvals classifier. New fixed 2-qubit
    gates are shaded grey. ](img/file804.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 8.9: PQCÂ #3 for the credit approvals classifier. New fixed 2-qubit gates
    are shaded grey.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar results can be achieved with the original QNN classifier trained on
    different subsets of the training dataset. These subsets are produced by drawing
    the bootstrap samples â€“ random samples with replacement â€“ from the original training
    dataset. The differently trained QNN classifiers can then be combined into a single
    strong classifier using the majority voting approach as described above.
  prefs: []
  type: TYPE_NORMAL
- en: 8.7.2 Quantum boosting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We started by introducing the concept of ensemble learning where predictions
    produced by various QNNs are combined into a more robust unified prediction via
    a classical majority voting method. However, we can take a different approach
    to ensemble learning: predictions of various classical classifiers can be treated
    as an input into the QNN that performs their aggregation and comes up with a unified
    prediction. In other words, the QNN operates as a quantum booster similar to the
    QUBO-based QBoost model introduced in ChapterÂ [4](Chapter_4.xhtml#x1-820004).'
  prefs: []
  type: TYPE_NORMAL
- en: Let us come back to the classical benchmarks used in SectionÂ [8.5](#x1-1740005).
    There are four different machine learning models performing binary classifications.
    Their outputs ("0" for ClassÂ 0 and "1" for ClassÂ 1) are inputs into a 4-qubit
    QNN classifier. Since all quantum registers are initialised as ![|0âŸ©](img/file805.jpg),
    the outputs of individual classifiers can be encoded by either doing nothing for
    ClassÂ 0 output (which is equivalent to applying an identity operatorÂ I) or by
    applying a NOT gateÂ X for ClassÂ 1 output.
  prefs: []
  type: TYPE_NORMAL
- en: '![FigureÂ 8.10: Embedding of a 4-qubit QNN onto the bridge section of Rigettiâ€™s
    Aspen system. ](img/file806.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 8.10: Embedding of a 4-qubit QNN onto the bridge section of Rigettiâ€™s
    Aspen system.'
  prefs: []
  type: TYPE_NORMAL
- en: FigureÂ [8.10](#8.10) shows how a 4-qubit QNN can be efficiently embedded on
    the QPU and FigureÂ [8.11](#8.11) shows the corresponding parameterised quantum
    circuit with adjustable one-qubit gates (R[X]*,*R[Y]) and fixed two-qubit gates
    (CZ).
  prefs: []
  type: TYPE_NORMAL
- en: '![FigureÂ 8.11: QBoost circuit. The sample encoding gateÂ G is either an identity
    gateÂ I if the input isÂ "0", or a NOT gateÂ X if the input isÂ "1". ](img/file807.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 8.11: QBoost circuit. The sample encoding gateÂ G is either an identity
    gateÂ I if the input isÂ "0", or a NOT gateÂ X if the input isÂ "1".'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning can improve QNN performance in the same way it improves performance
    of the classical weak learners.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this chapter, we introduced the concept of a quantum neural network as a
    parameterised quantum circuit trained as a classifier. We considered two approaches
    to training QNNs: differentiable (gradient descent) and non-differentiable (Particle
    Swarm Optimisation) methods. Gradient descent is generally faster but can face
    the problem of barren plateaus (vanishing gradients). The evolutionary search
    heuristics may be slower but can handle the presence of multiple local minima
    and strike the right balance between exploration and exploitation.'
  prefs: []
  type: TYPE_NORMAL
- en: We also explored the embedding of QNNs on the NISQ QPUs with limited connectivity
    between the qubits. As an example, we considered Rigettiâ€™s Aspen system and proposed
    an efficient embedding scheme that mirrors the "tree structure" architecture of
    the QNN.
  prefs: []
  type: TYPE_NORMAL
- en: Once our QNN was fully specified and embedded into a QPU graph, we investigated
    its performance on a real-world dataset of credit approvals and provided comparisons
    with several standard classical classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at several ensemble learning techniques that assist in improving
    QNN performance in the context of a hybrid quantum-classical protocol.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study a powerful generative QML model â€“ the Quantum
    Circuit Born Machine â€“ which is a direct quantum counterpart of the classical
    Restricted Boltzmann Machine we considered in ChapterÂ [5](Chapter_5.xhtml#x1-960005).
  prefs: []
  type: TYPE_NORMAL
- en: Join our bookâ€™s Discord space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/quantum](https://packt.link/quantum)'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
