["```py\nidx = pd.IndexSlice\nwith pd.HDFStore('../../data/assets.h5') as store:\n    prices = (store['quandl/wiki/prices']\n              .loc[idx['2000':'2018', :], 'adj_close']\n              .unstack('ticker'))\nprices.info()\nDatetimeIndex: 4706 entries, 2000-01-03 to 2018-03-27\nColumns: 3199 entries, A to ZUMZ \n```", "```py\nmonthly_prices = prices.resample('M').last() \n```", "```py\noutlier_cutoff = 0.01\ndata = pd.DataFrame()\nlags = [1, 2, 3, 6, 9, 12]\nfor lag in lags:\n    data[f'return_{lag}m'] = (monthly_prices\n                           .pct_change(lag)\n                           .stack()\n                           .pipe(lambda x: \n                                 x.clip(lower=x.quantile(outlier_cutoff),\n                                        upper=x.quantile(1-outlier_cutoff)))\n                           .add(1)\n                           .pow(1/lag)\n                           .sub(1)\n                           )\ndata = data.swaplevel().dropna()\ndata.info()\nMultiIndex: 521806 entries, (A, 2001-01-31 00:00:00) to (ZUMZ, 2018-03-\n                             31 00:00:00)\nData columns (total 6 columns):\nreturn_1m 521806 non-null float64\nreturn_2m 521806 non-null float64\nreturn_3m 521806 non-null float64\nreturn_6m 521806 non-null float64\nreturn_9m 521806 non-null float64\nreturn_12m 521806 non-null float6 \n```", "```py\nfor lag in [2,3,6,9,12]:\n    data[f'momentum_{lag}'] = data[f'return_{lag}m'].sub(data.return_1m)\ndata[f'momentum_3_12'] = data[f'return_12m'].sub(data.return_3m) \n```", "```py\nfor t in range(1, 7):\n    data[f'return_1m_t-{t}'] = data.groupby(level='ticker').return_1m.shift(t) \n```", "```py\nfor t in [1,2,3,6,12]:\n    data[f'target_{t}m'] = (data.groupby(level='ticker')\n                            [f'return_{t}m'].shift(-t)) \n```", "```py\nfactors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\nfactor_data = web.DataReader('F-F_Research_Data_5_Factors_2x3', \n              'famafrench', start='2000')[0].drop('RF', axis=1)\nfactor_data.index = factor_data.index.to_timestamp()\nfactor_data = factor_data.resample('M').last().div(100)\nfactor_data.index.name = 'date'\nfactor_data = factor_data.join(data['return_1m']).sort_index()\nT = 24\nbetas = (factor_data\n         .groupby(level='ticker', group_keys=False)\n         .apply(lambda x: PandasRollingOLS(window=min(T, x.shape[0]-1), y=x.return_1m, x=x.drop('return_1m', axis=1)).beta)) \n```", "```py\nfor lag in [2,3,6,9,12]:\n    data[f'momentum_{lag}'] = data[f'return_{lag}m'].sub(data.return_1m)\ndata[f'momentum_3_12'] = data[f'return_12m'].sub(data.return_3m) \n```", "```py\ndates = data.index.get_level_values('date')\ndata['year'] = dates.year\ndata['month'] = dates.month \n```", "```py\nfor t in range(1, 7):\n    data[f'return_1m_t-{t}'] = data.groupby(level='ticker').return_1m.shift(t) \n```", "```py\nfor t in [1,2,3,6,12]:\n    data[f'target_{t}m'] = (data.groupby(level='ticker')\n                            [f'return_{t}m'].shift(-t)) \n```", "```py\nwith pd.HDFStore(DATA_STORE) as store:\n    data = (store['quandl/wiki/prices']\n            .loc[idx['2007':'2010', 'AAPL'],\n                 ['adj_open', 'adj_high', 'adj_low', 'adj_close', \n                  'adj_volume']]\n            .unstack('ticker')\n            .swaplevel(axis=1)\n            .loc[:, 'AAPL']\n            .rename(columns=lambda x: x.replace('adj_', ''))) \n```", "```py\nfrom talib import RSI, BBANDS\nup, mid, low = BBANDS(data.close, timeperiod=21, nbdevup=2, nbdevdn=2, \n                      matype=0)\nrsi = RSI(adj_close, timeperiod=14) \n```", "```py\ndata = pd.DataFrame({'AAPL': data.close, 'BB Up': up, 'BB Mid': mid, \n                     'BB down': low, 'RSI': rsi})\nfig, axes= plt.subplots(nrows=2, figsize=(15, 8))\ndata.drop('RSI', axis=1).plot(ax=axes[0], lw=1, title='Bollinger Bands')\ndata['RSI'].plot(ax=axes[1], lw=1, title='Relative Strength Index')\naxes[1].axhline(70, lw=1, ls='--', c='k')\naxes[1].axhline(30, lw=1, ls='--', c='k') \n```", "```py\nwith pd.HDFStore(DATA_STORE) as store:\n    sp500 = store['sp500/stooq'].loc['2008': '2009', 'close'] \n```", "```py\nfrom pykalman import KalmanFilter\nkf = KalmanFilter(transition_matrices = [1],\n                  observation_matrices = [1],\n                  initial_state_mean = 0,\n                  initial_state_covariance = 1,\n                  observation_covariance=1,\n                  transition_covariance=.01) \n```", "```py\nstate_means, _ = kf.filter(sp500) \n```", "```py\nsp500_smoothed = sp500.to_frame('close')\nsp500_smoothed['Kalman Filter'] = state_means\nfor months in [1, 2, 3]:\n    sp500_smoothed[f'MA ({months}m)'] = (sp500.rolling(window=months * 21)\n                                         .mean())\nax = sp500_smoothed.plot(title='Kalman Filter vs Moving Average',\n                         figsize=(14, 6), lw=1, rot=0) \n```", "```py\nsignal = (pd.read_hdf(DATA_STORE, 'sp500/stooq')\n          .loc['2008': '2009']\n          .close.pct_change()\n          .dropna()) \n```", "```py\nimport pywt\npywt.families(short=False)\n['Haar', 'Daubechies',  'Symlets',  'Coiflets',  'Biorthogonal',  'Reverse biorthogonal',  'Discrete Meyer (FIR Approximation)',  'Gaussian',  'Mexican hat wavelet',  'Morlet wavelet',  'Complex Gaussian wavelets',   'Shannon wavelets',  'Frequency B-Spline wavelets',  'Complex Morlet wavelets'] \n```", "```py\nwavelet = \"db6\"\nfor i, scale in enumerate([.1, .5]):\n\n    coefficients = pywt.wavedec(signal, wavelet, mode='per')\n    coefficients[1:] = [pywt.threshold(i, value=scale*signal.max(), mode='soft') for i in coefficients[1:]]\n    reconstructed_signal = pywt.waverec(coefficients, wavelet, mode='per')\n    signal.plot(color=\"b\", alpha=0.5, label='original signal', lw=2, \n                 title=f'Threshold Scale: {scale:.1f}', ax=axes[i])\n    pd.Series(reconstructed_signal, index=signal.index).plot(c='k', label='DWT smoothing}', linewidth=1, ax=axes[i]) \n```", "```py\n$ QUANDL_API_KEY=<yourkey> zipline ingest [-b <bundle>] \n```", "```py\nfrom zipline.api import attach_pipeline, pipeline_output, record\nfrom zipline.pipeline import Pipeline, CustomFactor\nfrom zipline.pipeline.factors import Returns, AverageDollarVolume\nfrom zipline import run_algorithm\nMONTH, YEAR = 21, 252\nN_LONGS = N_SHORTS = 25\nVOL_SCREEN = 1000\nclass MeanReversion(CustomFactor):\n    \"\"\"Compute ratio of latest monthly return to 12m average,\n       normalized by std dev of monthly returns\"\"\"\n    inputs = [Returns(window_length=MONTH)]\n    window_length = YEAR\n    def compute(self, today, assets, out, monthly_returns):\n        df = pd.DataFrame(monthly_returns)\n        out[:] = df.iloc[-1].sub(df.mean()).div(df.std())\ndef compute_factors():\n    \"\"\"Create factor pipeline incl. mean reversion,\n        filtered by 30d Dollar Volume; capture factor ranks\"\"\"\n    mean_reversion = MeanReversion()\n    dollar_volume = AverageDollarVolume(window_length=30)\n    return Pipeline(columns={'longs'  : mean_reversion.bottom(N_LONGS),\n                             'shorts' : mean_reversion.top(N_SHORTS),\n                             'ranking': \n                          mean_reversion.rank(ascending=False)},\n                          screen=dollar_volume.top(VOL_SCREEN)) \n```", "```py\ndef initialize(context):\n    \"\"\"Setup: register pipeline, schedule rebalancing,\n        and set trading params\"\"\"\n    attach_pipeline(compute_factors(), 'factor_pipeline')\ndef before_trading_start(context, data):\n    \"\"\"Run factor pipeline\"\"\"\n    context.factor_data = pipeline_output('factor_pipeline')\n    record(factor_data=context.factor_data.ranking)\n    assets = context.factor_data.index\n    record(prices=data.current(assets, 'price')) \n```", "```py\nstart, end = pd.Timestamp('2015-01-01', tz='UTC'), pd.Timestamp('2018-\n             01-01', tz='UTC')\ncapital_base = 1e7\nperformance = run_algorithm(start=start,\n                            end=end,\n                            initialize=initialize,\n                            before_trading_start=before_trading_start,\n                            capital_base=capital_base)\nperformance.to_pickle('single_factor.pickle') \n```", "```py\nfrom quantopian.research import run_pipeline\nfrom quantopian.pipeline import Pipeline\nfrom quantopian.pipeline.data.builtin import USEquityPricing\nfrom quantopian.pipeline.data.morningstar import income_statement, \n     operation_ratios, balance_sheet\nfrom quantopian.pipeline.data.psychsignal import stocktwits\nfrom quantopian.pipeline.factors import CustomFactor, \n     SimpleMovingAverage, Returns\nfrom quantopian.pipeline.filters import QTradableStocksUS \n```", "```py\nclass AggregateFundamentals(CustomFactor):\n    def compute(self, today, assets, out, inputs):\n        out[:] = inputs[0] \n```", "```py\ndef compute_factors():\n    universe = QTradableStocksUS()\n    profitability = (AggregateFundamentals(inputs=\n                     [income_statement.gross_profit],\n                                           window_length=YEAR) /\n                     balance_sheet.total_assets.latest).rank(mask=universe)\n    roic = operation_ratios.roic.latest.rank(mask=universe)\n    ebitda_yield = (AggregateFundamentals(inputs=\n                             [income_statement.ebitda],\n                                          window_length=YEAR) /\n                    USEquityPricing.close.latest).rank(mask=universe)\n    mean_reversion = MeanReversion().rank(mask=universe)\n    price_momentum = Returns(window_length=QTR).rank(mask=universe)\n    sentiment = SimpleMovingAverage(inputs=[stocktwits.bull_minus_bear],\n                                    window_length=5).rank(mask=universe)\n    factor = profitability + roic + ebitda_yield + mean_reversion + \n             price_momentum + sentiment\n    return Pipeline(\n            columns={'Profitability'      : profitability,\n                     'ROIC'               : roic,\n                     'EBITDA Yield'       : ebitda_yield,\n                     \"Mean Reversion (1M)\": mean_reversion,\n                     'Sentiment'          : sentiment,\n                     \"Price Momentum (3M)\": price_momentum,\n                     'Alpha Factor'       : factor}) \n```", "```py\nperformance = pd.read_pickle('single_factor.pickle')\nprices = pd.concat([df.to_frame(d) for d, df in performance.prices.items()],axis=1).T\nprices.columns = [re.findall(r\"\\[(.+)\\]\", str(col))[0] for col in \n                  prices.columns]\nprices.index = prices.index.normalize()\nprices.info()\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 755 entries, 2015-01-02 to 2017-12-29\nColumns: 1661 entries, A to ZTS\ndtypes: float64(1661) \n```", "```py\nHOLDING_PERIODS = (5, 10, 21, 42)\nQUANTILES = 5\nalphalens_data = get_clean_factor_and_forward_returns(factor=factor_data,\n                                     prices=prices,\n                                     periods=HOLDING_PERIODS,\n                                     quantiles=QUANTILES)\nDropped 14.5% entries from factor data: 14.5% in forward returns computation and 0.0% in binning phase (set max_loss=0 to see potentially suppressed Exceptions). max_loss is 35.0%, not exceeded: OK! \n```", "```py\nfrom alphalens.performance import mean_return_by_quantile\nfrom alphalens.plotting import plot_quantile_returns_bar\nmean_return_by_q, std_err = mean_return_by_quantile(alphalens_data)\nplot_quantile_returns_bar(mean_return_by_q); \n```", "```py\nfrom alphalens.plotting import plot_cumulative_returns_by_quantile\nmean_return_by_q_daily, std_err =\n     mean_return_by_quantile(alphalens_data, by_date=True)\nplot_cumulative_returns_by_quantile(mean_return_by_q_daily['5D'], \n     period='5D'); \n```", "```py\nfrom alphalens.plotting import plot_quantile_returns_violin\nplot_quantile_returns_violin(mean_return_by_q_daily); \n```", "```py\nfrom alphalens.performance import factor_information_coefficient\nfrom alphalens.plotting import plot_ic_ts\nic = factor_information_coefficient(alphalens_data)\nplot_ic_ts(ic[['5D']]) \n```", "```py\nic = factor_information_coefficient(alphalens_data)\nic_by_year = ic.resample('A').mean()\nic_by_year.index = ic_by_year.index.year\nic_by_year.plot.bar(figsize=(14, 6)) \n```", "```py\ncreate_turnover_tear_sheet(alphalens_data) \n```"]