- en: '18'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs for Financial Time Series and Satellite Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduce the first of several specialized deep learning
    architectures that we will cover in *Part 4*. Deep **convolutional neural networks**
    (**CNNs**) have enabled superhuman performance in various computer vision tasks
    such as classifying images and video and detecting and recognizing objects in
    images. CNNs can also extract signals from time-series data that shares certain
    characteristics with image data and have been successfully applied to speech recognition
    (Abdel-Hamid et al. 2014). Moreover, they have been shown to deliver state-of-the-art
    performance on time-series classification across various domains (Ismail Fawaz
    et al. 2019).
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are named after a linear algebra operation called a **convolution** that
    replaces the general matrix multiplication typical of feedforward networks (discussed
    in the last chapter) in at least one of their layers. We will show how convolutions
    work and why they are particularly well suited to data with a certain regular
    structure typically found in images but also present in time series.
  prefs: []
  type: TYPE_NORMAL
- en: Research into **CNN architectures** has proceeded very rapidly, and new architectures
    that improve benchmark performance continue to emerge. We will describe a set
    of building blocks consistently used by successful applications. We will also
    demonstrate how **transfer learning** can speed up learning by using pretrained
    weights for CNN layers closer to the input while fine-tuning the final layers
    to a specific task. We will also illustrate how to use CNNs for the specific computer
    vision task of **object detection**.
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs can help build a **trading strategy** by generating signals from images
    or (multiple) time-series data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Satellite data** may signal future commodity trends, including the supply
    of certain crops or raw materials via aerial images of agricultural areas, mines,
    or transport networks like oil tankers. **Surveillance camera** footage, for example,
    from shopping malls, could be used to track and predict consumer activity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time-series data** encompasses a very broad range of data sources and CNNs
    have been shown to deliver high-quality classification results by exploiting their
    structural similarity with images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will create a trading strategy based on predictions of a CNN that uses time-series
    data that's been deliberately formatted like images and demonstrate how to build
    a CNN to classify satellite images.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, in this chapter, you will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How CNNs employ several building blocks to efficiently model grid-like data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training, tuning, and regularizing CNNs for images and time-series data using
    TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using transfer learning to streamline CNNs, even with less data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing a trading strategy using return predictions by a CNN trained on time-series
    data formatted like images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to classify satellite images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  prefs: []
  type: TYPE_NORMAL
- en: How CNNs learn to model grid-like data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CNNs are conceptually similar to feedforward **neural networks** (**NNs**):
    they consist of units with parameters called weights and biases, and the training
    process adjusts these parameters to optimize the network''s output for a given
    input according to a loss function. They are most commonly used for classification.
    Each unit uses its parameters to apply a linear operation to the input data or
    activations received from other units, typically followed by a nonlinear transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: The overall network models a **differentiable function** that maps raw data,
    such as image pixels, to class probabilities using an output activation function
    like softmax. CNNs use an objective function such as cross-entropy loss to measure
    the quality of the output with a single metric. They also rely on the gradients
    of the loss with respect to the network parameter to learn via backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward NNs with fully connected layers do not scale well to high-dimensional
    image data with a large number of pixel values. Even the low-resolution images
    included in the CIFAR-10 dataset that we'll use in the next section contain 32×32
    pixels with up to 256 different color values represented by 8 bits each. With
    three channels, for example, for the red, green, and blue channels of the RGB
    color model, a single unit in a fully connected input layer implies 32 × 32 ×
    3=3,072 weights. A more standard resolution of 640×480 pixels already yields closer
    to 1 million weights for a single input unit. Deep architectures with several
    layers of meaningful width quickly lead to an exploding number of parameters that
    make overfitting during training all but certain.
  prefs: []
  type: TYPE_NORMAL
- en: 'A fully connected feedforward NN makes no assumptions about the local structure
    of the input data so that arbitrarily reordering the features has no impact on
    the training result. By contrast, CNNs make the **key assumption** that the **data
    has a grid-like topology** and that the **local structure matters**. In other
    words, they encode the assumption that the input has a structure typically found
    in image data: pixels form a two-dimensional grid, possibly with several channels
    to represent the components of the color signal. Furthermore, the values of nearby
    pixels are likely more relevant to detect key features such as edges and corners
    than faraway data points. Naturally, initial CNN applications such as handwriting
    recognition focused on image data.'
  prefs: []
  type: TYPE_NORMAL
- en: Over time, however, researchers recognized **similar characteristics in time-series
    data**, broadening the scope for the productive use of CNNs. Time-series data
    consists of measurements at regular intervals that create a one-dimensional grid
    along the time axis, such as the lagged returns for a given ticker. There can
    also be a second dimension with additional features for this ticker and the same
    time periods. Finally, we could represent additional tickers using the third dimension.
  prefs: []
  type: TYPE_NORMAL
- en: A common CNN use case beyond images includes audio data, either in a one-dimensional
    waveform in the time domain or, after a Fourier transform, as a two-dimensional
    spectrum in the frequency domain. CNNs also play a key role in AlphaGo, the first
    algorithm to win a game of Go against humans, where they evaluated different positions
    on the grid-like board.
  prefs: []
  type: TYPE_NORMAL
- en: The most important element to encode the **assumption of a grid-like topology**
    is the **convolution** operation that gives CNNs their name, combined with **pooling**.
    We will see that the specific assumptions about the functional relationship between
    input and output data imply that CNNs need far fewer parameters and compute more
    efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explain how convolution and pooling layers learn filters
    that extract local features and why these operations are particularly suitable
    for data with the structure just described. State-of-the-art CNNs combine many
    of these basic building blocks to achieve the layered representation learning
    described in the previous chapter. We conclude by describing key architectural
    innovations over the last decade that saw enormous performance improvements.
  prefs: []
  type: TYPE_NORMAL
- en: From hand-coding to learning filters from data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For image data, this local structure has traditionally motivated the development
    of hand-coded filters that extract such patterns for the use as features in **machine
    learning** (**ML**) models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 18.1* displays the effect of simple filters designed to detect certain
    edges. The notebook `filter_example.ipynb` illustrates how to use hand-coded filters
    in a convolutional network and visualizes the resulting transformation of the
    image. The filters are simple [-1, 1] patterns arranged in a ![](img/B15439_18_001.png)
    matrix, shown in the upper right of the figure. Below each filter, its effects
    are shown; they are a bit subtle and will be easier to spot in the accompanying
    notebook.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.1: The result of basic edge filters applied to an image'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers, by contrast, are designed to learn such local feature
    representations from the data. A key insight is to restrict their input, called
    the **receptive field**, to a small area of the input so it captures basic pixel
    constellations that reflect common patterns like edges or corners. Such patterns
    may occur anywhere in an image, though, so CNNs also need to recognize similar
    patterns in different locations and possibly with small variations.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequent layers then learn to synthesize these local features to detect **higher-order
    features**. The linked resources on GitHub include examples of how to visualize
    the filters learned by a deep CNN using some of the deep architectures that we
    present in the next section on reference architectures.
  prefs: []
  type: TYPE_NORMAL
- en: How the elements of a convolutional layer operate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Convolutional layers integrate **three architectural ideas** that enable the
    learning of feature representations that are to some degree invariant to shifts,
    changes in scale, and distortion:'
  prefs: []
  type: TYPE_NORMAL
- en: Sparse rather than dense connectivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight sharing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spatial or temporal downsampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, convolutional layers allow for inputs of variable size. We will walk
    through a typical convolutional layer and describe each of these ideas in turn.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 18.2* outlines the set of operations that typically takes place in
    a three-dimensional convolutional layer, assuming image data is input with the
    three dimensions of height, width, and depth, or the number of channels. The range
    of pixel values depends on the bit representation, for example, [0, 255] for 8
    bits. Alternatively, the width axis could represent time, the height different
    features, and the channels could capture observations on distinct objects such
    as tickers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.2: Typical operations in a two-dimensional convolutional layer'
  prefs: []
  type: TYPE_NORMAL
- en: Successive computations process the input through the convolutional, detector,
    and pooling stages that we describe in the next three sections. In the example
    depicted in *Figure 18.2*, the convolutional layer receives three-dimensional
    input and produces an output of the same dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: State-of-the-art CNNs are composed of several such layers of varying sizes that
    are either stacked on top of each other or operate in parallel on different branches.
    With each layer, the network can detect higher-level, more abstract features.
  prefs: []
  type: TYPE_NORMAL
- en: The convolution stage – extracting local features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first stage applies a filter, also called the **kernel**, to overlapping
    patches of the input image. The filter is a matrix of a much smaller size than
    the input so that its receptive field is limited to a few contiguous values such
    as pixels or time-series values. As a result, it focuses on local patterns and
    dramatically reduces the number of parameters and computations relative to a fully
    connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: A complete convolutional layer has several **feature maps** organized as depth
    slices (depicted in *Figure 18.2*) so that each layer can extract multiple features.
  prefs: []
  type: TYPE_NORMAL
- en: From filters to feature maps
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While scanning the input, the kernel is convolved with each input segment covered
    by its receptive field. The convolution operation is simply the dot product between
    the filter weights and the values of the matching input area after both have been
    reshaped to vectors. Each convolution thus produces a single number, and the entire
    scan yields a feature map. Since the dot product is maximized for identical vectors,
    the feature map indicates the degree of activation for each input region.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 18.3* illustrates the result of the scan of a ![](img/B15439_18_002.png)
    input using a ![](img/B15439_18_003.png) filter with given values, and how the
    activation in the upper-right corner of the feature map results from the dot product
    of the flattened input region and the kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.3: From convolutions to a feature map'
  prefs: []
  type: TYPE_NORMAL
- en: The most important aspect is that the **filter values are the parameters** of
    the convolutional layers, **learned from the data** during training to minimize
    the chosen loss function. In other words, CNNs learn useful feature representations
    by finding kernel values that activate input patterns that are most useful for
    the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: How to scan the input – strides and padding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The **stride** defines the step size used for scanning the input, that is,
    the number of pixels to shift horizontally and vertically. Smaller strides scan
    more (overlapping) areas but are computationally more expensive. Four options
    are commonly used when the filter does not fit the input perfectly and partially
    crosses the image boundary during the scan:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Valid convolution**: Discards scans where the image and filter do not perfectly
    match'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Same convolution**: Zero-pads the input to produce a feature map of equal
    size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Full convolution**: Zero-pads the input so that each pixel is scanned an
    equal number of times, including pixels at the border (to avoid oversampling pixels
    closer to the center)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Causal**: Zero-pads the input only on the left so that the output does not
    depend on an input from a later period; maintains the temporal order for time-series
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choices depend on the nature of the data and where useful features are most
    likely located. In combination with the number of depth slices, they determine
    the output size of the convolution stage. The Stanford lecture notes by Andrew
    Karpathy (see GitHub) contain helpful examples using NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter sharing for robust features and fast computation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The location of salient features may vary due to distortion or shifts. Furthermore,
    elementary feature detectors are likely useful across the entire image. CNNs encode
    these assumptions by sharing or tying the weights for the filter in a given depth
    slice.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, each depth slice specializes in a certain pattern and the number
    of parameters is further reduced. Weight sharing works less well, however, when
    images are spatially centered and key patterns are less likely to be uniformly
    distributed across the input area.
  prefs: []
  type: TYPE_NORMAL
- en: The detector stage – adding nonlinearity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The feature maps are usually passed through a nonlinear transformation. The
    **rectified linear unit** (**ReLU**) that we encountered in the last chapter is
    a common function for this purpose. ReLUs replace negative activations element-wise
    by zero and mitigate the risk of vanishing gradients found in other activation
    functions such as tanh (see *Chapter 17*, *Deep Learning for Trading*).
  prefs: []
  type: TYPE_NORMAL
- en: 'A popular alternative is the **softplus function**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_004.png)'
  prefs: []
  type: TYPE_IMG
- en: In contrast to ReLU, it has a derivative everywhere, namely the sigmoid function
    that we used for logistic regression (see *Chapter 7*, *Linear Models – From Risk
    Factors to Return Forecasts*).
  prefs: []
  type: TYPE_NORMAL
- en: The pooling stage – downsampling the feature maps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last stage of the convolutional layer may downsample the feature map''s
    input representation to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce its dimensionality and prevent overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lower the computational cost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable basic translation invariance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This assumes that the precise location of the features is not only less important
    for identifying a pattern but can even be harmful because it will likely vary
    for different instances of the target. Pooling lowers the spatial resolution of
    the feature map as a simple way to render the location information less precise.
    However, this step is optional and many architectures use pooling only for some
    layers or not at all.
  prefs: []
  type: TYPE_NORMAL
- en: A common pooling operation is **max pooling**, which uses only the maximum activation
    value from (typically) non-overlapping subregions. For a small ![](img/B15439_18_005.png)
    feature map, for instance, ![](img/B15439_18_001.png) max pooling outputs the
    maximum for each of the four non-overlapping ![](img/B15439_18_001.png) areas.
    Less common pooling operators use the average or the median. Pooling does not
    add or learn new parameters but the size of the input window and possibly the
    stride are additional hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of CNN architectures – key innovations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several CNN architectures have pushed performance boundaries over the past two
    decades by introducing important innovations. Predictive performance growth accelerated
    dramatically with the arrival of big data in the form of ImageNet (Fei-Fei 2015)
    with 14 million images assigned to 20,000 classes by humans via Amazon's Mechanical
    Turk. The **ImageNet Large Scale Visual Recognition Challenge** (**ILSVRC**) became
    the focal point of CNN progress around a slightly smaller set of 1.2 million images
    from 1,000 classes.
  prefs: []
  type: TYPE_NORMAL
- en: It is useful to be familiar with the **reference architectures** dominating
    these competitions for practical reasons. As we will see in the next section on
    working with CNNs for image data, they offer a good starting point for standard
    tasks. Moreover, **transfer learning** allows us to address many computer vision
    tasks by building on a successful architecture with pretrained weights. Transfer
    learning not only speeds up architecture selection and training but also enables
    successful applications on much smaller datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, many publications refer to these architectures, and they often
    serve as a basis for networks tailored to segmentation or localization tasks.
    We will further describe some landmark architectures in the section on image classification
    and transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Performance breakthroughs and network size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The left side of *Figure 18.4* plots the top-1 accuracy against the computational
    cost of a variety of network architectures. It suggests a positive relationship
    between the number of parameters and performance, but also shows that the marginal
    benefit of more parameters declines and that architectural design and innovation
    also matter.
  prefs: []
  type: TYPE_NORMAL
- en: The right side plots the top-1 accuracy per parameter for all networks. Several
    new architectures target use cases on less powerful devices such as mobile phones.
    While they do not achieve state-of-the-art performance, they have found much more
    efficient implementations. See the resources on GitHub for more details on these
    architectures and the analysis behind these charts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.4: Predictive performance and computational complexity'
  prefs: []
  type: TYPE_NORMAL
- en: Lessons learned
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some of the lessons learned from 20 years of CNN architecture developments,
    especially since 2012, include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Smaller convolutional** filters perform better (possibly except at the first
    layer) because several small filters can substitute for a larger filter at a lower
    computational cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1 × 1 convolutions** reduce the dimensionality of feature maps so that the
    network can learn a larger number overall.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Skip connections** are able to create multiple paths through the network
    and enable the training of much higher-capacity CNNs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs for satellite images and object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we demonstrate how to solve key computer vision tasks such
    as image classification and object detection. As mentioned in the introduction
    and in *Chapter 3*, *Alternative Data for Finance – Categories and Use Cases*,
    image data can inform a trading strategy by providing clues about future trends,
    changing fundamentals, or specific events relevant to a target asset class or
    investment universe. Popular examples include exploiting satellite images for
    clues about the supply of agricultural commodities, consumer and economic activity,
    or the status of manufacturing or raw material supply chains. Specific tasks might
    include the following, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image classification**: Identifying whether cultivated land for certain crops
    is expanding, or predicting harvest quality and quantities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object detection**: Counting the number of oil tankers on a certain transport
    route or the number of cars in a parking lot, or identifying the locations of
    shoppers in a mall'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we'll demonstrate how to design CNNs to automate the extraction
    of such information, both from scratch using popular architectures and via transfer
    learning that fine-tunes pretrained weights to a given task. We'll also demonstrate
    how to detect objects in a given scene.
  prefs: []
  type: TYPE_NORMAL
- en: We will introduce key CNN architectures for these tasks, explain why they work
    well, and show how to train them using TensorFlow 2\. We will also demonstrate
    how to source pretrained weights and fine-tune time. Unfortunately, satellite
    images with information directly relevant for a trading strategy are very costly
    to obtain and are not readily available. We will, however, demonstrate how to
    work with the EuroSat dataset to build a classifier that identifies different
    land uses. This brief introduction to CNNs for computer vision aims to demonstrate
    how to approach common tasks that you will likely need to tackle when aiming to
    design a trading strategy based on images relevant to the investment universe
    of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: All the libraries we introduced in the last chapter provide support for convolutional
    layers; we'll focus on the Keras interface of TensorFlow 2\. We are first going
    to illustrate the LeNet5 architecture using the MNIST handwritten digit dataset.
    Next, we'll demonstrate the use of data augmentation with AlexNet on CIFAR-10,
    a simplified version of the original ImageNet. Then we'll continue with transfer
    learning based on state-of-the-art architectures before we apply what we've learned
    to actual satellite images. We conclude with an example of object detection in
    real-life scenes.
  prefs: []
  type: TYPE_NORMAL
- en: LeNet5 – The first CNN with industrial applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Yann LeCun, now the Director of AI Research at Facebook, was a leading pioneer
    in CNN development. In 1998, after several iterations starting in the 1980s, LeNet5
    became the first modern CNN used in real-world applications that introduced several
    architectural elements still relevant today.
  prefs: []
  type: TYPE_NORMAL
- en: LeNet5 was published in a very instructive paper, *Gradient-Based Learning Applied
    to Document Recognition* (LeCun et al. 1989), that laid out many of the central
    concepts. Most importantly, it promoted the insight that convolutions with learnable
    filters are effective at extracting related features at multiple locations with
    few parameters. Given the limited computational resources at the time, efficiency
    was of paramount importance.
  prefs: []
  type: TYPE_NORMAL
- en: LeNet5 was designed to recognize the handwriting on checks and was used by several
    banks. It established a new benchmark for classification accuracy, with a result
    of 99.2 percent on the MNIST handwritten digit dataset. It consists of three convolutional
    layers, each containing a nonlinear tanh transformation, a pooling operation,
    and a fully connected output layer. Throughout the convolutional layers, the number
    of feature maps increases while their dimensions decrease. It has a total of 60,850
    trainable parameters (Lecun et al. 1998).
  prefs: []
  type: TYPE_NORMAL
- en: '"Hello World" for CNNs – handwritten digit classification'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we'll implement a slightly simplified version of LeNet5 to
    demonstrate how to build a CNN using a TensorFlow implementation. The original
    MNIST dataset contains 60,000 grayscale images in ![](img/B15439_18_008.png) pixel
    resolution, each containing a single handwritten digit from 0 to 9\. A good alternative
    is the more challenging but structurally similar Fashion MNIST dataset that we
    encountered in *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation with
    Unsupervised Learning*. See the `digit_classification_with_lenet5` notebook for
    implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can load it in Keras out of the box:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 18.5* shows the first ten images in the dataset and highlights significant
    variation among instances of the same digit. On the right, it shows how the pixel
    values for an individual image range from 0 to 255:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.5: MNIST sample images'
  prefs: []
  type: TYPE_NORMAL
- en: 'We rescale the pixel values to the range [0, 1] to normalize the training data
    and facilitate the backpropagation process and convert the data to 32-bit floats,
    which reduce memory requirements and computational cost while providing sufficient
    precision for our use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Defining the LeNet5 architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can define a simplified version of LeNet5 that omits the original final
    layer containing radial basis functions as follows, using the default "valid"
    padding and single-step strides unless defined otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The summary indicates that the model thus defined has over 300,000 parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We compile with `sparse_crossentropy_loss`, which accepts integers rather than
    one-hot-encoded labels and the original stochastic gradient optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Training and evaluating the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we are ready to train the model. The model expects four-dimensional input,
    so we reshape accordingly. We use the standard batch size of 32 and an 80:20 train-validation
    split. Furthermore, we leverage checkpointing to store the model weights if the
    validation error improves, and make sure the dataset is randomly shuffled. We
    also define an `early_stopping` callback to interrupt training once the validation
    accuracy no longer improves for 20 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The training history records the last improvement after 81 epochs that take
    around 4 minutes on a single GPU. The test accuracy of this sample run is 99.09
    percent, almost exactly the same result as for the original LeNet5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For comparison, a simple two-layer feedforward network achieves "only" 97.04
    percent test accuracy (see the notebook). The LeNet5 improvement on MNIST is,
    in fact, modest. Non-neural methods have also achieved classification accuracies
    greater than or equal to 99 percent, including K-nearest neighbors and support
    vector machines. CNNs really shine with more challenging datasets as we will see
    next.
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet – reigniting deep learning research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AlexNet, developed by Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton at the
    University of Toronto, dramatically reduced the error rate and significantly outperformed
    the runner-up at the 2012 ILSVRC, achieving a top-5 error of 16 percent versus
    26 percent (Krizhevsky, Sutskever, and Hinton 2012). This breakthrough triggered
    a renaissance in ML research and put deep learning for computer vision firmly
    on the global technology map.
  prefs: []
  type: TYPE_NORMAL
- en: The AlexNet architecture is similar to LeNet, but much deeper and wider. It
    is often credited with discovering **the importance of depth** with around 60
    million parameters, exceeding LeNet5 by a factor of 1,000, a testament to increased
    computing power, especially the use of GPUs, and much larger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: It included convolutions stacked on top of each other rather than combining
    each convolution with a pooling stage, and successfully used dropout for regularization
    and ReLU for efficient nonlinear transformations. It also employed data augmentation
    to increase the number of training samples, added weight decay, and used a more
    efficient implementation of convolutions. It also accelerated training by distributing
    the network over two GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook `image_classification_with_alexnet.ipynb` has a slightly simplified
    version of AlexNet tailored to the CIFAR-10 dataset that contains 60,000 images
    from 10 of the original 1,000 classes. It has been compressed to a ![](img/B15439_18_009.png)
    pixel resolution from the original ![](img/B15439_18_010.png), but still has three
    color channels.
  prefs: []
  type: TYPE_NORMAL
- en: See the notebook `image_classification_with_alexnet` for implementation details;
    we will skip over some repetitive steps here.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing CIFAR-10 data using image augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CIFAR-10 can also be downloaded using TensorFlow's Keras interface, and we rescale
    the pixel values and one-hot encode the ten class labels as we did with MNIST
    in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: We first train a two-layer feedforward network on 50,000 training samples for
    45 epochs to achieve a test accuracy of 45.78 percent. We also experiment with
    a three-layer convolutional net with over 528,000 parameters that achieves 74.51
    percent test accuracy (see the notebook).
  prefs: []
  type: TYPE_NORMAL
- en: 'A common trick to enhance performance is to artificially increase the size
    of the training set by creating synthetic data. This involves randomly shifting
    or horizontally flipping the image or introducing noise into the image. TensorFlow
    includes an `ImageDataGenerator` class for this purpose. We can configure it and
    fit the training data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The result shows how the augmented images (in low 32×32 resolution) have been
    altered in various ways as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.6: Original and augmented samples'
  prefs: []
  type: TYPE_NORMAL
- en: The test accuracy for the three-layer CNN improves modestly to 76.71 percent
    after training on the larger, augmented data.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the model architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We need to adapt the AlexNet architecture to the lower dimensionality of CIFAR-10
    images relative to the ImageNet samples used in the competition. To this end,
    we use the original number of filters but make them smaller (see the notebook
    for implementation details).
  prefs: []
  type: TYPE_NORMAL
- en: The summary (see the notebook) shows the five convolutional layers followed
    by two fully connected layers with frequent use of batch normalization, for a
    total of 21.5 million parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing AlexNet performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to AlexNet, we trained a 2-layer feedforward NN and a 3-layer CNN,
    the latter with and without image augmentation. After 100 epochs (with early stopping
    if the validation accuracy does not improve for 20 rounds), we obtain the cross-validation
    trajectories and test accuracy for the four models, as displayed in *Figure 18.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.7: Validation performance and test accuracy on CIFAR-10'
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet achieves the highest test accuracy with 79.33 percent after some 35
    epochs, closely followed by the shallower CNN with augmented images at 78.29 percent
    that trains for longer due to the larger dataset. The feedforward NN performs
    much worse than on MNIST on this more complex dataset, with a test accuracy of
    43.05 percent.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning – faster training with less data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In practice, sometimes we do not have enough data to train a CNN from scratch
    with random initialization. **Transfer learning** is an ML technique that repurposes
    a model trained on one set of data for another task. Naturally, it works if the
    learning from the first task carries over to the task of interest. If successful,
    it can lead to better performance and faster training that requires less labeled
    data than training a neural network from scratch on the target task.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative approaches to transfer learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The transfer learning approach to CNN relies on pretraining on a very large
    dataset like ImageNet. The goal is for the convolutional filters to extract a
    feature representation that generalizes to new images. In a second step, it leverages
    the result to either initialize and retrain a new CNN or use it as input to a
    new network that tackles the task of interest.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed, CNN architectures typically use a sequence of convolutional layers
    to detect hierarchical patterns, adding one or more fully connected layers to
    map the convolutional activations to the outcome classes or values. The output
    of the last convolutional layer that feeds into the fully connected part is called
    the bottleneck features. We can use the **bottleneck features** of a pretrained
    network as inputs into a new fully connected network, usually after applying a
    ReLU activation function.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we freeze the convolutional layers and **replace the dense part
    of the network**. An additional benefit is that we can then use inputs of different
    sizes because it is the dense layers that constrain the input size.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can use the bottleneck features as **inputs into a different
    machine learning algorithm**. In the AlexNet architecture, for instance, the bottleneck
    layer computes a vector with 4,096 entries for each ![](img/B15439_18_010.png)
    input image. We then use this vector as features for a new model.
  prefs: []
  type: TYPE_NORMAL
- en: We also can go a step further and not only replace and retrain the final layers
    using new data but also **fine-tune the weights of the pretrained CNN**. To achieve
    this, we continue training, either only for later layers while freezing the weights
    of some earlier layers, or for all layers. The motivation is presumably to preserve
    more generic patterns learned by lower layers, such as edge or color blob detectors,
    while allowing later layers of the CNN to adapt to the details of a new task.
    ImageNet, for example, contains a wide variety of dog breeds, which may lead to
    feature representations specifically useful for differentiating between these
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Building on state-of-the-art architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transfer learning permits us to leverage top-performing architectures without
    incurring the potentially fairly GPU- and data-intensive training. We briefly
    outline the key characteristics of a few additional popular architectures that
    are popular starting points.
  prefs: []
  type: TYPE_NORMAL
- en: VGGNet – more depth and smaller filters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The runner-up in ILSVRC 2014 was developed by Oxford University's Visual Geometry
    Group (VGG, Simonyan 2015). It demonstrated the effectiveness of **much smaller**
    ![](img/B15439_18_012.png) **convolutional filters** combined in sequence and
    reinforced the importance of depth for strong performance. VGG16 contains 16 convolutional
    and fully connected layers that only perform ![](img/B15439_18_012.png) convolutions
    and ![](img/B15439_18_014.png) pooling (see *Figure 18.5*).
  prefs: []
  type: TYPE_NORMAL
- en: VGG16 has **140 million parameters** that increase the computational costs of
    training and inference as well as the memory requirements. However, most parameters
    are in the fully connected layers that were since discovered not to be essential
    so that removing them greatly reduces the number of parameters without negatively
    impacting performance.
  prefs: []
  type: TYPE_NORMAL
- en: GoogLeNet – fewer parameters through Inception
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Christian Szegedy at Google reduced the computational costs using more efficient
    CNN implementations to facilitate practical applications at scale. The resulting
    GoogLeNet (Szegedy et al. 2015) won the ILSVRC 2014 with only 4 million parameters
    due to the **Inception module**, compared to AlexNet's 60 million and VGG16's
    140 million.
  prefs: []
  type: TYPE_NORMAL
- en: The Inception module builds on the **network-in-network concept** that uses
    ![](img/B15439_18_015.png) convolutions to compress a deep stack of convolutional
    filters and thus reduce the cost of computation. The module uses parallel ![](img/B15439_18_015.png),
    ![](img/B15439_18_017.png), and ![](img/B15439_18_018.png) filters, combining
    the latter two with ![](img/B15439_18_015.png) convolutions to reduce the dimensionality
    of the filters passed in by the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, it uses average pooling instead of fully connected layers on top
    of the convolutional layers to eliminate many of the less impactful parameters.
    There have been several enhanced versions, most recently Inception-v4.
  prefs: []
  type: TYPE_NORMAL
- en: ResNet – shortcut connections beyond human performance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The **residual network** **(ResNet)** architecture was developed at Microsoft
    and won the ILSVRC 2015\. It pushed the top-5 error to 3.7 percent, below the
    level of human performance on this task of around 5 percent (He et al. 2015).
  prefs: []
  type: TYPE_NORMAL
- en: It introduces identity shortcut connections that skip several layers and overcome
    some of the challenges of training deep networks, enabling the use of hundreds
    or even over a thousand layers. It also heavily uses batch normalization, which
    was shown to allow higher learning rates and be more forgiving about weight initialization.
    The architecture also omits the fully connected final layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the last chapter, the training of deep networks faces the notorious
    vanishing gradient challenge: as the gradient propagates to earlier layers, repeated
    multiplication of small weights risks shrinking the gradient toward zero. Hence,
    increasing depth may limit learning.'
  prefs: []
  type: TYPE_NORMAL
- en: The shortcut connection that skips two or more layers has become one of the
    most popular developments in CNN architectures and triggered numerous research
    efforts to refine and explain its performance. See the references on GitHub for
    additional information.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning with VGG16 in practice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern CNNs can take weeks to train on multiple GPUs on ImageNet, but fortunately,
    many researchers share their final weights. TensorFlow 2, for example, contains
    pretrained models for several of the reference architectures discussed previously,
    namely VGG16 and its larger version, VGG19, ResNet50, InceptionV3, and InceptionResNetV2,
    as well as MobileNet, DenseNet, NASNet, and MobileNetV2.
  prefs: []
  type: TYPE_NORMAL
- en: How to extract bottleneck features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The notebook `bottleneck_features.ipynb` illustrates how to download the pretrained
    VGG16 model, either with the final layers to generate predictions or without the
    final layers, as illustrated in *Figure 18.8*, to extract the outputs produced
    by the bottleneck features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.8: The VGG16 architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow 2 makes it very straightforward to download and use pretrained models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use this model for predictions like any other Keras model: we pass
    in seven sample images and obtain class probabilities for each of the 1,000 ImageNet
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To exclude the fully connected layers, just add the keyword `include_top=False`.
    Predictions are now output by the final convolutional layer `block5_pool` and
    match this layer''s shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: By omitting the fully connected layers and keeping only the convolutional modules,
    we are no longer forced to use a fixed input size for the model such as the original
    ![](img/B15439_18_010.png) ImageNet format. Instead, we can adapt the model to
    arbitrary input sizes.
  prefs: []
  type: TYPE_NORMAL
- en: How to fine-tune a pretrained model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will demonstrate how to freeze some or all of the layers of a pretrained
    model and continue training using a new fully-connected set of layers and data
    with a different format (see the notebook `transfer_learning.ipynb` for code examples,
    adapted from a TensorFlow 2 tutorial).
  prefs: []
  type: TYPE_NORMAL
- en: We use the VGG16 weights, pretrained on ImageNet with TensorFlow's built-in
    cats versus dogs images (see the notebook on how to source the dataset).
  prefs: []
  type: TYPE_NORMAL
- en: 'Preprocessing resizes all images to ![](img/B15439_18_021.png) pixels. We indicate
    the new input size as we instantiate the pretrained VGG16 instance and then freeze
    all weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape of the model output for 32 sample images now matches that of the
    last convolutional layer in the headless model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can append new layers to the headless model using either the Sequential
    or the Functional API. For the Sequential API, adding `GlobalAveragePooling2D`,
    `Dense`, and `Dropout` layers works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We set `from_logits=True` for the `BinaryCrossentropy` loss because the model
    provides a linear output. The summary shows how the new model combines the pretrained
    VGG16 convolutional layers and the new final layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: See the notebook for the Functional API version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prior to training the new final layer, the pretrained VGG16 delivers a validation
    accuracy of 48.75 percent. Now we proceed to train the model for 10 epochs as
    follows, adjusting only the final layer weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '10 epochs boost validation accuracy above 94 percent. To fine-tune the model,
    we can unfreeze the VGG16 models and continue training. Note that you should only
    do so after training the new final layers: randomly initialized classification
    layers will likely produce large gradient updates that can eliminate the pretraining
    results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To unfreeze parts of the model, we select a layer, after which we set the weights
    to `trainable`; in this case, layer 12 of the total 19 layers in the VGG16 architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now just recompile the model and continue training for up to 50 epochs using
    early stopping, starting in epoch 10 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 18.9* shows how the validation accuracy increases substantially, reaching
    97.89 percent after another 22 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.9: Cross-validation performance: accuracy and cross-entropy loss'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is an important technique when training data is limited as
    is very often the case in practice. While cats and dogs are unlikely to produce
    tradeable signals, transfer learning could certainly help improve the accuracy
    of predictions on a relevant alternative dataset, such as the satellite images
    that we'll tackle next.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying satellite images with transfer learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Satellite images figure prominently among alternative data (see *Chapter 3*,
    *Alternative Data for Finance – Categories and Use Cases*). For instance, commodity
    traders may rely on satellite images to predict the supply of certain crops or
    resources by monitoring, activity on farms, at mining sites, or oil tanker traffic.
  prefs: []
  type: TYPE_NORMAL
- en: The EuroSat dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To illustrate working with this type of data, we load the EuroSat dataset included
    in the TensorFlow 2 datasets (Helber et al. 2019). The EuroSat dataset includes
    around 27,000 images in ![](img/B15439_18_022.png) format that represent 10 different
    types of land uses. *Figure 18.10* displays an example for each label:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.10: Ten types of land use contained in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: A time series of similar data could be used to track the relative sizes of cultivated,
    industrial, and residential areas or the status of specific crops to predict harvest
    quantities or quality, for example, for wine.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a very deep CNN – DenseNet201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Huang et al. (2018) developed a new architecture dubbed **densely connected**
    based on the insight that CNNs can be deeper, more accurate, and more efficient
    to train if they contain shorter connections between layers close to the input
    and those close to the output.
  prefs: []
  type: TYPE_NORMAL
- en: One architecture, labeled **DenseNet201**, connects each layer to every other
    layer in a feedforward fashion. It uses the feature maps of all preceding layers
    as inputs, while each layer's own feature maps become inputs into all subsequent
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We download the DenseNet201 architecture from `tensorflow.keras.applications`
    and replace its final layers with the following dense layers interspersed with
    batch normalization to mitigate exploding or vanishing gradients in this very
    deep network with over 700 layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Model training and results evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use 10 percent of the training images for validation purposes and achieve
    the best out-of-sample classification accuracy of 97.96 percent after 10 epochs.
    This exceeds the performance cited in the original paper for the best-performing
    ResNet-50 architecture with a 90-10 split.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.11: Cross-validation performance'
  prefs: []
  type: TYPE_NORMAL
- en: There would likely be additional performance gains from augmenting the relatively
    small training set.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection and segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Image classification is a fundamental computer vision task that requires labeling
    an image based on certain objects it contains. Many practical applications, including
    investment and trading strategies, require additional information:'
  prefs: []
  type: TYPE_NORMAL
- en: The **object detection** task requires not only the identification but also
    the spatial location of all objects of interest, typically using bounding boxes.
    Several algorithms have been developed to overcome the inefficiency of brute-force
    sliding-window approaches, including region proposal methods (R-CNN; see for example
    Ren et al. 2015) and the **You Only Look Once** (**YOLO**) real-time object detection
    algorithm (Redmon 2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **object segmentation** task goes a step further and requires a class label
    and an outline of every object in the input image. This may be useful to count
    objects such as oil tankers, individuals, or cars in an image and evaluate a level
    of activity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic segmentation**, also called scene parsing, makes dense predictions
    to assign a class label to each pixel in the image. As a result, the image is
    divided into semantic regions and each pixel is assigned to its enclosing object
    or region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection requires the ability to distinguish between several classes
    of objects and to decide how many and which of these objects are present in an
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A prominent example is Ian Goodfellow''s identification of house numbers from
    Google''s **Street View House Numbers** (**SVHN**) dataset (Goodfellow 2014).
    It requires the model to identify the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How many of up to five digits make up the house number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The correct digit for each component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proper order of the constituent digits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will show how to preprocess the irregularly shaped source images, adapt the
    VGG16 architecture to produce multiple outputs, and train the final layer, before
    fine-tuning the pretrained weights to address the task.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the source images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The notebook `svhn_preprocessing.ipynb` contains code to produce a simplified,
    cropped dataset that uses bounding box information to create regularly shaped
    ![](img/B15439_18_023.png) images containing the digits; the original images are
    of arbitrary shape (Netzer 2011).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.12: Cropped sample images of the SVHN dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The SVHN dataset contains house numbers with up to five digits and uses the
    class 10 if a digit is not present. However, since there are very few examples
    with five digits, we limit the images to those including up to four digits only.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning with a custom final layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The notebook `svhn_object_detection.ipynb` illustrates how to apply transfer
    learning to a deep CNN based on the VGG16 architecture, as outlined in the previous
    section. We will describe how to create new final layers that produce several
    outputs to meet the three SVHN task objectives, including one prediction of how
    many digits are present, and one for the value of each digit in the order they
    appear.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best-performing architecture on the original dataset has eight convolutional
    layers and two final fully connected layers. We will use **transfer learning**,
    departing from the VGG16 architecture. As before, we import the VGG16 network
    pretrained on ImageNet weights, remove the layers after the convolutional blocks,
    freeze the weights, and create new dense and predictive layers as follows using
    the Functional API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The prediction layer combines the four-class output for the number of digits
    `n_digits` with four outputs that predict which digit is present at that position.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom loss function and evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The custom output requires us to define a loss function that captures how well
    the model is meeting its objective. We would also like to measure accuracy in
    a way that reflects predictive accuracy tailored to the specific labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the custom loss, we average the cross-entropy over the five categorical
    outputs, namely the number of digits and their respective values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To measure predictive accuracy, we compare the five predictions with the corresponding
    label values and average the share of correct matches over the batch of samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we integrate the base and final layers and compile the model with
    the custom loss and accuracy metric as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Fine-tuning the VGG16 weights and final layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We train the new final layers for 14 periods and continue fine-tuning all VGG16
    weights, as in the previous section, for another 23 epochs (using early stopping
    in both cases).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following charts show the training and validation accuracy and the loss
    over the entire training period. As we unfreeze the VGG16 weights after the initial
    training period, the accuracy drops and then improves, achieving a validation
    performance of 94.52 percent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.13: Cross-validation performance'
  prefs: []
  type: TYPE_NORMAL
- en: See the notebook for additional implementation details and an evaluation of
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: Lessons learned
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can achieve decent levels of accuracy using only the small training set.
    However, state-of-the-art performance achieves an error rate of only 1.02 percent
    ([https://benchmarks.ai/svhn](https://benchmarks.ai/svhn)). To get closer, the
    most important step is to increase the amount of training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two easy ways to accomplish this: we can include the larger number
    of samples included in the **extra** dataset, and we can use image augmentation
    (see the *AlexNet: reigniting deep learning research* section). The currently
    best-performing approach relies heavily on augmentation learned from data (Cubuk
    2019).'
  prefs: []
  type: TYPE_NORMAL
- en: CNNs for time-series data – predicting returns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs were originally developed to process image data and have achieved superhuman
    performance on various computer vision tasks. As discussed in the first section,
    time-series data has a grid-like structure similar to that of images, and CNNs
    have been successfully applied to one-, two- and three-dimensional representations
    of temporal data.
  prefs: []
  type: TYPE_NORMAL
- en: The application of CNNs to time series will most likely bear fruit if the data
    meets the model's key assumption that local patterns or relationships help predict
    the outcome. In the time-series context, local patterns could be autocorrelation
    or similar non-linear relationships at relevant intervals. Along the second and
    third dimensions, local patterns imply systematic relationships among different
    components of a multivariate series or among these series for different tickers.
    Since locality matters, it is important that the data is organized accordingly,
    in contrast to feed-forward networks where shuffling the elements of any dimension
    does not negatively affect the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we provide a relatively simple example using a one-dimensional
    convolution to model an autoregressive process (see *Chapter 9*, *Time-Series
    Models for Volatility Forecasts and Statistical Arbitrage*) that predicts future
    returns based on lagged returns. Then we replicate a recent research paper that
    achieved good results by formatting multivariate time-series data like images
    to predict returns. We will also develop and test a trading strategy based on
    the signals contained in the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: An autoregressive CNN with 1D convolutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will introduce the time series use case for CNN using a univariate autoregressive
    asset return model. More specifically, the model receives the most recent 12 months
    of returns and uses a single layer of one-dimensional convolutions to predict
    the subsequent month.
  prefs: []
  type: TYPE_NORMAL
- en: 'The requisite steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the rolling 12 months of lagged returns and corresponding outcomes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the model architecture
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the model and evaluating the results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following sections, we'll describe each step in turn; the notebook `time_series_prediction`
    contains the code samples for this section.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we''ll select the adjusted close price for all Quandl Wiki stocks since
    2000 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we resample the price data to month-end frequency, compute returns, and
    set monthly returns over 100 percent to missing as they likely represent data
    errors. Then we drop tickers with missing observations, retaining 1,511 stocks
    with 215 observations each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To create the rolling series of 12 lagged monthly returns with their corresponding
    outcome, we iterate over rolling 13-month slices and append the transpose of each
    slice to a list after assigning the outcome date to the index. After completing
    the loop, we concatenate the DataFrames in the list as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We end up with over 305,000 pairs of outcomes and lagged returns for the 2001-2017
    period:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'When we compute the information coefficient for each lagged return and the
    outcome, we find that only lag 5 is not statistically significant:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.14: Information coefficient with respect to forward return by lag'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the model architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we''ll define the model architecture using TensorFlow''s Keras interface.
    We combine a one-dimensional convolutional layer with max pooling and batch normalization
    to produce a real-valued scalar output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The one-dimensional convolution computes the sliding dot product of a (regularized)
    vector of length 4 with each input sequence of length 12, using causal padding
    to maintain the temporal order (see the *How to scan the input: strides and padding*
    section). The resulting 32 feature maps have the same length, 12, as the input
    that max pooling in groups of size 4 reduces to 32 vectors of length 3.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model outputs the weighted average plus the bias of the flattened and normalized
    single vector of length 96, and has 449 trainable parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The notebook wraps the model generation and subsequent compilation into a `get_model()`
    function that parametrizes the model configuration to facilitate experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: Model training and performance evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We train the model on five years of data for each ticker to predict the first
    month after this period and repeat this procedure 36 times using the `MultipleTimeSeriesCV`
    we developed in *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*.
    See the notebook for the training loop that follows the pattern demonstrated in
    the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use early stopping after five epochs to simplify the exposition, resulting
    in a positive bias so that the results have only illustrative character. Training
    length varies from 1 to 27 epochs, with a median of 5 epochs, which demonstrates
    that the model can often only learn very limited amounts of systematic information
    from the past returns. Thus cherry-picking the results yields a cumulative average
    information coefficient of around 4, as shown in *Figure 18.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.15: (Biased) out-of-sample information coefficients for best epochs'
  prefs: []
  type: TYPE_NORMAL
- en: We'll now proceed to a more complex example of using CNNs for multiple time-series
    data.
  prefs: []
  type: TYPE_NORMAL
- en: CNN-TA – clustering time series in 2D format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To exploit the grid-like structure of time-series data, we can use CNN architectures
    for univariate and multivariate time series. In the latter case, we consider different
    time series as channels, similar to the different color signals.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach converts a time series of alpha factors into a two-dimensional
    format to leverage the ability of CNNs to detect local patterns. Sezer and Ozbayoglu
    (2018) propose **CNN-TA**, which computes 15 technical indicators for different
    intervals and uses hierarchical clustering (see *Chapter 13*, *Data-Driven Risk
    Factors and Asset Allocation with Unsupervised Learning*) to locate indicators
    that behave similarly close to each other in a two-dimensional grid.
  prefs: []
  type: TYPE_NORMAL
- en: The authors train a CNN similar to the CIFAR-10 example we used earlier to predict
    whether to buy, hold, or sell an asset on a given day. They compare the CNN performance
    to "buy-and-hold" and other models and find that it outperforms all alternatives
    using daily price series for Dow 30 stocks and the nine most-traded ETFs over
    the 2007-2017 time period.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we experiment with this approach using daily US equity price
    data and demonstrate how to compute and convert a similar set of indicators into
    image format. Then we train a CNN to predict daily returns and evaluate a simple
    long-short strategy based on the resulting signals.
  prefs: []
  type: TYPE_NORMAL
- en: Creating technical indicators at different intervals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first select a universe of the 500 most-traded US stocks from the Quandl
    Wiki dataset by dollar volume for rolling five-year periods for 2007-2017\. See
    the notebook `engineer_cnn_features.ipynb` for the code examples in this section
    and some additional implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our features consist of 15 technical indicators and risk factors that we compute
    for 15 different intervals and then arrange them in a ![](img/B15439_18_024.png)
    grid. The following table lists some of the technical indicators; in addition,
    we follow the authors in using the following metrics (see the *Appendix* for additional
    information):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weighted and exponential moving averages** (**WMA** and **EMA**) of the close
    price'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rate of change** (**ROC**) of the close price'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chande Momentum Oscillator** (**CMO**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chaikin A/D Oscillators** (**ADOSC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average Directional Movement Index** (**ADX**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15439_18_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.16: Technical indicators'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each indicator, we vary the time period from 6 to 20 to obtain 15 distinct
    measurements. For example, the following code example computes the **relative
    strength index** (**RSI**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'For the **Normalized Average True Range** (**NATR**) that requires several
    inputs, the computation works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: See the TA-Lib documentation for further details.
  prefs: []
  type: TYPE_NORMAL
- en: Computing rolling factor betas for different horizons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We also use **five Fama-French risk factors** (**Fama** and French, 2015; see
    *Chapter 4*, *Financial Feature Engineering – How to Research Alpha Factors*).
    They reflect the sensitivity of a stock''s returns to factors consistently demonstrated
    to impact equity returns. We capture these factors by computing the coefficients
    of a rolling OLS regression of a stock''s daily returns on the returns of portfolios
    designed to reflect the underlying drivers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Equity risk premium**: Value-weighted returns of US stocks minus the 1-month
    US Treasury bill rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Size** (**SMB**): Returns of stocks categorized as **Small** (by market cap)
    **Minus** those of **Big equities**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value** (**HML**): Returns of stocks with **High** book-to-market value **Minus**
    those with a **Low value**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Investment** (**CMA**): Returns differences for companies with **Conservative**
    investment expenditures **Minus** those with **Aggressive spending**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Profitability** (**RMW**): Similarly, return differences for stocks with
    **Robust** profitability **Minus** that with a **Weak** metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We source the data from Kenneth French''s data library using `pandas_datareader`
    (see *Chapter 4*, *Financial Feature Engineering – How to Research Alpha Factors*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we apply statsmodels'' `RollingOLS()` to run regressions over windowed
    periods of different lengths, ranging from 15 to 90 days. We set the `params_only`
    parameter on the `.fit()` method to speed up computation and capture the coefficients
    using the `.params` attribute of the fitted `factor_model`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Features selecting based on mutual information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next step is to select the 15 most relevant features from the 20 candidates
    to fill the 15×15 input grid. The code examples for the following steps are in
    the notebook `convert_cnn_features_to_image_format`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we estimate the mutual information for each indicator and the
    15 intervals with respect to our target, the one-day forward returns. As discussed
    in *Chapter 4*, *Financial Feature Engineering – How to Research Alpha Factors*,
    scikit-learn provides the `mutual_info_regression()` function that makes this
    straightforward, albeit time-consuming and memory-intensive. To accelerate the
    process, we randomly sample 100,000 observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The left panel in *Figure 18.16* shows the mutual information, averaged across
    the 15 intervals for each indicator. NATR, PPO, and Bollinger Bands are most important
    from this metric''s perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.17: Mutual information and two-dimensional grid layout for time series'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical feature clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The right panel in *Figure 18.16* sketches the 15 X 15 two-dimensional feature
    grid that we will feed into our CNN. As discussed in the first section of this
    chapter, CNNs rely on the locality of relevant patterns that is typically found
    in images where nearby pixels are closely related and changes from one pixel to
    the next are often gradual.
  prefs: []
  type: TYPE_NORMAL
- en: To organize our indicators in a similar fashion, we will follow Sezer and Ozbayoglu's
    approach of applying hierarchical clustering. The goal is to identify features
    that behave similarly and order the columns and the rows of the grid accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can build on SciPy''s `pairwise_distance()`, `linkage()`, and `dendrogram()`
    functions that we introduced in *Chapter 13*, *Data-Driven Risk Factors and Asset
    Allocation with Unsupervised Learning* alongside other forms of clustering. We
    create a helper function that standardizes the input column-wise to avoid distorting
    distances among features due to differences in scale, and use the Ward criterion
    that merges clusters to minimize variance. The function returns the order of the
    leaf nodes in the dendrogram that in turn displays the successive formation of
    larger clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'To obtain the optimized order of technical indicators in the columns and the
    different intervals in the rows, we use NumPy''s `.reshape()` method to ensure
    that the dimension we would like to cluster appears in the columns of the two-dimensional
    array we pass to `cluster_features()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 18.18* shows the dendrograms for both the row and column features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.18: Dendrograms for row and column features'
  prefs: []
  type: TYPE_NORMAL
- en: We reorder the features accordingly and store the result as inputs for the CNN
    that we will create in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and training a convolutional neural network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we are ready to design, train, and evaluate a CNN following the steps outlined
    in the previous section. The notebook `cnn_for_trading.ipynb` contains the relevant
    code examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'We again closely follow the authors in creating a CNN with 2 convolutional
    layers with kernel size 3 and 16 and 32 filters, respectively, followed by a max
    pooling layer of size 2\. We flatten the output of the last stack of filters and
    connect the resulting 1,568 outputs to a dense layer of size 32, applying 25 and
    50 percent dropout probability to the incoming and outcoming connections to mitigate
    overfitting. The following table summarizes the CNN structure that contains 55,041
    trainable parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We cross-validate the model with the `MutipleTimeSeriesCV` train and validation
    set index generator introduced in *Chapter 7*, *Linear Models – From Risk Factors
    to Return Forecasts*. We provide 5 years of trading days during the training period
    in batches of 64 random samples and validate using the subsequent 3 months, covering
    the years 2014-2017\.
  prefs: []
  type: TYPE_NORMAL
- en: 'We scale the features to the range [-1, 1] and again use NumPy''s `.reshape()`
    method to create the requisite ![](img/Image78145.png) format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Training and validation follow the process laid out in *Chapter 17*, *Deep Learning
    for Trading*, relying on checkpointing to store weights after each epoch and generate
    predictions for the best-performing iterations without the need for costly retraining.
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the model''s predictive accuracy, we compute the daily **information
    coefficient** (**IC**) for the validation set like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We train the model for up to 10 epochs using **stochastic gradient descent**
    with **Nesterov** momentum (see *Chapter 17*, *Deep Learning for Trading*) and
    find that the best performing epochs, 8 and 9, achieve a (low) daily average IC
    of around 0.009.
  prefs: []
  type: TYPE_NORMAL
- en: Assembling the best models to generate tradeable signals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To reduce the variance of the test-period forecasts, we generate and average
    the predictions for the 3 models that perform best during cross-validation, which
    here correspond to training for 4, 8, and 9 epochs. As in the previous time-series
    example, the relatively short training period underscores that the amount of signals
    in financial time series is low compared to the systematic information contained
    in, for example, image data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `generate_predictions()` function reloads the model weights and returns
    the forecasts for the target period:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We store the predictions and proceed to backtest a trading strategy based on
    these daily return forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: Backtesting a long-short trading strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To get a sense of the signal quality, we compute the spread between equally
    weighted portfolios invested in stocks selected according to the signal quintiles
    using Alphalens (see *Chapter 4*, *Financial Feature Engineering – How to Research
    Alpha Factors*).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 18.19* shows that for a one-day investment horizon, this naive strategy
    would have earned a bit over four basis points per day during the 2013-2017 period:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.19: Alphalens signal quality evaluation'
  prefs: []
  type: TYPE_NORMAL
- en: We translate this slightly encouraging result into a simple strategy that enters
    long (short) positions for the 25 stocks with the highest (lowest) return forecasts,
    trading on a daily basis. *Figure 18.20* shows that this strategy is competitive
    with the S&P 500 benchmark over much of the backtesting period (left panel), resulting
    in a 35.6 percent cumulative return and a Sharpe ratio of 0.53 (before transaction
    costs; right panel)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_18_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.20: Backtest performance in- and out-of-sample'
  prefs: []
  type: TYPE_NORMAL
- en: Summary and lessons learned
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It appears that the CNN is able to extract meaningful information from the time
    series of alpha factors converted into a two-dimensional grid. Experimentation
    with different architectures and training parameters shows that the result is
    not very robust and slight modifications can yield significantly worse performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tuning attempts also surface the notorious difficulties in successfully training
    a deep NN, especially when the signal-to-noise ratio is low: too complex a network
    or the wrong optimizer can lead the CNN to a local optimum where it always predicts
    a constant value.'
  prefs: []
  type: TYPE_NORMAL
- en: The most important step to improve the results and obtain a performance closer
    to that achieved by the authors (using different outcomes) would be to revisit
    the features. There are many alternatives to different intervals of a limited
    set of technical indicators. Any appropriate number of time-series features could
    be arranged in a rectangular *n*×*m* format and benefit from the CNN's ability
    to learn local patterns. The choice of *n* indicators and *m* intervals just makes
    it easier to organize the rows and the columns of the two-dimensional grid. Give
    it a shot!
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the authors take a classification approach to the algorithmically
    labeled buy, hold, and sell outcomes (see the paper for an outline of the computation),
    whereas our experiment applied regression to the daily returns. The Alphalens
    chart in *Figure 18.18* suggests that longer holding periods (especially 10 days)
    might work better, so there is also scope for adjusting the strategy accordingly
    or switching to a classification approach.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced CNNs, a specialized NN architecture that has
    taken cues from our (limited) understanding of human vision and performs particularly
    well on grid-like data. We covered the central operation of convolution or cross-correlation
    that drives the discovery of filters that in turn detect features useful to solve
    the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: We reviewed several state-of-the-art architectures that are good starting points,
    especially because transfer learning enables us to reuse pretrained weights and
    reduce the otherwise rather computationally and data-intensive training effort.
    We also saw that Keras makes it relatively straightforward to implement and train
    a diverse set of deep CNN architectures.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we turn our attention to recurrent neural networks that
    are designed specifically for sequential data, such as time-series data, which
    is central to investment and trading.
  prefs: []
  type: TYPE_NORMAL
