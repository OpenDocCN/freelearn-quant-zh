["```py\nbase_estimator = DecisionTreeClassifier(criterion='gini', \n                                        splitter='best',\n                                        max_depth=1, \n                                        min_samples_split=2, \n                                        min_samples_leaf=20, \n                                        min_weight_fraction_leaf=0.0,\n                                        max_features=None, \n                                        random_state=None, \n                                        max_leaf_nodes=None, \n                                        min_impurity_decrease=0.0, \n                                        min_impurity_split=None)\n```", "```py\nada_clf = AdaBoostClassifier(base_estimator=base_estimator,\n                             n_estimators=200,\n                             learning_rate=1.0,\n                             algorithm='SAMME.R',\n                             random_state=42)\n```", "```py\ncv = OneStepTimeSeriesSplit(n_splits=12, test_period_length=1, shuffle=True)\ndef run_cv(clf, X=X_dummies, y=y, metrics=metrics, cv=cv, fit_params=None):\n    return cross_validate(estimator=clf,\n                          X=X,\n                          y=y,\n                          scoring=list(metrics.keys()),\n                          cv=cv,\n                          return_train_score=True,\n                          n_jobs=-1,                    # use all cores\n                          verbose=1,\n                          fit_params=fit_params)\n```", "```py\ngb_clf = GradientBoostingClassifier(loss='deviance',                # deviance = logistic reg; exponential: AdaBoost\n                                    learning_rate=0.1,              # shrinks the contribution of each tree\n                                    n_estimators=100,               # number of boosting stages\n                                    subsample=1.0,                  # fraction of samples used t fit base learners\n                                    criterion='friedman_mse',       # measures the quality of a split\n                                    min_samples_split=2,            \n                                    min_samples_leaf=1, \n                                    min_weight_fraction_leaf=0.0,   # min. fraction of sum of weights\n                                    max_depth=3,                    # opt value depends on interaction\n                                    min_impurity_decrease=0.0, \n                                    min_impurity_split=None, \n                                    max_features=None, \n                                    max_leaf_nodes=None, \n                                    warm_start=False, \n                                    presort='auto',\n                                    validation_fraction=0.1, \n                                    tol=0.0001)\n```", "```py\ngb_cv_result = run_cv(gb_clf, y=y_clean, X=X_dummies_clean)\ngb_result = stack_results(gb_cv_result)\n```", "```py\ncv = OneStepTimeSeriesSplit(n_splits=12)\n\nparam_grid = dict(\n        n_estimators=[100, 300],\n        learning_rate=[.01, .1, .2],\n        max_depth=list(range(3, 13, 3)),\n        subsample=[.8, 1],\n        min_samples_split=[10, 50],\n        min_impurity_decrease=[0, .01],\n        max_features=['sqrt', .8, 1]\n)\n```", "```py\ngs = GridSearchCV(gb_clf,\n                   param_grid,\n                   cv=cv,\n                   scoring='roc_auc',\n                   verbose=3,\n                   n_jobs=-1,\n                   return_train_score=True)\ngs.fit(X=X, y=y)\n\n# persist result using joblib for more efficient storage of large numpy arrays\njoblib.dump(gs, 'gbm_gridsearch.joblib')\n```", "```py\npd.Series(gridsearch_result.best_params_)\nlearning_rate              0.01\nmax_depth                  9.00\nmax_features               1.00\nmin_impurity_decrease      0.01\nmin_samples_split         10.00\nn_estimators             300.00\nsubsample                  0.80\n\ngridsearch_result.best_score_\n0.6853\n```", "```py\nbest_model = gridsearch_result.best_estimator_\npreds= best_model.predict(test_feature_data)\nroc_auc_score(y_true=test_target, y_score=preds)\n0.6622\n\n```", "```py\ncat_cols = ['year', 'month', 'age', 'msize', 'sector']\ndata = {}\nfor fold, (train_idx, test_idx) in enumerate(kfold.split(features)):\n    print(fold, end=' ', flush=True)\n    if model == 'xgboost':\n        data[fold] = {'train': xgb.DMatrix(label=target.iloc[train_idx],\n                                           data=features.iloc[train_idx],\n                                           nthread=-1),                  # use avail. threads\n                      'valid': xgb.DMatrix(label=target.iloc[test_idx],\n                                           data=features.iloc[test_idx],\n                                           nthread=-1)}\n    elif model == 'lightgbm':\n        train = lgb.Dataset(label=target.iloc[train_idx],\n                            data=features.iloc[train_idx],\n                            categorical_feature=cat_cols,\n                            free_raw_data=False)\n\n        # align validation set histograms with training set\n        valid = train.create_valid(label=target.iloc[test_idx],\n                                   data=features.iloc[test_idx])\n\n        data[fold] = {'train': train.construct(),\n                      'valid': valid.construct()}\n\n    elif model == 'catboost':\n        # get categorical feature indices\n        cat_cols_idx = [features.columns.get_loc(c) for c in cat_cols]\n        data[fold] = {'train': Pool(label=target.iloc[train_idx],\n                                    data=features.iloc[train_idx],\n                                    cat_features=cat_cols_idx),\n\n                      'valid': Pool(label=target.iloc[test_idx],\n                                    data=features.iloc[test_idx],\n                                    cat_features=cat_cols_idx)}\n```", "```py\nparam_grid = dict(\n        # common options\n        learning_rate=[.01, .1, .3],\n        colsample_bytree=[.8, 1],  # except catboost\n\n        # lightgbm\n        num_leaves=[2 ** i for i in range(9, 14)],\n        boosting=['gbdt', 'dart'],\n        min_gain_to_split=[0, 1, 5],  # not supported on GPU\n\nall_params = list(product(*param_grid.values()))\nn_models = len(all_params) # max number of models to cross-validate\nshuffle(all_params)\n```", "```py\nGBM = 'lightgbm'\nfor test_param in all_params:\n    cv_params = get_params(GBM)\n    cv_params.update(dict(zip(param_grid.keys(), test_param)))\n    if GBM == 'lightgbm':\n        cv_params['max_depth'] = int(ceil(np.log2(cv_params['num_leaves'])))\n    results[n] = run_cv(test_params=cv_params,\n                        data=datasets,\n                        n_splits=n_splits,\n                        gb_machine=GBM)\n```", "```py\ndef run_cv(test_params, data, n_splits=10):\n    \"\"\"Train-Validate with early stopping\"\"\"\n    result = []\n    cols = ['rounds', 'train', 'valid']\n    for fold in range(n_splits):\n        train = data[fold]['train']\n        valid = data[fold]['valid']\n\n        scores = {}\n        model = lgb.train(params=test_params,\n                          train_set=train,\n                          valid_sets=[train, valid],\n                          valid_names=['train', 'valid'],\n                          num_boost_round=250,\n                          early_stopping_rounds=25,\n                          verbose_eval=50,\n                          evals_result=scores)\n\n        result.append([model.current_iteration(),\n                       scores['train']['auc'][-1],\n                       scores['valid']['auc'][-1]])\n\n    return pd.DataFrame(result, columns=cols)\n```", "```py\nfig, axes = plot_partial_dependence(gbrt=gb_clf,\n                                    X=X_dummies_clean,\n                                    features=['month_9', 'return_1m', 'return_3m', ('return_1m', 'return_3m')],\n                                    feature_names=['month_9','return_1m', 'return_3m'],\n                                    percentiles=(0.01, 0.99),\n                                    n_jobs=-1,\n                                    n_cols=2,\n                                    grid_resolution=250)\n```", "```py\ntargets = ['return_1m', 'return_3m']\ntarget_feature = [X_dummies_clean.columns.get_loc(t) for t in targets]\npdp, axes = partial_dependence(gb_clf,\n                               target_feature,\n                               X=X_dummies_clean,\n                               grid_resolution=100)\n\nXX, YY = np.meshgrid(axes[0], axes[1])\nZ = pdp[0].reshape(list(map(np.size, axes))).T\n\nfig = plt.figure(figsize=(14, 8))\nax = Axes3D(fig)\nsurf = ax.plot_surface(XX, YY, Z,\n                       rstride=1,\n                       cstride=1,\n                       cmap=plt.cm.BuPu,\n                       edgecolor='k')\nax.set_xlabel(' '.join(targets[0].split('_')).capitalize())\nax.set_ylabel(' '.join(targets[1].split('_')).capitalize())\nax.set_zlabel('Partial Dependence')\nax.view_init(elev=22, azim=30)\n```", "```py\n# load JS visualization code to notebook\nshap.initjs()\n\n# explain the model's predictions using SHAP values\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values, X_test, show=False)\n```", "```py\nshap.force_plot(explainer.expected_value, shap_values[:1000,:], X_test.iloc[:1000])\n```", "```py\nshap.dependence_plot(\"return_1m\", shap_values, X_test, interaction_index=2, title='Interaction between 1- and 3-Month Returns')\n```"]