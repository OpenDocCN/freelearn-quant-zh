- en: '22'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '22'
- en: Deep Reinforcement Learning – Building a Trading Agent
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习 - 构建交易代理
- en: In this chapter, we'll introduce **reinforcement learning** (**RL**), which
    takes a different approach to **machine learning** (**ML**) than the supervised
    and unsupervised algorithms we have covered so far. RL has attracted enormous
    attention as it has been the main driver behind some of the most exciting AI breakthroughs,
    like AlphaGo. David Silver, AlphaGo's creator and the lead RL researcher at Google-owned
    DeepMind, recently won the prestigious 2019 ACM Prize in Computing "for breakthrough
    advances in computer game-playing." We will see that the interactive and online
    nature of RL makes it particularly well-suited to the trading and investment domain.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍**强化学习**（**RL**），它与我们迄今为止涵盖的监督和无监督算法的**机器学习**（**ML**）方法不同。RL 吸引了极大的关注，因为它是一些最令人兴奋的人工智能突破的主要推动力，比如
    AlphaGo。AlphaGo 的创造者、Google 拥有的 DeepMind 的首席 RL 研究员 David Silver 最近荣获了 2019 年的重要
    ACM 计算奖，以表彰其在计算机游戏中取得的突破性进展。我们将看到，RL 的交互式和在线特性使其特别适合于交易和投资领域。
- en: RL models **goal-directed learning by an agent** that interacts with a typically
    stochastic environment that the agent has incomplete information about. RL aims
    to automate how the agent makes decisions to achieve a long-term objective by
    learning the value of states and actions from a reward signal. The ultimate goal
    is to derive a policy that encodes behavioral rules and maps states to actions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: RL 模型通过与通常具有不完全信息的随机环境交互的代理进行**目标导向学习**。RL 的目标是通过从奖励信号中学习状态和动作的价值来自动化代理如何做出决策以实现长期目标。最终目标是推导出一个策略，该策略编码了行为规则并将状态映射到动作。
- en: RL is considered **most similar to human learning** that results from taking
    actions in the real world and observing the consequences. It differs from supervised
    learning because it optimizes the agent's behavior one trial-and-error experience
    at a time based on a scalar reward signal, rather than by generalizing from correctly
    labeled, representative samples of the target concept. Moreover, RL does not stop
    at making predictions. Instead, it takes an end-to-end perspective on goal-oriented
    decision-making by including actions and their consequences.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: RL 被认为是**最接近人类学习**的方法，它是通过在现实世界中采取行动并观察后果而产生的。它与监督学习不同，因为它根据一个标量奖励信号一次优化代理的行为体验，而不是通过从正确标记的、代表性的目标概念样本中泛化。此外，RL
    不仅仅停留在做出预测。相反，它采用了端到端的目标导向决策视角，包括动作及其后果。
- en: In this chapter, you will learn how to formulate an RL problem and apply various
    solution methods. We will cover model-based and model-free methods, introduce
    the OpenAI Gym environment, and combine deep learning with RL to train an agent
    that navigates a complex environment. Finally, we'll show you how to adapt RL
    to algorithmic trading by modeling an agent that interacts with the financial
    market to optimize its profit objective.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何制定 RL 问题并应用各种解决方法。我们将涵盖基于模型和无模型方法，介绍 OpenAI Gym 环境，并将深度学习与 RL 结合起来，训练一个在复杂环境中导航的代理。最后，我们将向您展示如何通过建模与金融市场互动的代理来调整
    RL，以优化其利润目标。
- en: 'More specifically, after reading this chapter, you will be able to:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在阅读本章后，您将能够：
- en: Define a **Markov decision problem** (**MDP**)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义**马尔可夫决策问题**（**MDP**）
- en: Use value and policy iteration to solve an MDP
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用值迭代和策略迭代来解决 MDP
- en: Apply Q-learning in an environment with discrete states and actions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在具有离散状态和动作的环境中应用 Q 学习
- en: Build and train a deep Q-learning agent in a continuous environment
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在连续环境中构建和训练一个深度 Q 学习代理
- en: Use OpenAI Gym to train an RL trading agent
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 OpenAI Gym 训练 RL 交易代理
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 GitHub 仓库的相应目录中找到本章的代码示例和额外资源的链接。笔记本包括图像的彩色版本。
- en: Elements of a reinforcement learning system
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习系统的要素
- en: 'RL problems feature several elements that set them apart from the ML settings
    we have covered so far. The following two sections outline the key features required
    for defining and solving an RL problem by learning a policy that automates decisions.
    We''ll use the notation and generally follow *Reinforcement Learning: An Introduction*
    (Sutton and Barto 2018) and David Silver''s UCL Courses on RL ([https://www.davidsilver.uk/teaching/](https://www.davidsilver.uk/teaching/)),
    which are recommended for further study beyond the brief summary that the scope
    of this chapter permits.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题具有几个元素，使它们与我们迄今为止涵盖的机器学习设置有所不同。以下两个部分概述了定义和解决强化学习问题所需的关键特征，通过学习一个自动化决策的策略。我们将使用符号和通常遵循*强化学习：导论*（Sutton
    和 Barto 2018）以及 David Silver 的 UCL 强化学习课程 ([https://www.davidsilver.uk/teaching/](https://www.davidsilver.uk/teaching/))，这些都是进一步研究的推荐材料，超出了本章范围的简要概述。
- en: RL problems aim to solve for actions that **optimize the agent's objective,
    given some observations about the environment**. The environment presents information
    about its state to the agent, assigns rewards for actions, and transitions the
    agent to new states, subject to probability distributions the agent may or may
    not know. It may be fully or partially observable, and it may also contain other
    agents. The structure of the environment has a strong impact on the agent's ability
    to learn a given task, and typically requires significant up-front design effort
    to facilitate the training process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题旨在解决**优化代理目标的行动**，考虑到对环境的一些观察。环境向代理提供其状态的信息，为动作分配奖励，并使代理转移到新状态，受到代理可能知道的概率分布的约束。它可能是完全或部分可观察的，还可能包含其他代理。环境的结构对代理学习给定任务的能力有着很大的影响，通常需要大量的前期设计工作来促进训练过程。
- en: RL problems differ based on the complexity of the environment's state and agent's
    action spaces, which can be either discrete or continuous. Continuous actions
    and states, unless discretized, require machine learning to approximate a functional
    relationship between states, actions, and their values. They also require generalization
    because the agent almost certainly experiences only a subset of the potentially
    infinite number of states and actions during training.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题基于环境状态和代理动作空间的复杂性而有所不同，可以是离散的或连续的。连续的动作和状态，除非离散化，否则需要机器学习来近似状态、动作和其价值之间的函数关系。它们还需要泛化，因为代理在训练期间几乎肯定只经历了潜在无限数量的状态和动作的子集。
- en: 'Solving complex decision problems usually requires a simplified model that
    isolates the key aspects. *Figure 22.1* highlights the **salient features of an
    RL problem**. These typically include:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 解决复杂的决策问题通常需要一个简化的模型，将关键方面隔离出来。*图22.1*突显了**强化学习问题的显著特征**。这些特征通常包括：
- en: Observations by the agent on the state of the environment
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理对环境状态的观察
- en: A set of actions available to the agent
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组可供代理选择的动作
- en: A policy that governs the agent's decisions
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理代理决策的策略
- en: '![](img/B15439_22_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图22.1](img/B15439_22_01.png)'
- en: 'Figure 22.1: Components of an RL system'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.1：强化学习系统的组成部分
- en: In addition, the environment emits a **reward signal** (that may be negative)
    as the agent's action leads to a transition to a new state. At its core, the agent
    usually learns a **value function** that informs its judgment of the available
    actions. The agent's objective function processes the reward signal and translates
    the value judgments into an optimal policy.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，环境会发出**奖励信号**（可能是负数），因为代理的动作导致了状态转换到一个新状态。在其核心，代理通常学习一个**价值函数**，它指导其对可用动作的判断。代理的目标函数处理奖励信号，并将价值判断转化为最优策略。
- en: The policy – translating states into actions
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 - 将状态转换为动作
- en: At any point in time, **the policy defines the agent's behavior**. It maps any
    state the agent may encounter to one or several actions. In an environment with
    a limited number of states and actions, the policy can be a simple lookup table
    that's filled in during training.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何时刻，**策略定义了代理的行为**。它将代理可能遇到的任何状态映射到一个或多个动作。在具有有限状态和动作数量的环境中，策略可以是一个简单的查找表，在训练期间填充。
- en: With continuous states and actions, the policy takes the form of a function
    that machine learning can help to approximate. The policy may also involve significant
    computation, as in the case of AlphaZero, which uses tree search to decide on
    the best action for a given game state. The policy may also be stochastic and
    assign probabilities to actions, given a state.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 借助连续状态和动作，策略采用机器学习可以帮助近似的函数形式。策略也可能涉及大量计算，例如 AlphaZero，它使用树搜索来决定给定游戏状态的最佳动作。策略也可能是随机的，并给定一个状态，为动作分配概率。
- en: Rewards – learning from actions
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励——从动作中学习
- en: The reward signal is a single value that the environment sends to the agent
    at each time step. The agent's objective is typically to **maximize the total
    reward received over time**. Rewards can also be a stochastic function of the
    state and the actions. They are typically discounted to facilitate convergence
    and reflect the time decay of value.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励信号是环境在每个时间步发送给代理程序的单个值。代理的目标通常是**最大化随时间接收到的总奖励**。奖励也可以是状态和动作的随机函数。它们通常被折扣以促进收敛并反映价值的时间衰减。
- en: Rewards are the **only way for the agent to learn** about the value of its decisions
    in a given state and to modify the policy accordingly. Due to its critical impact
    on the agent's learning, the reward signal is often the most challenging part
    of designing an RL system.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励是代理程序了解其在给定状态下决策价值的**唯一途径**，并相应调整策略。由于奖励信号对代理程序学习的重要影响，因此奖励信号通常是设计强化学习系统中最具挑战性的部分。
- en: Rewards need to clearly communicate what the agent should accomplish (as opposed
    to how it should do so) and may require domain knowledge to properly encode this
    information. For example, the development of a trading agent may need to define
    rewards for buy, hold, and sell decisions. These may be limited to profit and
    loss, but also may need to include volatility and risk considerations, such as
    drawdown.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励需要清楚地传达代理应该完成的任务（而不是如何完成），可能需要领域知识来正确编码这些信息。例如，交易代理的开发可能需要为买入、持有和卖出决策定义奖励。这些可能仅限于利润和损失，但也可能需要包括波动率和风险考虑，例如回撤。
- en: The value function – optimal choice for the long run
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 价值函数——长期选择的最优解
- en: 'The reward provides immediate feedback on actions. However, solving an RL problem
    requires decisions that create value in the long run. This is where the value
    function comes in: it summarizes the utility of states or of actions in a given
    state in terms of their long-term reward.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励提供了对动作的即时反馈。然而，解决强化学习问题需要能够从长远角度创造价值的决策。这就是价值函数的作用所在：它总结了状态或给定状态下动作的效用，以长期奖励的形式。
- en: In other words, the value of a state is the total reward an agent can expect
    to obtain in the future when starting in that state. The immediate reward may
    be a good proxy of future rewards, but the agent also needs to account for cases
    where low rewards are followed by much better outcomes that are likely to follow
    (or the reverse).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，状态的价值是代理程序在未来在该状态下可以期望获得的总奖励。即时奖励可能是未来奖励的良好代理，但代理还需要考虑到低奖励后面很可能出现的更好结果（或者相反）。
- en: Hence, **value estimates aim to predict future rewards**. Rewards are the key
    inputs, and the goal of making value estimates is to achieve more rewards. However,
    RL methods focus on learning accurate values that enable good decisions while
    efficiently leveraging the (often limited) experience.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**值估计旨在预测未来的奖励**。奖励是关键输入，而进行值估计的目标是获得更多的奖励。然而，强化学习方法专注于学习准确的值，以便在有效利用（通常有限的）经验的同时做出良好的决策。
- en: There are also RL approaches that do not rely on value functions, such as randomized
    optimization methods like genetic algorithms or simulated annealing, which aim
    to find optimal behaviors by efficiently exploring the policy space. The current
    interest in RL, however, is mostly driven by methods that directly or indirectly
    estimate the value of states and actions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些强化学习方法不依赖于值函数，例如基因算法或模拟退火等随机优化方法，这些方法旨在通过有效地探索策略空间来找到最佳行为。然而，目前对强化学习的兴趣主要受到直接或间接估计状态和动作价值的方法驱动。
- en: '**Policy gradient methods** are a new development that relies on a parameterized,
    differentiable policy that can be directly optimized with respect to the objective
    using gradient descent (Sutton et al. 2000). See the resources on GitHub that
    include abstracts of key papers and algorithms beyond the scope of this chapter.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: With or without a model – look before you leap?
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Model-based RL** approaches learn a model of the environment to allow the
    agent to plan ahead by predicting the consequences of its actions. Such a model
    may be used, for example, to predict the next state and reward based on the current
    state and action. This is the **basis for planning**, that is, deciding on the
    best course of action by considering possible futures before they materialize.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Simpler **model-free methods**, in contrast, learn from **trial and error**.
    Modern RL methods span the gamut from low-level trial-and-error methods to high-level,
    deliberative planning. The right approach depends on the complexity and learnability
    of the environment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: How to solve reinforcement learning problems
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RL methods aim to learn from experience how to take actions that achieve a long-term
    goal. To this end, the agent and the environment interact over a sequence of discrete
    time steps via the interface of actions, state observations, and rewards described
    in the previous section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Key challenges in solving RL problems
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Solving RL problems requires addressing two unique challenges: the credit-assignment
    problem and the exploration-exploitation trade-off.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Credit assignment
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In RL, reward signals can occur significantly later than actions that contributed
    to the result, complicating the association of actions with their consequences.
    For example, when an agent takes 100 different positions and trades repeatedly,
    how does it realize that certain holdings performed much better than others if
    it only learns about the portfolio return?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: The **credit-assignment problem** is the challenge of accurately estimating
    the benefits and costs of actions in a given state, despite these delays. RL algorithms
    need to find a way to distribute the credit for positive and negative outcomes
    among the many decisions that may have been involved in producing it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Exploration versus exploitation
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dynamic and interactive nature of RL implies that the agent needs to estimate
    the value of the states and actions before it has experienced all relevant trajectories.
    While it is able to select an action at any stage, these decisions are based on
    incomplete learning, yet generate the agent's first insights into the optimal
    choices of its behavior.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Partial visibility into the value of actions creates the risk of decisions that
    only exploit past (successful) experience rather than exploring uncharted territory.
    Such choices limit the agent's exposure and prevent it from learning an optimal
    policy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: An RL algorithm needs to balance this **exploration-exploitation trade-off**—too
    little exploration will likely produce biased value estimates and suboptimal policies,
    whereas too little exploitation prevents learning from taking place in the first
    place.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: RL算法需要平衡这种探索-利用的权衡——太少的探索可能会产生偏见的值估计和次优策略，而太少的利用则会阻止学习的发生。
- en: Fundamental approaches to solving RL problems
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决强化学习问题的基本方法
- en: 'There are numerous approaches to solving RL problems, all of which involve
    finding rules for the agent''s optimal behavior:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 解决RL问题有许多方法，所有这些方法都涉及找到代理的最优行为规则：
- en: '**Dynamic programming** (**DP**) methods make the often unrealistic assumption
    of complete knowledge of the environment, but they are the conceptual foundation
    for most other approaches.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DP方法做出了完全了解环境的常常不切实际的假设，但它们是大多数其他方法的概念基础。
- en: '**Monte Carlo** (**MC**) methods learn about the environment and the costs
    and benefits of different decisions by sampling entire state-action-reward sequences.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡洛（MC）方法通过对整个状态-动作-奖励序列进行采样来学习环境以及不同决策的成本和收益。
- en: '**Temporal difference** (**TD**) learning significantly improves sample efficiency
    by learning from shorter sequences. To this end, it relies on **bootstrapping**,
    which is defined as refining its estimates based on its own prior estimates.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD学习通过从更短的序列中学习显著提高了样本效率。为此，它依赖于引导，即根据其自身的先前估计来优化其估计。
- en: When an RL problem includes well-defined transition probabilities and a limited
    number of states and actions, it can be framed as a finite **Markov decision process**
    (**MDP**) for which DP can compute an exact solution. Much of the current RL theory
    focuses on finite MDPs, but practical applications are used for (and require)
    more general settings. Unknown transition probabilities require efficient sampling
    to learn about their distribution.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个RL问题包括明确定义的转移概率以及有限数量的状态和动作时，它可以被构建为一个有限MDP，对于这个MDP，DP可以计算出一个精确的解。当前RL理论的大部分关注点都集中在有限MDP上，但实际应用需要更一般的设置。未知的转移概率需要高效的采样来学习它们的分布。
- en: 'Approaches to continuous state and/or action spaces often leverage **machine
    learning** to approximate a value or policy function. They integrate supervised
    learning and, in particular, deep learning methods like those discussed in the
    previous four chapters. However, these methods face **distinct challenges** in
    the RL context:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续状态和/或动作空间的方法通常利用机器学习来近似值函数或策略函数。它们集成了监督学习，特别是前四章讨论的深度学习方法。然而，在RL环境中，这些方法面临着明显的挑战：
- en: The **reward signal** does not directly reflect the target concept, like a labeled
    training sample.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励信号不直接反映目标概念，就像标记的训练样本一样。
- en: The **distribution of the observations** depends on the agent's actions and
    the policy, which is itself the subject of the learning process.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察的分布取决于代理的动作和策略，策略本身是学习过程的主题。
- en: The following sections will introduce and demonstrate various solution methods.
    We'll start with the DP methods value iteration and policy iteration, which are
    limited to finite MDP with known transition probabilities. As we will see in the
    following section, they are the foundation for Q-learning, which is based on TD
    learning and does not require information about transition probabilities. It aims
    for similar outcomes as DP but with less computation and without assuming a perfect
    model of the environment. Finally, we'll expand the scope to continuous states
    and introduce deep Q-learning.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节将介绍和演示各种解决方法。我们将从值迭代和策略迭代的DP方法开始，这些方法仅限于已知转移概率的有限MDP。正如我们将在接下来的部分看到的那样，它们是Q-learning的基础，Q-learning基于TD学习，并且不需要关于转移概率的信息。它的目标与DP类似，但计算量较少，而且不需要假设环境的完美模型。最后，我们将扩展范围到连续状态，并介绍深度Q-learning。
- en: Solving dynamic programming problems
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决动态规划问题
- en: Finite MDPs are a simple yet fundamental framework. We will introduce the trajectories
    of rewards that the agent aims to optimize, define the policy and value functions
    used to formulate the optimization problem, and the Bellman equations that form
    the basis for the solution methods.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有限MDP是一个简单但基本的框架。我们将介绍代理人试图优化的奖励轨迹，定义用于制定优化问题的策略和值函数，以及构成解决方法基础的贝尔曼方程。
- en: Finite Markov decision problems
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有限马尔可夫决策问题
- en: MDPs frame the agent-environment interaction as a sequential decision problem
    over a series of time steps *t* =1, …, *T* that constitute an episode. Time steps
    are assumed as discrete, but the framework can be extended to continuous time.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The abstraction afforded by MDPs makes its application easily adaptable to many
    contexts. The time steps can be at arbitrary intervals, and actions and states
    can take any form that can be expressed numerically.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: The Markov property implies that the current state completely describes the
    process, that is, the process has no memory. Information from past states adds
    no value when trying to predict the process's future. Due to these properties,
    the framework has been used to model asset prices subject to the efficient market
    hypothesis discussed in *Chapter 5*, *Portfolio Optimization and Performance Evaluation*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Sequences of states, actions, and rewards
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MDPs proceed in the following fashion: at each step *t*, the agent observes
    the environment''s state ![](img/B15439_22_001.png) and selects an action ![](img/B15439_22_002.png),
    where *S* and *A* are the sets of states and actions, respectively. At the next
    time step *t+1*, the agent receives a reward ![](img/B15439_22_003.png) and transitions
    to state *S*[t][+1]. Over time, the MDP gives rise to a trajectory *S*[0], *A*[0],
    *R*[1], *S*[1], *A*[1], *R*[1], … that continues until the agent reaches a terminal
    state and the episode ends.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Finite MDPs with a limited number of actions *A*, states *S*, and rewards *R*
    include well-defined discrete probability distributions over these elements. Due
    to the Markov property, these distributions only depend on the previous state
    and action.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'The probabilistic nature of trajectories implies that the agent maximizes the
    expected sum of future rewards. Furthermore, rewards are typically discounted
    using a factor ![](img/B15439_22_004.png) to reflect their time value. In the
    case of tasks that are not episodic but continue indefinitely, a discount factor
    strictly less than 1 is necessary to avoid infinite rewards and ensure convergence.
    Therefore, the agent maximizes the discounted, expected sum of future returns
    *R*[t], denoted as *G*[t]:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_22_005.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: 'This relationship can also be defined recursively because the sum starting
    at the second step is the same as *G*[t][+1] discounted once:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_22_006.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: We will see later that this type of recursive relationship is frequently used
    to formulate RL algorithms.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Value functions – how to estimate the long-run reward
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As introduced previously, a policy ![](img/B15439_22_009.png) maps all states
    to probability distributions over actions so that the probability of choosing
    action *A*[t] in state *S*[t] can be expressed as ![](img/B15439_22_007.png).
    The value function estimates the long-run return for each state or state-action
    pair. It is fundamental to find the policy that is the optimal mapping of states
    to actions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'The state-value function ![](img/B15439_22_008.png) for policy ![](img/B15439_22_009.png)
    gives the long-term value *v* of a specific state *s* as the expected return *G*
    for an agent that starts in *s* and then always follows policy ![](img/B15439_22_009.png).
    It is defined as follows, where ![](img/B15439_22_011.png) refers to the expected
    value when the agent follows policy ![](img/B15439_22_009.png):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于策略![](img/B15439_22_009.png)，状态值函数![](img/B15439_22_008.png)给出了特定状态*s*的长期价值*v*，作为代理从*s*开始然后始终遵循策略![](img/B15439_22_009.png)的预期回报*G*。它的定义如下，其中![](img/B15439_22_011.png)是指当代理遵循策略![](img/B15439_22_009.png)时的预期值：
- en: '![](img/B15439_22_013.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_013.png)'
- en: 'Similarly, we can compute the **state-action value function** *q*(*s*,*a*)
    as the expected return of starting in state *s*, taking action, and then always
    following the policy ![](img/B15439_22_009.png):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以计算**状态动作值函数** *q*(*s*,*a*)，作为在状态*s*开始，采取行动，然后始终遵循策略![](img/B15439_22_009.png)的预期回报：
- en: '![](img/B15439_22_014.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_014.png)'
- en: The Bellman equations
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: 'The Bellman equations define a recursive relationship between the value functions
    for all states *s* in *S* and any of their successor states *s′* under a policy
    ![](img/B15439_22_009.png). They do so by decomposing the value function into
    the immediate reward and the discounted value of the next state:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程定义了所有状态*s*在*S*中的值函数与任何其后继状态*s′*之间的递归关系，其遵循策略![](img/B15439_22_009.png)。它们通过将值函数分解为即时奖励和下一状态的折现值来实现这一点：
- en: '![](img/B15439_22_016.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_016.png)'
- en: This equation says that for a given policy, the value of a state must equal
    the expected value of its successor states under the policy, plus the expected
    reward earned from arriving at that successor state.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程表示，对于给定策略，一个状态的值必须等于其在该策略下的后继状态的预期值，加上到达该后继状态时所获得的预期奖励。
- en: This implies that, if we know the values of the successor states for the currently
    available actions, we can look ahead one step and compute the expected value of
    the current state. Since it holds for all states *S*, the expression defines a
    set of ![](img/B15439_22_017.png) equations. An analogous relationship holds for
    ![](img/B15439_22_018.png).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果我们知道当前可用操作的后继状态的值，我们可以向前看一步，计算当前状态的预期值。由于它对所有状态*S*都成立，该表达式定义了一组![](img/B15439_22_017.png)方程。对于![](img/B15439_22_018.png)，也存在类似的关系。
- en: '*Figure 22.2* summarizes this recursive relationship: in the current state,
    the agent selects an action *a* based on the policy ![](img/B15439_22_009.png).
    The environment responds by assigning a reward that depends on the resulting new
    state *s′*:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*图22.2*总结了这种递归关系：在当前状态下，代理根据策略![](img/B15439_22_009.png)选择一个动作*a*。环境通过分配一个取决于结果新状态*s′*的奖励来做出响应：'
- en: '![](img/B15439_22_02.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_02.png)'
- en: 'Figure 22.2: The recursive relationship expressed by the Bellman equation'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.2：贝尔曼方程表达的递归关系
- en: From a value function to an optimal policy
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从值函数到最优策略
- en: 'The solution to an RL problem is a policy that optimizes the cumulative reward.
    Policies and value functions are closely connected: an optimal policy yields a
    value estimate for each state ![](img/B15439_22_020.png) or state-action pair
    ![](img/B15439_22_021.png) that is at least as high as for any other policy since
    the value is the cumulative reward under the given policy. Hence, the optimal
    value functions ![](img/B15439_22_022.png) and ![](img/B15439_22_023.png) implicitly
    define optimal policies and solve the MDP.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题的解是一个优化累积奖励的策略。策略和值函数紧密相关：一个最优策略为每个状态![](img/B15439_22_020.png)或状态-动作对![](img/B15439_22_021.png)提供的值估计至少与任何其他策略的值相同，因为该值是给定策略下的累积奖励。因此，最优值函数![](img/B15439_22_022.png)和![](img/B15439_22_023.png)隐式定义了最优策略并解决了MDP。
- en: 'The optimal value functions ![](img/B15439_22_024.png) and ![](img/B15439_22_025.png)
    also satisfy the Bellman equations from the previous section. These Bellman optimality
    equations can omit the explicit reference to a policy as it is implied by ![](img/B15439_22_024.png)
    and ![](img/B15439_22_025.png). For ![](img/B15439_22_028.png), the recursive
    relationship equates the current value to the sum of the immediate reward from
    choosing the best action in the current state, as well as the expected discounted
    value of the successor states:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最优值函数 ![](img/B15439_22_024.png) 和 ![](img/B15439_22_025.png) 也满足前一节中的贝尔曼方程。这些贝尔曼最优方程可以省略对策略的显式引用，因为它被
    ![](img/B15439_22_024.png) 和 ![](img/B15439_22_025.png) 隐含。对于 ![](img/B15439_22_028.png)，递归关系将当前值等同于选择当前状态中最佳动作的即时奖励之和，以及后继状态的期望折现值：
- en: '![](img/B15439_22_029.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_029.png)'
- en: 'For the optimal state-action value function ![](img/B15439_22_030.png), the
    Bellman optimality equation decomposes the current state-action value into the
    sum of the reward for the implied current action and the discounted expected value
    of the best action in all successor states:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最佳状态-动作值函数 ![](img/B15439_22_030.png)，贝尔曼最优方程将当前状态-动作值分解为隐含当前动作的奖励与所有后继状态中最佳动作的期望值的折现期望值之和：
- en: '![](img/B15439_22_031.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_031.png)'
- en: The optimality conditions imply that the best policy is to always select the
    action that maximizes the expected value in a greedy fashion, that is, to only
    consider the result of a single time step.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最优性条件暗示了最佳策略是始终选择最大化贪婪方式中的期望值的动作，即仅考虑单个时间步骤的结果。
- en: The optimality conditions defined by the two previous expressions are nonlinear
    due to the max operator and lack a closed-form solution. Instead, MDP solutions
    rely on an iterative solution - like policy and value iteration or Q-learning,
    which we will cover next.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由前两个表达式定义的最优性条件由于 max 操作符是非线性的，缺乏封闭形式的解。相反，MDP 解决方案依赖于迭代解法 - 如策略和值迭代或 Q-learning，我们将在下一节中介绍。
- en: Policy iteration
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略迭代
- en: DP is a general method for solving problems that can be decomposed into smaller,
    overlapping subproblems with a recursive structure that permit the reuse of intermediate
    results. MDPs fit the bill due to the recursive Bellman optimality equations and
    the cumulative nature of the value function. More specifically, the **principle
    of optimality** applies because an optimal policy consists of picking an optimal
    action and then following an optimal policy.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DP 是一种解决可以分解为具有递归结构并允许重复使用中间结果的较小、重叠子问题的一般方法。由于递归贝尔曼最优方程和值函数的累积特性，MDP 符合这一要求。更具体地说，**最优性原理**适用于最优策略，因为最优策略包括选择最优动作然后遵循最优策略。
- en: DP requires knowledge of the MDP's transition probabilities. This is often not
    the case, but many methods for more general cases follow an approach similar to
    DP and learn the missing information from the data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: DP 需要了解 MDP 的转移概率。通常情况下并非如此，但许多更一般情况下的方法都采用类似于 DP 的方法，并从数据中学习缺失的信息。
- en: DP is useful for **prediction tasks** that estimate the value function and the
    control task that focuses on optimal decisions and outputs a policy (while also
    estimating a value function in the process).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: DP 对于估计值函数的**预测任务**和专注于最佳决策并输出策略的控制任务非常有用（在此过程中也估计值函数）。
- en: 'The policy iteration algorithm to find an optimal policy repeats the following
    two steps until the policy has converged, that is, no longer changes more than
    a given threshold:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最优策略的策略迭代算法重复以下两个步骤，直到策略收敛，即不再发生变化超过给定阈值：
- en: '**Policy evaluation**: Update the value function based on the current policy.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**策略评估**：根据当前策略更新值函数。'
- en: '**Policy improvement**: Update the policy so that actions maximize the expected
    one-step value.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**策略改进**：更新策略，使动作最大化期望的一步值。'
- en: Policy evaluation relies on the Bellman equation to estimate the value function.
    More specifically, it selects the action determined by the current policy and
    sums the resulting reward, as well as the discounted value of the next state,
    to update the value for the current state.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 策略评估依赖于贝尔曼方程来估计值函数。更具体地说，它选择由当前策略确定的动作，并将导致的奖励以及下一个状态的折现值相加，以更新当前状态的值。
- en: Policy improvement, in turn, alters the policy so that for each state, the policy
    produces the action that produces the highest value in the next state. This improvement
    is called greedy because it only considers the return of a single time step. Policy
    iteration always converges to an optimal policy and often does so in relatively
    few iterations.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Value iteration
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Policy iteration requires the evaluation of the policy for all states after
    each iteration. The evaluation can be costly, as discussed previously, for search-tree-based
    policies, for example.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '**Value iteration** simplifies this process by collapsing the policy evaluation
    and improvement step. At each time step, it iterates over all states and selects
    the best greedy action based on the current value estimate for the next state.
    Then, it uses the one-step lookahead implied by the Bellman optimality equation
    to update the value function for the current state.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'The corresponding update rule for the value function ![](img/B15439_22_032.png)
    is almost identical to the policy evaluation update; it just adds the maximization
    over the available actions:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_22_033.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: The algorithm stops when the value function has converged and outputs the greedy
    policy derived from its value function estimate. It is also guaranteed to converge
    to an optimal policy.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Generalized policy iteration
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In practice, there are several ways to truncate policy iteration; for example,
    by evaluating the policy *k* times before improving it. This just means that the
    *max* operator will only be applied at every *k*^(th) iteration.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'Most RL algorithms estimate value and policy functions and rely on the interaction
    of policy evaluation and improvement to converge to a solution, as illustrated
    in *Figure 22.3*. The general approach improves the policy with respect to the
    value function while adjusting the value function so that it matches the policy:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_22_03.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.3: Convergence of policy evaluation and improvement'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Convergence requires that the value function be consistent with the policy,
    which, in turn, needs to stabilize while acting greedily with respect to the value
    function. Thus, both processes stabilize only when a policy has been found that
    is greedy with respect to its own evaluation function. This implies that the Bellman
    optimality equation holds, and thus that the policy and the value function are
    optimal.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming in Python
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we''ll apply value and policy iteration to a toy environment
    that consists of a ![](img/B15439_22_034.png) grid, as depicted in *Figure 22.4*,
    with the following features:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '**States**: 11 states represented as two-dimensional coordinates. One field
    is not accessible and the top two states in the right-most column are terminal,
    that is, they end the episode.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions**: Movements of one step up, down, left, or right. The environment
    is randomized so that actions can have unintended outcomes. For each action, there
    is an 80 percent probability of moving to the expected state, and 10 percent each
    of moving in an adjacent direction (for example, right or left instead of up,
    or up/down instead of right).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：向上、向下、向左或向右移动一步。环境是随机的，因此动作可能会产生意外的结果。对于每个动作，有80%的概率移动到预期状态，并且有10%的概率移动到相邻方向（例如，向右或向左而不是向上，或者向上/向下而不是向右）。'
- en: '**Rewards**: As depicted in the left panel, each state results in -.02 except
    the +1/-1 rewards in the terminal states.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：如左图所示，除了终止状态的+1/-1奖励外，每个状态都会产生-.02的奖励。'
- en: '![](img/B15439_22_04.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_04.png)'
- en: 'Figure 22.4: 3×4 gridworld rewards, value function, and optimal policy'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.4：3×4网格世界奖励，值函数和最优策略
- en: Setting up the gridworld
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置网格世界
- en: 'We will begin by defining the environment parameters:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始定义环境参数：
- en: '[PRE0]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will frequently need to convert between 1D and 2D representations, so we
    will define two helper functions for this purpose; states are one-dimensional,
    and cells are the corresponding 2D coordinates:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要在1D和2D表示之间进行转换，因此我们将为此定义两个辅助函数；状态是一维的，而单元格是相应的2D坐标：
- en: '[PRE1]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Furthermore, we will precompute some data points to make the code more concise:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将预先计算一些数据点以使代码更简洁：
- en: '[PRE2]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We store the rewards for each state:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们存储每个状态的奖励：
- en: '[PRE3]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To account for the probabilistic environment, we also need to compute the probability
    distribution over the actual move for a given action:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考虑到概率环境，我们还需要计算给定动作的实际移动的概率分布：
- en: '[PRE4]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, we are ready to compute the transition matrix, which is the key input to
    the MDP.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备计算转移矩阵，这是MDP的关键输入。
- en: Computing the transition matrix
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算转移矩阵
- en: The **transition matrix** defines the probability of ending up in a certain
    state *S* for each previous state and action *A* ![](img/B15439_22_035.png). We
    will demonstrate `pymdptoolbox` and use one of the formats available to specify
    transitions and rewards. For both transition probabilities, we will create a NumPy
    array with dimensions ![](img/B15439_22_036.png).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**转移矩阵** 定义了对于每个先前状态和动作 *A*，以及每个状态 *S* 的结束概率！[](img/B15439_22_035.png)。我们将演示`pymdptoolbox`并使用其中一种可用于指定转移和奖励的格式。对于转移概率，我们将创建一个具有维度![](img/B15439_22_036.png)的NumPy数组。'
- en: 'We first compute the target cell for each starting cell and move:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算每个起始单元格和移动的目标单元格：
- en: '[PRE5]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following function uses the arguments starting `state`, `action`, and `outcome`
    to fill in the transition probabilities and rewards:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数使用开始`state`、`action`和`outcome`参数来填充转移概率和奖励：
- en: '[PRE6]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We generate the transition and reward values by creating placeholder data structures
    and iterating over the Cartesian product of ![](img/B15439_22_037.png), as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过创建占位数据结构并迭代![](img/B15439_22_037.png)的笛卡尔积来生成转移和奖励值，如下所示：
- en: '[PRE7]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Implementing the value iteration algorithm
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现值迭代算法
- en: 'We first create the value iteration algorithm, which is slightly simpler because
    it implements policy evaluation and improvement in a single step. We capture the
    states for which we need to update the value function, excluding terminal states
    that have a value of 0 for lack of rewards (+1/-1 are assigned to the starting
    state), and skip the blocked cell:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建值迭代算法，稍微简单一些，因为它在单个步骤中实现策略评估和改进。我们捕获需要更新值函数的状态，排除了值为0的终止状态（由于缺乏奖励，+1/-1分配给起始状态），并跳过阻塞的单元格：
- en: '[PRE8]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we initialize the value function and set the discount factor gamma and
    the convergence threshold `epsilon`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们初始化值函数，并设置折扣因子 gamma 和收敛阈值`epsilon`：
- en: '[PRE9]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The algorithm updates the value function using the Bellman optimality equation,
    as described previously, and terminates when the L1 norm of *V* changes to less
    than epsilon in absolute terms:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 算法使用贝尔曼最优方程更新值函数，如前所述，并在*V*的L1范数绝对值小于 epsilon 时终止：
- en: '[PRE10]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The algorithm converges in 16 iterations and 0.0117s. It produces the following
    optimal value estimate, which, together with the implied optimal policy, is depicted
    in the right panel of *Figure 22.4*, earlier in this section:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在16次迭代和0.0117秒内收敛。它产生以下最优值估计，连同隐含的最优策略，如本节之前的*图22.4*右图所示：
- en: '[PRE11]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Defining and running policy iteration
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义并运行策略迭代
- en: 'Policy iterations involve separate evaluation and improvement steps. We define
    the improvement part by selecting the action that maximizes the sum of the expected
    reward and next-state value. Note that we temporarily fill in the rewards for
    the terminal states to avoid ignoring actions that would lead us there:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 政策迭代包括单独的评估和改进步骤。我们通过选择最大化预期奖励和下一个状态值的和的动作来定义改进部分。请注意，我们临时填充终端状态的奖励以避免忽略会导致我们到达那里的动作：
- en: '[PRE12]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We initialize the value function as before and also include a random starting
    policy:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像以前一样初始化值函数，并且还包括一个随机起始策略：
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The algorithm alternates between policy evaluation for a greedily selected
    action and policy improvement until the policy stabilizes:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在贪婪选择的动作的政策评估和政策改进之间交替，直到策略稳定为止：
- en: '[PRE14]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Policy iteration converges after only three iterations. The policy stabilizes
    before the algorithm finds the optimal value function, and the optimal policy
    differs slightly, most notably by suggesting "up" instead of the safer "left"
    for the field next to the negative terminal state. This can be avoided by tightening
    the convergence criteria, for example, by requiring a stable policy of several
    rounds or by adding a threshold for the value function.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 政策迭代在仅三次迭代后收敛。在算法找到最优值函数之前，策略会稳定下来，而最优策略略有不同，最明显的是建议在负终端状态旁边的场地上“向上”而不是更安全的“向左”。通过缩紧收敛标准（例如，要求几轮稳定的策略或为值函数添加阈值），可以避免这种情况。
- en: Solving MDPs using pymdptoolbox
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用pymdptoolbox解决MDP问题
- en: We can also solve MDPs using the Python library `pymdptoolbox`, which includes
    a few other algorithms, including Q-learning.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用Python库`pymdptoolbox`来解决MDP问题，其中包括一些其他算法，包括Q-learning。
- en: 'To run value iteration, just instantiate the corresponding object with the
    desired configuration options, rewards, and transition matrices before calling
    the `.run()` method:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行值迭代，只需在调用`.run()`方法之前，使用所需的配置选项、奖励和转移矩阵实例化相应的对象：
- en: '[PRE15]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The value function estimate matches the result in the previous section:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数估计与上一节的结果相匹配：
- en: '[PRE16]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Policy iteration works similarly:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 政策迭代工作方式类似：
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It also yields the same policy, but the value function varies by run and does
    not need to achieve the optimal value before the policy converges.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 它也产生相同的策略，但是值函数会根据运行而变化，并且在策略收敛之前不需要达到最优值。
- en: Lessons learned
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 吸取的教训
- en: The right panel we saw earlier in *Figure 22.4* shows the optimal value estimate
    produced by value iteration and the corresponding greedy policy. The negative
    rewards, combined with the uncertainty in the environment, produce an optimal
    policy that involves moving away from the negative terminal state.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前在*图22.4*中看到的右侧面板显示了值迭代产生的最优值估计以及相应的贪婪策略。负奖励与环境的不确定性相结合，产生了一个最优策略，涉及远离负终端状态。
- en: The results are sensitive to both the rewards and the discount factor. The cost
    of the negative state affects the policy in the surrounding fields, and you should
    modify the example in the corresponding notebook to identify threshold levels
    that alter the optimal action selection.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 结果对奖励和折扣因子都很敏感。负状态的成本影响周围字段的策略，您应修改相应笔记本中的示例以识别改变最优动作选择的阈值水平。
- en: Q-learning – finding an optimal policy on the go
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning - 边走边学习寻找最优策略
- en: Q-learning was an early RL breakthrough when developed by Chris Watkins for
    his PhD thesis ([http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf](http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf))
    (1989). It introduces incremental dynamic programming to learn to control an MDP
    without knowing or modeling the transition and reward matrices that we used for
    value and policy iteration in the previous section. A convergence proof followed
    3 years later (Christopher J. C. H. Watkins and Dayan 1992).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning是早期的强化学习突破，由克里斯·沃特金斯（Chris Watkins）为他的博士论文开发（[http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf](http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf)）（1989）。它引入了增量动态规划来学习控制MDP，而不知道或建模我们在前一节中用于值和策略迭代的转移和奖励矩阵。
    3年后进行了收敛证明（Christopher J.C.H. Watkins和Dayan 1992）。
- en: Q-learning directly optimizes the action-value function *q* to approximate *q**.
    The learning proceeds "off-policy," that is, the algorithm does not need to select
    actions based on the policy implied by the value function alone. However, convergence
    requires that all state-action pairs continue to be updated throughout the training
    process. A straightforward way to ensure this is through an ![](img/B15439_22_038.png)-greedy
    policy.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习直接优化动作值函数*q*以逼近*q**。学习进行“离策略”，即，算法不需要仅根据值函数隐含的策略选择动作。然而，收敛需要所有状态-动作对在整个训练过程中持续更新。确保这一点的一种简单方法是通过**-贪婪策略**。
- en: Exploration versus exploitation – ![](img/B15439_22_039.png)-greedy policy
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索与利用 – **-贪婪策略**
- en: An ![](img/B15439_22_0391.png)**-greedy policy** is a simple policy that ensures
    the exploration of new actions in a given state while also exploiting the learning
    experience . It does this by randomizing the selection of actions. An ![](img/B15439_22_038.png)-greedy
    policy selects an action randomly with a probability of ![](img/B15439_22_038.png),
    and the best action according to the value function otherwise.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**-贪婪策略**是一种简单的策略，它确保在给定状态下探索新的动作，同时也利用了学习经验。它通过随机选择动作来实现这一点。一个**-贪婪策略**以概率![](img/B15439_22_038.png)随机选择一个动作，否则选择值函数最优的动作。
- en: The Q-learning algorithm
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q学习算法
- en: 'The algorithm keeps improving a state-action value function after random initialization
    for a given number of episodes. At each time step, it chooses an action based
    on an ![](img/B15439_22_042.png)-greedy policy, and uses a learning rate ![](img/B15439_22_043.png)
    to update the value function, as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在随机初始化后的给定数量的剧集中不断改进状态-动作值函数。在每个时间步长，它根据一个**-贪婪策略**选择一个动作，并使用学习率![](img/B15439_22_043.png)来更新值函数，如下所示：
- en: '![](img/B15439_22_044.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_044.png)'
- en: Note that the algorithm does not compute expected values based on the transition
    probabilities. Instead, it learns the *Q* function from the rewards *R*[t] produced
    by the ![](img/B15439_22_045.png)-greedy policy and its current estimate of the
    discounted value function for the next state.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，该算法不会根据转移概率计算期望值。相反，它从**-贪婪策略**产生的奖励*R*[t]和下一个状态的折现值函数的当前估计中学习*Q*函数。
- en: The use of the estimated value function to improve this very estimate is called
    **bootstrapping**. The Q-learning algorithm is part of the **temporal difference**
    (**TD**) **learning** algorithms. TD learning does not wait until receiving the
    final reward for an episode. Instead, it updates its estimates using the values
    of intermediate states that are closer to the final reward. In this case, the
    intermediate state is one time step ahead.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用估计值函数来改进这个估计本身被称为**自举**。Q学习算法是**时间差**（**TD**）**学习**算法的一部分。TD学习不会等待收到剧集的最终奖励。相反，它使用更接近最终奖励的中间状态的值来更新其估计。在这种情况下，中间状态是一步。
- en: How to train a Q-learning agent using Python
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用Python训练一个Q学习智能体
- en: 'In this section, we will demonstrate how to build a Q-learning agent using
    the ![](img/B15439_22_046.png) grid of states from the previous section. We will
    train the agent for 2,500 episodes, using a learning rate of ![](img/B15439_22_047.png)
    and ![](img/B15439_22_048.png) for the ![](img/B15439_22_042.png)-greedy policy
    (see the notebook `gridworld_q_learning.ipynb` for details):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何使用上一节中的状态网格构建一个Q学习智能体。我们将训练智能体进行2,500个剧集，使用学习速率![](img/B15439_22_047.png)和![](img/B15439_22_048.png)进行**-贪婪策略**（有关详细信息，请参见笔记本`gridworld_q_learning.ipynb`）：
- en: '[PRE18]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we will randomly initialize the state-action value function as a NumPy
    array with dimensions *number of states × number of actions*:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将随机初始化状态-动作值函数作为NumPy数组，维度为*状态数×动作数*：
- en: '[PRE19]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The algorithm generates 2,500 episodes that start at a random location and
    proceed according to the ![](img/B15439_22_045.png)-greedy policy until termination,
    updating the value function according to the Q-learning rule:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法生成2,500个从随机位置开始并根据**-贪婪策略**进行的剧集，直到终止，根据Q学习规则更新值函数：
- en: '[PRE20]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The episodes take 0.6 seconds and converge to a value function fairly close
    to the result of the value iteration example from the previous section. The `pymdptoolbox`
    implementation works analogously to previous examples (see the notebook for details).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 每个情节需要0.6秒，并收敛到与前一节中值迭代示例结果相当接近的值函数。`pymdptoolbox`实现与以前的示例类似（详情请参见笔记本）。
- en: Deep RL for trading with the OpenAI Gym
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenAI Gym进行交易的深度RL
- en: In the previous section, we saw how Q-learning allows us to learn the optimal
    state-action value function *q** in an environment with discrete states and discrete
    actions using iterative updates based on the Bellman equation.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看到了Q学习如何让我们在具有离散状态和离散动作的环境中使用基于贝尔曼方程的迭代更新来学习最优状态-动作值函数*q**。
- en: In this section, we will take RL one step closer to the real world and upgrade
    the algorithm to **continuous states** (while keeping actions discrete). This
    implies that we can no longer use a tabular solution that simply fills an array
    with state-action values. Instead, we will see how to **approximate q* using a
    neural network** (**NN**), which results in a deep Q-network. We will first discuss
    how deep learning integrates with RL before presenting the deep Q-learning algorithm,
    as well as various refinements that accelerate its convergence and make it more
    robust.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将强化学习（RL）迈向真实世界，将算法升级为**连续状态**（同时保持动作离散）。这意味着我们不能再使用简单地填充数组状态-动作值的表格解决方案。相反，我们将看到如何使用神经网络来**近似q**，从而得到深度Q网络。在介绍深度Q学习算法之前，我们将首先讨论深度学习与RL的整合，以及各种加速其收敛并使其更加健壮的改进。
- en: Continuous states also imply a **more complex environment**. We will demonstrate
    how to work with OpenAI Gym, a toolkit for designing and comparing RL algorithms.
    First, we'll illustrate the workflow by training a deep Q-learning agent to navigate
    a toy spaceship in the Lunar Lander environment. Then, we'll proceed to **customize
    OpenAI Gym** to design an environment that simulates a trading context where an
    agent can buy and sell a stock while competing against the market.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 连续状态还意味着**更复杂的环境**。我们将演示如何使用OpenAI Gym，一个用于设计和比较RL算法的工具包。首先，我们将通过训练一个深度Q学习代理程序来演示工作流程，以在月球着陆器环境中导航一个玩具飞船。然后，我们将继续**自定义OpenAI
    Gym**，设计一个模拟交易情境的环境，其中代理可以买卖股票，并与市场竞争。
- en: Value function approximation with neural networks
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用神经网络进行值函数近似
- en: Continuous state and/or action spaces imply an **infinite number of transitions**
    that make it impossible to tabulate the state-action values, as in the previous
    section. Rather, we approximate the Q function by learning a continuous, parameterized
    mapping from training samples.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 连续状态和/或动作空间意味着**无限数量的转换**，使得不可能像前一节那样制表状态-动作值。相反，我们通过学习连续参数化映射来近似Q函数。
- en: 'Motivated by the success of NNs in other domains, which we discussed in the
    previous chapters in *Part 4*, deep NNs have also become popular for approximating
    value functions. However, **machine learning in the RL context**, where the data
    is generated by the interaction of the model with the environment using a (possibly
    randomized) policy, **faces distinct challenges**:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 受到在其他领域中NN成功的启发，我们在*Part 4*中讨论过，深度NN也因近似值函数而变得流行起来。然而，在RL环境中，数据由模型与使用（可能是随机的）策略与环境进行交互生成，面临着**不同的挑战**：
- en: With continuous states, the agent will fail to visit most states and thus needs
    to generalize.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于连续状态，代理将无法访问大多数状态，因此需要进行泛化。
- en: Whereas supervised learning aims to generalize from a sample of independently
    and identically distributed samples that are representative and correctly labeled,
    in the RL context, there is only one sample per time step, so learning needs to
    occur online.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在监督学习中，旨在从独立同分布且具有代表性且正确标记的样本中概括出来，而在强化学习（RL）环境中，每个时间步只有一个样本，因此学习需要在线进行。
- en: Furthermore, samples can be highly correlated when sequential states are similar
    and the behavior distribution over states and actions is not stationary, but rather
    changes as a result of the agent's learning.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，当连续状态时，样本可能高度相关，当连续状态相似且行为分布在状态和动作上不是固定的，而是由于代理的学习而发生变化时，样本可能高度相关。
- en: We will look at several techniques that have been developed to address these
    additional challenges.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍几种已开发的技术来解决这些额外的挑战。
- en: The Deep Q-learning algorithm and extensions
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度Q学习算法及其扩展
- en: Deep Q-learning estimates the value of the available actions for a given state
    using a deep neural network. DeepMind introduced this technique in *Playing Atari
    with Deep Reinforcement Learning* (Mnih et al. 2013), where agents learned to
    play games solely from pixel input.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 学习通过深度神经网络估计给定状态的可用动作的价值。DeepMind 在 *使用深度强化学习玩 Atari 游戏*（Mnih 等人，2013）中介绍了这项技术，代理程序仅从像素输入中学习玩游戏。
- en: The Deep Q-learning algorithm approximates the action-value function *q* by
    learning a set of weights ![](img/B15439_10_006.png) of a multilayered **deep
    Q-network** (**DQN**) that maps states to actions so that ![](img/B15439_22_052.png).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 学习算法通过学习一组权重 ![](img/B15439_10_006.png) 的多层**深度 Q 网络**（**DQN**）来近似动作价值函数
    *q*，该函数将状态映射到动作，使得 ![](img/B15439_22_052.png)。
- en: 'The algorithm applies gradient descent based on a loss function that computes
    the squared difference between the DQN''s estimate of the target:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法应用基于损失函数的梯度下降，计算目标 DQN 的估计之间的平方差：
- en: '![](img/B15439_22_053.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_053.png)'
- en: 'and its estimate of the action-value of the current state-action pair ![](img/B15439_22_054.png)
    to learn the network parameters:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 并根据当前状态-动作对的动作价值估计 ![](img/B15439_22_054.png) 来学习网络参数：
- en: '![](img/B15439_22_055.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_055.png)'
- en: Both **the target and the current estimate depend on the DQN weights**, underlining
    the distinction from supervised learning where targets are fixed prior to training.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标和当前估计都依赖于 DQN 权重**，突显了与监督学习的区别，在监督学习中，目标在训练之前是固定的。'
- en: Rather than computing the full gradient, the Q-learning algorithm uses **stochastic
    gradient descent** (**SGD**) and updates the weights ![](img/B15439_22_056.png)
    after each time step *i*. To explore the state-action space, the agent uses an
    ![](img/B15439_22_057.png)-greedy policy that selects a random action with probability
    ![](img/B15439_22_067.png) and follows a greedy policy that selects the action
    with the highest predicted *q*-value otherwise.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 该 Q 学习算法不计算完整梯度，而是使用**随机梯度下降**（**SGD**）并在每个时间步 *i* 后更新权重 ![](img/B15439_22_056.png)。为了探索状态-动作空间，代理程序使用一个
    ![](img/B15439_22_057.png)-贪婪策略，以概率选择一个随机动作，否则按照最高预测 *q* 值选择动作。
- en: The basic **DQN architecture has been refined** in several directions to make
    the learning process more efficient and improve the final result; Hessel et al.
    (2017) combined these innovations in the **Rainbow agent** and demonstrated how
    each contributes to significantly higher performance across the Atari benchmarks.
    The following subsections summarize some of these innovations.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的**DQN 架构已经得到改进**，以使学习过程更加高效，并改善最终结果；Hessel 等人（2017）将这些创新组合成**Rainbow 代理**，并展示了每个创新如何显著提高
    Atari 基准测试的性能。以下各小节总结了其中一些创新。
- en: (Prioritized) Experience replay – focusing on past mistakes
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: （优先）经验回放 – 关注过去的错误
- en: Experience replay stores a history of the state, action, reward, and next state
    transitions experienced by the agent. It randomly samples mini-batches from this
    experience to update the network weights at each time step before the agent selects
    an *ε*-greedy action.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 经验回放存储代理程序经历的状态、动作、奖励和下一个状态转换的历史记录。它从这些经验中随机抽样小批量，在每个时间步更新网络权重，然后代理程序选择一个*ε*-贪婪动作。
- en: Experience replay increases sample efficiency, reduces the autocorrelation of
    samples collected during online learning, and limits the feedback due to current
    weights producing training samples that can lead to local minima or divergence
    (Lin and Mitchell 1992).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 经验回放提高了样本效率，减少了在线学习期间收集的样本的自相关性，并限制了由当前权重产生的反馈，这些反馈可能导致局部最小值或发散（Lin 和 Mitchell
    1992）。
- en: This technique was later refined to prioritize experience that is more important
    from a learning perspective. Schaul et al. (2015) approximated the value of a
    transition by the size of the TD error that captures how "surprising" the event
    was for the agent. In practice, it samples historical state transitions using
    their associated TD error rather than uniform probabilities.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 此技术后来被进一步改进，以优先考虑从学习角度更重要的经验。Schaul 等人（2015）通过 TD 误差的大小来近似转换的价值，该误差捕捉了该事件对代理程序的“惊讶程度”。实际上，它使用其关联的
    TD 误差而不是均匀概率对历史状态转换进行抽样。
- en: The target network – decorrelating the learning process
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标网络 – 解耦学习过程
- en: To further weaken the feedback loop from the current network parameters on the
    NN weight updates, the algorithm was extended by DeepMind in *Human-level control
    through deep reinforcement learning* (Mnih et al. 2015) to use a slowly-changing
    target network.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步削弱当前网络参数对 NN 权重更新的反馈循环，DeepMind 在 *Human-level control through deep reinforcement
    learning*（Mnih et al. 2015）中将算法扩展为使用缓慢变化的目标网络。
- en: 'The target network has the same architecture as the Q-network, but its weights
    ![](img/B15439_22_058.png) are only updated periodically after ![](img/B15439_22_059.png)
    steps when they are copied from the Q-network and held constant otherwise. The
    target network **generates the TD target predictions**, that is, it takes the
    place of the Q-network to estimate:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网络具有与 Q 网络相同的架构，但其权重为 ![](img/B15439_22_058.png)，仅在每隔 ![](img/B15439_22_059.png)
    步更新一次，当它们从 Q 网络复制并保持不变时。目标网络**生成 TD 目标预测**，即它取代 Q 网络来估计：
- en: '![](img/B15439_22_060.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_060.png)'
- en: Double deep Q-learning – decoupling action and prediction
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双深度 Q 学习 – 分离行动和预测
- en: Q-learning has been shown to overestimate the action values because it purposely
    samples maximal estimated action values.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 存在过高估计行动价值的问题，因为它有意地采样最大估计行动价值。
- en: This bias can negatively affect the learning process and the resulting policy
    if it does not apply uniformly and alters action preferences, as shown in *Deep
    Reinforcement Learning with Double Q-learning* (van Hasselt, Guez, and Silver
    2015).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这种偏见不是均匀应用并且改变行动偏好，它可能会对学习过程和结果的政策产生负面影响，就像 *Deep Reinforcement Learning with
    Double Q-learning*（van Hasselt, Guez, and Silver 2015）中所示的那样。
- en: 'To decouple the estimation of action values from the selection of actions,
    the **Double DQN** (**DDQN**) algorithm uses the weights ![](img/B15439_22_061.png)
    of one network to select the best action given the next state, as well as the
    weights ![](img/B15439_22_062.png) of another network, to provide the corresponding
    action value estimate:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将行动价值的估计与行动的选择分离，**双深度 Q 学习**（**DDQN**）算法使用一个网络的权重 ![](img/B15439_22_061.png)
    来选择给定下一个状态的最佳行动，以及另一个网络的权重 ![](img/B15439_22_062.png) 来提供相应的行动价值估计：
- en: '![](img/B15439_22_063.png).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B15439_22_063.png)。'
- en: One option is to randomly select one of two identical networks for training
    at each iteration so that their weights will differ. A more efficient alternative
    is to rely on the target network to provide ![](img/B15439_22_064.png) instead.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 一个选项是在每次迭代时随机选择两个相同网络中的一个进行训练，以使它们的权重不同。更有效的替代方法是依靠目标网络提供 ![](img/B15439_22_064.png)。
- en: Introducing the OpenAI Gym
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 OpenAI Gym
- en: OpenAI Gym is an RL platform that provides standardized environments to test
    and benchmark RL algorithms using Python. It is also possible to extend the platform
    and register custom environments.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 是一个提供标准化环境以测试和基准 RL 算法的 RL 平台，使用 Python。也可以扩展该平台并注册自定义环境。
- en: 'The **Lunar Lander v2** (**LL**) environment requires the agent to control
    its motion in two dimensions based on a discrete action space and low-dimensional
    state observations that include position, orientation, and velocity. At each time
    step, the environment provides an observation of the new state and a positive
    or negative reward. Each episode consists of up to 1,000 time steps. *Figure 22.5*
    shows selected frames from a successful landing after 250 episodes by the agent
    we will train later:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lunar Lander v2**（**LL**）环境要求代理根据离散行动空间和包括位置、方向和速度在内的低维状态观察来控制其在二维中的运动。在每个时间步长，环境提供新状态的观察和正面或负面的奖励。每个事件最多包含
    1,000 个时间步。*图 22.5* 展示了我们稍后将训练的代理在经过 250 个事件后成功着陆时的选定帧：'
- en: '![](img/B15439_22_05.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_05.png)'
- en: 'Figure 22.5: RL agent''s behavior during the Lunar Lander episode'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.5：月球着陆器（Lunar Lander）事件期间 RL 代理的行为
- en: More specifically, the **agent observes eight aspects of the state**, including
    six continuous and two discrete elements. Based on the observed elements, the
    agent knows its location, direction, and speed of movement and whether it has
    (partially) landed. However, it does not know in which direction it should move,
    nor can it observe the inner state of the environment to understand the rules
    that govern its motion.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，**代理观察到状态的八个方面**，包括六个连续和两个离散元素。根据观察到的元素，代理知道自己的位置、方向和移动速度，以及是否（部分）着陆。然而，它不知道应该朝哪个方向移动，也不能观察环境的内部状态以了解规则来控制其运动。
- en: At each time step, the agent controls its motion using one of **four discrete
    actions**. It can do nothing (and continue on its current path), fire its main
    engine (to reduce downward motion), or steer toward the left or right using the
    respective orientation engines. There are no fuel limitations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步长，智能体使用**四种离散动作**来控制其运动。它可以什么都不做（继续当前路径），启动主引擎（减少向下运动），或使用相应的方向引擎向左或向右转向。没有燃料限制。
- en: The goal is to land the agent between two flags on a landing pad at coordinates
    (0, 0), but landing outside of the pad is possible. The agent accumulates rewards
    in the range of 100-140 for moving toward the pad, depending on the exact landing
    spot. However, a move away from the target negates the reward the agent would
    have gained by moving toward the pad. Ground contact by each leg adds 10 points,
    while using the main engine costs -0.3 points.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是在坐标（0,0）的着陆垫上的两个旗帜之间着陆智能体，但也可以在垫子外着陆。智能体朝着垫子移动，积累的奖励在100-140之间，具体取决于着陆点。然而，远离目标的移动会抵消智能体通过朝着垫子移动而获得的奖励。每条腿的接地都会增加10分，而使用主引擎会消耗-0.3点。
- en: An episode terminates if the agent lands or crashes, adding or subtracting 100
    points, respectively, or after 1,000 time steps. Solving LL requires achieving
    a cumulative reward of at least 200 on average over 100 consecutive episodes.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果智能体着陆或坠毁，则一集结束，分别添加或减去100分，或在1000个时间步之后结束。解决 LL 需要在100个连续集合上平均获得至少200的累积奖励。
- en: How to implement DDQN using TensorFlow 2
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 TensorFlow 2 实现 DDQN
- en: The notebook `03_lunar_lander_deep_q_learning` implements a DDQN agent using
    TensorFlow 2 that learns to solve OpenAI Gym's **Lunar Lander** 2.0 (**LL**) environment.
    The notebook `03_lunar_lander_deep_q_learning` contains a TensorFlow 1 implementation
    that was discussed in the first edition and runs significantly faster because
    it does not rely on eager execution and also converges sooner. This section highlights
    key elements of the implementation; please see the notebook for much more extensive
    details.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`03_lunar_lander_deep_q_learning`使用 TensorFlow 2 实现了一个 DDQN 代理程序，该程序学习解决
    OpenAI Gym 的**月球着陆器**2.0（**LL**）环境。笔记本`03_lunar_lander_deep_q_learning`包含了在第一版中讨论的
    TensorFlow 1 实现，运行速度显著更快，因为它不依赖于急切执行，并且更快地收敛。本节重点介绍了实现的关键元素；更详细的细节请参阅笔记本。
- en: Creating the DDQN agent
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 DDQN 代理
- en: We create our `DDQNAgent` as a Python class to integrate the learning and execution
    logic with the key configuration parameters and performance tracking.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`DDQNAgent`创建为一个 Python 类，以将学习和执行逻辑与关键配置参数和性能跟踪集成在一起。
- en: 'The agent''s `__init__()` method takes, as arguments, information on:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的`__init__()`方法接受以下信息作为参数：
- en: The **environment characteristics**, like the number of dimensions for the state
    observations and the number of actions available to the agent.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境特征**，比如状态观测的维度数量以及智能体可用的动作数量。'
- en: The decay of the randomized exploration for the **ε-greedy policy**.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ε-贪婪策略**的随机探索衰减。'
- en: The **neural network architecture** and the parameters for **training** and
    target network updates.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络架构**和**训练**和目标网络更新的参数。'
- en: '[PRE21]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Adapting the DDQN architecture to the Lunar Lander
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 DDQN 架构调整为月球着陆器
- en: The DDQN architecture was first applied to the Atari domain with high-dimensional
    image observations and relied on convolutional layers. The LL's lower-dimensional
    state representation makes fully connected layers a better choice (see *Chapter
    17*, *Deep Learning for Trading*).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 首次将 DDQN 架构应用于具有高维图像观察的雅达利领域，并依赖于卷积层。LL 的较低维状态表示使得全连接层成为更好的选择（见*第17章*，*交易深度学习*）。
- en: More specifically, the network maps eight inputs to four outputs, representing
    the Q values for each action, so that it only takes a single forward pass to compute
    the action values. The DQN is trained on the previous loss function using the
    Adam optimizer. The agent's DQN uses three densely connected layers with 256 units
    each and L2 activity regularization. Using a GPU via the TensorFlow Docker image
    can significantly speed up NN training performance (see *Chapter 17* and *Chapter
    18*, *CNNs for Financial Time Series and Satellite Images*).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，网络将八个输入映射到四个输出，表示每个动作的 Q 值，因此只需进行一次前向传递即可计算动作值。DQN 使用 Adam 优化器对先前的损失函数进行训练。代理的
    DQN 使用每个具有 256 单元的三个密集连接层和 L2 活动正则化。通过 TensorFlow Docker 镜像使用 GPU 可以显著加快 NN 训练性能（见*第17章*和*第18章*，*金融时间序列和卫星图像的
    CNN*）。
- en: The `DDQNAgent` class's `build_model()` method creates the primary online and
    slow-moving target networks based on the `architecture` parameter, which specifies
    the number of layers and their number of units.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`DDQNAgent`类的`build_model()`方法根据`architecture`参数创建主要的在线和缓慢移动的目标网络，该参数指定了层的数量和它们的单元数量。'
- en: 'We set `trainable` to `True` for the primary online network and to `False`
    for the target network. This is because we simply periodically copy the online
    NN weights to update the target network:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 对于主要的在线网络，我们将`trainable`设置为`True`，对于目标网络，我们将其设置为`False`。这是因为我们只是周期性地将在线NN的权重复制以更新目标网络：
- en: '[PRE22]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Memorizing transitions and replaying the experience
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 记忆转换和重播体验
- en: To enable experience replay, the agent memorizes each state transition so it
    can randomly sample a mini-batch during training. The `memorize_transition()`
    method receives the observation on the current and next state provided by the
    environment, as well as the agent's action, the reward, and a flag that indicates
    whether the episode is completed.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用经验重播，代理记忆每个状态转换，以便在训练期间随机抽样小批量。`memorize_transition()`方法接收环境提供的当前和下一个状态的观察、代理的动作、奖励以及指示情节是否完成的标志。
- en: 'It tracks the reward history and length of each episode, applies exponential
    decay to epsilon at the end of each period, and stores the state transition information
    in a buffer:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 它跟踪奖励历史和每个情节的长度，在每个周期结束时对epsilon进行指数衰减，并将状态转换信息存储在缓冲区中：
- en: '[PRE23]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The replay of the memorized experience begins as soon as there are enough samples
    to create a full batch. The `experience_replay()` method predicts the Q values
    for the next states using the online network and selects the best action. It then
    selects the predicted *q* values for these actions from the target network to
    arrive at the TD `targets`.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦有足够的样本创建完整的批次，记忆的重播就开始了。`experience_replay()`方法使用在线网络预测下一个状态的Q值，并选择最佳动作。然后，它从目标网络中选择这些动作的预测*q*值，以得到TD`targets`。
- en: 'Next, it trains the primary network using a single batch of current state observations
    as input, the TD targets as the outcome, and the mean-squared error as the loss
    function. Finally, it updates the target network weights every ![](img/B15439_22_065.png)
    steps:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，它使用单个批次的当前状态观察作为输入，TD目标作为输出，并将均方误差作为损失函数训练主要网络。最后，它每隔![](img/B15439_22_065.png)步更新一次目标网络的权重：
- en: '[PRE24]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The notebook contains additional implementation details for the ε-greedy policy
    and the target network weight updates.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本包含ε-贪心策略和目标网络权重更新的其他实现细节。
- en: Setting up the OpenAI environment
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置OpenAI环境
- en: 'We will begin by instantiating and extracting key parameters from the LL environment:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先实例化并从LL环境中提取关键参数：
- en: '[PRE25]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will also use the built-in wrappers that permit the periodic storing of
    videos that display the agent''s performance:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用内置的包装器，允许周期性地存储显示代理性能的视频：
- en: '[PRE26]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: When running on a server or Docker container without a display, you can use
    `pyvirtualdisplay`.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有显示器的服务器或Docker容器上运行时，您可以使用`pyvirtualdisplay`。
- en: Key hyperparameter choices
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键的超参数选择
- en: 'The agent''s performance is quite sensitive to several hyperparameters. We
    will start with the discount and learning rates:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的性能对几个超参数非常敏感。我们将从折扣率和学习率开始：
- en: '[PRE27]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will update the target network every 100 time steps, store up to 1 million
    past episodes in the replay memory, and sample mini-batches of 1,024 from memory
    to train the agent:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每隔100个时间步更新目标网络，在回放内存中存储最多100万个过去的情节，并从内存中对训练代理进行1,024个小批量的抽样：
- en: '[PRE28]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The ε-greedy policy starts with pure exploration at ![](img/B15439_22_066.png),
    linear decay to 0.01 over 250 episodes, and exponential decay thereafter:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ε-贪心策略从纯探索开始，线性衰减至![](img/B15439_22_066.png)，然后在250个情节后以指数衰减：
- en: '[PRE29]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The notebook contains the training loop, including experience replay, SGD, and
    slow target network updates.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本包含训练循环，包括经验重播、SGD和缓慢的目标网络更新。
- en: Lunar Lander learning performance
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 月球着陆器的学习表现
- en: The preceding hyperparameter settings enable the agent to solve the environment
    in around 300 episodes using the TensorFlow 1 implementation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的超参数设置使得代理能够在约300个情节内使用TensorFlow 1实现解决环境。
- en: 'The left panel of *Figure 22.6* shows the episode rewards and their moving
    average over 100 periods. The right panel shows the decay of exploration and the
    number of steps per episode. There is a stretch of some 100 episodes that often
    take 1,000 time steps each while the agent reduces exploration and "learns how
    to fly" before starting to land fairly consistently:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_22_06.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.6: The DDQN agent''s performance in the Lunar Lander environment'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple trading agent
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this and the following sections, we will adapt the deep RL approach to design
    an agent that learns how to trade a single asset. To train the agent, we will
    set up a simple environment with a limited set of actions, a relatively low-dimensional
    state with continuous observations, and other parameters.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, the **environment** samples a stock price time series for
    a single ticker using a random start date to simulate a trading period that, by
    default, contains 252 days or 1 year. Each **state observation** provides the
    agent with the historical returns for various lags and some technical indicators,
    like the **relative strength index** (**RSI**).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent can choose from three **actions**:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '**Buy**: Invest all capital for a long position in the stock.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flat**: Hold cash only.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sell short**: Take a short position equal to the amount of capital.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment accounts for **trading cost**, set to 10 basis points by default,
    and deducts one basis point per period without trades. The **reward** of the agent
    consists of the daily return minus trading costs.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The environment tracks the **net asset value** (**NAV**) of the agent's portfolio
    (consisting of a single stock) and compares it against the market portfolio, which
    trades frictionless to raise the bar for the agent.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'An episode begins with a starting NAV of 1 unit of cash:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: If the NAV drops to 0, the episode ends with a loss.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the NAV hits 2.0, the agent wins.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This setting limits complexity as it focuses on a single stock and abstracts
    from position sizing to avoid the need for continuous actions or a larger number
    of discrete actions, as well as more sophisticated bookkeeping. However, it is
    useful to demonstrate how to customize an environment and permits for extensions.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: How to design a custom OpenAI trading environment
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To build an agent that learns how to trade, we need to create a market environment
    that provides price and other information, offers relevant actions, and tracks
    the portfolio to reward the agent accordingly. For a description of the efforts
    to build a large-scale, real-world simulation environment, see Byrd, Hybinette,
    and Balch (2019).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym allows for the design, registration, and utilization of environments
    that adhere to its architecture, as described in the documentation. The file `trading_env.py`
    contains the following code examples, which illustrate the process unless noted
    otherwise.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: The trading environment consists of three classes that interact to facilitate
    the agent's activities. The `DataSource` class loads a time series, generates
    a few features, and provides the latest observation to the agent at each time
    step. `TradingSimulator` tracks the positions, trades and cost, and the performance.
    It also implements and records the results of a buy-and-hold benchmark strategy.
    `TradingEnvironment` itself orchestrates the process. We will briefly describe
    each in turn; see the script for implementation details.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Designing a DataSource class
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we code up a `DataSource` class to load and preprocess historical stock
    data to create the information used for state observations and rewards. In this
    example, we will keep it very simple and provide the agent with historical data
    on a single stock. Alternatively, you could combine many stocks into a single
    time series, for example, to train the agent on trading the S&P 500 constituents.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'We will load the adjusted price and volume information for one ticker from
    the Quandl dataset, in this case for AAPL with data from the early 1980s until
    2018:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `preprocess_data()` method creates several features and normalizes them.
    The most recent daily returns play two roles:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: An element of the observations for the current state
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The net of trading costs and, depending on the position size, the reward for
    the last period
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The method takes the following steps, among others (refer to the *Appendix*
    for details on the technical indicators):'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `DataSource` class keeps track of episode progress, provides fresh data
    to `TradingEnvironment` at each time step, and signals the end of the episodes:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The TradingSimulator class
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The trading simulator computes the agent's reward and tracks the net asset values
    of the agent and "the market," which executes a buy-and-hold strategy with reinvestment.
    It also tracks the positions and the market return, computes trading costs, and
    logs the results.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important method of this class is the `take_step` method, which computes
    the agent''s reward based on its current position, the latest stock return, and
    the trading costs (slightly simplified; see the script for full details):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The TradingEnvironment class
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `TradingEnvironment` class subclasses `gym.Env` and drives the environment
    dynamics. It instantiates the `DataSource` and `TradingSimulator` objects and
    sets the action and state-space dimensionality, with the latter depending on the
    ranges of the features defined by `DataSource`:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The two key methods of `TradingEnvironment` are `.reset()` and `.step()`. The
    former initializes the `DataSource` and `TradingSimulator` instances, as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Each time step relies on `DataSource` and `TradingSimulator` to provide a state
    observation and reward the most recent action:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Registering and parameterizing the custom environment
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before using the custom environment, just as for the Lunar Lander environment,
    we need to register it with the `gym` package, provide information about the `entry_point`
    in terms of module and class, and define the maximum number of steps per episode
    (the following steps occur in the `q_learning_for_trading` notebook):'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用自定义环境之前，就像对待月球着陆器环境一样，我们需要将其注册到 `gym` 包中，提供关于 `entry_point` 的信息，即模块和类，并定义每个剧集的最大步数（以下步骤发生在
    `q_learning_for_trading` 笔记本中）：
- en: '[PRE37]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can instantiate the environment using the desired trading costs and ticker:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用所需的交易成本和股票代码实例化环境：
- en: '[PRE38]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Deep Q-learning on the stock market
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 股票市场上的深度 Q 学习
- en: The notebook `q_learning_for_trading` contains the DDQN agent training code;
    we will only highlight noteworthy differences from the previous example.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本 `q_learning_for_trading` 包含 DDQN 代理训练代码；我们只会突出显示与先前示例有显著不同的地方。
- en: Adapting and training the DDQN agent
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整和训练 DDQN 代理
- en: 'We will use the same DDQN agent but simplify the NN architecture to two layers
    of 64 units each and add dropout for regularization. The online network has 5,059
    trainable parameters:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的 DDQN 代理，但简化 NN 架构为每层 64 个单元的两层，并添加了用于正则化的 dropout。在线网络有 5059 个可训练参数：
- en: '[PRE39]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The training loop interacts with the custom environment in a manner very similar
    to the Lunar Lander case. While the episode is active, the agent takes the action
    recommended by its current policy and trains the online network using experience
    replay after memorizing the current transition. The following code highlights
    the key steps:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环与自定义环境的交互方式与月球着陆器案例非常相似。当剧集处于活动状态时，代理根据其当前策略采取行动，并在记忆当前转换后使用经验回放来训练在线网络。以下代码突出显示了关键步骤：
- en: '[PRE40]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We let exploration continue for 2,000 1-year trading episodes, corresponding
    to about 500,000 time steps; we use linear decay of ε from 1.0 to 0.1 over 500
    periods with exponential decay at a factor of 0.995 thereafter.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们让探索持续进行 2000 个 1 年的交易周期，相当于约 500,000 个时间步；我们在 500 个周期内使用 ε 的线性衰减从 1.0 到 0.1，之后以指数衰减因子
    0.995 进行指数衰减。
- en: Benchmarking DDQN agent performance
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基准 DDQN 代理的表现
- en: To compare the DDQN agent's performance, we not only track the buy-and-hold
    strategy but also generate the performance of a random agent.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较 DDQN 代理的表现，我们不仅追踪买入持有策略，还生成一个随机代理的表现。
- en: '*Figure 22.7* shows the rolling averages over the last 100 episodes of three
    cumulative return values for the 2,000 training periods (left panel), as well
    as the share of the last 100 episodes when the agent outperformed the buy-and-hold
    period (right panel). It uses AAPL stock data, for which there are some 9,000
    daily price and volume observations:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 22.7* 显示了 2000 个训练周期（左侧面板）中最近 100 个剧集的三个累积回报值的滚动平均值，以及代理超过买入持有期的最近 100 个剧集的份额（右侧面板）。它使用了
    AAPL 股票数据，其中包含约 9000 个每日价格和交易量观测值：'
- en: '![](img/B15439_22_07.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_07.png)'
- en: 'Figure 22.7: Trading agent performance relative to the market'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.7：交易代理的表现相对于市场
- en: This shows how the agent's performance improves steadily after 500 episodes,
    from the level of a random agent, and starts to outperform the buy-and-hold strategy
    toward the end of the experiment more than half of the time.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了代理在 500 个剧集后的表现稳步提高，从随机代理的水平开始，并在实验结束时开始超过买入持有策略超过一半的时间。
- en: Lessons learned
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学到的经验
- en: This relatively simple agent uses no information beyond the latest market data
    and the reward signal compared to the machine learning models we covered elsewhere
    in this book. Nonetheless, it learns to make a profit and achieve performance
    similar to that of the market (after training on 2,000 years' worth of data, which
    takes only a fraction of the time on a GPU).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这个相对简单的代理程序除了最新的市场数据和奖励信号外，没有使用其他信息，与我们在本书其他部分介绍的机器学习模型相比。尽管如此，它学会了盈利，并且在训练了
    2000 年的数据之后，它的表现与市场相似（在 GPU 上只需花费一小部分时间）。
- en: Keep in mind that using a single stock also increases the risk of overfitting
    to the data—by a lot. You can test your trained agent on new data using the saved
    model (see the notebook for Lunar Lander).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，只使用一支股票也会增加过拟合数据的风险——相当多。您可以使用保存的模型在新数据上测试您训练过的代理（请参阅月球着陆器的笔记本）。
- en: In summary, we have demonstrated the mechanics of setting up an RL trading environment
    and experimented with a basic agent that uses a small number of technical indicators.
    You should try to extend both the environment and the agent, for example, to choose
    from several assets, size the positions, and manage risks.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is often considered the **most promising approach to
    algorithmic trading** because it most accurately models the task an investor is
    facing. However, our dramatically simplified examples illustrate that creating
    a realistic environment poses a considerable challenge. Moreover, deep reinforcement
    learning that has achieved impressive breakthroughs in other domains may face
    greater obstacles given the noisy nature of financial data, which makes it even
    harder to learn a value function based on delayed rewards.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, the substantial interest in this subject makes it likely that institutional
    investors are working on larger-scale experiments that may yield tangible results.
    An interesting complementary approach beyond the scope of this book is **Inverse
    Reinforcement Learning**, which aims to identify the reward function of an agent
    (for example, a human trader) given its observed behavior; see Arora and Doshi
    (2019) for a survey and Roa-Vicens et al. (2019) for an application on trading
    in the limit-order book context.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced a different class of machine learning problems
    that focus on automating decisions by agents that interact with an environment.
    We covered the key features required to define an RL problem and various solution
    methods.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to frame and analyze an RL problem as a finite Markov decision problem,
    as well as how to compute a solution using value and policy iteration. We then
    moved on to more realistic situations, where the transition probabilities and
    rewards are unknown to the agent, and saw how Q-learning builds on the key recursive
    relationship defined by the Bellman optimality equation in the MDP case. We saw
    how to solve RL problems using Python for simple MDPs and more complex environments
    with Q-learning.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: We then expanded our scope to continuous states and applied the Deep Q-learning
    algorithm to the more complex Lunar Lander environment. Finally, we designed a
    simple trading environment using the OpenAI Gym platform, and also demonstrated
    how to train an agent to learn how to make a profit while trading a single stock.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter, we'll present a few conclusions and key takeaways
    from our journey through this book and lay out some steps for you to consider
    as you continue building your skills to use machine learning for trading.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
