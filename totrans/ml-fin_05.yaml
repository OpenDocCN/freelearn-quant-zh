- en: Chapter 5. Parsing Textual Data with Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章：使用自然语言处理解析文本数据
- en: It's no accident that Peter Brown, Co-CEO of Renaissance Technologies, one of
    the most successful quantitative hedge funds of all time, had previously worked
    at IBM, where he applied machine learning to natural language problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 彼得·布朗（Peter Brown）是文艺复兴科技公司（Renaissance Technologies）的联合首席执行官，该公司是有史以来最成功的量化对冲基金之一。他曾在IBM工作，并在该公司应用机器学习解决自然语言问题，这一切都不是巧合。
- en: As we've explored in earlier chapters, in today's world, information drives
    finance, and the most important source of information is written and spoken language.
    Ask any finance professional what they are actually spending time on, and you
    will find that a significant part of their time is spent on reading. This can
    cover everything from reading headlines on tickers, to reading a Form 10K, the
    financial press, or various analyst reports; the list goes on and on. Automatically
    processing this information can increase the speed of trades occurring and widen
    the breadth of information considered for trades while at the same time reducing
    overall costs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在之前的章节中探讨过的那样，在今天的世界中，信息驱动金融，而信息最重要的来源就是书面和口语语言。问问任何金融专业人士他们到底在花时间做什么，你会发现他们大部分时间都在阅读。这包括从阅读股票行情头条，到阅读10-K表格、金融新闻或各种分析师报告，列表几乎无穷无尽。自动处理这些信息可以加快交易速度，扩大交易时考虑的信息范围，同时降低总体成本。
- en: '**Natural language processing** (**NLP**) is making inroads into the finance
    sector. As an example, insurance companies are increasingly looking to process
    claims automatically, while retail banks try to streamline their customer service
    and offer better products to their clients. The understanding of text is increasingly
    becoming the go-to application of machine learning within the finance sector.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）正在逐步进入金融领域。例如，保险公司越来越倾向于自动处理索赔，而零售银行则试图简化客户服务并为客户提供更好的产品。文本理解正越来越成为金融领域中机器学习的首选应用。'
- en: Historically, NLP has relied on hand-crafted rules that were created by linguists.
    Today, the linguists are being replaced by neural networks that are able to learn
    the complex, and often hard to codify, rules of language.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，NLP依赖于语言学家手工创建的规则。如今，语言学家正被神经网络取代，这些神经网络能够学习复杂的、往往难以编码的语言规则。
- en: In this chapter, you will learn how to build powerful natural language models
    with Keras, as well as how to use the spaCy NLP library.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，您将学习如何使用Keras构建强大的自然语言模型，并了解如何使用spaCy NLP库。
- en: 'The focus of this chapter will be on the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点将放在以下内容：
- en: Fine-tuning spaCy's models for your own custom applications
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调spaCy的模型以适应您自己的自定义应用
- en: Finding parts of speech and mapping the grammatical structure of sentences
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找词性并映射句子的语法结构
- en: Using techniques such as Bag-of-Words and TF-IDF for classification
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用如词袋模型和TF-IDF等技术进行分类
- en: Understanding how to build advanced models with the Keras functional API
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何使用Keras函数式API构建高级模型
- en: Training models to focus with attention, as well as to translate sentences with
    a sequence to sequence (seq2seq) model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型聚焦注意力，以及使用序列到序列（seq2seq）模型翻译句子
- en: So, let's get started!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始吧！
- en: An introductory guide to spaCy
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: spaCy入门指南
- en: spaCy is a library for advanced NLP. The library, which is pretty fast to run,
    also comes with a range of useful tools and pretrained models that make NLP easier
    and more reliable. If you've installed Kaggle, you won't need to download spaCy,
    as it comes preinstalled with all the models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy是一个用于高级NLP的库。这个库运行速度非常快，同时还配备了一系列有用的工具和预训练模型，使得NLP变得更加简便和可靠。如果您已经安装了Kaggle，则无需单独下载spaCy，因为它已随所有模型一起预安装。
- en: To use spaCy locally, you will need to install the library and download its
    pretrained models separately.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 若要在本地使用spaCy，您需要分别安装该库并下载其预训练模型。
- en: 'To install the library, we simply need to run the following command:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装该库，只需运行以下命令：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: This chapter makes use of the English language models, but more are
    available. Most features are available in English, German, Spanish, Portuguese,
    French, Italian, and Dutch. Entity recognition is available for many more languages
    through the multi-language model.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：本章使用的是英文语言模型，但还有更多可用的模型。大多数功能在英语、德语、西班牙语、葡萄牙语、法语、意大利语和荷兰语中均可用。通过多语言模型，实体识别功能支持更多语言。'
- en: The core of spaCy is made up of the `Doc` and `Vocab` classes. A `Doc` instance
    contains one document, including its text, tokenized version, and recognized entities.
    The `Vocab` class, meanwhile, keeps track of all the common information found
    across documents.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy的核心由`Doc`和`Vocab`类组成。`Doc`实例包含一篇文档，包括其文本、分词版本和已识别的实体。而`Vocab`类则负责追踪文档中出现的所有共享信息。
- en: spaCy is useful for its pipeline features, which contain many of the parts needed
    for NLP. If this all seems a bit abstract right now, don't worry, as this section
    will show you how to use spaCy for a wide range of practical tasks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy因其管道特性而非常有用，这些特性包含了自然语言处理所需的许多部分。如果这些内容现在看起来有些抽象，别担心，本节将向你展示如何使用spaCy完成各种实用任务。
- en: Tip
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: You can find the data and code for this section on Kaggle at [https://www.kaggle.com/jannesklaas/analyzing-the-news](https://www.kaggle.com/jannesklaas/analyzing-the-news).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Kaggle上找到本节的相关数据和代码，地址是[https://www.kaggle.com/jannesklaas/analyzing-the-news](https://www.kaggle.com/jannesklaas/analyzing-the-news)。
- en: The data that we'll use for this first section is from a collection of 143,000
    articles taken from 15 American publications. The data is spread out over three
    files. We are going to load them separately, merge them into one large DataFrame,
    and then delete the individual DataFrames in order to save memory.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于本节的数据来自15家美国出版物的143,000篇文章。数据分布在三个文件中。我们将分别加载这些文件，将它们合并成一个大型数据框，然后删除单独的数据框以节省内存。
- en: 'To achieve this, we must run:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们必须运行以下代码：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As a result of running the preceding code, the data will end up looking like
    this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码后，数据将最终呈现如下所示：
- en: '| id | title | publication | author | date | year | month | url | content |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| id | 标题 | 出版物 | 作者 | 日期 | 年 | 月 | URL | 内容 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 17283 | House Republicans Fret... | New York Times | Carl Hulse | 2016-12-31
    | 2016.0 | 12.0 | NaN | WASHINGTON — Congressional Republicans... |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 17283 | 众议院共和党人担忧... | 《纽约时报》 | 卡尔·赫尔斯 | 2016-12-31 | 2016.0 | 12.0 | NaN
    | 华盛顿 — 国会共和党人... |'
- en: After getting our data to this state, we can then plot the distribution of publishers
    to get an idea of what kind of news we are dealing with.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据处理到此状态后，我们可以绘制出版商的分布图，从而了解我们处理的是哪种类型的新闻。
- en: 'To achieve this, we must run the following code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们必须运行以下代码：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After successfully running this code, we''ll see this chart showing the distribution
    of news sources from our dataset:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 成功运行此代码后，我们将看到一个展示数据集中新闻来源分布的图表：
- en: '![An introductory guide to spaCy](img/B10354_05_01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![spaCy简介指南](img/B10354_05_01.jpg)'
- en: News page distribution
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻页面分布
- en: As you can see in the preceding graph the dataset that we extracted contains
    no articles from classical financial news media, instead it mostly contains articles
    from mainstream and politically oriented publications.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，我们提取的数据集没有来自传统金融新闻媒体的文章，而是大多来自主流和政治导向的出版物。
- en: Named entity recognition
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: A common task in NLP is **named entity recognition** (**NER**). NER is all about
    finding things that the text explicitly refers to. Before discussing more about
    what is going on, let's jump right in and do some hands-on NER on the first article
    in our dataset.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理中的常见任务是**命名实体识别**（**NER**）。NER的核心是找到文本中明确提到的实体。在深入讨论发生了什么之前，我们先直接开始对数据集中的第一篇文章进行命名实体识别（NER）。
- en: 'The first thing we need to do is load spaCy, in addition to the model for English
    language processing:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是加载spaCy，并加载英语语言处理的模型：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we must select the text of the article from our data:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要从数据中选择文章的文本：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we''ll run this piece of text through the English language model pipeline.
    This will create a `Doc` instance, something we explained earlier on in this chapter.
    The file will hold a lot of information, including the named entities:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将把这段文本输入到英语语言模型管道中。这将创建一个`Doc`实例，这是我们在本章前面解释过的。该文件将包含大量信息，包括命名实体：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'One of the best features of spaCy is that it comes with a handy visualizer
    called `displacy`, which we can use to show the named entities in text. To get
    the visualizer to generate the display, based on the text from our article, we
    must run this code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy最棒的功能之一是它带有一个叫做`displacy`的可视化工具，我们可以用它来展示文本中的命名实体。为了让可视化工具基于我们文章中的文本生成展示，我们必须运行以下代码：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With that command now executed, we''ve done three important things, which are:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 执行完该命令后，我们完成了三项重要的工作，分别是：
- en: We've passed the document
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经传递了文档。
- en: We have specified that we would like to render entities
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经指定希望渲染实体。
- en: We let `displacy` know that we are running this in a Jupyter notebook so that
    rendering works correctly
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们让`displacy`知道我们正在Jupyter笔记本中运行，这样渲染才能正确工作。
- en: '![Named entity recognition](img/B10354_05_02.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![命名实体识别](img/B10354_05_02.jpg)'
- en: The output of the previous NER using spaCy tags
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 之前使用spaCy标签进行NER的输出
- en: And voilà! As you can see, there are a few mishaps, such as blank spaces being
    classified as organizations, and "Obama" being classified as a place.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 看！如你所见，存在一些小问题，例如空白被分类为组织，“奥巴马”被分类为地点。
- en: So, why has this happened? It's because the tagging has been done by a neural
    network and neural networks are strongly dependent on the data that they were
    trained on. So, because of these imperfections, we might find that we need to
    fine-tune the tagging model for our own purposes, and in a minute, we will see
    how that works.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么会发生这种情况呢？这是因为标注是通过神经网络完成的，而神经网络强烈依赖于它们训练时的数据。因此，由于这些不完美，我们可能会发现需要为我们的目的微调标注模型，稍后我们将看到它是如何工作的。
- en: You can also see in our output that NER offers a wide range of tags, some of
    which come with strange abbreviations. For now, don't worry as we will examine
    a full list of tags later on in this chapter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的输出中，你还可以看到NER提供了多种标签，其中一些标签带有奇怪的缩写。现在不必担心，因为我们将在本章后面检查一份完整的标签列表。
- en: 'Right now, let''s answer a different question: what organizations does the
    news in our dataset write about? To make this exercise run faster, we will create
    a new pipeline in which we will disable everything but NER.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回答另一个问题：我们数据集中报道了哪些组织？为了让这个练习运行得更快，我们将创建一个新的管道，其中只启用NER，禁用其他所有功能。
- en: 'To find out the answer to this question, we must first run the following code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出这个问题的答案，我们必须首先运行以下代码：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the next step, we''ll loop over the first 1,000 articles from our dataset,
    which can be done with the following code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将遍历数据集中的前1,000篇文章，可以通过以下代码完成：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The code we''ve just created has nine key points. Let''s take a minute to break
    it down, so we are confident in understanding what we''ve just written. Note that
    in the preceding code, the hashtag, `#`, refers to the number it relates to in
    this following list:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建的代码有九个关键点。让我们花点时间来分解它，这样我们可以确认自己理解了我们刚刚写的内容。请注意，在前面的代码中，井号`#`指的是它在下列清单中的对应编号：
- en: We get the content of the article at row `i`.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获取第`i`行的文章内容。
- en: We get the id of the article.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获取文章的id。
- en: We run the article through the pipeline.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过管道处理这篇文章。
- en: For all the entities found, we save the text, index of the first and last character,
    as well as the label. This only happens if the tag consists of more than white
    spaces and dashes. This removes some of the mishaps we encountered earlier when
    the classification tagged empty segments or delimiters.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于找到的所有实体，我们保存文本、首尾字符的索引以及标签。只有当标签不只是空格和破折号时，才会执行此操作。这消除了我们之前遇到的一些问题，当时分类错误地标注了空段落或分隔符。
- en: We create a pandas DataFrame out of the array of tuples created.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从创建的元组数组中创建一个pandas DataFrame。
- en: We add the id of the article to all records of our named entities.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将文章的id添加到我们所有的命名实体记录中。
- en: We add the DataFrame containing all the tagged entities of one document to a
    list. This way, we can build a collection of tagged entities over a larger number
    of articles.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将包含一个文档所有标注实体的DataFrame添加到一个列表中。通过这种方式，我们可以在更多的文章中构建标注实体的集合。
- en: We concatenate all DataFrames in the list, meaning that we create one big table
    with all tags.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将列表中的所有DataFrame连接起来，意味着我们创建了一个包含所有标签的大表。
- en: For easier use, we give the columns meaningful names
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了方便使用，我们给列赋予了有意义的名称。
- en: 'Now that we''ve done that, the next step is to plot the distribution of the
    types of entities that we found. This code will produce a chart which can be created
    with the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 做完这一切后，下一步是绘制我们发现的实体类型的分布图。以下代码将生成一个可以通过以下代码创建的图表：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output of the code being this graph:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的输出是这个图表：
- en: '![Named entity recognition](img/B10354_05_03.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![命名实体识别](img/B10354_05_03.jpg)'
- en: spaCy tag distribution
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy标签分布
- en: 'After seeing the preceding graph, it is a fair question to ask which categories
    spaCy can identify and where they come from. The English language NER that comes
    with spaCy is a neural network trained on the *OntoNotes 5.0 corpus*, meaning
    it can recognize the following categories:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**PERSON**: People, including fictional characters'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ORG**: Companies, agencies, institutions'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPE**: Places including countries, cities, and states'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DATE**: Absolute (for example, January 2017) or relative dates (for example,
    two weeks)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CARDINAL**: Numerals that are not covered by other types'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NORP**: Nationalities or religious or political groups'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ORDINAL**: "first," "second," and so on'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TIME**: Times shorter than a day (for example, two hours)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WORK_OF_ART**: Titles of books, songs, and so on'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LOC**: Locations that are not `GPE`s, for example, mountain ranges or streams'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MONEY**: Monetary values'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FAC**: Facilities such as airports, highways or bridges'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PERCENT**: Percentages'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EVENT**: Named hurricanes, battles, sporting events, and so on'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**QUANTITY**: Measurements such as weights or distance'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LAW**: Named documents that are laws'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PRODUCT**: Objects, vehicles, food, and so on'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LANGUAGE**: Any named language'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this list, we will now look at the 15 most frequently named organizations,
    categorized as ORG. As part of this, we will produce a similar graph showing us
    that information.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the graph, we must run the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The resulting code will give us the following graph:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![Named entity recognition](img/B10354_05_04.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: spaCy organization distance
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, political institutions such as the *senate* are most frequently
    named in our news dataset. Likewise, some companies, such as *Volkswagen*, that
    were in the center of media attention can also be found in the chart. Take a minute
    to also notice how **the White House** and **White House** are listed as two separate
    organizations, despite us knowing they are the same entity.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your needs, you might want to do some post-processing, such as removing
    "the" from organization names. Python comes with a built-in string replacement
    method that you can use with pandas. This would allow you to achieve post-processing.
    However, this is not something we will cover in depth here.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Should you want to look at it in more detail, you can get the documentation
    and example from the following link: [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.replace.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.replace.html)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Also, note how **Trump** is shown here as an organization. However, if you look
    at the tagged text, you will also see that "Trump" is tagged several times as
    an NORP, a political organization. This has happened because the NER infers the
    type of tag from the context. Since Trump is the U.S. president, his name often
    gets used in the same context as (political) organizations.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: This pretrained NER gives you a powerful tool that can solve many common NLP
    tasks. So, in reality, from here you could conduct all kinds of other investigations.
    For example, we could fork the notebook to see whether The New York Times is mentioned
    as different entities more often than the Washington Post or Breitbart.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预训练的NER为你提供了一个强大的工具，可以解决许多常见的NLP任务。所以，实际上，从这里你可以进行各种其他的调查。例如，我们可以分叉这个笔记本，看看《纽约时报》是否比《华盛顿邮报》或布赖特巴特（Breitbart）更多地被提及为不同的实体。
- en: Fine-tuning the NER
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调命名实体识别（NER）
- en: A common issue you may find is that the pretrained NER does not perform well enough
    on the specific types of text that you want it to work with. To solve this problem,
    you will need to fine-tune the NER model by training it with custom data. Achieving
    this will be the focus of this section.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现的一个常见问题是，预训练的NER在特定类型的文本上表现不够好。为了解决这个问题，你需要通过用自定义数据训练它来微调NER模型。实现这一目标将是本节的重点。
- en: 'The training data you''re using should be in a form like this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用的训练数据应该是以下格式：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see, you provide a list of tuples of the string, together with the
    start and end points, as well as the types of entities you want to tag. Data such
    as this is usually collected through manual tagging, often on platforms such as
    Amazon's **Mechanical Turk** (**MTurk**).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，你提供了一个包含字符串、起始和结束点以及你想要标注的实体类型的元组列表。此类数据通常通过人工标注收集，常见的平台包括亚马逊的**Mechanical
    Turk**（**MTurk**）。
- en: The company behind spaCy, Explosion AI, also make a (paid) data tagging system
    called *Prodigy*, which enables efficient data collection. Once you have collected
    enough data, you can either fine-tune a pretrained model or initialize a completely
    new model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy背后的公司Explosion AI还提供了一个（付费的）数据标注系统，叫做*Prodigy*，它可以实现高效的数据收集。一旦你收集了足够的数据，你可以微调一个预训练模型，或者初始化一个完全新的模型。
- en: 'To load and fine-tune a model, we need to use the `load()` function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载并微调模型，我们需要使用`load()`函数：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Alternatively, to create a new and empty model from scratch that is ready for
    the English language, use the `blank` function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，要从头开始创建一个新的空白模型并准备好用于英语语言，可以使用`blank`函数：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Either way, we need to get access to the NER component. If you have created
    a blank model, you'll need to create an NER pipeline component and add it to the
    model.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，我们都需要访问NER组件。如果你创建了一个空白模型，你需要创建一个NER管道组件并将其添加到模型中。
- en: 'If you have loaded an existing model, you can just access its existing NER
    by running the following code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你加载了一个现有的模型，可以通过运行以下代码直接访问其现有的NER：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The next step is to ensure that our NER can recognize the labels we have. Imagine
    our data contained a new type of named entity such as `ANIMAL`. With the `add_label`
    function, we can add a label type to an NER.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是确保我们的NER能够识别我们所拥有的标签。假设我们的数据包含了一种新的命名实体类型，比如`ANIMAL`。通过`add_label`函数，我们可以向NER添加一个标签类型。
- en: 'The code to achieve this can be seen below, but don''t worry if it doesn''t
    make sense right now, we''ll break it down on the next page:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点的代码如下所示，但如果现在不太明白也没关系，我们会在下一页详细解释：
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'What we''ve just written is made up of 10 key elements:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚写的内容由10个关键元素组成：
- en: We disable all pipeline components that are not the NER by first getting a list
    of all the components that are not the NER and then disabling them for training.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们禁用所有不是NER的管道组件，首先获取所有不是NER的组件列表，然后在训练中禁用它们。
- en: Pretrained models come with an optimizer. If you have a blank model, you will
    need to create a new optimizer. Note that this also resets the model weights.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预训练模型自带优化器。如果你有一个空白模型，你需要创建一个新的优化器。请注意，这也会重置模型的权重。
- en: We now train for a number of epochs, in this case, 5.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在进行多次训练周期，在这种情况下是5个。
- en: At the beginning of each epoch, we shuffle the training data using Python's
    built-in `random` module.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个训练周期的开始，我们使用Python内置的`random`模块对训练数据进行洗牌。
- en: We create an empty dictionary to keep track of the losses.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个空字典来跟踪损失值。
- en: We then loop over the text and annotations in the training data.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们遍历训练数据中的文本和注释。
- en: '`nlp.update` performs one forward and backward pass, and updates the neural
    network weights. We need to supply text and annotations, so that the function
    can figure out how to train a network from it.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nlp.update`执行一次前向和反向传播，并更新神经网络的权重。我们需要提供文本和注释，以便该函数可以根据这些信息来训练网络。'
- en: We can manually specify the dropout rate we want to use while training.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以手动指定训练时要使用的dropout率。
- en: We pass a stochastic gradient descent optimizer that performs the model updates.
    Note that you cannot just pass a Keras or TensorFlow optimizer here, as spaCy
    has its own optimizers.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can also pass a dictionary to write losses that we can later print to monitor
    progress.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you''ve run the code, the output should look something like this:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: What you are seeing is the loss value of a part of the spaCy pipeline, in this
    case, the **named entity recognition** (**NER**) engine. Similar to the cross-entropy
    loss we discussed in previous chapters, the actual value is hard to interpret
    and does not tell you very much. What matters here is that the loss is decreasing
    over time and that it reaches a value much lower than the initial loss.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech (POS) tagging
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On Tuesday, October 10, 2017, between 9:34 AM and 9:36 AM, the US Dow Jones
    newswire encountered a technical error that resulted in it posting some strange
    headlines. One of them was, "Google to buy Apple." These four words managed to
    send Apple stock up over two percent.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: The algorithmic trading systems obviously failed here to understand that such
    an acquisition would be impossible as Apple had a market capitalization of $800
    billion at the time, coupled with the fact that the move would likely not find
    regulatory approval.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: So, the question arises, why did the trading algorithms choose to buy stock
    based on these four words? The answer is through **part-of-speech** (**POS**)
    tagging. POS tagging allows an understanding of which words take which function
    in a sentence and how the words relate to each other.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'spaCy comes with a handy, pretrained POS tagger. In this section we''re going
    to apply this to the Google/Apple news story. To start the POS tagger, we need
    to run the following code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Again, we will load the pretrained English model and run our sentence through
    it. Then we'll use `displacy` just as we did for NER.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the graphics fit better in this book, we will set the `distance` option
    to something shorter than the default, in this case, 1,120, so that words get
    displayed closer together, as we can see in the following diagram:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![Part-of-speech (POS) tagging](img/B10354_05_05.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: spaCy POS tagger
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the POS tagger identified **buy** as a verb and **Google** and
    **Apple** as the nouns in the sentence. It also identified that **Apple** is the
    object the action is applied to and that **Google** is applying the action.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'We can access this information for nouns through this code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After running the preceding code, we get the following table featured as the
    result:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '| Text | Root Text | Root dep | Root Head Text |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| Google | Google | ROOT | Google |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| Apple | Apple | dobj | buy |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: In our example, Google is the root of the sentence, while Apple is the object
    of the sentence. The verb applied to Apple is "buy."
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: From there, it is only a hard-coded model of price developments under an acquisition
    (demand for the target stock goes up and with it the price) and a stock lookup
    table to a simple event-driven trading algorithm. Making these algorithms understand
    the context and plausibility is another story, however.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，它只不过是一个硬编码的模型，用于描述收购过程中的价格变化（目标股票的需求上升，价格随之上升），以及一个股票查询表，最终成为一个简单的事件驱动的交易算法。然而，让这些算法理解上下文和合理性是另一回事。
- en: Rule-based matching
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于规则的匹配
- en: Before deep learning and statistical modeling took over, NLP was all about rules.
    That's not to say that rule-based systems are dead! They are often easy to set
    up and perform very well when it comes to doing simple tasks.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习和统计建模占主导地位之前，自然语言处理（NLP）完全是基于规则的。并不是说基于规则的系统已经死掉了！它们通常易于设置，并且在执行简单任务时表现非常好。
- en: Imagine you wanted to find all mentions of Google in a text. Would you really
    train a neural network-based named entity recognizer? If you did, you would have
    to run all of the text through the neural network and then look for Google in
    the entity texts. Alternatively, would you rather just search for text that exactly
    matches Google with a classic search algorithm? Well, we're in luck, as spaCy
    comes with an easy-to-use, rule-based matcher that allows us to do just that.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你想在文本中找到所有提到Google的地方。你真的会训练一个基于神经网络的命名实体识别器吗？如果是这样，你必须将所有文本通过神经网络处理，然后在实体文本中查找Google。或者，你宁愿使用经典的搜索算法直接搜索与Google完全匹配的文本吗？幸运的是，spaCy提供了一个易于使用的基于规则的匹配器，让我们能够做到这一点。
- en: 'Before we start this section, we first must make sure that we reload the English
    language model and import the matcher. This is a very simple task that can be
    done by running the following code:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本节之前，我们首先必须确保重新加载英语语言模型并导入匹配器。这是一个非常简单的任务，可以通过运行以下代码来完成：
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The matcher searches for patterns, which we encode as a list of dictionaries.
    It operates token by token, that is, word for word, except for punctuation and
    numbers, where a single symbol can be a token.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配器搜索模式，我们将其编码为字典列表。它按标记逐个操作，也就是说，逐字匹配，标点符号和数字除外，其中一个符号也可以是一个标记。
- en: 'As a starting example, let''s search for the phrase "hello, world." To do this,
    we would define a pattern as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个入门示例，让我们搜索短语"hello, world"。为了做到这一点，我们可以定义一个模式，如下所示：
- en: '[PRE20]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This pattern is fulfilled if the lower case first token is `hello`. The `LOWER`
    attribute checks if both words would match if they were both converted to lowercase.
    That means if the actual token text is "Hello" or "HELLO," then it would also
    fulfill the requirement. The second token has to be punctuation to pick up the
    comma, so the phrases "hello. world" or "hello! world" would both work, but not
    "hello world."
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果第一个小写字母的标记是`hello`，则满足此模式。`LOWER`属性检查如果两个单词都转换为小写，是否会匹配。这意味着如果实际的标记文本是"Hello"或"HELLO"，那么它也会满足要求。第二个标记必须是标点符号来匹配逗号，因此像"hello.
    world"或"hello! world"这样的短语都有效，但"hello world"则不行。
- en: The lower case of the third token has to be "world," so "WoRlD" would also be
    fine.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个标记的小写字母必须是"world"，因此"WoRlD"也是可以的。
- en: 'The possible attributes for a token can be the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 标记的可能属性包括以下几种：
- en: '`ORTH`: The token text has to match exactly'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ORTH`：标记文本必须完全匹配'
- en: '`LOWER`: The lower case of the token has to match'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LOWER`：标记的小写字母必须匹配'
- en: '`LENGTH`: The length of the token text has to match'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LENGTH`：标记文本的长度必须匹配'
- en: '`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`: The token text has to consist of alphanumeric
    characters, ASCII symbols, or digits'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IS_ALPHA`、`IS_ASCII`、`IS_DIGIT`：标记文本必须由字母数字字符、ASCII符号或数字组成'
- en: '`IS_LOWER`, `IS_UPPER`, `IS_TITLE`: The token text has to be lower case, upper case,
    or title case'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IS_LOWER`、`IS_UPPER`、`IS_TITLE`：标记文本必须是小写、大写或标题大小写'
- en: '`IS_PUNCT`, `IS_SPACE`, `IS_STOP`: The token text has to be punctuation, white space,
    or a stop word'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IS_PUNCT`、`IS_SPACE`、`IS_STOP`：标记文本必须是标点符号、空白或停用词'
- en: '`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`: The token has to resemble a number, URL, or email'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LIKE_NUM`、`LIKE_URL`、`LIKE_EMAIL`：标记必须像数字、URL或电子邮件'
- en: '`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`: The token''s position, tag, dependency,
    lemma, or shape has to match'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`POS`、`TAG`、`DEP`、`LEMMA`、`SHAPE`：标记的位置、标签、依赖关系、词元或形态必须匹配'
- en: '`ENT_TYPE`: The token''s entity type from NER has to match'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ENT_TYPE`：标记的实体类型必须与命名实体识别（NER）中的类型匹配'
- en: spaCy's lemmatization is extremely useful. A lemma is the base version of a
    word. For example, "was" is a version of "be," so "be" is the lemma for "was"
    but also for "is." spaCy can lemmatize words in context, meaning it uses the surrounding
    words to determine what the actual base version of a word is.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy的词形还原功能非常有用。词根是一个词的基本形式。例如，“was”是“be”的一种形式，所以“be”是“was”的词根，也适用于“is”。spaCy可以根据上下文进行词形还原，这意味着它使用周围的词语来确定一个词的实际基本形式。
- en: 'To create a matcher, we have to pass on the vocabulary the matcher works on.
    In this case, we can just pass the vocabulary of our English language model by
    running the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个匹配器，我们必须传递匹配器操作的词汇表。在这个例子中，我们可以通过运行以下命令传递我们英语语言模型的词汇表：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In order to add the required attributes to our matcher, we must call the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了向我们的匹配器添加所需的属性，我们必须调用以下命令：
- en: '[PRE22]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `add` function expects three arguments. The first is a name of the pattern,
    in this case, `HelloWorld`, so that we can keep track of the patterns we added.
    The second is a function that can process matches once found. Here we pass `None`,
    meaning no function will be applied, though we will use this tool later. Finally,
    we need to pass the list of token attributes we want to search for.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`add`函数需要三个参数。第一个是模式的名称，在这里是`HelloWorld`，以便我们跟踪已添加的模式。第二个是一个可以处理找到的匹配项的函数。在这里我们传递`None`，这意味着不会应用任何函数，尽管我们稍后会使用这个工具。最后，我们需要传递要搜索的词元属性列表。'
- en: 'To use our matcher, we can simply call `matcher(doc)`. This will give us back
    all the matches that the matcher found. We can call this by running the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用我们的匹配器，我们可以简单地调用`matcher(doc)`。这将返回匹配器找到的所有匹配项。我们可以通过运行以下命令来调用它：
- en: '[PRE23]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If we print out the matches, we can see the structure:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印出匹配项，我们可以看到其结构：
- en: '[PRE24]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The first thing in a match is the hash of the string found. This is just to
    identify what was found internally; we won't use it here. The next two numbers
    indicate the range in which the matcher found something, here tokens 0 to 3.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配的第一件事是找到的字符串的哈希值。这只是为了在内部标识找到的内容；在这里我们不会使用它。接下来的两个数字表示匹配器找到内容的范围，这里是从0到3的词元。
- en: 'We can get the text back by indexing the original document:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过索引原始文档来获取文本：
- en: '[PRE25]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the next section we will look at how we can add custom functions to matchers.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看看如何将自定义函数添加到匹配器中。
- en: Adding custom functions to matchers
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向匹配器添加自定义函数
- en: Let's move on to a more complex case. We know that the iPhone is a product.
    However, the neural network-based matcher often classifies it as an organization.
    This happens because the word "iPhone" gets used a lot in a similar context as organizations,
    as in "The iPhone offers..." or "The iPhone sold...."
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们转向一个更复杂的例子。我们知道iPhone是一个产品。然而，基于神经网络的匹配器经常将其分类为一个组织。这是因为“iPhone”一词在类似的上下文中经常用作组织名称，如“iPhone提供……”或“iPhone出售……”。
- en: Let's build a rule-based matcher that always classifies the word "iPhone" as a product
    entity.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个基于规则的匹配器，始终将“iPhone”分类为产品实体。
- en: First, we have to get the hash of the word PRODUCT. Words in spaCy can be uniquely
    identified by their hash. Entity types also get identified by their hash. To set an
    entity of the product type, we have to be able to provide the hash for the entity
    name.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须获取“PRODUCT”一词的哈希值。spaCy中的单词可以通过它们的哈希值唯一标识。实体类型也通过它们的哈希值来标识。为了设置一个产品类型的实体，我们必须能够提供该实体名称的哈希值。
- en: 'We can get the name from the language model''s vocabulary by running the following:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下命令从语言模型的词汇表中获取名称：
- en: '[PRE27]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we need to define an `on_match` rule. This function will be called every
    time the matcher finds a match. `on_match` rules have four arguments:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义一个`on_match`规则。每次匹配器找到匹配项时，都会调用此函数。`on_match`规则有四个参数：
- en: '`matcher`: The matcher that made the match.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matcher`: 进行匹配的匹配器。'
- en: '`doc`: The document the match was made in.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc`: 进行匹配的文档。'
- en: '`i`: The index of a match. The first match in a document would have index zero,
    the second would have index one, and so on.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`i`: 匹配项的索引。文档中的第一个匹配项的索引为零，第二个匹配项的索引为一，以此类推。'
- en: '`matches`: A list of all matches made.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matches`: 所有匹配项的列表。'
- en: 'There are two things happening in our `on_match` rule:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`on_match`规则中有两件事在发生：
- en: '[PRE28]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s break down what they are:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解它们是什么：
- en: We index all matches to find our match at index `i`. One match is a tuple of a `match_id`,
    the start of the match, and the end of the match.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们索引所有匹配项，以便找到索引为`i`的匹配项。一个匹配项是一个包含`match_id`、匹配开始位置和匹配结束位置的元组。
- en: We add a new entity to the document's named entities. An entity is a tuple of
    the hash of the type of entity (the hash of the word `PRODUCT` here), the start
    of the entity, and the end of the entity. To append an entity, we have to nest
    it in another tuple. Tuples that only contain one value need to include a comma
    at the end. It is important not to overwrite `doc.ents`, as we otherwise would
    remove all the entities that we have already found.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have an `on_match` rule, we can define our matcher.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'We should note that matchers allow us to add multiple patterns, so we can add
    a matcher for just the word "iPhone" and another pattern for the word "iPhone"
    together with a version number, as in "iPhone 5":'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: So, what makes these commands work?
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: We define the first pattern.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We define the second pattern.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a new empty matcher.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We add the patterns to the matcher. Both will fall under the rule called `iPhone`,
    and both will call our `on_match` rule called `add_product_ent`.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will now pass one of the news articles through the matcher:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This code is relatively simple, with only two steps:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: We run the text through the pipeline to create an annotated document.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We run the document through the matcher. This modifies the document created
    in the step before. We do not care as much about the matches but more about how
    the `on_match` method adds the matches as entities to our documents.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that the matcher is set up, we need to add it to the pipeline so that spaCy
    can use it automatically. This will be the focus in the next section.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Adding the matcher to the pipeline
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Calling the matcher separately is somewhat cumbersome. To add it to the pipeline,
    we have to wrap it into a function, which we can achieve by running the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The spaCy pipeline calls the components of the pipeline as functions and always
    expects the annotated document to be returned. Returning anything else could break the
    pipeline.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then add the matcher to the main pipeline, as can be seen in the following code:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The matcher is now the last piece of the pipeline. From this point onward iPhones
    will now get tagged based on the matcher's rules.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'And boom! All mentions of the word "iPhone" (case independent) are now tagged
    as named entities of the product type. You can validate this by displaying the
    entities with `displacy` as we have done in the following code:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The results of that code can be seen in the following screenshot:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding the matcher to the pipeline](img/B10354_05_06.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: spaCy now finds the iPhone as a product
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Combining rule-based and learning-based systems
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One especially interesting aspect of spaCy's pipeline system is that it is relatively
    easy to combine different aspects of it. We can, for example, combine neural network-based
    named entity recognition with a rule-based matcher in order to find something
    such as executive compensation information.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Executive compensation is often reported in the press but hard to find in aggregate.
    One possible rule-based matching pattern for executive compensation could look
    like this:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: A matcher looking for this pattern would pick up any combination of a person's
    name, for example, John Appleseed, or Daniel; any version of the word receive,
    for example, received, receives, and so on; followed by an expression of money,
    for example, $4 million.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: This matcher could be run over a large text corpus with the `on_match` rule
    handily saving the found snippets into a database. The machine learning approach
    for naming entities and the rule-based approach go hand in hand seamlessly.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Since there is much more training data available with annotations for names
    and money, rather than statements about executive education, it is much easier
    to combine the NER with a rule-based method rather than training a new NER.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regular expressions, or regexes, are a powerful form of rule-based matching.
    Invented back in the 1950s, they were, for a very long time, the most useful way
    to find things in text and proponents argue that they still are.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: No chapter on NLP would be complete without mentioning regexes. With that being
    said, this section is by no means a complete regex tutorial. It's intended to
    introduce the general idea and show how regexes can be used in Python, pandas,
    and spaCy.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: A very simple regex pattern could be "a." This would only find instances of
    the lower-case letter a followed by a dot. However, regexes also allow you to
    add ranges of patterns; for example, "[a-z]." would find any lower-case letter
    followed by a dot, and "xy." would find only the letters "x" or "y" followed by
    a dot.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Regex patterns are case sensitive, so "A-Z" would only capture upper-case letters.
    This is useful if we are searching for expressions in which the spelling is frequently
    different; for example, the pattern "seriali[sz]e" would catch the British as
    well as the American English version of the word.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The same goes for numbers. "0-9" captures all numbers from 0 to 9\. To find
    repetitions, you can use "*," which captures zero or more occurrences, or "+,"
    which captures one or more occurrences. For example, "[0-9]+" would capture any
    series of numbers, which might be useful when looking for years. While "[A-Z][a-z]
    + [0-9] +," for example, would find all words starting with a capital letter followed
    by one or more digit, such as "March 2018" but also "Jaws 2."
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Curly brackets can be used to define the number of repetitions. For instance,
    "[0-9]{4}" would find number sequences with exactly four digits. As you can see,
    a regex does not make any attempt to understand what is in the text, but rather
    offers a clever method of finding text that matches patterns.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'A practical use case in the financial industry is finding the VAT number of
    companies in invoices. These follow a pretty strict pattern in most countries
    that can easily be encoded. VAT numbers in the Netherlands, for example, follow
    this regex pattern: "NL[0-9]{9}B[0-9]{2}".'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Using Python's regex module
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Python has a built-in tool for regexes called `re`. While it does not need
    to be installed because it is part of Python itself, we can import it with the
    following code:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Imagine we are working on an automatic invoice processor, and we want to find
    the VAT number of the company that sent us the invoice. For simplicity''s sake,
    we''re going to only deal with Dutch VAT numbers (the Dutch for "VAT" is "BTW").
    As mentioned before, we know the pattern for a Dutch VAT number is as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'A string for finding a BTW number might look like this:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'So, to find all the occurrences of a BTW number in the string, we can call
    `re.findall`, which will return a list of all strings matching the pattern found.
    To call this, we simply run:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '`re` also allows the passing of flags to make the development of regex patterns
    a bit easier. For example, to ignore the case of letters when matching a regular
    expression, we can add a `re.IGNORECASE` flag, like we''ve done here:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Often, we are interested in a bit more information about our matches. To this
    end, there is a `match` object. `re.search` yields a `match` object for the first
    match found:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can get more information out of this object, such as the location of our
    match, simply by running:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The span, the start and the end of our match, is the characters 18 to 32.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Regex in pandas
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The data for NLP problems often comes in pandas DataFrames. Luckily for us,
    pandas natively supports regex. If, for example, we want to find out whether any
    of the articles in our news dataset contain a Dutch BTW number, then we can pass
    the following code:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This would yield all the articles that include a Dutch BTW number, but unsurprisingly
    no articles in our dataset do.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: When to use regexes and when not to
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A regex is a powerful tool, and this very short introduction does not do it
    justice. In fact, there are several books longer than this one written purely
    on the topic of regexes. However, for the purpose of this book, we're only going
    to briefly introduce you to the topic.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: A regex, as a tool, works well on simple and clear-to-define patterns. VAT/BTW
    numbers are a perfect example, as are email addresses and phone numbers, both
    of which are very popular use cases for regexes. However, a regex fails when the
    pattern is hard to define or if it can only be inferred from context. It is not
    possible to create a rule-based named entity recognizer that can spot that a word
    refers to the name of a person, because names follow no clear distinguishing pattern.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: So, the next time you are looking to find something that is easy to spot for
    a human but hard to describe in rules, use a machine learning-based solution.
    Likewise, the next time you are looking for something clearly encoded, such as
    a VAT number, use regexes.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: A text classification task
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common NLP task is to classify text. The most common text classification is
    done in sentiment analysis, where texts are classified as positive or negative.
    In this section, we will consider a slightly harder problem, classifying whether
    a tweet is about an actual disaster happening or not.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Today, investors have developed a number of ways to gain information from tweets.
    Twitter users are often faster than news outlets to report disasters, such as
    a fire or a flood. In the case of finance, this speed advantage can be used and
    translated to event-driven trading strategies.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: However, not all tweets that contain words associated with disasters are actually
    about disasters. A tweet such as, "California forests on fire near San Francisco"
    is a tweet that should be taken into consideration, whereas "California this weekend
    was on fire, good times in San Francisco" can safely be ignored.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the task here is to build a classifier that separates the tweets
    that relate to real disasters from irrelevant tweets. The dataset that we are
    using consists of hand-labeled tweets that were obtained by searching Twitter
    for words common to disaster tweets such as "ablaze" or "fire."
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: In preparation for this section, the code and data can be found on
    Kaggle at [https://www.kaggle.com/jannesklaas/nl](https://www.kaggle.com/jannesklaas/nl).'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preparing the text is a task in its own right. This is because in the real world,
    text is often messy and cannot be fixed with a few simple scaling operations.
    For instance, people can often make typos after adding unnecessary characters
    as they are adding text encodings that we cannot read. NLP involves its own set
    of data cleaning challenges and techniques.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Sanitizing characters
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To store text, computers need to encode the characters into bits. There are
    several different ways to do this, and not all of them can deal with all the characters
    out there.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: It is good practice to keep all the text files in one encoding scheme, usually
    UTF-8, but of course, that does not always happen. Files might also be corrupted,
    meaning that a few bits are off, therefore rendering some characters unreadable.
    Therefore, before we do anything else, we need to sanitize our inputs.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Python offers a helpful `codecs` library, which allows us to deal with different
    encodings. Our data is UTF-8 encoded, but there are a few special characters in
    there that cannot be read easily. Therefore, we have to sanitize our text of these
    special characters, which we can do by running the following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In the preceding code, `codecs.open` acts as a stand-in replacement for Python's
    standard file opening function. It returns a file object, which we can later read
    line by line. We specify the input path that we want to read the file (with `r`),
    the expected encoding, and what to do with errors. In this case, we are going
    to replace the errors with a special unreadable character marker.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'To write to the output file, we can just use Python''s standard `open()` function.
    This function will create a file at the specified file path we can write to:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now that''s done, all we have to do is loop over the lines in our input file
    that we read with our `codecs` reader and save it as a regular CSV file again.
    We can achieve this by running the following:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Likewise, it''s good practice to close the file objects afterward, which we
    can do by running:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now we can read the sanitized CSV file with pandas:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Lemmatization
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lemmas have already made several appearances throughout this chapter. A lemma
    in the field of linguistics, also called a headword, is the word under which the
    set of related words or forms appears in a dictionary. For example, "was" and
    "is" appear under "be," "mice" appears under "mouse," and so on. Quite often,
    the specific form of a word does not matter very much, so it can be a good idea
    to convert all your text into its lemma form.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'spaCy offers a handy way to lemmatize text, so once again, we''re going to
    load a spaCy pipeline. Only that in this case, we don''t need any pipeline module
    aside from the tokenizer. The tokenizer splits the text into separate words, usually
    by spaces. These individual words, or tokens, can then be used to look up their
    lemma. In our case, it looks like this:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Lemmatization can be slow, especially for big files, so it makes sense to track
    our progress. `tqdm` allows us to show progress bars on the pandas `apply` function.
    All we have to do is import `tqdm` as well as the notebook component for pretty
    rendering in our work environment. We then have to tell `tqdm` that we would like
    to use it with pandas. We can do this by running the following:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We can now run `progress_apply` on a DataFrame just as we would use the standard
    `apply` method, but here it has a progress bar.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'For each row, we loop over the words in the `text` column and save the lemma
    of the word in a new `lemmas` column:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Our `lemmas` column is now full of lists, so to turn the lists back into text,
    we will join all of the elements of the lists with a space as a separator, as
    we can see in the following code:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Preparing the target
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several possible prediction targets in this dataset. In our case,
    humans were asked to rate a tweet, and, they were given three options, `Relevant`,
    `Not Relevant`, and `Can''t Decide`, as the lemmatized text shows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The tweets where humans cannot decide whether it is about a real disaster are
    not interesting to us. Therefore, we will just remove the category, *Can''t Decide*,
    which we can do in the following code:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We are also only interested in mapping text to relevance, therefore we can
    drop all the other metadata and just keep these two columns, which we do here:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Finally, we''re going to convert the target into numbers. This is a binary
    classification task, as there are only two categories. So, we map `Relevant` to
    `1` and `Not Relevant` to `0`:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Preparing the training and test sets
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we start building models, we''re going to split our data into two sets,
    the training dataset and the test dataset. To do this we simply need to run the
    following code:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Bag-of-words
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A simple yet effective way of classifying text is to see the text as a bag-of-words.
    This means that we do not care for the order in which words appear in the text,
    instead we only care about which words appear in the text.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: One of the ways of doing a bag-of-words classification is by simply counting
    the occurrences of different words from within a text. This is done with a so-called
    **count vector**. Each word has an index, and for each text, the value of the
    count vector at that index is the number of occurrences of the word that belong
    to the index.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Picture this as an example: the count vector for the text "I see cats and dogs
    and elephants" could look like this:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '| i | see | cats | and | dogs | elephants |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 | 2 | 1 | 1 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: In reality, count vectors are pretty sparse. There are about 23,000 different
    words in our text corpus, so it makes sense to limit the number of words we want
    to include in our count vectors. This could mean excluding words that are often
    just gibberish or typos with no meaning. As a side note, if we kept all the rare
    words, this could be a source of overfitting.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'We are using `sklearn`''s built-in count vectorizer. By setting `max_features`,
    we can control how many words we want to consider in our count vector. In this
    case, we will only consider the 10,000 most frequent words:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Our count vectorizer can now transform texts into count vectors. Each count
    vector will have 10,000 dimensions:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Once we have obtained our count vectors, we can then perform a simple logistic
    regression on them. While we could use Keras for logistic regression, as we did
    in the first chapter of this book, it is often easier to just use the logistic
    regression class from scikit-learn:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now that we have predictions from our logistic regressor, we can measure the
    accuracy of it with `sklearn`:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: As you can see, we've got 80% accuracy, which is pretty decent for such a simple
    method. A simple count vector-based classification is useful as a baseline for
    more advanced methods, which we will be discussing later.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**TF-IDF** stands for **Term Frequency, Inverse Document Frequency**. It aims
    to address a problem of simple word counting, that being words that frequently
    appear in a text are important, while words that appear in *all* texts are not
    important.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: The TF component is just like a count vector, except that TF divides the counts
    by the total number of words in a text. Meanwhile, the IDF component is the logarithm
    of the total number of texts in the entire corpus divided by the number of texts
    that include a specific word.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF is the product of these two measurements. TF-IDF vectors are like count
    vectors, except they contain the TF-IDF scores instead of the counts. Rare words
    will gain a high score in the TF-IDF vector.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'We create TF-IDF vectors just as we created count vectors with `sklearn`:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Once we have the TF-IDF vectors, we can train a logistic regressor on them
    just like we did for count vectors:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'In this case, TF-IDF does slightly worse than count vectors. However, because
    the performance difference is very small, this poorer performance might be attributable
    to chance in this case:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Topic modeling
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A final, very useful application of word counting is topic modeling. Given a
    set of texts, are we able to find clusters of topics? The method to do this is
    called **Latent Dirichlet Allocation** (**LDA**).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: The code and data for this section can be found on Kaggle at [https://www.kaggle.com/jannesklaas/topic-modeling-with-lda](https://www.kaggle.com/jannesklaas/topic-modeling-with-lda).'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'While the name is quite a mouth full, the algorithm is a very useful one, so
    we will look at it step by step. LDA makes the following assumption about how
    texts are written:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: First, a topic distribution is chosen, say 70% machine learning and 30% finance.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, the distribution of words for each topic is chosen. For example, the topic
    "machine learning" might be made up of 20% the word "tensor," 10% the word "gradient,"
    and so on. This means that our topic distribution is a *distribution of distributions*,
    also called a Dirichlet distribution.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the text gets written, two probabilistic decisions are made for each word:
    first, a topic is chosen from the distribution of topics in the document. Then,
    a word is chosen for the distribution of words in that document.'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that not all documents in a corpus have the same distribution of topics.
    We need to specify a fixed number of topics. In the learning process, we start
    out by assigning each word in the corpus randomly to one topic. For each document,
    we then calculate the following:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic modeling](img/B10354_05_001.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
- en: 'The preceding formula is the probability of each topic, *t,* to be included
    in document *d*. For each word, we then calculate:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic modeling](img/B10354_05_002.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
- en: 'That is the probability of a word, *w,* to belong to a topic, *t*. We then
    assign the word to a new topic, *t,* with the following probability:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic modeling](img/B10354_05_003.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
- en: In other words, we assume that all of the words are already correctly assigned
    to a topic except for the word currently under consideration. We then try to assign
    words to topics to make documents more homogenous in their topic distribution.
    This way, words that actually belong to a topic cluster together.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn offers an easy-to-use LDA tool that will help us achieve this.
    To use this, we must first create a new LDA analyzer and specify the number of
    topics, called components that we expect.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be done by simply running the following:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We then create count vectors, just as we did for the bag-of-words analysis.
    For LDA, it is important to remove frequent words that don''t mean anything, such
    as "an" or "the," so-called stop words. `CountVectorizer` comes with a built-in
    stopword dictionary that removes these words automatically. To use this, we''ll
    need to run the following code:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Next, we fit the LDA to the count vectors:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'To inspect our results, we can print out the most frequent words for each topic.
    To this end, we first need to specify the number of words per topic we want to
    print, in this case 5. We also need to extract the mapping word count vector indices
    to words:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now we can loop over the topics of the LDA, in order to print the most frequent
    words:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: As you can see, the LDA seems to have discovered the grouping into serious tweets
    and non-serious ones by itself without being given the targets.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: This method is very useful for classifying news articles, too. Back in the world
    of finance, investors might want to know if there is a news article mentioning
    a risk factor they are exposed to. The same goes for support requests for consumer-facing
    organizations, which can be clustered this way.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The order of words in a text matters. Therefore, we can expect higher performance
    if we do not just look at texts in aggregate but see them as a sequence. This
    section makes use of a lot of the techniques discussed in the previous chapter;
    however, here we're going to add a critical ingredient, word vectors.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Words and word tokens are categorical features. As such, we cannot directly
    feed them into a neural network. Previously, we have dealt with categorical data
    by turning it into one-hot encoded vectors. Yet for words, this is impractical.
    Since our vocabulary is 10,000 words, each vector would contain 10,000 numbers
    that are all zeros except for one. This is highly inefficient, so instead, we
    will use an embedding.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: In practice, embeddings work like a lookup table. For each token, they store
    a vector. When the token is given to the embedding layer, it returns the vector
    for that token and passes it through the neural network. As the network trains,
    the embeddings get optimized as well.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Remember that neural networks work by calculating the derivative of the loss
    function with respect to the parameters (weights) of the model. Through backpropagation,
    we can also calculate the derivative of the loss function with respect to the
    input of the model. Thus we can optimize the embeddings to deliver ideal inputs
    that help our model.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing for training with word vectors
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we start with training word embeddings, we need to do some preprocessing
    steps. Namely, we need to assign each word token a number and create a NumPy array
    full of sequences.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Assigning numbers to tokens makes the training process smoother and decouples
    the tokenization process from the word vectors. Keras has a `Tokenizer` class,
    which can create numeric tokens for words. By default, this tokenizer splits text
    by spaces. While this works mostly fine in English, it can be problematic and
    cause issues in other languages. A key learning point to take away is that it's
    better to tokenize the text with spaCy first, as we already did for our two previous
    methods, and then assign numeric tokens with Keras.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Tokenizer` class also allows us to specify how many words we want to consider,
    so once again we will only use the 10,000 most used words, which we can specify
    by running:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The tokenizer works a lot like `CountVectorizer` from `sklearn`. First, we
    create a new `tokenizer` object. Then we fit the tokenizer, and finally, we can
    transform the text into tokenized sequences:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The `sequences` variable now holds all of our texts as numeric tokens. We can
    look up the mapping of words to numbers from the tokenizer''s word index with
    the following code:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: As you can see, frequently used words such as "the" have lower token numbers
    than less frequent words such as "movie." You can also see that `word_index` is
    a dictionary. If you are using your model in production, you can save this dictionary
    to disk in order to convert words into tokens at a later time.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need to turn our sequences into sequences of equal length. This
    is not always necessary, as some model types can deal with sequences of different
    lengths, but it usually makes sense and is often required. We will examine which
    models need equal length sequences in the next section on building custom NLP
    models.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras'' `pad_sequences` function allows us to easily bring all of the sequences
    to the same length by either cutting off sequences or adding zeros at the end.
    We will bring all the tweets to a length of 140 characters, which for a long time
    was the maximum length tweets could have:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Finally, we split our data into a training and validation set:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Now we are ready to train our own word vectors.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 'Embeddings are their own layer type in Keras. To use them, we have to specify
    how large we want the word vectors to be. The 50-dimensional vector that we have
    chosen to use is able to capture good embeddings even for quite large vocabularies.
    Additionally, we also have to specify how many words we want embeddings for and
    how long our sequences are. Our model is now a simple logistic regressor that
    trains its own embeddings:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Notice how we do not have to specify an input shape. Even specifying the input
    length is only necessary if the following layers require knowledge of the input
    length. `Dense` layers require knowledge about the input size, but since we are
    using dense layers directly, we need to specify the input length here.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'Word embeddings have *many* parameters. This is something you can see if you
    are printing out the models summary:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: As you can see, the embedding layer has 50 parameters for 10,000 words equaling
    500,000 parameters in total. This makes training slower and can increase the chance
    of overfitting.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is for us to compile and train our model as usual:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: This model achieves about 76% accuracy on the test set but over 90% accuracy
    on the training set. However, the large number of parameters in the custom embeddings
    has led us to overfitting. To avoid overfitting and reduce training time, it's
    often better to use pretrained word embeddings.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: Loading pretrained word vectors
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like in computer vision, NLP models can benefit from using pretrained pieces
    of other models. In this case, we will use the pretrained GloVe vectors. **GloVe**
    stands for **Global Vectors** for Word 8 and is a project of the Stanford NLP
    group. GloVe provides different sets of vectors trained in different texts.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will be using word embeddings trained on Wikipedia texts
    as well as the Gigaword dataset. In total, the vectors were trained on a text
    of 6 billion tokens.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: With all that being said, there are alternatives to GloVe, such as Word2Vec.
    Both GloVe and Word2Vec are relatively similar, although the training method for
    them is different. They each have their strengths and weaknesses, and in practice
    it is often worth trying out both.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: A nice feature of GloVe vectors is that they encode word meanings in vector
    space so that "word algebra" becomes possible. The vector for "king" minus the
    vector for "man" plus the vector for "woman," for example, results in a vector
    pretty close to "queen." This means the differences between the vectors for "man"
    and "woman" are the same as the differences for the vectors of "king" and "queen,"
    as the differentiating features for both are nearly the same.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Equally, words describing similar things such as "frog" and "toad" are very
    close to each other in the GloVe vector space. Encoding semantic meanings in vectors
    offer a range of other exciting opportunities for document similarity and topic
    modeling, as we will see later in this chapter. Semantic vectors are also pretty
    useful for a wide range of NLP tasks, such as our text classification problem.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual GloVe vectors are in a text file. We will use the 50-dimensional
    embeddings trained on 6 billion tokens. To do this, we need to open the file:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Then we create an empty dictionary that will later map words to embeddings:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'In the dataset, each line represents a new word embedding. The line starts
    with the word, and the embedding values follow. We can read out the embeddings
    like this:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'But what does that mean? Let''s take a minute to break down the meaning behind
    the code, which has six key elements:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: We loop over all lines in the file. Each line contains a word and embedding.
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We split the line by whitespace.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first thing in the line is always the word.
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then come the embedding values. We immediately transform them into a NumPy array
    and make sure that they are all floating-point numbers, that is, decimals.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then save the embedding vector in our embedding dictionary.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we are done with it, we close the file.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As a result of running this code, we now have a dictionary mapping words to
    their embeddings:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: This version of GloVe has vectors for 400,000 words, which should be enough
    to cover most of the words that we will encounter. However, there might be some
    words where we still do not have a vector. For these words, we will just create
    random vectors. To make sure these vectors are not too far off, it is a good idea
    to use the same mean and standard deviation for the random vectors as from the trained
    vectors.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we need to calculate the mean and standard deviation for the GloVe
    vectors:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Our embedding layer will be a matrix with a row for each word and a column
    for each element of the embedding. Therefore, we need to specify how many dimensions
    one embedding has. The version of GloVe we loaded earlier has 50-dimensional vectors:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Next, we need to find out how many words we actually have. Although we have
    set the maximum to 10,000, there might be fewer words in our corpus. At this point,
    we also retrieve the word index from the tokenizer, which we will use later:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'To create our embedding matrix, we first create a random matrix with the same
    `mean` and `std` as the embeddings:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Embedding vectors need to be in the same position as their token number. A
    word with token 1 needs to be in row 1 (rows start with zero), and so on. We can
    now replace the random embeddings for the words for which we have trained embeddings:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'This command has four key elements that we should explore in more detail before
    we move on:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: We loop over all the words in the word index.
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we are above the number of words we want to use, we do nothing.
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We get the embedding vector for the word. This operation might return none if there
    is no embedding for this word.
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there is an embedding vector, we put it in the embedding matrix.
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To use the pretrained embeddings, we just have to set the weights in the embedding
    layer to the embedding matrix that we just created. To make sure the carefully
    created weights are not destroyed, we are going to set the layer to be non-trainable,
    which we can achieve by running the following:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: This model can be compiled and trained just like any other Keras model. You
    will notice that it trains much faster than the model in which we trained our
    own embeddings and suffers less from overfitting. However, the overall performance
    on the test set is roughly the same.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings are pretty cool in reducing training time and helping to build
    accurate models. However, semantic embeddings go further. They can, for example,
    be used to measure how similar two texts are on a semantical level, even if they
    include different words.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Time series models with word vectors
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text is a time series. Different words follow each other and the order in which
    they do matters. Therefore, every neural network-based technique from the previous
    chapter can also be used for NLP. In addition, there are some building blocks
    that were not introduced in [Chapter 4](ch04.xhtml "Chapter 4. Understanding Time
    Series"), *Understanding Time Series* that are useful for NLP.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with an LSTM, otherwise known as long short-term memory. All you
    have to change from the implementation in the last chapter is that the first layer
    of the network should be an embedding layer. This example below uses a `CuDNNLSTM`
    layer, which trains much faster than a regular `LSTM` layer.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 'Other than this, the layer remains the same. If you do not have a GPU, replace
    `CuDNNLSTM` with `LSTM`:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'One technique used frequently in NLP but less frequently in time series forecasting
    is a bidirectional **recurrent neural network** (**RNN**). A bidirectional RNN
    is effectively just two RNNs where one gets fed the sequence forward, while the
    other one gets fed the sequence backward:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '![Time series models with word vectors](img/B10354_05_07.jpg)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
- en: A bidirectional RNN
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras, there is a `Bidirectional` layer that we can wrap any RNN layer around,
    such as an `LSTM`. We achieve this in the following code:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Word embeddings are great because they enrich neural networks. They are a space-efficient
    and powerful method that allows us to transform words into numbers that a neural
    network can work with. With that being said, there are more advantages to encoding
    semantics as vectors, such as how we can perform vector math on them! This is
    useful if we want to measure the similarity between two texts, for instance.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: Document similarity with word embeddings
  id: totrans-461
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The practical use case of word vectors is to compare the semantic similarity
    between documents. If you are a retail bank, insurance company, or any other company
    that sells to end users, you will have to deal with support requests. You'll often
    find that many customers have similar requests, so by finding out how similar
    texts are semantically, previous answers to similar requests can be reused, and
    your organization's overall service can be improved.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: 'spaCy has a built-in function to measure the similarity between two sentences.
    It also comes with pretrained vectors from the Word2Vec model, which is similar
    to GloVe. This method works by averaging the embedding vectors of all the words
    in a text and then measuring the cosine of the angle between the average vectors.
    Two vectors pointing in roughly the same direction will have a high similarity
    score, whereas vectors pointing in different directions will have a low similarity
    score. This is visualized in the following graph:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: '![Document similarity with word embeddings](img/B10354_05_08.jpg)'
  id: totrans-464
  prefs: []
  type: TYPE_IMG
- en: Similarity vectors
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the similarity between two phrases by running the following command:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'As you can see, these requests are pretty similar, achieving a rate of 70%:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: As you can see, their similarity score is quite high. This simple averaging
    method works pretty decently. It is not, however, able to capture things such
    as negations or a single deviating vector, which might not influence the average
    too much.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: For example, "I would like to close a checking account" has a semantically different
    meaning than, "I would like to open a checking account." However, the model sees them
    as being pretty similar. Yet, this approach is still useful and a good illustration
    of the advantages of representing semantics as vectors.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: A quick tour of the Keras functional API
  id: totrans-473
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve used sequential models. In the sequential model, layers get
    stacked on top of each other when we call `model.add()`. The advantage of the
    functional API is that it is simple and prevents errors. The disadvantage is that
    it only allows us to stack layers linearly:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '![A quick tour of the Keras functional API](img/B10354_05_09.jpg)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
- en: GoogLeNet Architecture from Szegedy and others' "Going Deeper with Convolutions"
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the preceding GoogLeNet architecture. While the graph is very
    detailed, what we need to take away is the fact that the model is not just a number
    of layers stacked on top of each other. Instead, there are multiple layers in
    parallel; in this case, the model has three outputs. However, the question remains,
    how did the authors build this complicated model? The sequential API wouldn't
    have allowed them to, but the functional API makes it easy to string up layers
    like a pearl string and create architectures such as the preceding one.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: For many NLP applications, we need more complex models in which, for example,
    two separate layers run in parallel. In the Keras functional API, we have more
    control and can specify how layers should be connected. We can use this to create
    much more advanced and complex models.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the functional API a lot more from now on. This section of the
    chapter aims to provide a brief overview of the Keras functional API, as we will
    be going into much more depth in later chapters. Firstly, let''s look at a simple
    two-layer network in both the sequential and functional way:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The preceding model is a simple model implemented in the sequential API. Take
    note that this is how we have done it throughout this book so far. We will now
    implement the same model in the functional API:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Notice the differences to the sequential API:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: Instead of defining the model first with `model = Sequential()`, you now define
    the computational graph first and then turn it into a model using the `Model`
    class.
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inputs are now their own layer.
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of using `model.add()`, you define the layer and then pass on an input
    layer or the output tensor of the previous layer.
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You create models by stringing layers on a chain. `Dense(64)(model_input)`,
    for instance, returns a tensor. You pass on this tensor to the next layer, like
    in `Activation('relu')(x)`. This function will return a new output tensor, which
    you can pass to the next layer, and so on. This way, you create a computational
    graph like a chain.
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To create a model, you pass the model input layer as well as the final output
    tensor of your graph into the `Model` class.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Functional API models can be used just like sequential API models. In fact,
    from the output of this model''s summary, you can see it is pretty much the same
    as the model we just created with the sequential API:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: You can see that the functional API can connect layers in more advanced ways
    than the sequential API. We can also separate the layer creation and connection
    step. This keeps the code clean and allows us to use the same layer for different
    purposes.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code segment will create the exact same model as the preceding
    segment, but with separate layer creation and connection steps:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Layers can be reused. For example, we could train some layers in one computational
    graph and then use them for another, as we will do in the section on seq2seq models
    later in the chapter.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: 'One more caveat before we move on to use the functional API to build advanced
    models. We should note that the activation function of any layer can also be specified
    directly in the layer. So far, we have used a separate activation layer, which
    increases clarity but is not strictly required. A `Dense` layer with a `relu`
    activation function can also be specified as:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: When using the functional API, this can be easier than adding an activation
    function.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: Attention
  id: totrans-499
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Are you paying attention? If so, certainly not to everyone equally. In any
    text, some words matter more than others. An attention mechanism is a way for
    a neural network to *focus* on a certain element in a sequence. Focusing, for
    neural networks, means amplifying what is important:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: '![Attention](img/B10354_05_10.jpg)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
- en: An example of an attention mechanism
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention layers are fully connected layers that take in a sequence and output
    the weighting for a sequence. The sequence is then multiplied with the weightings:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Let''s break down the sequence we''ve just created. As you can see, it''s made
    up of nine key elements:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: Our input has the shape `(batch_size, time_steps, input_dim)`, where `time_steps`
    is the length of the sequence, and `input_dim` is the dimensionality of the input.
    If we applied this directly to a text series with the embeddings used, `input_dim`
    would be 50, the same as the embedding dimensionality.
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then swap (permute) the axis for `time_steps` and `input_dim` so that the tensor
    has a shape of `(batch_size, input_dim, time_steps)`.
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If everything went fine, our tensor is already in the shape that we want it
    to be in. Here we are adding a reshaping operation just to be sure.
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now comes the trick. We run our input through a `dense` layer with a `softmax`
    activation. This will generate a weighting for each element in the series, just
    as shown previously. This `dense` layer is what is trained inside the `attention`
    block.
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By default, the `dense` layer computes attention for each input dimension individually.
    That is, for our word vectors, it would compute 50 different weightings. That
    can be useful if we are working with time series models where the input dimensions
    actually represent different things. In this case, we want to weight words as
    a whole.
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To create one attention value per word, we average the attention layer across
    the input dimensions. Our new tensor has the shape `(batch_size, 1, time_steps)`.
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In order to multiply the attention vector with the input, we need to repeat
    the weightings across the input dimension. After repetition, the tensor has the shape
    `(batch_size, input_dim, time_steps)` again, but with the same weights across
    the `input_dim` dimension.
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To match the shape of the input, we permute the axis for `time_steps` and `input_dim`
    back, so that the attention vector once again has a shape of `(batch_size, time_steps,
    input_dim)`.
  id: totrans-513
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we apply the attention to the input by element-wise multiplying the attention
    vector with the input. We return the resulting tensor.
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following flowchart gives an overview of the process:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: '![Attention](img/B10354_05_11.jpg)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
- en: Attention block
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how the preceding function defines takes a tensor as an input, defines
    a graph, and returns a tensor. We can now call this function as part of our model
    building process:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: In this case, we are using the attention block right after the embeddings. This
    means that we can amplify or suppress certain word embeddings. Equally, we could
    use the attention block after the LSTM. In many cases, you will find attention
    blocks to be powerful tools in your arsenal when it comes to building models that
    deal with any kind of sequence, especially in NLP.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: 'To become more comfortable with how the functional API strings up layers and
    how the attention block reshapes tensors, take a look at this model summary:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: This model can be trained, just as any Keras model can be, and achieves around
    80% accuracy on the validation set.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: Seq2seq models
  id: totrans-525
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2016, Google announced that it had replaced the entire Google Translate algorithm
    with a single neural network. The special thing about the Google Neural Machine
    Translation system is that it translates mutliple languages "end-to-end" using
    only a single model. It works by encoding the semantics of a sentence and then
    decoding the semantics into the desired output language.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: The fact that such a system is possible at all baffled many linguists and other
    researchers, as it shows that machine learning can create systems that accurately
    capture high-level meanings and semantics without being given any explicit rules.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: These semantic meanings are represented as an encoding vector, and while we
    don't quite yet know how to interpret these vectors, there are a lot of useful
    applications for them. Translating from one language to another is one such popular
    method, but we could use a similar approach to "translate" a report into a summary.
    Text summarization has made great strides, but the downside is that it requires
    a lot of computing power to deliver meaningful results, so we will be focusing
    on language translation.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: Seq2seq architecture overview
  id: totrans-529
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If all phrases had the exact same length, we could simply use an LSTM (or multiple
    LSTMs). Remember that an LSTM can also return a full sequence of the same length
    as the input sequence. However, in many cases, sequences will not have the same
    length.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: 'To deal with different lengths of phrases, we''ll need to create an encoder
    that aims to capture the sentence''s semantic meaning. We then create a decoder
    that has two inputs: the *encoded semantics* and the *sequence* that was already
    produced. The decoder then predicts the next item in the sequence. For our character-level
    translator, it looks like this:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: '![Seq2seq architecture overview](img/B10354_05_12.jpg)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
- en: Seq2seq architecture overview
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: Note how the output of the decoder is used as the input of the decoder again.
    This process is only stopped once the decoder produces a `<STOP>` tag, which indicates
    that the sequence is over.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-535
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: The data and code for this section can be found on Kaggle at [https://www.kaggle.com/jannesklaas/a-simple-seq2seq-translat](https://www.kaggle.com/jannesklaas/a-simple-seq2seq-translat).'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: The data
  id: totrans-537
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use a dataset of English phrases and their translation. This dataset was
    obtained from the **Tabotea** project, a translation database, and you can find
    the file attached to the code on Kaggle. We implement this model on a character
    level, which means that unlike previous models, we won't tokenize words, but characters.
    This makes the task harder for our network because it now has to also learn how
    to spell words! However, on the other hand, there are a lot fewer characters than
    words, therefore we can just one-hot encode characters instead of having to work
    with embeddings. This makes our model a bit simpler.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, we have to set a few parameters:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: But what are the parameters that we've set up?
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: Batch size for training.
  id: totrans-542
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of epochs to train for.
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dimensionality of the encoding vectors. How many numbers we use to encode the
    meaning of a sentence.
  id: totrans-544
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A number of samples to train on. The whole dataset has about 140,000 samples.
    However, we will train on fewer for memory and time reasons.
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The path to the data `.txt` file on disk.
  id: totrans-546
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Input (English) and target (French) is tab delimited in the data file. Each
    row represents a new phrase. The translations are separated by a tab (escaped
    character: `\t`). So, we loop over the lines and read out inputs and targets by
    splitting the lines at the tab symbol.'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: To build up our tokenizer, we also need to know which characters are present
    in our dataset. So, for all of the characters, we need to check whether they are
    already in our set of seen characters, and if not, add them to it.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we must first set up the holding variables for texts and characters:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Then we loop over as many lines as we want samples and extract the texts and
    characters:'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Let''s break this code down so that we can understand it in more detail:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: Input and target are split by tabs, English TAB French, so we split the lines by
    tabs to obtain input and target texts.
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use `\t` as the "start sequence" character for the targets, and `\n` as "end sequence"
    character. This way, we know when to stop decoding.
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We loop over the characters in the input text, adding all characters that we have not
    seen yet to our set of input characters.
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We loop over the characters in the output text, adding all characters that we have
    not seen yet to our set of output characters.
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encoding characters
  id: totrans-558
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now need to create lists of alphabetically sorted input and output characters,
    which we can do by running:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'We''re also going to count how many input and output characters we have. This
    is important since we need to know how many dimensions our one-hot encodings should
    have. We can find this by writing the following:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  id: totrans-562
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Instead of using the Keras tokenizer, we will build our own dictionary mapping
    characters to token numbers. We can do this by running the following:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'We can see how this works by printing the token numbers for all characters
    in a short sentence:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  id: totrans-566
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Next, we build up our model training data. Remember that our model has two
    inputs but only one output. While our model can handle sequences of any length,
    it is handy to prepare the data in NumPy and thus to know how long our longest
    sequence is:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  id: totrans-569
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Now we prepare input and output data for our model. `encoder_input_data` is
    a 3D array of shape `(num_pairs, max_english_sentence_length, num_english_characters)`
    containing a one-hot vectorization of the English sentences:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  id: totrans-572
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '`decoder_input_data` is a 3D array of shape `(num_pairs, max_french_sentence_length,
    num_french_characters)` containing a one-hot vectorization of the French sentences:'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '`decoder_target_data` is the same as `decoder_input_data` but offset by one
    timestep. `decoder_target_data[:, t, :]` will be the same as `decoder_input_data[:,
    t + 1, :]`.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  id: totrans-576
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: You can see that the input and output of the decoder are the same except that
    the output is one timestep ahead. This makes sense when you consider that we feed
    an unfinished sequence into the decoder and want it to predict the next character.
    We will use the functional API to create a model with two inputs.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that the decoder also has two inputs: the *decoder inputs* and
    the *encoded semantics*. The encoded semantics, however, are not directly the
    outputs of the encoder LSTM but its *states*. In an LSTM, states are the hidden
    memory of the cells. What happens is that the first "memory" of our decoder is
    the encoded semantics. To give the decoder this first memory, we can initialize
    its states with the states of the decoder LSTM.'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: 'To return states, we have to set the `return_state` argument, configuring an
    RNN layer to return a list where the first entry is the outputs and the next entries
    are the internal RNN states. Once again, we are using `CuDNNLSTM`. If you do not
    have a GPU, replace it with `LSTM`, but note that training this model without
    a GPU can take a very long time to complete:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  id: totrans-580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'Let''s look at the four key elements of the code:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: We create an input layer for our encoder
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create the LSTM encoder
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We link the LSTM encoder to the input layer and get back the outputs and states
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We discard `encoder_outputs` and only keep the states
  id: totrans-585
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we define the decoder. The decoder uses the states of the encoder as initial
    states for its decoding LSTM.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: 'You can think of it like this: imagine you were a translator translating English
    to French. When tasked with translating, you would first listen to the English
    speaker and form ideas about what the speaker wants to say in your head. You would
    then use these ideas to form a French sentence expressing the same idea.'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to understand that we are not just passing a variable, but
    a piece of the computational graph. This means that we can later backpropagate
    from the decoder to the encoder. In the case of our previous analogy, you might
    think that your French translation suffered from a poor understanding of the English
    sentence, so you might start changing your English comprehension based on the
    outcomes of your French translation, for example:'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  id: totrans-589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'The preceding code is made up of four key elements:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: Set up the decoder inputs.
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set up our decoder to return full output sequences, and to return internal
    states as well. We don't use the return states in the training model, but we will
    use them for inference.
  id: totrans-592
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect the decoder to the decoder inputs and specify the internal state. As mentioned
    previously, we don't use the internal states of the decoder for training, so we
    discard them here.
  id: totrans-593
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we need to decide which character we want to use as the next character.
    This is a classification task, so we will use a simple `Dense` layer with a `softmax`
    activation function.
  id: totrans-594
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We now have the pieces we need to define our model with two inputs and one
    output:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  id: totrans-596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'If you have the `graphviz` library installed, you can visualize the model very
    nicely using the following code lines. Unfortunately, however, this code snippet
    won''t work on Kaggle:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  id: totrans-598
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'As you can see, this visualization is represented in the following diagram:'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: '![Encoding characters](img/B10354_05_13.jpg)'
  id: totrans-600
  prefs: []
  type: TYPE_IMG
- en: Seq2seq visual
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now compile and train the model. Since we have to choose between a
    number of possible characters to output next, this is basically a multi-class
    classification task. Therefore, we''ll use a categorical cross-entropy loss:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'The training process takes about 7 minutes on a GPU. However, if we were to
    plot the model''s progress, you can see that it''s overfitting:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: '![Encoding characters](img/B10354_05_14.jpg)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
- en: Seq2seq overfitting
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: The reason it's overfitting is largely because we used only 10,000 sentence
    pairs of only relatively short sentences. To get a bigger model, a real translation
    or summarization system would have to be trained on many more examples. To allow
    you to follow the examples without owning a massive datacenter, we are just using
    a smaller model to give an example of what a seq2seq architecture can do.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: Creating inference models
  id: totrans-608
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overfitting or not, we would like to use our model now. Using a seq2seq model
    for inference, in this case for doing translations, requires us to build a separate
    inference model that uses the weights trained in the training model, but does
    the routing a bit differently. More specifically, we will separate the encoder
    and decoder. This way, we can first create the encoding once and then use it for
    decoding instead of creating it again and again.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder model maps from the encoder inputs to the encoder states:'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  id: totrans-611
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'The decoder model then takes in the encoder memory plus its own memory from
    the last character as an input. It then spits out a prediction plus its own memory
    to be used for the next character:'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  id: totrans-613
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'Let''s look at the six elements of this code:'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: The encoder memory consists of two states. We need to create two inputs for both
    of them.
  id: totrans-615
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then combine the two states into one memory representation.
  id: totrans-616
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then connect the decoder LSTM we trained earlier to the decoder inputs and
    the encoder memory.
  id: totrans-617
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We combine the two states of the decoder LSTM into one memory representation.
  id: totrans-618
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We reuse the dense layer of the decoder to predict the next character.
  id: totrans-619
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we set up the decoder model to take in the character input as well
    as the state input and map it to the character output as well as the state output.
  id: totrans-620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Making translations
  id: totrans-621
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now start to use our model. To do this, we must first create an index
    that maps tokens to characters again:'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: When we translate a phrase, we must first encode the input. We'll then loop,
    feeding the decoder states back into the decoder until we receive a STOP; in our
    case, we use the tab character to signal STOP.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: '`target_seq` is a NumPy array representing the last character predicted by
    the decoder:'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  id: totrans-626
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'For the final time in this chapter, let''s break down the code:'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: Encode the input as state vectors
  id: totrans-628
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an empty target sequence of length one
  id: totrans-629
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Populate the first character of the target sequence with the start character
  id: totrans-630
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There was no stop sign, and the decoded sequence is empty so far
  id: totrans-631
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loop until we receive a stop sign
  id: totrans-632
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get output and internal states of the decoder
  id: totrans-633
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the predicted token (the token with the highest probability)
  id: totrans-634
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the character belonging to the token number
  id: totrans-635
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Append a character to the output
  id: totrans-636
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Exit condition: either hit max length or find stop character'
  id: totrans-637
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target sequence (of length one)
  id: totrans-638
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update states
  id: totrans-639
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we can translate English into French! At least for some phrases, it works
    quite well. Given that we did not supply our model with any rules about French
    words or grammar, this is quite impressive. Translation systems such as Google
    Translate, of course, use much bigger datasets and models, but the underlying
    principles are the same.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: 'To translate a text, we first create a placeholder array full of zeros:'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  id: totrans-642
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'We then one-hot encode all characters in the text by setting the element at
    the index of the characters'' token numbers to `1`:'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  id: totrans-644
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'This will print out the characters'' token numbers alongside the character
    and its position in the text:'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  id: totrans-646
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'Now we can feed this placeholder into our decoder:'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  id: totrans-648
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'And we get the translation back:'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  id: totrans-650
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: Seq2seq models are useful not only for translating between languages. They can
    be trained on just about anything that takes a sequence as an input and also outputs
    a sequence.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: Remember our forecasting task from the last chapter? The winning solution to
    the forecasting problem was a seq2seq model. Text summarization is another useful
    application. Seq2seq models can also be trained to output a series of actions,
    such as a sequence of trades that would minimize the impact of a large order.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-653
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''re at the end of the chapter, let''s see what we''ve learned.
    To finish this chapter, I''ve included three exercises that will challenge you
    based on what we''ve covered in this chapter:'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: Add an extra layer to the encoder of the translation model. The translation
    model might work better if it had a bit more capacity to learn the structure of
    French sentences. Adding one more LSTM layer will be a good exercise to learn
    about the functional API.
  id: totrans-655
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add attention to the encoder of the translation model. Attention will allow
    the model to focus on the (English) words that really matter for translation.
    It is best to use attention as the last layer. This task is a bit harder than
    the previous one, but you will understand the inner workings of attention much
    better.
  id: totrans-656
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visit *Daily News for Stock Market Prediction* at [https://www.kaggle.com/aaron7sun/stocknews](https://www.kaggle.com/aaron7sun/stocknews).
    The task is to use the daily news as an input to predict stock prices. There are
    a number of kernels already that can help you with this. Use what you have learned
    in this chapter to predict some stock prices!
  id: totrans-657
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-658
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you have learned the most important NLP techniques. There
    was a lot that we''ve learned, and here''s a big list of things we covered in
    this chapter and everything you should now feel confident about understanding:'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: Finding named entities
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning spaCy's models for your own custom applications
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding parts of speech and mapping the grammatical structure of sentences
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using regular expressions
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing text data for classification tasks
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using techniques such as bag-of-words and TF-IDF for classification
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling the topics present in a text with LDA
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using pretrained word embeddings
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building advanced models with the Keras functional API
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training your model to focus on attention
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating sentences with the seq2seq model
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You now have a big set of tools in your toolbox that will allow you to tackle
    NLP problems. Throughout the rest of this book, you will see some of these techniques
    again, being used in different contexts to solve hard problems. These techniques
    are useful across the industry, from retail banking to hedge fund investing. While
    the problem your institution is trying to solve might require a bit of tweaking,
    the general approaches are quite transferable.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will look at a technique that has gained a lot of attention
    since DeepMind beat a human Go champion: reinforcement learning. This technique
    is especially useful when working in financial markets and is in many ways a natural
    extension of what many quantitative investment firms are already doing. So, stay
    tuned, and I''ll see you on the other side.'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
