- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Deep Learning for Trading
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交易的深度学习
- en: This chapter kicks off Part 4, which covers how several **deep learning** (**DL**)
    modeling techniques can be useful for investment and trading. DL has achieved
    numerous **breakthroughs in many domains**, ranging from image and speech recognition
    to robotics and intelligent agents that have drawn widespread attention and revived
    large-scale research into **artificial intelligence** (**AI**). The expectations
    are high that the rapid development will continue and many more solutions to difficult
    practical problems will emerge.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开启了第4部分，涵盖了几种**深度学习**（**DL**）建模技术如何对投资和交易有用。DL已经在许多领域取得了许多**突破**，从图像和语音识别到机器人和智能代理，引起了广泛关注，并重振了对**人工智能**（**AI**）的大规模研究。人们对快速发展有着很高的期望，并且预计将会出现更多解决困难实际问题的解决方案。
- en: In this chapter, we will present **feedforward neural networks** to introduce
    key elements of working with neural networks relevant to the various DL architectures
    covered in the following chapters. More specifically, we will demonstrate how
    to train large models efficiently using the **backpropagation algorithm** and
    manage the risks of overfitting. We will also show how to use the popular TensorFlow
    2 and PyTorch frameworks, which we will leverage throughout Part 4.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍**前馈神经网络**，以介绍与后续章节中涵盖的各种DL架构相关的神经网络工作要素。具体来说，我们将演示如何使用**反向传播算法**高效训练大型模型，并管理过拟合的风险。我们还将展示如何使用流行的TensorFlow
    2和PyTorch框架，这些框架将贯穿第4部分。
- en: Finally, we will develop, backtest, and evaluate a trading strategy based on
    signals generated by a deep feedforward neural network. We will design and tune
    the neural network and analyze how key hyperparameter choices affect its performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将基于由深度前馈神经网络生成的信号开发、回测和评估交易策略。我们将设计和调整神经网络，并分析关键超参数选择如何影响其性能。
- en: 'In summary, after reading this chapter and reviewing the accompanying notebooks,
    you will know about:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在阅读本章并审阅随附的笔记本后，您将了解：
- en: How DL solves AI challenges in complex domains
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习如何解决复杂领域的AI挑战
- en: Key innovations that have propelled DL to its current popularity
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推动DL走向当前流行的关键创新
- en: How feedforward networks learn representations from data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络如何从数据中学习表示
- en: Designing and training deep **neural networks** (**NNs**) in Python
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中设计和训练深度**神经网络**（**NNs**）
- en: Implementing deep NNs using Keras, TensorFlow, and PyTorch
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras、TensorFlow和PyTorch实现深度神经网络
- en: Building and tuning a deep NN to predict asset returns
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和调整深度神经网络以预测资产回报
- en: Designing and backtesting a trading strategy based on deep NN signals
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和回测基于深度神经网络信号的交易策略
- en: In the following chapters, we will build on this foundation to design various
    architectures suitable for different investment applications with a particular
    focus on alternative text and image data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将在此基础上设计各种适用于不同投资应用的架构，特别关注替代文本和图像数据。
- en: These include **recurrent neural networks** (**RNNs**) tailored to sequential
    data such as time series or natural language, and **convolutional neural networks**
    (**CNNs**), which are particularly well suited to image data but can also be used
    with time-series data. We will also cover deep unsupervised learning, including
    autoencoders and **generative adversarial networks** (**GANs**) as well as reinforcement
    learning to train agents that interactively learn from their environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包括针对序列数据（如时间序列或自然语言）量身定制的**循环神经网络**（**RNNs**），以及特别适用于图像数据但也可以与时间序列数据一起使用的**卷积神经网络**（**CNNs**）。我们还将涵盖深度无监督学习，包括自动编码器和**生成对抗网络**（**GANs**），以及强化学习来训练能够与环境进行交互式学习的代理程序。
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库的相应目录中找到本章的代码示例和附加资源链接。笔记本中包括图片的彩色版本。
- en: Deep learning – what's new and why it matters
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习 - 新技术和重要性
- en: The **machine learning** (**ML**) algorithms covered in *Part 2* work well on
    a wide variety of important problems, including on text data, as demonstrated
    in *Part 3*. They have been less successful, however, in solving central AI problems
    such as recognizing speech or classifying objects in images. These limitations
    have motivated the development of DL, and the recent DL breakthroughs have greatly
    contributed to a resurgence of interest in AI. For a comprehensive introduction
    that includes and expands on many of the points in this section, see Goodfellow,
    Bengio, and Courville (2016), or for a much shorter version, see LeCun, Bengio,
    and Hinton (2015).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二部分*中涵盖的**机器学习**（**ML**）算法在解决各种重要问题方面表现良好，包括文本数据，如*第三部分*所示。然而，它们在解决诸如识别语音或对图像中的对象进行分类等核心人工智能问题上成功较少。这些限制促使了深度学习的发展，最近的深度学习突破大大促进了对人工智能的兴趣再度增长。有关包含并扩展本节许多观点的全面介绍，请参见Goodfellow、Bengio和Courville（2016），或者看看LeCun、Bengio和Hinton（2015）的简短版本。'
- en: In this section, we outline how DL overcomes many of the limitations of other
    ML algorithms. These limitations particularly constrain performance on high-dimensional
    and unstructured data that requires sophisticated efforts to extract informative
    features.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了深度学习如何克服其他机器学习算法的许多限制。这些限制特别限制了在需要复杂努力提取信息特征的高维和非结构化数据上的性能。
- en: The ML techniques we covered in *Parts 2* and *3* are best suited for processing
    structured data with well-defined features. We saw, for example, how to convert
    text data into tabular data using the document-text matrix in *Chapter 14*, *Text
    Data for Trading – Sentiment Analysis*. DL overcomes the **challenge of designing
    informative features**, possibly by hand, by learning a representation of the
    data that better captures its characteristics with respect to the outcome.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第二*和*第三*部分中介绍的机器学习技术最适合处理具有明确定义特征的结构化数据。例如，我们看到如何使用*第14章*中的文档-文本矩阵将文本数据转换为表格数据，*交易的文本数据-情感分析*。深度学习通过学习数据的表示来克服**设计信息特征**的挑战，可能需要手动进行，从而更好地捕捉其与结果相关的特征。
- en: More specifically, we'll see how DL learns a **hierarchical representation of
    the data**, and why this approach works well for high-dimensional, unstructured
    data. We will describe how NNs employ a multilayered, deep architecture to compose
    a set of nested functions and discover a hierarchical structure. These functions
    compute successive and increasingly abstract representations of the data in each
    layer based on the learning of the previous layer. We will also look at how the
    backpropagation algorithm adjusts the network parameters so that these representations
    best meet the model's objective.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们将看到深度学习如何学习数据的**分层表示**，以及为什么这种方法对于高维、非结构化数据效果很好。我们将描述神经网络如何使用多层、深度架构来组成一组嵌套函数并发现分层结构。这些函数根据前一层的学习计算数据的连续和越来越抽象的表示。我们还将看看反向传播算法如何调整网络参数，以便这些表示最好地满足模型的目标。
- en: We will also briefly outline how DL fits into the evolution of AI and the diverse
    set of approaches that aim to achieve the current goals of AI.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将简要概述深度学习如何融入人工智能的演变以及旨在实现当前人工智能目标的各种方法。
- en: Hierarchical features tame high-dimensional data
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分层特征驯服了高维数据
- en: As discussed throughout *Part 2*, the key challenge of supervised learning is
    to generalize from training data to new samples. Generalization becomes exponentially
    more difficult as the dimensionality of the data increases. We encountered the
    root causes of these difficulties as the curse of dimensionality in *Chapter 13*,
    *Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*第二部分*中所讨论的那样，监督学习的关键挑战是从训练数据推广到新样本。随着数据的维度增加，泛化变得指数级困难。我们在*第13章*中遇到了这些困难的根本原因，即无监督学习的数据驱动风险因素和资产配置的**维度诅咒**。
- en: 'One aspect of this curse is that volume grows exponentially with the number
    of dimensions: for a hypercube with edge length 10, volume increases from 10³
    to 10⁴ as its dimensionality increases from three to four. Conversely, the **data
    density for a given sample size drops exponentially**. In other words, the number
    of observations required to maintain a certain density grows exponentially.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个诅咒的一个方面是，体积随着维度的增加而呈指数增长：对于边长为10的超立方体，随着维度从三增加到四，体积从10³增加到10⁴。相反，对于给定样本大小，**数据密度会呈指数级下降**。换句话说，为了保持一定的密度，所需的观察次数呈指数增长。
- en: Another aspect is that functional relationships between the features and the
    output can become more complex when they are allowed to vary across a growing
    number of dimensions. As discussed in *Chapter 6*, *The Machine Learning Process*,
    ML algorithms struggle to learn **arbitrary functions in a high-dimensional space**
    because the number of candidates grows exponentially while the density of the
    data available to infer the relationship drops simultaneously. To mitigate this
    problem, algorithms hypothesize that the target function belongs to a certain
    class and impose constraints on the search for the optimal solution within that
    class for the problem at hand.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个方面是，当特征与输出之间的功能关系被允许跨越越来越多的维度变化时，它们变得更加复杂。正如*第6章*，*机器学习过程*中讨论的那样，ML算法在高维空间中学习**任意函数**时会遇到困难，因为候选者数量呈指数级增长，而可用于推断关系的数据密度同时下降。为了缓解这个问题，算法假设目标函数属于某个特定的类，并对在解决当前问题时在该类中寻找最佳解的搜索施加约束。
- en: Furthermore, algorithms typically assume that the output at a new point should
    be similar to the output at nearby training points. This prior **assumption of
    smoothness** or local constancy posits that the learned function will not change
    much in a small region, as illustrated by the k-nearest neighbor algorithm (see
    *Chapter 6*, *The Machine Learning Process*). However, as data density drops exponentially
    with a growing number of dimensions, the distance between training samples naturally
    rises. The notion of nearby training examples thus becomes less meaningful as
    the potential complexity of the target function increases.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，算法通常假设新点的输出应与附近训练点的输出相似。这种先验**平滑性假设**或局部恒定性假设，即学习的函数在小区域内不会发生太大变化，正如k最近邻算法所示（参见*第6章*，*机器学习过程*）。然而，随着维度数量的增加，数据密度指数级下降，训练样本之间的距离自然上升。因此，随着目标函数的潜在复杂性增加，附近训练示例的概念变得不太有意义。
- en: For traditional ML algorithms, the number of parameters and required training
    samples is generally proportional to the number of regions in the input space
    that the algorithm is able to distinguish. DL is designed to overcome the challenges
    of learning an exponential number of regions from a limited number of training
    points by assuming that a hierarchy of features generates the data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于传统的ML算法，所需参数和训练样本的数量通常与算法能够区分的输入空间中的区域数量成正比。DL旨在通过假设特征的层次结构生成数据，从而克服从有限数量的训练点学习指数数量的区域的挑战。
- en: DL as representation learning
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DL作为表示学习
- en: Many AI tasks like image or speech recognition require knowledge about the world.
    One of the key challenges is to encode this knowledge so a computer can utilize
    it. For decades, the development of ML systems required considerable domain expertise
    to transform the raw data (such as image pixels) into an internal representation
    that a learning algorithm could use to detect or classify patterns.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人工智能任务，如图像或语音识别，需要关于世界的知识。其中一个关键挑战是对这些知识进行编码，以便计算机可以利用它。几十年来，ML系统的发展需要相当的领域专业知识，以将原始数据（如图像像素）转换为学习算法可以用来检测或分类模式的内部表示。
- en: Similarly, how much value an ML algorithm adds to a trading strategy depends
    greatly on our ability to engineer features that represent the predictive information
    in the data so that the algorithm can process it. Ideally, the features capture
    independent drivers of the outcome, as discussed in *Chapter 4*, *Financial Feature
    Engineering – How to Research Alpha Factors*, and throughout *Parts 2* and *3*
    when designing and evaluating factors that capture trading signals.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，ML算法对交易策略增加了多少价值，很大程度上取决于我们能够工程化特征，以表示数据中的预测信息，以便算法可以处理它。理想情况下，特征应捕获结果的独立驱动因素，正如在*第4章*，*金融特征工程-如何研究Alpha因子*和*第2部分*和*第3部分*中讨论的，在设计和评估捕获交易信号的因子时。
- en: Rather than relying on hand-designed features, representation learning allows
    an ML algorithm to automatically discover the representation of the data most
    useful for detecting or classifying patterns. DL combines this technique with
    specific assumptions about the nature of the features. See Bengio, Courville,
    and Vincent (2013) for additional information.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与依赖手工设计的特征不同，表示学习使ML算法能够自动发现对于检测或分类模式最有用的数据表示。DL将这种技术与关于特征性质的特定假设相结合。有关更多信息，请参见Bengio、Courville和Vincent（2013）。
- en: How DL extracts hierarchical features from data
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DL如何从数据中提取层次特征
- en: The core idea behind DL is that a multi-level hierarchy of features has generated
    the data. Consequently, a DL model encodes the prior belief that the target function
    is composed of a nested set of simpler functions. This assumption permits an exponential
    gain in the number of regions that can be distinguished for a given number of
    training samples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: DL 背后的核心思想是一个多层次的特征层次结构生成了数据。因此，DL 模型编码了目标函数由一组嵌套的简单函数组成的先验信念。这一假设允许在给定数量的训练样本的情况下，区分的区域数量呈指数增长。
- en: In other words, DL is a representation learning method that extracts a hierarchy
    of concepts from the data. It learns this hierarchical representation by **composing
    simple but non-linear functions** that successively transform the representation
    of one level (starting with the input data) into a new representation at a higher,
    slightly more abstract level. By combining enough of these transformations, DL
    is able to learn very complex functions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，DL 是一种从数据中提取概念层次结构的表示学习方法。它通过**组合简单但非线性的函数**来学习这种层次结构表示，这些函数逐步地将一个级别（从输入数据开始）的表示转换为稍微更抽象的高级表示。通过组合足够多的这些转换，DL
    能够学习非常复杂的函数。
- en: Applied to a **classification task**, for example, higher levels of representation
    tend to amplify the aspects of the data most helpful for discriminating objects
    while suppressing irrelevant sources of variation. As we will see in more detail
    in *Chapter 18*, *CNNs for Financial Time Series and Satellite Images*, raw image
    data is just a two- or three-dimensional array of pixel values. The first layer
    of representation typically learns features that focus on the presence or absence
    of edges at particular orientations and locations. The second layer often learns
    motifs that depend on particular edge arrangements, regardless of small variations
    in their positions. The following layer may assemble motifs to represent parts
    of relevant objects, and subsequent layers would detect objects as combinations
    of these parts.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于**分类任务**时，例如，更高层次的表示往往会放大对区分对象最有帮助的数据方面，同时抑制无关的变化源。正如我们将在*第18章*更详细地看到的，*用于金融时间序列和卫星图像的CNNs*，原始图像数据只是像素值的二维或三维数组。表示的第一层通常学习侧重于特定方向和位置的边缘的存在或缺失的特征。第二层通常学习依赖于特定边缘排列的主题，而不考虑它们位置的小变化。接下来的层可能会组装这些主题以表示相关对象的部分，随后的层将检测到对象作为这些部分的组合。
- en: The **key breakthrough of DL** is that a general-purpose learning algorithm
    can extract hierarchical features suitable for modeling high-dimensional, unstructured
    data in a way that is infinitely more scalable than human engineering. It is thus
    no surprise that the rise of DL parallels the large-scale availability of unstructured
    image or text data. To the extent that these data sources also figure prominently
    among alternative data, DL has become highly relevant for algorithmic trading.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: DL 的**关键突破**在于一个通用的学习算法可以提取适合于对高维、非结构化数据进行建模的层次特征，而这种方式的可扩展性比人类工程学无限得多。因此，DL
    的崛起与非结构化图像或文本数据的大规模可用性并不奇怪。在这些数据源在替代数据中也占据重要地位的程度上，DL 对算法交易变得高度相关。
- en: Good and bad news – the universal approximation theorem
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 好消息和坏消息 - 通用逼近定理
- en: The **universal approximation theorem** formalizes the ability of NNs to capture
    arbitrary relationships between input and output data. George Cybenko (1989) demonstrated
    that single-layer NNs using sigmoid activation functions can represent any continuous
    function on a closed and bounded subset of `Rn.` Kurt Hornik (1991) further showed
    that it is not the specific shape of the activation function but rather the **multilayered
    architecture** that enables the hierarchical feature representation, which in
    turn allows NNs to approximate universal functions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用逼近定理**正式化了 NNs 捕捉输入和输出数据之间任意关系的能力。George Cybenko（1989）证明了使用 sigmoid 激活函数的单层
    NNs 可以表示`Rn`的闭合和有界子集上的任何连续函数。Kurt Hornik（1991）进一步表明，能够进行分层特征表示的不是特定形状的激活函数，而是**多层次架构**，这进而使得
    NNs 能够逼近通用函数。'
- en: However, the theorem does not help us identify the network architecture required
    to represent a specific target function. We will see in the last section of this
    chapter that there are numerous parameters to optimize, including the network's
    width and depth, the number of connections between neurons, and the type of activation
    functions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，该定理并不能帮助我们确定表示特定目标函数所需的网络架构。我们将在本章的最后一节中看到，有许多参数需要优化，包括网络的宽度和深度、神经元之间的连接数量以及激活函数的类型。
- en: Furthermore, the ability to represent arbitrary functions does not imply that
    a network can actually learn the parameters for a given function. It took over
    two decades for backpropagation, the most popular learning algorithm for NNs to
    become effective at scale. Unfortunately, given the highly nonlinear nature of
    the optimization problem, there is no guarantee that it will find the absolute
    best rather than just a relatively good solution.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，能够表示任意函数并不意味着网络实际上可以学习给定函数的参数。过去20多年来，反向传播，即用于神经网络的最流行的学习算法之一，才在规模上变得有效。不幸的是，考虑到优化问题的高度非线性性质，不能保证它会找到绝对最佳解而不仅仅是相对好的解决方案。
- en: How DL relates to ML and AI
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习与机器学习和人工智能的关系
- en: AI has a long history, going back at least to the 1950s as an academic field
    and much longer as a subject of human inquiry, but has experienced several waves
    of ebbing and flowing enthusiasm since (see Nilsson, 2009, for an in-depth survey).
    ML is an important subfield with a long history in related disciplines such as
    statistics and became prominent in the 1980s. As we have just discussed, and as
    depicted in *Figure 17.1*, DL is a form of representation learning and is itself
    a subfield of ML.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能有着悠久的历史，至少可以追溯到20世纪50年代作为一个学术领域，而作为人类探究的主题则更久远，但自那时以来，经历了几次热情的高涨和低落（有关深入调查，请参见尼尔森，2009年）。机器学习是一个重要的子领域，在统计学等相关学科中有着悠久的历史，并在20世纪80年代变得突出起来。正如我们刚刚讨论的那样，并且在*图17.1*中所示，深度学习是一种表示学习，本身是机器学习的一个子领域。
- en: The initial goal of AI was to achieve **general AI**, conceived as the ability
    to solve problems considered to require human-level intelligence, and to reason
    and draw logical conclusions about the world and automatically improve itself.
    AI applications that do not involve ML include knowledge bases that encode information
    about the world, combined with languages for logical operations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的最初目标是实现**通用人工智能**，即被认为需要人类级智能来解决的问题，并对世界进行推理和逻辑推断，并自动改进自己的能力。不涉及机器学习的人工智能应用包括编码有关世界的信息的知识库，以及用于逻辑操作的语言。
- en: Historically, much AI effort went into developing **rule-based systems** that
    aimed to capture expert knowledge and decision-making rules, but hard-coding these
    rules frequently failed due to excessive complexity. In contrast, ML implies a
    **probabilistic approach** that learns rules from data and aims at circumventing
    the limitations of human-designed rule-based systems. It also involves a shift
    to narrower, task-specific objectives.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在历史上，大量的人工智能工作都致力于开发**基于规则的系统**，旨在捕获专家知识和决策规则，但是由于过度复杂而硬编码这些规则的尝试经常失败。相比之下，机器学习意味着从数据中学习规则的**概率方法**，并旨在规避人为设计的基于规则系统的限制。它还涉及到更窄、任务特定目标的转变。
- en: The following figure sketches the relationship between the various AI subfields,
    outlines their goals, and highlights their relevance on a timeline.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下图勾勒了各种人工智能子领域之间的关系，概述了它们的目标，并突出了它们在时间线上的相关性。
- en: '![](img/B15439_17_01.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_01.png)'
- en: 'Figure 17.1: AI timeline and subfields'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1：人工智能时间线和子领域
- en: In the next section, we will see how to actually build a neural network.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将看到如何实际构建神经网络。
- en: Designing an NN
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计神经网络
- en: DL relies on **NNs**, which consist of a few key building blocks, which in turn
    can be configured in a multitude of ways. In this section, we introduce how NNs
    work and illustrate their most important components used to design different architectures.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习依赖于**神经网络**，它由一些关键构建模块组成，而这些模块又可以以多种方式配置。在本节中，我们介绍了神经网络的工作原理，并阐述了用于设计不同架构的最重要组成部分。
- en: '(Artificial) NNs were originally inspired by biological models of learning
    like the human brain, either in an attempt to mimic how it works and achieve similar
    success, or to gain a better understanding through simulation. Current NN research
    draws less on neuroscience, not least since our understanding of the brain has
    not yet reached a sufficient level of granularity. Another constraint is overall
    size: even if the number of neurons used in NNs continued to double every year
    since their inception in the 1950s, they would only reach the scale of the human
    brain around 2050.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: （人工）神经网络最初受到了生物学习模型的启发，例如人脑，要么试图模仿其工作原理并取得类似的成功，要么通过模拟来更好地理解。当前的神经网络研究不太依赖于神经科学，至少因为我们对大脑的理解尚未达到足够的细致程度。另一个约束是总体规模：即使从20世纪50年代开始，神经网络中使用的神经元数量每年都以指数倍增长，它们也只会在2050年左右达到人脑的规模。
- en: We will also explain how **backpropagation**, often simply called **backprop**,
    uses gradient information (the value of the partial derivative of the cost function
    with respect to a parameter) to adjust all neural network parameters based on
    training errors. The composition of various nonlinear modules implies that the
    optimization of the objective function can be quite challenging. We also introduce
    refinements of backpropagation that aim to accelerate the learning process.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将解释**反向传播**，通常简称为**反向传播**，如何使用梯度信息（损失函数对参数的偏导数值）根据训练误差调整所有神经网络参数。各种非线性模块的组合意味着目标函数的优化可能非常具有挑战性。我们还介绍了旨在加速学习过程的反向传播的改进。
- en: A simple feedforward neural network architecture
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个简单的前馈神经网络架构
- en: In this section, we introduce **feedforward NNs**,which are based on the **multilayer
    perceptron** (**MLP**) and consist of one or more hidden layers that connect the
    input to the output layer. In feedforward NNs, information only flows from input
    to output, such that they can be represented as directed acyclic graphs, as in
    the following figure. In contrast, **recurrent neural networks** (**RNNs**; see
    *Chapter 19*, *RNNs for Multivariate Time Series and Sentiment Analysis*) include
    loops from the output back to the input to track or memorize past patterns and
    events.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了基于**多层感知器**（**MLP**）的**前馈神经网络**（**feedforward NNs**），它由一个或多个连接输入与输出层的隐藏层组成。在前馈神经网络中，信息只从输入流向输出，因此它们可以被表示为有向无环图，如下图所示。相比之下，**循环神经网络**（**RNNs**；见*第19章*，*用于多元时间序列和情感分析的RNNs*）包括从输出回到输入的循环，以跟踪或记忆过去的模式和事件。
- en: We will first describe the feedforward NN architecture and how to implement
    it using NumPy. Then we will explain how backpropagation learns the NN weights
    and implement it in Python to train a binary classification network that produces
    perfect results even though the classes are not linearly separable. See the notebook
    `build_and_train_feedforward_nn` for implementation details.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先描述前馈神经网络的架构以及如何使用NumPy实现它。然后我们将解释反向传播如何学习神经网络的权重，并在Python中实现它，以训练一个二分类网络，即使类别不是线性可分的也能产生完美的结果。有关实现细节，请参见笔记本`build_and_train_feedforward_nn`。
- en: A feedforward NN consists of several **layers**, each of which receives a sample
    of input data and produces an output. The **chain of transformations** starts
    with the input layer, which passes the source data to one of several internal
    or hidden layers, and ends with the output layer, which computes a result for
    comparison with the sample's output value.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络由多个**层**组成，每个层接收一份输入数据样本并产生一个输出。**变换链**始于输入层，将源数据传递给几个内部或隐藏层之一，以输出层结束，该层计算与样本输出值进行比较的结果。
- en: The hidden and output layers consist of nodes or neurons. Nodes of a **fully
    connected** or dense layer connect to some or all nodes of the previous layer.
    The network architecture can be summarized by its depth, measured by the number
    of hidden layers, or the width and the number of nodes of each layer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层和输出层由节点或神经元组成。**全连接**或密集层的节点连接到上一层的一些或所有节点。网络架构可以通过其深度（由隐藏层的数量衡量）、每个层的宽度和节点数量来总结。
- en: Each connection has a **weight** used to compute a linear combination of the
    input values. A layer may also have a **bias** node that always outputs a 1 and
    is used by the nodes in the subsequent layer, like a constant in linear regression.
    The goal of the training phase is to learn values for these weights that optimize
    the network's predictive performance.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 每个连接都有一个用于计算输入值的线性组合的**权重**。一层也可以有一个**偏差**节点，它始终输出1，并由后续层中的节点使用，类似于线性回归中的常数。训练阶段的目标是学习这些权重的值，以优化网络的预测性能。
- en: Each node of the hidden layers computes the **dot product** of the weights and
    the output of the previous layer. An **activation function** transforms the result,
    which becomes the input to the subsequent layer. This transformation is typically
    nonlinear (like the sigmoid function used for logistic regression; see *Chapter
    7*, *Linear Models – From Risk Factors to Return Forecasts,* on linear models)
    so that the network can learn nonlinear relationships; we'll discuss common activation
    functions in the next section. The output layer computes the linear combination
    of the output of the last hidden layer with its weights and uses an activation
    function that matches the type of ML problem.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 每个隐藏层节点计算前一层的输出和权重的**点积**。**激活函数**转换结果，成为后续层的输入。此转换通常是非线性的（就像逻辑回归中使用的 Sigmoid
    函数一样；参见 *第7章*，*线性模型 - 从风险因素到回报预测*，关于线性模型），以便网络可以学习非线性关系；我们将在下一节讨论常见的激活函数。输出层计算最后一个隐藏层的输出与其权重的线性组合，并使用与
    ML 问题类型匹配的激活函数。
- en: The computation of the network output from the inputs thus flows through a chain
    of nested functions and is called **forward propagation**. *Figure 17.2* illustrates
    a single-layer feedforward NN with a two-dimensional input vector, a hidden layer
    of width three, and two nodes in the output layer. This architecture is simple
    enough, so we can still easily graph it yet illustrate the key concepts.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，网络输出的计算从输入中流经一系列嵌套函数，并称为**前向传播**。*图17.2*说明了一个具有二维输入向量、宽度为三的隐藏层和两个输出层节点的单层前馈
    NN。这种架构足够简单，因此我们仍然可以轻松地绘制它，并且说明关键概念。
- en: '![](img/B15439_17_02.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_02.png)'
- en: 'Figure 17.2: A feedforward architecture with one hidden layer'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.2：具有一个隐藏层的前馈架构
- en: The **network graph** shows that each of the three hidden layer nodes (not counting
    the bias) has three weights, one for the input layer bias and two for each of
    the two input variables. Similarly, each output layer node has four weights to
    compute the product sum or dot product of the hidden layer bias and activations.
    In total, there are 17 parameters to be learned.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络图**显示，每个隐藏层节点（不包括偏差）都有三个权重，一个用于输入层偏差，两个用于每个两个输入变量。同样，每个输出层节点都有四个权重来计算隐藏层偏差和激活的乘积和。总共有17个要学习的参数。'
- en: The **forward propagation** panel on the right of the figure lists the computations
    for an example node at the hidden and output layers, *h* and *o*, respectively.
    The first node in the hidden layer applies the sigmoid function to the linear
    combination *z* of its weights and inputs akin to logistic regression. The hidden
    layer thus runs three logistic regressions in parallel, while the backpropagation
    algorithm ensures that their parameters will most likely differ to best inform
    subsequent layers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的**前向传播**面板列出了隐藏层和输出层中一个示例节点 *h* 和 *o* 的计算，分别表示隐藏层和输出层。隐藏层中的第一个节点对其权重和输入的线性组合
    *z* 应用 Sigmoid 函数，类似于逻辑回归。因此，隐藏层同时运行三个逻辑回归，并且反向传播算法确保它们的参数很可能不同，以最好地通知后续层。
- en: The output layer uses a **softmax** activation function (see *Chapter 6*, *The
    Machine Learning Process*) that generalizes the logistic sigmoid function to multiple
    classes. It adjusts the dot product of the hidden layer output with its weight
    to represent probabilities for the classes (only two in this case to simplify
    the presentation).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层使用**softmax**激活函数（参见 *第6章*，*机器学习过程*），该函数将逻辑 Sigmoid 函数推广到多个类别。它调整了隐藏层输出与其权重的点积，以表示类别的概率（在这种情况下仅为两个以简化演示）。
- en: 'The forward propagation can also be expressed as nested functions, where *h*
    again represents the hidden layer and *o* the output layer to produce the NN estimate
    of the output: ![](img/B15439_17_001.png).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播也可以表示为嵌套函数，其中 *h* 再次表示隐藏层，*o* 表示输出层以产生 NN 对输出的估计：![](img/B15439_17_001.png)。
- en: Key design choices
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键设计选择
- en: Some NN design choices resemble those for other supervised learning models.
    For example, the output is dictated by the type of the ML problem such as regression,
    classification, or ranking. Given the output, we need to select a cost function
    to measure prediction success and failure, and an algorithm that optimizes the
    network parameters to minimize the cost.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一些神经网络设计选择与其他监督学习模型相似。例如，输出取决于ML问题的类型，如回归、分类或排名。给定输出，我们需要选择一个成本函数来衡量预测成功和失败，并选择一个算法来优化网络参数以最小化成本。
- en: NN-specific choices include the numbers of layers and nodes per layer, the connections
    between nodes of different layers, and the type of activation functions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: NN特定的选择包括层数和每层节点的数量，不同层节点之间的连接以及激活函数的类型。
- en: 'A key concern is **training efficiency**: the functional form of activations
    can facilitate or hinder the flow of the gradient information available to the
    backpropagation algorithm that adjusts the weights in response to training errors.
    Functions with flat regions for large input value ranges have a very low gradient
    and can impede training progress when parameter values get stuck in such a range.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键问题是**训练效率**：激活的功能形式可以促进或阻碍可用于反向传播算法的梯度信息的流动，该算法根据训练错误调整权重。具有大输入值范围的平坦区域的函数具有非常低的梯度，当参数值停留在这种范围内时，可以阻碍训练进度。
- en: Some architectures add **skip connections** that establish direct links beyond
    neighboring layers to facilitate the flow of gradient information. On the other
    hand, the deliberate omission of connections can reduce the number of parameters
    to limit the network's capacity and possibly lower the generalization error, while
    also cutting the computational cost.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一些架构添加了**跳跃连接**，建立了超出相邻层的直接链接，以促进梯度信息的流动。另一方面，有意省略连接可以减少参数数量，限制网络的容量，并可能降低泛化误差，同时也减少了计算成本。
- en: Hidden units and activation functions
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐藏单元和激活函数
- en: Several nonlinear activation functions besides the sigmoid function have been
    used successfully. Their design remains an area of research because they are the
    key element that allows the NN to learn nonlinear relationships. They also have
    a critical impact on the training process because their derivatives determine
    how errors translate into weight adjustments.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 除了sigmoid函数之外，还有几种非线性激活函数被成功使用。它们的设计仍然是一个研究领域，因为它们是允许NN学习非线性关系的关键元素。它们也对训练过程有关键影响，因为它们的导数决定了错误如何转化为权重调整。
- en: A very popular activation function is the **rectiﬁed linear unit** (**ReLU**).
    The activation is computed as *g*(*z*) = max(0, *z*) for a given activation *z*,
    resulting in a functional form similar to the payoff for a call option. The derivative
    is constant whenever the unit is active. ReLUs are usually combined with an affine
    input transformation that requires the presence of a bias node. Their discovery
    has greatly improved the performance of feedforward networks compared to sigmoid
    units, and they are often recommended as the default. There are several ReLU extensions
    that aim to address the limitations of ReLU to learn via gradient descent when
    they are not active and their gradient is zero (Goodfellow, Bengio, and Courville,
    2016).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常流行的激活函数是**修正线性单元**（**ReLU**）。激活被计算为*g*(*z*) = max(0, *z*)，对于给定的激活*z*，结果形式类似于看涨期权的支付。当单元处于活跃状态时，导数是常数。ReLU通常与需要存在偏置节点的仿射输入变换结合使用。它们的发现极大地改善了前馈网络的性能，与S型单元相比，它们经常被推荐为默认选项。有几个ReLU扩展旨在解决ReLU在不活动时学习梯度下降时的限制和它们的梯度为零的问题（Goodfellow、Bengio和Courville，2016）。
- en: Another alternative to the logistic function σ is the **hyperbolic tangent function
    tanh**, which produces output values in the ranges [-1, 1]. They are closely related
    because ![](img/B15439_17_002.png). Both functions suffer from saturation because
    their gradient becomes very small for very low and high input values. However,
    tanh often performs better because it more closely resembles the identity function
    so that for small activation values, the network behaves more like a linear model,
    which in turn facilitates training.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于逻辑函数σ的另一种选择是**双曲正切函数tanh**，它产生在范围[-1, 1]的输出值。它们是密切相关的，因为![](img/B15439_17_002.png)。两个函数都受到饱和的影响，因为它们的梯度在非常低和高的输入值时变得非常小。然而，tanh通常表现更好，因为它更接近恒等函数，所以对于小的激活值，网络的行为更像是一个线性模型，这反过来又促进了训练。
- en: Output units and cost functions
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出单元和成本函数
- en: 'The choice of NN output format and cost function depends on the type of supervised
    learning problem:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: NN输出格式和成本函数的选择取决于监督学习问题的类型：
- en: '**Regression problems** use a linear output unit that computes the dot product
    of its weights with the final hidden layer activations, typically in conjunction
    with mean squared error cost'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归问题**使用线性输出单元，计算其权重与最终隐藏层激活的点积，通常与均方误差成本一起使用。'
- en: '**Binary classification** uses sigmoid output units to model a Bernoulli distribution
    just like logistic regression with hidden activations as input'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元分类**使用sigmoid输出单元来模拟伯努利分布，就像逻辑回归一样，其中隐藏激活作为输入'
- en: '**Multiclass problems** rely on softmax units that generalize the logistic
    sigmoid and model a discrete distribution over more than two classes, as demonstrated
    earlier'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类问题**依赖于softmax单元，它们推广了逻辑sigmoid并模拟了超过两个类别的离散分布，正如之前展示的那样'
- en: Binary and multiclass problems typically use cross-entropy loss, which significantly
    improves training efficacy compared to mean squared error (see *Chapter 6*, *The
    Machine Learning Process*, for additional information on loss functions).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 二元和多类问题通常使用交叉熵损失，与均方误差相比，这显着提高了训练效果（有关损失函数的其他信息，请参见*第6章*，*机器学习过程*）。
- en: How to regularize deep NNs
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何正则化深度NN
- en: The downside of the capacity of NNs to approximate arbitrary functions is the
    greatly increased risk of overfitting. The best **protection against overfitting**
    is to train the model on a larger dataset. Data augmentation, such as creating
    slightly modified versions of images, is a powerful alternative approach. The
    generation of synthetic financial training data for this purpose is an active
    research area that we will address in *Chapter 20*, *Autoencoders for Conditional
    Risk Factors and Asset Pricing* (see, for example, Fu et al. 2019).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: NN近似任意函数的容量的缺点是过度拟合的风险大大增加。对**过度拟合的最佳保护**是在更大的数据集上训练模型。数据增强，例如创建图像的略微修改版本，是一个强大的替代方法。为此目的生成合成金融训练数据是一个活跃的研究领域，我们将在*第20章*，*自编码器用于条件风险因素和资产定价*中讨论这一点（例如，Fu等人2019年）。
- en: As an alternative or complement to obtaining more data, regularization can help
    mitigate the risk of overfitting. For all models discussed so far in this book,
    there is some form of regularization that modiﬁes the learning algorithm to reduce
    its generalization error without negatively affecting its training error. Examples
    include the penalties added to the ridge and lasso regression objectives and the
    split or depth constraints used with decision trees and tree-based ensemble models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 作为获取更多数据的替代或补充，正则化可以帮助减轻过度拟合的风险。在本书中到目前为止讨论的所有模型中，都有一些形式的正则化，它修改学习算法以减少其泛化误差，而不会对其训练误差产生负面影响。示例包括添加到岭和套索回归目标中的惩罚以及用于决策树和基于树的集成模型的分割或深度约束。
- en: Frequently, regularization takes the form of a soft constraint on the parameter
    values that trades off some additional bias for lower variance. A common practical
    finding is that the model with the lowest generalization error is not the model
    with the exact right size of parameters, but rather a larger model that has been
    well regularized. Popular NN regularization techniques that can be used in combination
    include parameter norm penalties, early stopping, and dropout.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 经常，正则化采用对参数值的软约束形式，以权衡一些额外的偏差以获得较低的方差。一个常见的实际发现是，具有最低泛化误差的模型不是具有精确正确参数大小的模型，而是一个经过很好正则化的更大的模型。可以结合使用的流行的NN正则化技术包括参数范数惩罚，提前停止和丢弃。
- en: Parameter norm penalties
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数范数惩罚
- en: We encountered **parameter norm penalties** for lasso and ridge regression as
    **L1 and L2 regularization**, respectively, in *Chapter 7*, *Linear Models – From
    Risk Factors to Return Forecasts*. In the NN context, parameter norm penalties
    similarly modify the objective function by adding a term that represents the L1
    or L2 norm of the parameters, weighted by a hyperparameter that requires tuning.
    For NN, the bias parameters are usually not constrained, only the weights.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第7章*，*线性模型-从风险因素到收益预测*中遇到了**参数范数惩罚**，分别用作**L1和L2正则化**的套索和岭回归。在NN上下文中，参数范数惩罚通过添加一个代表参数的L1或L2范数的项来类似地修改目标函数，权重由需要调整的超参数加权。对于NN，偏置参数通常不受限制，只有权重。
- en: L1 regularization can produce sparse parameter estimates by reducing weights
    all the way to zero. L2 regularization, in contrast, preserves directions along
    which the parameters signiﬁcantly reduce the cost function. Penalties or hyperparameter
    values can vary across layers, but the added tuning complexity quickly becomes
    prohibitive.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: L1正则化可以通过将权重减少到零来产生稀疏的参数估计。相比之下，L2正则化保留了参数显著减少成本函数的方向。惩罚或超参数的值可以在各个层之间变化，但添加的调整复杂性很快变得令人难以承受。
- en: Early stopping
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 早停止
- en: 'We encountered **early stopping** as a regularization technique in *Chapter
    12*, *Boosting Your Trading Strategy*. It is perhaps the most common NN regularization
    method because it is both effective and simple to use: it monitors the model''s
    performance on a validation set and stops training when the performance ceases
    to improve for a certain number of observations to prevent overfitting.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第12章* *提升您的交易策略*中遇到了**早停止**作为一种正则化技术。它可能是最常见的神经网络正则化方法，因为它既有效又简单：它监视模型在验证集上的性能，并在一定数量的观察次数内停止训练，以防止过拟合。
- en: 'Early stopping can be viewed as **efficient hyperparameter selection** that
    automatically determines the correct amount of regularization, whereas parameter
    penalties require hyperparameter tuning to identify the ideal weight decay. Just
    be careful to avoid **lookahead bias**: backtest results will be exceedingly positive
    when early stopping uses out-of-sample data that would not be available during
    a real-life implementation of the strategy.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 早停止可以被看作是**有效的超参数选择**，它可以自动确定正确的正则化量，而参数惩罚则需要超参数调整来确定理想的权重衰减。只要注意避免**前瞻性偏见**：当早停止使用不可用于策略实施的样本外数据时，回测结果将过度正面。
- en: Dropout
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dropout
- en: '**Dropout** refers to the randomized omission of individual units with a given
    probability during forward or backward propagation. As a result, these omitted
    units do not contribute to the training error or receive updates.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dropout** 是指在前向或后向传播过程中以给定概率随机省略个别单元。因此，这些被省略的单元不会对训练误差做出贡献，也不会接收更新。'
- en: The technique is computationally inexpensive and does not constrain the choice
    of model or training procedure. While more iterations are necessary to achieve
    the same amount of learning, each iteration is faster due to the lower computational
    cost. Dropout reduces the risk of overfitting by preventing units from compensating
    for mistakes made by other units during the training process.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术计算成本低廉，不限制模型或训练过程的选择。虽然需要更多迭代才能达到相同的学习量，但由于计算成本较低，每次迭代速度更快。Dropout通过防止单元在训练过程中弥补其他单元的错误来降低过拟合的风险。
- en: Training faster – optimizations for deep learning
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更快地训练 - 深度学习的优化
- en: Backprop refers to the computation of the gradient of the cost function with
    respect to the internal parameter we wish to update and the use of this information
    to update the parameter values. The gradient is useful because it indicates the
    direction of parameter change that causes the maximal increase in the cost function.
    Hence, adjusting the parameters according to the negative gradient produces an
    optimal cost reduction, at least for a region very close to the observed samples.
    See Ruder (2017) for an excellent overview of key gradient descent optimization
    algorithms.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是指计算目标函数相对于我们希望更新的内部参数的梯度，并利用这些信息来更新参数值。梯度是有用的，因为它指示导致成本函数最大增加的参数变化方向。因此，根据负梯度调整参数会产生最佳的成本减少，至少对于非常接近观察样本的区域来说是这样。有关关键梯度下降优化算法的出色概述，请参阅Ruder（2017）。
- en: 'Training deep NNs can be time-consuming due to the nonconvex objective function
    and the potentially large number of parameters. Several challenges can significantly
    delay convergence, find a poor optimum, or cause oscillations or divergence from
    the target:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度神经网络可能会耗费大量时间，这是由于非凸目标函数和可能庞大的参数数量所导致的。几个挑战可能会显著延迟收敛，找到一个糟糕的最优解，或者导致振荡或偏离目标：
- en: '**Local minima** can prevent convergence to a global optimum and cause poor
    performance'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部极小值** 可能会阻止收敛到全局最优解并导致性能差。'
- en: '**Flat regions with low gradients** that are not a local minimum can also prevent
    convergence while most likely being distant from the global optimum'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有低梯度的**平坦区域** 可能不是局部最小值，也可能阻止收敛，但很可能远离全局最优解。
- en: '**Steep regions with high gradients** resulting from multiplying several large
    weights can cause excessive adjustments'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于乘以几个大权重而导致的具有高梯度的陡峭区域可能会导致过度调整
- en: Deep architectures or long-term dependencies in an RNN require the multiplication
    of many weights during backpropagation, leading to **vanishing gradients** so
    that at least parts of the NN receive few or no updates
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN中的深层结构或长期依赖性需要在反向传播过程中乘以许多权重，导致**梯度消失**，使得至少部分NN接收到少量或没有更新
- en: Several algorithms have been developed to address some of these challenges,
    namely variations of stochastic gradient descent and approaches that use adaptive
    learning rates. There is no single best algorithm, although adaptive learning
    rates have shown some promise.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发了几种算法来解决其中一些挑战，即随机梯度下降的变体和使用自适应学习率的方法。虽然自适应学习率并没有单一的最佳算法，但已经显示出一些希望。
- en: Stochastic gradient descent
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: 'Gradient descent iteratively adjusts these parameters using the gradient information.
    For a given parameter ![](img/B15439_17_003.png), the basic gradient descent rule
    adjusts the value by the negative gradient of the loss function with respect to
    this parameter, multiplied by a learning rate ![](img/B15439_17_004.png):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降通过梯度信息迭代地调整这些参数。对于给定的参数 ![](img/B15439_17_003.png)，基本梯度下降规则通过损失函数相对于该参数的负梯度乘以学习速率
    ![](img/B15439_17_004.png) 来调整该值：
- en: '![](img/B15439_17_005.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_005.png)'
- en: The gradient can be evaluated for all training data, a randomized batch of data,
    or individual observations (called online learning). Random samples give rise
    to **stochastic gradient descent** (**SGD**), which often leads to faster convergence
    if random samples are an unbiased estimate of the gradient direction throughout
    the training process.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度可以对所有训练数据、随机批量数据或单个观测（称为在线学习）进行评估。随机样本产生**随机梯度下降**（**SGD**），如果随机样本在整个训练过程中对梯度方向是无偏估计，则通常导致更快的收敛。
- en: 'However, there are numerous challenges: it can be difficult to define a learning
    rate or a rate schedule that facilitates efficient convergence ex ante—too low
    a rate prolongs the process, and too high a rate can lead to repeated overshooting
    and oscillation around or even divergence from a minimum. Furthermore, the same
    learning rate may not be adequate for all parameters, that is, in all directions
    of change.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，存在许多挑战：很难预先定义一个能够促进有效收敛的学习率或速率调度——太低的速率会延长过程，而太高的速率可能会导致反复超调和围绕最小值振荡甚至发散。此外，相同的学习率可能不适用于所有参数，即在所有变化方向上都不适用。
- en: Momentum
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动量
- en: A popular refinement of basic gradient descent adds momentum to **accelerate
    the convergence to a local minimum**. Illustrations of momentum often use the
    example of a local optimum at the center of an elongated ravine (while in practice
    the dimensionality would be much higher than three). It implies a minimum inside
    a deep and narrow canyon with very steep walls that have a large gradient on one
    side and a much gentler slope towards a local minimum at the bottom of this region
    on the other side. Gradient descent naturally follows the steep gradient and will
    make repeated adjustments up and down the walls of the canyons with much slower
    movements towards the minimum.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 基本梯度下降的一个流行改进是将动量添加到**加速收敛到局部最小值**。动量的图示经常使用一个位于细长峡谷中心的局部最优值的例子（实际上，维度要比三维高得多）。它意味着一个位于深而窄的峡谷内的最小值，该峡谷的壁非常陡峭，其中一侧的梯度很大，另一侧朝着该区域底部的局部最小值的斜坡要缓得多。梯度下降自然而然地沿着陡峭的梯度走，将反复调整上下峡谷的墙壁，向着最小值的方向移动得更慢。
- en: 'Momentum aims to address such a situation by **tracking recent directions**
    and adjusting the parameters by a weighted average of the most recent gradient
    and the currently computed value. It uses a momentum term ![](img/B15439_17_037.png)
    to weigh the contribution of the latest adjustment to this iteration''s update
    *v*[t]:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 动量旨在通过**跟踪最近的方向**并通过最近的梯度和当前计算值的加权平均来调整参数来解决这种情况。它使用动量项 ![](img/B15439_17_037.png)
    来衡量最新调整对此迭代更新 *v*[t] 的贡献：
- en: '![](img/B15439_17_006.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_006.png)'
- en: '**Nesterov momentum** is a simple change to normal momentum. Here, the gradient
    term is not computed at the current parameter space position ![](img/Image74692.png)
    but instead from an intermediate position. The goal is to correct for the momentum
    term overshooting or pointing in the wrong direction (Sutskever et al. 2013).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**Nesterov momentum** 是对普通动量的简单改进。在这里，梯度项不是在当前参数空间位置计算！[](img/Image74692.png)，而是从一个中间位置计算。其目标是纠正动量项过度偏离或指向错误方向（Sutskever等人，2013）。'
- en: Adaptive learning rates
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自适应学习率
- en: The choice of the appropriate learning rate is very challenging as highlighted
    in the previous subsection on stochastic gradient descent. At the same time, it
    is one of the most important parameters that strongly impacts training time and
    generalization performance.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当的学习率非常具有挑战性，正如前一小节中所强调的随机梯度下降。与此同时，它是最重要的参数之一，它强烈影响训练时间和泛化性能。
- en: While momentum addresses some of the issues with learning rates, it does so
    at the expense of introducing another hyperparameter, the **momentum rate**. Several
    algorithms aim to adapt the learning rate throughout the training process based
    on gradient information.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然动量解决了一些学习率的问题，但是以引入另一个超参数，**动量率** 为代价。几种算法旨在根据梯度信息在整个训练过程中自适应地调整学习率。
- en: AdaGrad
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: AdaGrad
- en: AdaGrad accumulates all historical, parameter-specific gradient information
    and continues to rescale the learning rate inversely proportional to the squared
    cumulative gradient for a given parameter. The goal is to slow down changes for
    parameters that have already changed a lot and to encourage adjustments for those
    that haven't.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad 累积所有历史的、参数特定的梯度信息，并继续根据给定参数的平方累积梯度来反比例地重新缩放学习率。其目标是减缓已经大量变化的参数的变化，鼓励还没有变化的参数进行调整。
- en: AdaGrad is designed to perform well on convex functions and has had a mixed
    performance in a DL context because it can reduce the learning rate too quickly
    based on early gradient information.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad 设计用于在凸函数上表现良好，在 DL 上表现不佳，因为它可能会根据早期梯度信息过快地降低学习率。
- en: RMSProp
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RMSProp
- en: RMSProp modifies AdaGrad to use an exponentially weighted average of the cumulative
    gradient information. The goal is to put more emphasis on recent gradients. It
    also introduces a new hyperparameter that controls the length of the moving average.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: RMSProp 修改了 AdaGrad 以使用累积梯度信息的指数加权平均值。其目标是更加强调最近的梯度。它还引入了一个新的超参数，用于控制移动平均的长度。
- en: RMSProp is a popular algorithm that often performs well, provided by the various
    libraries that we will introduce later and routinely used in practice.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: RMSProp 是一种流行的算法，通常表现良好，由我们稍后将介绍的各种库提供，并在实践中经常使用。
- en: Adam
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Adam
- en: Adam stands for **adaptive moment derivation** and combines aspects of RMSProp
    with Momentum. It is considered fairly robust and often used as the default optimization
    algorithm (Kingma and Ba, 2014).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 代表 **自适应动量估计**，将 RMSProp 的一些方面与动量结合起来。它被认为是相当稳健的，并经常用作默认的优化算法（Kingma 和
    Ba，2014）。
- en: 'Adam has several hyperparameters with recommended default values that may benefit
    from some tuning:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 有几个带有建议的默认值的超参数，可能会从一些调整中受益：
- en: '**alpha**: The learning rate or step size determines how much weights are updated
    so that larger (smaller) values speed up (slow down) learning before the rate
    is updated; many libraries use the 0.001 default'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**alpha**：学习率或步长确定更新权重的程度，较大（较小）的值在更新速度之前加快（减慢）学习；许多库使用默认值0.001'
- en: '**beta**[1]: The exponential decay rate for the first moment estimates; typically
    set to 0.9'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**beta**[1]：第一矩估计的指数衰减率；通常设置为0.9'
- en: '**beta**[2]. The exponential decay rate for the second-moment estimates; usually
    set to 0.999'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**beta**[2]：第二矩估计的指数衰减率；通常设置为0.999'
- en: '**epsilon**: A very small number to prevent division by zero; often set to
    1e-8'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**epsilon**：一个非常小的数字，用于防止除零；通常设置为1e-8'
- en: Summary – how to tune key hyperparameters
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结 - 如何调整关键的超参数
- en: Hyperparameter optimization aims at **tuning the capacity of the model** so
    that it matches the complexity of the relationship between the input of the data.
    Excess capacity makes overfitting likely and requires either more data that introduces
    additional information into the learning process, reducing the size of the model,
    or more aggressive use of the various regularization tools just described.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数优化旨在**调整模型的容量**，使其匹配数据输入之间的关系的复杂性。过多的容量会增加过拟合的可能性，需要更多的数据，将额外的信息引入到学习过程中，减小模型的大小，或更积极地使用刚刚描述的各种正则化工具。
- en: 'The **principal diagnostic tool** is the behavior of training and validation
    error described in *Chapter 6*, *The Machine Learning Process*: if the validation
    error worsens while the training error continues to drop, the model is overfitting
    because its capacity is too high. On the other hand, if performance falls short
    of expectations, increasing the size of the model may be called for.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**主要的诊断工具**是描述在*第6章*，*机器学习过程*中的训练和验证错误的行为：如果验证错误恶化，而训练错误继续下降，那么模型就是过度拟合的，因为其容量太高。另一方面，如果性能不符合预期，可能需要增加模型的大小。'
- en: 'The most important aspect of parameter optimization is the architecture itself
    as it largely determines the number of parameters: other things being equal, more
    or wider hidden layers increase the capacity. As mentioned before, the best performance
    is often associated with models that have excess capacity but are well regularized
    using mechanisms like dropout or L1/L2 penalties.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 参数优化最重要的方面是架构本身，因为它很大程度上决定了参数的数量：其他条件相同，更多或更宽的隐藏层会增加容量。正如前面提到的，最佳性能通常与具有过量容量但使用像dropout或L1/L2惩罚这样的机制进行了良好正则化的模型相关联。
- en: In addition to **balancing model size and regularization**, it is important
    to tune the **learning rate** because it can undermine the optimization process
    and reduce the effective model capacity. The adaptive optimization algorithms
    offer a good starting point as described for Adam, the most popular option.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 除了**平衡模型大小和正则化**之外，调整**学习率**也很重要，因为它可能会破坏优化过程并降低有效模型容量。自适应优化算法提供了一个很好的起点，就像Adam描述的那样，这是最流行的选项。
- en: A neural network from scratch in Python
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python从头开始的神经网络
- en: To gain a better understanding of how NNs work, we will formulate the single-layer
    architecture and forward propagation computations displayed in *Figure 17.2* using
    matrix algebra and implement it using NumPy. You can find the code samples in
    the notebook `build_and_train_feedforward_nn`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解NNs的工作原理，我们将使用矩阵代数来表述单层架构和*图17.2*中显示的前向传播计算，并使用NumPy实现它。你可以在笔记本`build_and_train_feedforward_nn`中找到代码示例。
- en: The input layer
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入层
- en: 'The architecture shown in *Figure 17.2* is designed for two-dimensional input
    data *X* that represents two different classes *Y*. In matrix form, both *X* and
    *Y* are of shape ![](img/B15439_17_009.png):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17.2*中显示的架构设计用于表示两个不同类别*Y*的二维输入数据*X*。以矩阵形式，*X*和*Y*的形状都是![](img/B15439_17_009.png)：'
- en: '![](img/B15439_17_010.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_010.png)'
- en: 'We will generate 50,000 random binary samples in the form of two concentric
    circles with different radius using scikit-learn''s `make_circles` function so
    that the classes are not linearly separable:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用scikit-learn的`make_circles`函数生成50,000个随机二进制样本，形成两个半径不同的同心圆，以便类别不是线性可分的：
- en: '[PRE0]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then convert the one-dimensional output into a two-dimensional array:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将一维输出转换为二维数组：
- en: '[PRE1]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Figure 17.3* shows a scatterplot of the data that is clearly not linearly
    separable:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17.3*显示了数据的散点图，很明显不是线性可分的：'
- en: '![](img/B15439_17_03.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_03.png)'
- en: 'Figure 17.3: Synthetic data for binary classification'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.3：二元分类的合成数据
- en: The hidden layer
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐藏层
- en: 'The hidden layer *h* projects the two-dimensional input into a three-dimensional
    space using the weights W^h and translates the result by the bias vector b^h.
    To perform this affine transformation, the hidden layer weights are represented
    by a ![](img/B15439_17_011.png) matrix W^h, and the hidden layer bias vector by
    a three-dimensional vector:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层*h*使用权重W^h将二维输入投影到三维空间，并通过偏置向量b^h将结果平移。为了执行这个仿射变换，隐藏层权重由一个![](img/B15439_17_011.png)矩阵W^h表示，而隐藏层偏置向量由一个三维向量表示：
- en: '![](img/B15439_17_012.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_012.png)'
- en: 'The hidden layer activations *H* result from the application of the sigmoid
    function to the dot product of the input data and the weights after adding the
    bias vector:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层激活 *H* 是通过将输入数据与加入偏置向量后的权重的点积应用于 sigmoid 函数而得到的：
- en: '![](img/B15439_17_013.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_013.png)'
- en: 'To implement the hidden layer using NumPy, we first define the `logistic` sigmoid
    function:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 NumPy 实现隐藏层，我们首先定义 `logistic` sigmoid 函数：
- en: '[PRE2]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then define a function that computes the hidden layer activations as a function
    of the relevant inputs, weights, and bias values:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义一个函数，根据相关的输入、权重和偏置值计算隐藏层激活：
- en: '[PRE3]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output layer
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出层
- en: 'The output layer compresses the three-dimensional hidden layer activations
    *H* back to two dimensions using a ![](img/B15439_17_014.png) weight matrix W^o
    and a two-dimensional bias vector **b**^o:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层使用一个 ![](img/B15439_17_014.png) 权重矩阵 W^o 和一个二维偏置向量 **b**^o 将三维隐藏层激活 *H* 压缩回两维：
- en: '![](img/B15439_17_015.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_015.png)'
- en: 'The linear combination of the hidden layer outputs results in an ![](img/B15439_17_016.png)
    matrix **Z**^o:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层输出的线性组合导致一个 ![](img/B15439_17_016.png) 矩阵 **Z**^o：
- en: '![](img/B15439_17_017.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_017.png)'
- en: 'The output layer activations are computed by the softmax function ![](img/B15439_17_018.png)
    that normalizes the **Z**^o to conform to the conventions used for discrete probability
    distributions:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层激活由 softmax 函数 ![](img/B15439_17_018.png) 计算，该函数将 **Z**^o 规范化以符合离散概率分布的惯例：
- en: '![](img/B15439_17_019.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_019.png)'
- en: 'We create a softmax function in Python as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Python 中创建一个 softmax 函数如下所示：
- en: '[PRE4]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As defined here, the output layer activations depend on the hidden layer activations
    and the output layer weights and biases:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如此定义，输出层激活取决于隐藏层激活和输出层权重和偏置：
- en: '[PRE5]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now we have all the components we need to integrate the layers and compute the
    NN output directly from the input.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了集成层并直接从输入计算 NN 输出所需的所有组件。
- en: Forward propagation
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正向传播
- en: 'The `forward_prop` function combines the previous operations to yield the output
    activations from the input data as a function of weights and biases:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward_prop` 函数将前述操作组合起来，从输入数据中产生输出激活作为权重和偏置的函数：'
- en: '[PRE6]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `predict` function produces the binary class predictions given weights,
    biases, and input data:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict` 函数根据权重、偏置和输入数据产生二元类别预测：'
- en: '[PRE7]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The cross-entropy cost function
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉熵损失函数
- en: 'The final piece is the cost function to evaluate the NN output based on the
    given label. The cost function *J* uses the cross-entropy loss ![](img/B15439_17_020.png),
    which sums the deviations of the predictions for each class *c* from the actual
    outcome:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一块是根据给定标签评估 NN 输出的损失函数。损失函数 *J* 使用交叉熵损失 ![](img/B15439_17_020.png)，它对每个类别
    *c* 的预测与实际结果的偏差进行求和：
- en: '![](img/B15439_17_021.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_021.png)'
- en: 'It takes the following form in Python:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，它的形式如下：
- en: '[PRE8]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How to implement backprop using Python
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 Python 实现反向传播
- en: To update the NN weights and bias values using backprop, we need to compute
    the gradient of the cost function. The gradient represents the partial derivative
    of the cost function with respect to the target parameter.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用反向传播更新神经网络的权重和偏置值，我们需要计算损失函数的梯度。梯度表示损失函数相对于目标参数的偏导数。
- en: How to compute the gradient
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何计算梯度
- en: The NN composes a set of nested functions as highlighted earlier. Hence, the
    gradient of the loss function with respect to internal, hidden parameters is computed
    using the chain rule of calculus.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: NN 组成一组嵌套函数，如前所述。因此，使用微积分的链式法则计算损失函数相对于内部隐藏参数的梯度。
- en: 'For scalar values, given the functions *z* = *h*(*x*) and *y* = *o*(*h*(*x*))
    = *o* (*z*), we compute the derivative of* y* with respect to *x* using the chain
    rule as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标量值，给定函数 *z* = *h*(*x*) 和 *y* = *o*(*h*(*x*)) = *o* (*z*)，我们使用链式法则计算 *y* 相对于
    *x* 的导数如下：
- en: '![](img/B15439_17_022.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_022.png)'
- en: 'For vectors, with ![](img/B15439_17_023.png) and ![](img/B15439_17_024.png)
    so that the hidden layer *h* maps from R^n to R^m and *z* = *h*(*x*) and *y* =
    *o* (*z*), we get:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于向量，有 ![](img/B15439_17_023.png) 和 ![](img/B15439_17_024.png)，使得隐藏层 *h* 从 R^n
    映射到 R^m，*z* = *h*(*x*)，*y* = *o* (*z*)，我们得到：
- en: '![](img/B15439_17_025.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_025.png)'
- en: 'We can express this more concisely using matrix notation using the ![](img/B15439_17_026.png)
    Jacobian matrix of *h*:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用矩阵表示更简洁地表达这一点，使用 *h* 的雅可比矩阵 ![](img/B15439_17_026.png)：
- en: '![](img/B15439_17_027.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_027.png)'
- en: 'which contains the partial derivatives for each of the *m* components of *z*
    with respect to each of the *n* inputs *x*. The gradient ![](img/B15439_17_028.png)
    of *y* with respect to *x* contains all partial derivatives and can thus be written
    as:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含了对于 *z* 的每个 *m* 组件相对于每个 *n* 输入 *x* 的偏导数。*y* 相对于 *x* 的梯度 ![](img/B15439_17_028.png)
    包含了所有的偏导数，因此可以写成：
- en: '![](img/B15439_17_029.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_029.png)'
- en: The loss function gradient
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数的梯度
- en: 'The derivative of the cross-entropy loss function *J* with respect to each
    output layer activation *i* = 1, ..., *N* is a very simple expression (see the
    notebook for details), shown below on the left for scalar values and on the right
    in matrix notation:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失函数 *J* 对于每个输出层激活 *i* = 1, ..., *N* 的导数是一个非常简单的表达式（详见笔记本），如下左侧为标量值，右侧为矩阵表示：
- en: '![](img/B15439_17_030.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_030.png)'
- en: 'We define `loss_gradient` function accordingly:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相应地定义`loss_gradient`函数：
- en: '[PRE9]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output layer gradients
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出层梯度
- en: 'To propagate the update back to the output layer weights, we use the gradient
    of the loss function *J* with respect to the weight matrix:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 要将更新传播回输出层权重，我们使用损失函数 *J* 对于权重矩阵的梯度：
- en: '![](img/B15439_17_031.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_031.png)'
- en: 'and for the bias:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 和偏置项的梯度：
- en: '![](img/B15439_17_032.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_032.png)'
- en: 'We can now define `output_weight_gradient` and `output_bias_gradient` accordingly,
    both taking the loss gradient ![](img/B15439_17_033.png) as input:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以相应地定义`output_weight_gradient`和`output_bias_gradient`，两者都以损失梯度 ![](img/B15439_17_033.png)
    作为输入：
- en: '[PRE10]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The hidden layer gradients
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐藏层梯度
- en: 'The gradient of the loss function with respect to the hidden layer values computes
    as follows, where ![](img/B15439_17_034.png) refers to the element-wise matrix
    product:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数对于隐藏层值的梯度计算如下，其中 ![](img/B15439_17_034.png) 表示逐元素的矩阵乘积：
- en: '![](img/B15439_17_035.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_035.png)'
- en: 'We define a `hidden_layer_gradient` function to encode this result:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个`hidden_layer_gradient`函数来编码这个结果：
- en: '[PRE11]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The gradients for hidden layer weights and biases are:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层权重和偏置的梯度为：
- en: '![](img/B15439_17_036.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_036.png)'
- en: 'The corresponding functions are:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的函数是：
- en: '[PRE12]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Putting it all together
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容整合起来
- en: 'To prepare for the training of our network, we create a function that combines
    the previous gradient definition and computes the relevant weight and bias updates
    from the training data and labels, and the current weight and bias values:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备训练我们的网络，我们创建一个函数，该函数结合了先前的梯度定义，并从训练数据和标签以及当前的权重和偏置值计算相关的权重和偏置更新：
- en: '[PRE13]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Testing the gradients
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试梯度
- en: The notebook contains a test function that compares the gradient derived previously
    analytically using multivariate calculus to a numerical estimate that we obtain
    by slightly perturbing individual parameters. The test function validates that
    the resulting change in output value is similar to the change estimated by the
    analytical gradient.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本包含一个测试函数，该函数将先前使用多元微积分解析导出的梯度与我们通过轻微扰动单个参数获得的数值估计进行比较。测试函数验证了输出值的变化与分析梯度估计的变化类似。
- en: Implementing momentum updates using Python
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 Python 实现动量更新
- en: 'To incorporate momentum into the parameter updates, define an `update_momentum`
    function that combines the results of the `compute_gradients` function we just
    used with the most recent momentum updates for each parameter matrix:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要将动量合并到参数更新中，定义一个`update_momentum`函数，该函数将我们刚刚使用的`compute_gradients`函数的结果与每个参数矩阵的最新动量更新组合起来：
- en: '[PRE14]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `update_params` function performs the actual updates:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`update_params`函数执行实际的更新：'
- en: '[PRE15]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Training the network
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练网络
- en: 'To train the network, we first randomly initialize all network parameters using
    a standard normal distribution (see the notebook). For a given number of iterations
    or epochs, we run momentum updates and compute the training loss as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练网络，我们首先使用标准正态分布随机初始化所有网络参数（参见笔记本）。对于给定的迭代次数或周期，我们运行动量更新并计算训练损失如下：
- en: '[PRE16]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*Figure 17.4* plots the training loss over 50,000 iterations for 50,000 training
    samples with a momentum term of 0.5 and a learning rate of 1e-4\. It shows that
    it takes over 5,000 iterations for the loss to start to decline but then does
    so very fast. We have not used SGD, which would have likely accelerated convergence
    significantly.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17.4* 绘制了使用动量项为0.5和学习率为1e-4的50,000个训练样本进行50,000次迭代的训练损失。它显示损失需要超过5,000次迭代才开始下降，但然后下降速度非常快。我们没有使用
    SGD，这可能会显著加速收敛。'
- en: '![](img/B15439_17_04.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_04.png)'
- en: 'Figure 17.4: Training loss per iteration'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.4：每次迭代的训练损失
- en: The plots in *Figure 17.5* show the function learned by the neural network with
    a three-dimensional hidden layer from two-dimensional data with two classes that
    are not linearly separable. The left panel displays the source data and the decision
    boundary that misclassifies very few data points and would further improve with
    continued training.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 17.5* 中的图表展示了具有三维隐藏层的神经网络从二维数据中学习的函数，这些数据有两个不是线性可分的类。左侧面板显示了源数据和决策边界，它误分类了非常少的数据点，并且随着持续训练将进一步改善。'
- en: 'The center panel shows the representation of the input data learned by the
    hidden layer. The network learns weights so that the projection of the input from
    two to three dimensions enables the linear separation of the two classes. The
    right plot shows how the output layer implements the linear separation in the
    form of a cutoff value of 0.5 in the output dimension:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 中央面板显示了隐藏层学习的输入数据的表示。网络学习权重，以便将输入从二维投影到三维，从而使得两个类能够线性分离。右侧图显示了输出层如何以 0.5 的输出维度值作为线性分离的截止值：
- en: '![](img/B15439_17_05.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_05.png)'
- en: 'Figure 17.5: Visualizing the function learned by the neural network'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.5：可视化神经网络学习的函数
- en: 'To **sum up**: we have seen how a very simple network with a single hidden
    layer with three nodes and a total of 17 parameters is able to learn how to solve
    a nonlinear classification problem using backprop and gradient descent with momentum.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**：我们已经看到一个非常简单的网络，只有一个包含三个节点的隐藏层和总共 17 个参数，能够学习如何使用反向传播和带动量的梯度下降来解决非线性分类问题。'
- en: We will next review how to use popular DL libraries that facilitate the design
    and fast training of complex architectures while using sophisticated techniques
    to prevent overfitting and evaluate the results.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将回顾如何使用流行的 DL 库，这些库有助于设计复杂的架构并进行快速训练，同时使用复杂技术来防止过拟合并评估结果。
- en: Popular deep learning libraries
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流行的深度学习库
- en: Currently, the most popular DL libraries are TensorFlow (supported by Google),
    Keras (led by Francois Chollet, now at Google), and PyTorch (supported by Facebook).
    Development is very active with PyTorch at version 1.4 and TensorFlow at 2.2 as
    of March 2020\. TensorFlow 2.0 adopted Keras as its main interface, effectively
    combining both libraries into one.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最流行的 DL 库是 TensorFlow（由 Google 支持）、Keras（由 Francois Chollet 领导，现在在 Google）和
    PyTorch（由 Facebook 支持）。开发非常活跃，截至 2020 年 3 月，PyTorch 版本为 1.4，TensorFlow 版本为 2.2。TensorFlow
    2.0 将 Keras 作为其主要接口，有效地将两个库合并为一个。
- en: All libraries provide the design choices, regularization methods, and backprop
    optimizations we discussed previously in this chapter. They also facilitate fast
    training on one or several **graphics processing units** (**GPUs**). The libraries
    differ slightly in their focus with TensorFlow originally designed for deployment
    in production and prevalent in the industry, while PyTorch has been popular among
    academic researchers; however, the interfaces are gradually converging.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的库都提供了我们在本章中讨论过的设计选择、正则化方法和反向传播优化。它们还能够在一个或多个**图形处理单元**（**GPU**）上进行快速训练。这些库在重点上略有不同，TensorFlow
    最初设计用于在生产中部署，在工业界很普遍，而 PyTorch 在学术研究者中很受欢迎；然而，接口正在逐渐趋同。
- en: We will illustrate the use of TensorFlow and PyTorch using the same network
    architecture and dataset as in the previous section.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与上一节相同的网络架构和数据集来说明 TensorFlow 和 PyTorch 的使用。
- en: Leveraging GPU acceleration
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用 GPU 加速
- en: DL is very computationally intensive, and good results often require large datasets.
    As a result, model training and evaluation can become rather time-consuming. GPUs
    are highly optimized for the matrix operations required by deep learning models
    and tend to have more processing power, rendering speedups of 10x or more not
    uncommon.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: DL 非常计算密集，而且好的结果通常需要大型数据集。因此，模型训练和评估可能会变得非常耗时。GPU 高度优化了深度学习模型所需的矩阵运算，并且往往具有更多的处理能力，使得加速
    10 倍或更多不罕见。
- en: All popular deep learning libraries support the use of a GPU, and some also
    allow for parallel training on multiple GPUs. The most common types of GPU are
    produced by NVIDIA, and configuration requires installation and setup of the CUDA
    environment. The process continues to evolve and can be somewhat challenging depending
    on your computational environment.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 所有流行的深度学习库都支持使用 GPU，并且一些还允许在多个 GPU 上进行并行训练。最常见的 GPU 类型由 NVIDIA 生产，配置需要安装和设置
    CUDA 环境。这个过程不断发展，取决于您的计算环境，可能会有一定挑战。
- en: A more straightforward way to leverage GPU is via the Docker virtualization
    platform. There are numerous images available that you can run in a local container
    managed by Docker that circumvents many of the driver and version conflicts that
    you may otherwise encounter. TensorFlow provides Docker images on its website
    that can also be used with Keras.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 GPU 的更简单方法是通过 Docker 虚拟化平台。有大量的镜像可供您在由 Docker 管理的本地容器中运行，避免了您可能遇到的许多驱动程序和版本冲突。TensorFlow
    在其网站上提供了 Docker 镜像，也可以与 Keras 一起使用。
- en: See GitHub for references and related instructions in the DL notebooks and the
    installation directory.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GitHub 上查看 DL 笔记本和安装目录中的参考和相关说明。
- en: How to use TensorFlow 2
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 TensorFlow 2
- en: TensorFlow became the leading deep learning library shortly after its release
    in September 2015, one year before PyTorch. TensorFlow 2 simplified the API that
    had grown increasingly complex over time by making the Keras API its principal
    interface.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 在 2015 年 9 月发布后不久成为了领先的深度学习库，在 PyTorch 之前一年。TensorFlow 2 简化了随着时间推移变得越来越复杂的
    API，通过将 Keras API 作为其主要接口。
- en: Keras was designed as a high-level API to accelerate the iterative workflow
    of designing and training deep neural networks with computational backends like
    TensorFlow, Theano, or CNTK. It has been integrated into TensorFlow in 2017\.
    You can also combine code from both libraries to leverage Keras' high-level abstractions
    as well as customized TensorFlow graph operations.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 被设计为高级 API，以加速使用 TensorFlow、Theano 或 CNTK 等计算后端设计和训练深度神经网络的迭代工作流程。它在 2017
    年被整合到 TensorFlow 中。您还可以结合两个库的代码，以利用 Keras 的高级抽象以及定制的 TensorFlow 图操作。
- en: In addition, TensorFlow adopts **eager execution**. Previously, you needed to
    define a complete computational graph for compilation into optimized operations.
    Running the compiled graph required the configuration of a session and the provision
    of the requisite data. Under eager execution, you can run TensorFlow operations
    on a line-by-line basis just like common Python code.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，TensorFlow 采用了**即时执行**。以前，您需要为编译成优化操作的完整计算图进行定义。运行编译后的图形需要配置会话并提供所需的数据。在即时执行下，您可以像常规
    Python 代码一样逐行运行 TensorFlow 操作。
- en: Keras supports both a slightly simpler Sequential API and a more flexible Functional
    API. We will introduce the former at this point and use the Functional API in
    more complex examples in the following chapters.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 支持稍微简单的 Sequential API 和更灵活的 Functional API。我们将在此介绍前者，并在后续章节中的更复杂示例中使用
    Functional API。
- en: To create a model, we just need to instantiate a `Sequential` object and provide
    a list with the sequence of standard layers and their configurations, including
    the number of units, type of activation function, or name.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建模型，我们只需要实例化一个`Sequential`对象，并提供一个包含标准层序列及其配置的列表，包括单元数、激活函数类型或名称。
- en: 'The first hidden layer needs information about the number of features in the
    matrix it receives from the input layer via the `input_shape` argument. In our
    simple case, there are just two. Keras infers the number of rows it needs to process
    during training, through the `batch_size` argument that we will pass to the `fit`
    method later in this section. TensorFlow infers the sizes of the inputs received
    by other layers from the previous layer''s `units` argument:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个隐藏层需要关于它从输入层通过`input_shape`参数接收到的矩阵中特征数的信息。在我们的简单案例中，只有两个。Keras 通过我们稍后将传递给本节中的`fit`方法的`batch_size`参数在训练期间推断需要处理的行数。TensorFlow
    通过前一层的`units`参数推断接收到的输入的大小：
- en: '[PRE17]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The Keras API provides numerous standard building blocks, including recurrent
    and convolutional layers, various options for regularization, a range of loss
    functions and optimizers, and also preprocessing, visualization, and logging (see
    the link to the TensorFlow documentation on GitHub for reference). It is also
    extensible.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Keras API提供了许多标准构建模块，包括循环和卷积层，各种正则化选项，一系列损失函数和优化器，以及预处理，可视化和日志记录（请参阅GitHub上的TensorFlow文档链接以供参考）。它也是可扩展的。
- en: 'The model''s `summary` method produces a concise description of the network
    architecture, including a list of the layer types and shapes and the number of
    parameters:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的`summary`方法生成对网络架构的简明描述，包括层类型和形状的列表以及参数数量：
- en: '[PRE18]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we compile the Sequential model to configure the learning process. To
    this end, we define the optimizer, the loss function, and one or several performance
    metrics to monitor during training:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编译Sequential模型以配置学习过程。为此，我们定义优化器，损失函数以及一种或多种在训练期间监视的性能指标：
- en: '[PRE19]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Keras uses callbacks to enable certain functionality during training, such
    as logging information for interactive display in TensorBoard (see the next section):'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Keras使用回调函数来在训练期间启用某些功能，例如在TensorBoard中记录信息以供交互式显示（见下一节）：
- en: '[PRE20]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To train the model, we call its `fit` method and pass several parameters in
    addition to the training data:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，我们调用它的`fit`方法，并在训练数据之外传递多个参数：
- en: '[PRE21]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: See the notebook for a visualization of the decision boundary that resembles
    the result from our earlier manual network implementation. The training with TensorFlow
    runs orders of magnitude faster, though.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅笔记本以可视化决策边界，它类似于我们先前的手动网络实现的结果。不过，使用TensorFlow进行训练速度快了几个数量级。
- en: How to use TensorBoard
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用TensorBoard
- en: TensorBoard is a great suite of visualization tools that comes with TensorFlow.
    It includes visualization tools to simplify the understanding, debugging, and
    optimization of NNs.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard是TensorFlow附带的一套优秀的可视化工具，包括可视化工具以简化对NNs的理解，调试和优化。
- en: You can use it to visualize the computational graph, plot various execution
    and performance metrics, and even visualize image data processed by the network.
    It also permits comparisons of different training runs.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用它来可视化计算图，绘制各种执行和性能指标，甚至可视化网络处理的图像数据。它还允许比较不同的训练运行。
- en: 'When you run the `how_to_use_tensorflow` notebook, with TensorFlow installed,
    you can launch TensorBoard from the command line:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`how_to_use_tensorflow`笔记本时，需要安装TensorFlow，然后可以从命令行启动TensorBoard：
- en: '[PRE22]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Alternatively, you can use it within your notebook by first loading the extension
    and then starting TensorBoard similarly by referencing the `log` directory:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以首先加载扩展程序，然后通过引用`log`目录类似地启动TensorBoard，在您的笔记本中使用它：
- en: '[PRE23]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: For starters, the visualizations include train and validation metrics (see the
    left panel of *Figure 17.6*).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，可视化包括训练和验证指标（请参阅*图17.6*的左面板）。
- en: In addition, you can view histograms of the weights and biases over various
    epochs (right panel of Figure 17.6; epochs evolve from back to front). This is
    useful because it allows you to monitor whether backpropagation succeeds in adjusting
    the weights as learning progresses and whether they are converging.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以查看各个时期的权重和偏差的直方图（图17.6的右面板；时期从后到前演变）。这很有用，因为它允许您监视反向传播是否成功地在学习过程中调整权重以及它们是否收敛。
- en: 'The values of weights should change from their initialization values over the
    course of several epochs and eventually stabilize:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的值应该在多个时期内从它们的初始化值改变并最终稳定：
- en: '![](img/B15439_17_06.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_06.png)'
- en: 'Figure 17.6: TensorBoard learning process visualization'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.6：TensorBoard学习过程可视化
- en: TensorBoard also lets you display and interactively explore the computational
    graph of your network, drilling down from the high-level structure to the underlying
    operations by clicking on the various nodes. The visualization for our simple
    example architecture (see the notebook) already includes numerous components but
    is very useful when debugging. For further reference, see the links on GitHub
    to more detailed tutorials.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard还允许您显示和交互式探索网络的计算图，通过单击各个节点从高级结构向下钻取到底层操作。我们简单示例架构的可视化（请参阅笔记本）已经包含了许多组件，但在调试时非常有用。有关更详细的参考，请参阅GitHub上更详细的教程链接。
- en: How to use PyTorch 1.4
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用PyTorch 1.4
- en: PyTorch was developed at the **Facebook AI Research** (**FAIR**) group led by
    Yann LeCunn, and the first alpha version released in September 2016\. It provides
    deep integration with Python libraries like NumPy that can be used to extend its
    functionality, strong GPU acceleration, and automatic differentiation using its
    autograd system. It provides more granular control than Keras through a lower-level
    API and is mainly used as a deep learning research platform but can also replace
    NumPy while enabling GPU computation.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是在由 Yann LeCunn 领导的**Facebook AI 研究**（**FAIR**）团队开发的，并于 2016 年 9 月发布了第一个
    alpha 版本。它与 NumPy 等 Python 库深度集成，可以用于扩展其功能，具有强大的 GPU 加速和使用其 autograd 系统进行自动微分。通过更低级别的
    API，它提供比 Keras 更细粒度的控制，并且主要用作深度学习研究平台，但也可以在启用 GPU 计算的同时替代 NumPy。
- en: It employs eager execution, in contrast to the static computation graphs used
    by, for example, Theano or TensorFlow. Rather than initially defining and compiling
    a network for fast but static execution, it relies on its autograd package for
    automatic differentiation of tensor operations; that is, it computes gradients
    "on the fly" so that network structures can be partially modified more easily.
    This is called **define-by-run**, meaning that backpropagation is defined by how
    your code runs, which in turn implies that every single iteration can be different.
    The PyTorch documentation provides a detailed tutorial on this.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 它采用即时执行，与 Theano 或 TensorFlow 等使用静态计算图的方式形成对比。与最初为了快速但静态执行而定义和编译网络不同，它依赖于其 autograd
    包来自动对张量操作进行微分；也就是说，它在“飞行中”计算梯度，以便更轻松地部分修改网络结构。这称为**按运行定义**，意味着反向传播是由代码运行方式定义的，这又意味着每次迭代都可能不同。PyTorch
    文档提供了关于此的详细教程。
- en: The resulting flexibility combined with an intuitive Python-first interface
    and speed of execution has contributed to its rapid rise in popularity and led
    to the development of numerous supporting libraries that extend its functionality.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 结合结果灵活性和直观的 Python 首选界面以及执行速度，这导致了它的迅速普及和众多支持库的开发，这些支持库扩展了其功能。
- en: Let's see how PyTorch and autograd work by implementing our simple network architecture
    (see the `how_to_use_pytorch` notebook for details).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过实现我们的简单网络架构来看看 PyTorch 和 autograd 如何工作（详细信息请参见`how_to_use_pytorch`笔记本）。
- en: How to create a PyTorch DataLoader
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何创建 PyTorch 的 DataLoader
- en: 'We begin by converting the NumPy or pandas input data to `torch` tensors. Conversion
    from and to NumPy is very straightforward:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将 NumPy 或 pandas 输入数据转换为`torch`张量。从 NumPy 到 PyTorch 的转换非常简单：
- en: '[PRE24]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can use these PyTorch tensors to instantiate first a `TensorDataset` and,
    in a second step, a `DataLoader` that includes information about `batch_size`:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些 PyTorch 张量首先实例化一个`TensorDataset`，然后在第二步实例化一个包含有关`batch_size`信息的`DataLoader`：
- en: '[PRE25]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: How to define the neural network architecture
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何定义神经网络架构
- en: PyTorch defines an NN architecture using the `Net()` class. The central element
    is the `forward` function. autograd automatically defines the corresponding `backward`
    function that computes the gradients.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 使用`Net()`类定义了一个 NN 架构。其核心元素是`forward`函数。autograd 自动定义了相应的`backward`函数来计算梯度。
- en: 'Any legal tensor operation is fair game for the `forward` function, providing
    a log of design flexibility. In our simple case, we just link the tensor through
    functional input-output relations after initializing their attributes:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 任何合法的张量操作都可以用于`forward`函数，提供了设计灵活性的记录。在我们的简单情况下，我们只是在初始化其属性后通过功能输入输出关系链接张量：
- en: '[PRE26]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We then instantiate a `Net()` object and can inspect the architecture as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们实例化一个`Net()`对象，并且可以按照以下方式检查其架构：
- en: '[PRE27]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To illustrate eager execution, we can also inspect the initialized parameters
    in the first tensor:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明即时执行，我们还可以检查第一个张量中初始化的参数：
- en: '[PRE28]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: To enable GPU processing, you can use `net.cuda()`. See the PyTorch documentation
    for placing tensors on CPU and/or one or more GPU units.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用 GPU 处理，您可以使用`net.cuda()`。请参阅 PyTorch 文档以将张量放置在 CPU 和/或一个或多个 GPU 单元上。
- en: 'We also need to define a loss function and the optimizer, using some of the
    built-in options:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要定义损失函数和优化器，使用一些内置选项：
- en: '[PRE29]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How to train the model
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何训练模型
- en: 'Model training consists of an outer loop for each epoch, that is, each pass
    over the training data, and an inner loop over the batches produced by the `DataLoader`.
    That executes the forward and backward passes of the learning algorithm. Some
    care needs to be taken to adjust data types to the requirements of the various
    objects and functions; for example, labels need to be integers and the features
    should be of type `float`:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练包括对每个 epoch 的外循环，即对训练数据的每次传递，以及对 `DataLoader` 产生的批次的内循环。这执行学习算法的前向和后向传递。需要小心地调整数据类型以满足各种对象和函数的要求；例如，标签需要是整数，特征应该是
    `float` 类型：
- en: '[PRE30]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The notebook also contains an example that uses the `livelossplot` package to
    plot losses throughout the training process as provided by Keras out of the box.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本还包含一个示例，使用 `livelossplot` 包绘制损失，这是由 Keras 提供的开箱即用的功能。
- en: How to evaluate the model predictions
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何评估模型预测
- en: 'To obtain predictions from our trained model, we pass it feature data and convert
    the prediction to a NumPy array. We get softmax probabilities for each of the
    two classes:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 要从我们训练的模型中获得预测，我们传递特征数据并将预测转换为 NumPy 数组。我们获得了每个类别的 softmax 概率：
- en: '[PRE31]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: From here on, we can proceed as before to compute loss metrics or visualize
    the result that again reproduces a version of the decision boundary we found earlier.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们可以像以前一样继续计算损失指标或可视化结果，再次生成我们之前找到的决策边界的一个版本。
- en: Alternative options
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可选方案
- en: The huge interest in DL has led to the development of several competing libraries
    that facilitate the design and training of NNs. The most prominent include the
    following examples (also see references on GitHub).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 对深度学习的巨大兴趣导致了几个竞争性库的开发，这些库促进了神经网络的设计和训练。最突出的包括以下示例（还请参阅 GitHub 上的参考资料）。
- en: Apache MXNet
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache MXNet
- en: MXNet, incubated at the Apache Foundation, is an open source DL software framework
    used to train and deploy deep NNs. It focuses on scalability and fast model training.
    They included the Gluon high-level interface to make it easy to prototype, train,
    and deploy DL models. MXNet has been picked by Amazon for deep learning on AWS.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet，由 Apache 基金会孵化，是一个用于训练和部署深度神经网络的开源深度学习软件框架。它专注于可扩展性和快速模型训练。他们包括了 Gluon
    高级接口，使得原型设计、训练和部署深度学习模型变得容易。MXNet 已被亚马逊选为 AWS 上的深度学习工具。
- en: Microsoft Cognitive Toolkit (CNTK)
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Microsoft Cognitive Toolkit（CNTK）
- en: The Cognitive Toolkit, previously known as CNTK, is Microsoft's contribution
    to the deep learning library collection. It describes an NN as a series of computational
    steps via a directed graph, similar to TensorFlow. In this directed graph, leaf
    nodes represent input values or network parameters, while other nodes represent
    matrix operations upon their inputs. CNTK allows users to build and combine popular
    model architectures ranging from deep feedforward NNs, convolutional networks,
    and recurrent networks (RNNs/LSTMs).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: Cognitive Toolkit，以前称为 CNTK，是微软对深度学习库的贡献。它将神经网络描述为通过有向图的一系列计算步骤，类似于 TensorFlow。在这个有向图中，叶节点代表输入值或网络参数，而其他节点代表对它们的输入进行的矩阵操作。CNTK
    允许用户构建和组合从深度前馈神经网络、卷积网络到循环网络（RNNs/LSTMs）的流行模型架构。
- en: Fastai
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: fastai
- en: The fastai library aims to simplify training NNs that are fast and accurate
    using modern best practices. These practices have emerged from research into DL
    at the company that makes both the software and accompanying courses available
    for free. Fastai includes support for models that process image, text, tabular,
    and collaborative filtering data.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: fastai 库旨在使用现代最佳实践简化训练快速而准确的神经网络。这些实践是从该公司对深度学习的研究中产生的，该公司提供了免费的软件和相关课程。fastai
    支持处理图像、文本、表格和协同过滤数据的模型。
- en: Optimizing an NN for a long-short strategy
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为长短策略优化神经网络
- en: In practice, we need to explore variations for the design options for the NN
    architecture and how we train it from those we outlined previously because we
    can never be sure from the outset which configuration best suits the data. In
    this section, we will explore various architectures for a simple feedforward NN
    to predict daily stock returns using the dataset developed in *Chapter 12* (see
    the notebook `preparing_the_model_data` in the GitHub directory for that chapter).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们需要探索对神经网络架构的设计选项以及我们如何训练它的变化，因为我们从一开始就无法确定哪种配置最适合数据。在本节中，我们将探讨使用在 *第
    12 章* 中开发的数据集预测每日股票回报的简单前馈神经网络的各种架构（请参见该章节的 GitHub 目录中的笔记本 `preparing_the_model_data`）。
- en: To this end, we will define a function that returns a TensorFlow model based
    on several architectural input parameters and cross-validate alternative designs
    using the `MultipleTimeSeriesCV` we introduced in *Chapter 7*, *Linear Models
    – From Risk Factors to Return Forecasts*. To assess the signal quality of the
    model predictions, we build a simple ranking-based long-short strategy based on
    an ensemble of the models that perform best during the in-sample cross-validation
    period. To limit the risk of false discoveries, we then evaluate the performance
    of this strategy for an out-of-sample test period.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将定义一个函数，根据几个架构输入参数返回一个 TensorFlow 模型，并使用我们在*第7章*，*线性模型 - 从风险因素到收益预测*中介绍的`MultipleTimeSeriesCV`交叉验证备选设计。为了评估模型预测的信号质量，我们构建了一个基于模型集成的简单基于排名的多头空头策略，在样本内交叉验证期间表现最佳的模型基础上。为了限制假发现的风险，我们然后评估该策略在样本外测试期间的表现。
- en: See the `optimizing_a_NN_architecture_for_trading` notebook for details.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细信息，请参见`optimizing_a_NN_architecture_for_trading`笔记本。
- en: Engineering features to predict daily stock returns
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工程特征以预测每日股票收益
- en: 'To develop our trading strategy, we use the daily stock returns for 995 US
    stocks for the eight-year period from 2010 to 2017\. We will use the features
    developed in *Chapter 12,* *Boosting Your Trading Strategy* that include volatility
    and momentum factors, as well as lagged returns with cross-sectional and sectoral
    rankings. We load the data as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发我们的交易策略，我们使用了从2010年到2017年的八年期间的995只美国股票的日收益。我们将使用在*第12章*，*提升您的交易策略*中开发的特征，其中包括波动率和动量因子，以及带有横截面和部门排名的滞后收益。我们按如下方式加载数据：
- en: '[PRE32]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Defining an NN architecture framework
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个神经网络架构框架
- en: To automate the generation of our TensorFlow model, we create a function that
    constructs and compiles the model based on arguments that can later be passed
    during cross-validation iterations.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 为了自动化生成我们的 TensorFlow 模型，我们创建了一个函数，根据后续可以在交叉验证迭代期间传递的参数来构建和编译模型。
- en: 'The following `make_model` function illustrates how to flexibly define various
    architectural elements for the search process. The `dense_layers` argument defines
    both the depth and width of the network as a list of integers. We also use `dropout`
    for regularization, expressed as a float in the range [0, 1] to define the probability
    that a given unit will be excluded from a training iteration:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的`make_model`函数说明了如何灵活定义搜索过程的各种架构元素。`dense_layers`参数将网络的深度和宽度定义为整数列表。我们还使用`dropout`进行正则化，表示为在[0,1]范围内的浮点数，用于定义在训练迭代中排除给定单元的概率：
- en: '[PRE33]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now we can turn to the cross-validation process to evaluate various NN architectures.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以转向交叉验证过程，评估各种神经网络架构。
- en: Cross-validating design options to tune the NN
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证设计选项以调整 NN
- en: 'We use the `MultipleTimeSeriesCV` to split the data into rolling training and
    validation sets comprising of 24 * 12 months of data, while keeping the final
    12 * 21 days of data (starting November 30, 2016) as a holdout test. We train
    each model for 48 21-day periods and evaluate its results over 3 21-day periods,
    implying 12 splits for cross-validation and test periods combined:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`MultipleTimeSeriesCV`将数据分割为滚动训练和验证集，包括24 * 12个月的数据，同时保留最后12 * 21天的数据（从2016年11月30日开始）作为保留测试。我们对每个模型进行48个21天期的训练，并在3个21天期内评估其结果，这意味着在交叉验证和测试期间共有12个拆分：
- en: '[PRE34]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we define a set of configurations for cross-validation. These include
    several options for two hidden layers and dropout probabilities; we''ll only use
    tanh activations because a trial run did not suggest significant differences compared
    to ReLU. (We could also try out different optimizers. but I recommend you do not
    run this experiment, to limit what is already a computationally intensive effort):'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为交叉验证定义一组配置。这些包括两个隐藏层和dropout概率的几个选项；我们只会使用tanh激活，因为一次试验没有显示出与ReLU相比的显著差异。（我们也可以尝试不同的优化器。但我建议您不要运行这个实验，以限制已经是计算密集型工作的内容）：
- en: '[PRE35]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'To run the cross-validation, we define a function that produces the train and
    validation data based on the integer indices produced by the `MultipleTimeSeriesCV`
    as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行交叉验证，我们定义一个函数，根据`MultipleTimeSeriesCV`生成的整数索引来生成训练和验证数据，如下所示：
- en: '[PRE36]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: During cross-validation, we train a model using one set of parameters from the
    previously defined grid for 20 epochs. After each epoch, we store a `checkpoint`
    that contains the learned weights that we can reload to quickly generate predictions
    for the best configuration without retraining.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证期间，我们使用之前定义的网格中的一组参数训练一个模型20个时期。每个时期结束后，我们存储一个包含学习权重的`checkpoint`，这样我们就可以重新加载它们，以快速生成最佳配置的预测，而无需重新训练。
- en: 'After each epoch, we compute and store the **information coefficient** (**IC**)
    for the validation set by day:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时期结束后，我们计算并存储验证集的**信息系数**（**IC**）按天计算：
- en: '[PRE37]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: With an NVIDIA GTX 1080 GPU, 20 epochs takes a bit over one hour with batches
    of 64 samples, and around 20 minutes with 256 samples.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NVIDIA GTX 1080 GPU，20个时期的批处理大小为64个样本的计算时间超过1小时，而批处理大小为256个样本则约为20分钟。
- en: Evaluating the predictive performance
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估预测性能
- en: 'Let''s first take a look at the five models that achieved the highest median
    daily IC during the cross-validation period. The following code computes these
    values:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一下在交叉验证期间实现了最高中位数日IC的五个模型。以下代码计算这些值：
- en: '[PRE38]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The resulting table shows that the architectures using 32 units in both layers
    and 16/8 in the first/second layer, respectively, performed best. These models
    also use `dropout` and were trained with batch sizes of 64 samples with the given
    number of epochs for all folds. The median IC values vary between 0.0236 and 0.0246:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表显示，使用32个单位的32个架构在两层中以及在第一/第二层中分别使用16/8的架构表现最佳。这些模型还使用了`dropout`，并且使用给定数量的时期对所有折叠进行了64个样本的批处理训练。中位数IC值在0.0236和0.0246之间变化：
- en: '| Dense Layers | Dropout | Batch Size | Epoch | IC |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 稠密层 | 丢失率 | 批次大小 | 时期 | IC |'
- en: '| (32, 32) | 0.1 | 64 | 7 | 0.0246 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| (32, 32) | 0.1 | 64 | 7 | 0.0246 |'
- en: '| (16, 8) | 0.2 | 64 | 14 | 0.0241 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| (16, 8) | 0.2 | 64 | 14 | 0.0241 |'
- en: '| (16, 8) | 0.1 | 64 | 3 | 0.0238 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| (16, 8) | 0.1 | 64 | 3 | 0.0238 |'
- en: '| (32, 32) | 0.1 | 64 | 10 | 0.0237 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| (32, 32) | 0.1 | 64 | 10 | 0.0237 |'
- en: '| (16, 8) | 0.2 | 256 | 3 | 0.0236 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| (16, 8) | 0.2 | 256 | 3 | 0.0236 |'
- en: Next, we'll take a look at how the parameter choices impact the predictive performance.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看参数选择如何影响预测性能。
- en: 'First, we visualize the daily information coefficient (averaged per fold) for
    different configurations by epoch to understand how the duration of training affects
    the predictive accuracy. The plots in *Figure 17.7*, however, highlight few conclusive
    patterns; the IC varies little across models and not particularly systematically
    across epochs:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过时期可视化不同配置的每折日信息系数（平均值），以了解训练持续时间如何影响预测准确性。然而，在*图17.7*中的图表突出显示出一些明确的模式；IC在模型之间变化很小，并且在时期之间并没有特别系统地变化：
- en: '![](img/B15439_17_07.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_07.png)'
- en: 'Figure 17.7: Information coefficients for various model configurations'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.7：各种模型配置的信息系数
- en: 'For more statistically robust insights, we run a linear regression using **ordinary
    least squares** (**OLS**) (see *Chapter 7*, *Linear Models – From Risk Factors
    to Return Forecasts*) using dummy variables for the layer, dropout, and batch
    size choices as well as for each epoch:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更具统计意义的见解，我们使用**普通最小二乘法**（**OLS**）进行线性回归（参见*第7章*，*线性模型 - 从风险因素到收益预测*），使用关于层、丢失率和批次大小选择以及每个时期的虚拟变量：
- en: '[PRE39]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The chart in *Figure 17.8* plots the confidence interval for each regression
    coefficient; if it does not include zero, then the coefficient is significant
    at the five percent level. The IC values on the y-axis reflect the differential
    from the constant (0.0027, p-value: 0.017) that represents the sample average
    over the configuration excluded while dropping one category of each dummy variable.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17.8*中的图表绘制了每个回归系数的置信区间；如果不包含零，则系数在百分之五的水平上是显著的。y轴上的IC值反映了与舍弃每个虚拟变量类别的配置的样本平均值相对差异（0.0027，p值：0.017）。'
- en: Across all configurations, batch size 256 and a dropout of 0.2 made significant
    (but small) positive contributions to performance. Similarly, training for seven
    epochs yielded slightly superior results. The regression is overall significant
    according to the F statistic but has a very low R2 value close to zero, underlining
    the high degree of noise in the data relative to the signal conveyed by the parameter
    choices.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有配置中，批处理大小为256和丢失率为0.2对性能产生了显著（但微小）的正面影响。类似地，训练七个时期产生了略微优越的结果。根据F统计量，回归总体上是显著的，但R2值非常低，接近零，强调了数据中噪音相对于参数选择传递的信号的高程度。
- en: '![](img/B15439_17_08.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_08.png)'
- en: 'Figure 17.8: OLS coefficients and confidence intervals'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.8：OLS系数和置信区间
- en: Backtesting a strategy based on ensembled signals
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于集成信号回测策略
- en: To translate our NN model into a trading strategy, we generate predictions,
    evaluate their signal quality, create rules that define how to trade on these
    predictions, and backtest the performance of a strategy that implements these
    rules. See the notebook `backtesting_with_zipline` for the code examples in this
    section.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 要将我们的NN模型转换为交易策略，我们生成预测，评估其信号质量，创建定义如何根据这些预测进行交易的规则，并回测实施这些规则的策略的性能。请参阅笔记本`backtesting_with_zipline`以获取本节中的代码示例。
- en: Ensembling predictions to produce tradeable signals
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成预测以产生可交易信号
- en: To reduce the variance of the predictions and hedge against in-sample overfitting,
    we combine the predictions of the best three models listed in the table in the
    previous section and average the result.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少预测的方差并对样本内过拟合进行套期保值，我们结合了在前一节表中列出的三个最佳模型的预测，并平均了结果。
- en: 'To this end, we define the following `generate_predictions()` function, which
    receives the model parameters as inputs, loads the weights for the models for
    the desired epoch, and creates forecasts for the cross-validation and out-of-sample
    periods (showing only the essentials here to save some space):'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们定义以下`generate_predictions()`函数，该函数接收模型参数作为输入，加载所需时期模型的权重，并为交叉验证和样本外期间创建预测（这里仅显示关键内容以节省空间）：
- en: '[PRE40]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We store the results for evaluation with Alphalens and a Zipline backtest.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Alphalens和Zipline回测来存储评估结果。
- en: Evaluating signal quality using Alphalens
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Alphalens评估信号质量
- en: 'To gain some insight into the signal content of the ensembled model predictions,
    we use Alphalens to compute the return differences for investments into five equal-weighted
    portfolios differentiated by the forecast quantiles (see *Figure 17.9*). The spread
    between the top and the bottom quintile equals around 8 bps for a one-day holding
    period, which implies an alpha of 0.094 and a beta of 0.107:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对集成模型预测的信号内容有所了解，我们使用Alphalens计算了根据预测分位数区分的五个等权重投资组合的回报差异（见*图17.9*）。在一个交易日的持有期间，最高分位和最低分位之间的差距约为8个基点，这意味着alpha为0.094，beta为0.107：
- en: '![](img/B15439_17_09.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_09.png)'
- en: 'Figure 17.9: Signal quality evaluation'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.9：信号质量评估
- en: Backtesting the strategy using Zipline
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Zipline回测策略
- en: Based on the Alphalens analysis, our strategy will enter long and short positions
    for the 50 stocks with the highest positive and lowest negative predicted returns,
    respectively, as long as there are at least 10 options on either side. The strategy
    trades every day.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Alphalens分析，我们的策略将为具有最高正预测回报和最低负预测回报的50只股票输入长和短头寸，只要每边至少有10个选项。该策略每天进行交易。
- en: 'The charts in *Figure 17.10* show that the strategy performs well in- and out-of-sample
    (before transaction costs):'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17.10*中的图表显示，该策略在样本内和样本外表现良好（在交易成本之前）：'
- en: '![](img/B15439_17_10.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_17_10.png)'
- en: 'Figure 17.10: In- and out-of-sample backtest performance'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.10：样本内和样本外回测表现
- en: It produces annualized returns of 22.8 percent over the 36-month period, 16.5
    percent for the 24 in-sample months, and 35.7 percent for the 12 out-of-sample
    months. The Sharpe ratio is 0.72 in-sample and 2.15 out-of-sample, delivering
    an alpha of 0.18 (0.29) and a beta of 0.24 (0.16) in/out of sample.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 它在36个月的期间内产生了年化收益率为22.8％，在样本内24个月为16.5％，在样本外12个月为35.7％。夏普比率在样本内为0.72，在样本外为2.15，提供了0.18（0.29）的alpha和0.24（0.16）的beta在/样本外。
- en: How to further improve the results
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何进一步改进结果
- en: The relatively simple architecture yields some promising results. To further
    improve performance, you can first and foremost add new features and more data
    to the model.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 相对简单的架构产生了一些有希望的结果。要进一步提高性能，首先可以添加新功能和更多数据到模型中。
- en: Alternatively, you can use more sophisticated architectures, including RNNs
    and CNNs, which are well suited to sequential data, whereas vanilla feedforward
    NNs are not designed to capture the ordered nature of the features.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用更复杂的架构，包括适用于顺序数据的RNN和CNN，而香草前馈NN并不设计捕获特征的有序性。
- en: We will turn to these specialized architectures in the following chapter.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章中转向这些专用架构。
- en: Summary
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced DL as a form of representation learning that
    extracts hierarchical features from high-dimensional, unstructured data. We saw
    how to design, train, and regularize feedforward neural networks using NumPy.
    We demonstrated how to use the popular DL libraries PyTorch and TensorFlow that
    are suitable for use cases from rapid prototyping to production deployments.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将 DL（深度学习）介绍为一种从高维、非结构化数据中提取层次特征的表征学习形式。我们看到了如何使用 NumPy 设计、训练和正则化前馈神经网络。我们演示了如何使用流行的
    DL 库 PyTorch 和 TensorFlow，这些库适用于从快速原型到生产部署的用例。
- en: Most importantly, we designed and tuned an NN using TensorFlow and were able
    to generate tradeable signals that delivered attractive returns during both the
    in-sample and out-of-sample periods.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，我们使用 TensorFlow 设计和调优了一个神经网络（NN），能够在样本内和样本外期间生成可交易的信号，从而获得了可观的回报。
- en: In the next chapter, we will explore CNNs, which are particularly well suited
    for image data but are also well-suited for sequential data.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨 CNNs，它们特别适用于图像数据，但也适用于序列数据。
