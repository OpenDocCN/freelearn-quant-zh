["```py\nvectorizer = TfidfVectorizer(max_df=.25, min_df=.01,\nstop_words='english',\nbinary=False)\ntrain_dtm = vectorizer.fit_transform(train_docs.article)\ntest_dtm = vectorizer.transform(test_docs.article)\n```", "```py\nsvd = TruncatedSVD(n_components=5, n_iter=5, random_state=42)\nsvd.fit(train_dtm)\nsvd.explained_variance*ratio* array([0.00187014, 0.01559661, 0.01389952, 0.01215842, 0.01066485])\n```", "```py\ntrain_doc_topics = svd.transform(train_dtm)\ntrain_doc_topics.shape\n(2175, 5)\n```", "```py\ni = randint(0, len(train_docs))\ntrain_docs.iloc[i, :2].append(pd.Series(doc_topics[i], index=topic_labels))\nCategory Politics\nHeading What the election should really be about?\nTopic 1 0.33\nTopic 2 0.18\nTopic 3 0.12\nTopic 4 0.02\nTopic 5 0.06\n```", "```py\nnmf = NMF(n_components=n_components,\nrandom_state=42,\nsolver='mu',\nbeta_loss='kullback-leibler',\nmax_iter=1000)\nnmf.fit(train_dtm)\n```", "```py\nnmf.reconstruction_err_\n316.2609400385988\n```", "```py\nlda = LatentDirichletAllocation(n_components=5, \n                                    n_jobs=-1, \n                                    max_iter=500,\n                                    learning_method='batch', \n                                    evaluate_every=5,\n                                    verbose=1, \n                                    random_state=42)\nldat.fit(train_dtm)\nLatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n             evaluate_every=5, learning_decay=0.7, learning_method='batch',\n             learning_offset=10.0, max_doc_update_iter=100, max_iter=500,\n             mean_change_tol=0.001, n_components=5, n_jobs=-1,\n             n_topics=None, perp_tol=0.1, random_state=42,\n             topic_word_prior=None, total_samples=1000000.0, verbose=1)\n```", "```py\njoblib.dump(lda, model_path / 'lda.pkl')\nlda = joblib.load(model_path / 'lda.pkl')\n```", "```py\ntrain_corpus = Sparse2Corpus(train_dtm, documents_columns=False)\ntest_corpus = Sparse2Corpus(test_dtm, documents_columns=False)\nid2word = pd.Series(vectorizer.get_feature_names()).to_dict()\n```", "```py\nLdaModel(corpus=None, \n         num_topics=100, \n         id2word=None, \n         distributed=False, \n         chunksize=2000, # No of doc per training chunk.\n         passes=1,       # No of passes through corpus during training\n         update_every=1, # No of docs to be iterated through per update\n         alpha='symmetric', \n         eta=None,       # a-priori belief on word probability\n         decay=0.5,    # % of lambda forgotten when new doc is examined\n         offset=1.0,   # controls slow down of first few iterations.\n         eval_every=10,   # how often estimate log perplexity (costly)\n         iterations=50,         # Max. of iterations through the corpus\n         gamma_threshold=0.001, # Min. change in gamma to continue\n         minimum_probability=0.01, # Filter topics with lower \n                                     probability\n         random_state=None, \n         ns_conf=None, \n         minimum_phi_value=0.01, # lower bound on term probabilities\n         per_word_topics=False,  #  Compute most word-topic \n                                    probabilities\n         callbacks=None, \n         dtype=<class 'numpy.float32'>)\n```", "```py\nlda = LdaModel(corpus=train_corpus,\nnum_topics=5,\nid2word=id2word)\n```", "```py\ncoherence = lda_gensim.top_topics(corpus=train_corpus,  coherence='u_mass')\n```", "```py\ntopic_coherence = []\ntopic_words = pd.DataFrame()\nfor t in range(len(coherence)):\n    label = topic_labels[t]\n    topic_coherence.append(coherence[t][1])\n    df = pd.DataFrame(coherence[t][0], columns=[(label, 'prob'), (label, 'term')])\n    df[(label, 'prob')] = df[(label, 'prob')].apply(lambda x: '{:.2%}'.format(x))\n    topic_words = pd.concat([topic_words, df], axis=1)\n\ntopic_words.columns = pd.MultiIndex.from_tuples(topic_words.columns)\npd.set_option('expand_frame_repr', False)\nprint(topic_words.head())\npd.Series(topic_coherence, index=topic_labels).plot.bar();\n```", "```py\ndocuments = []\nfor transcript in earnings_path.iterdir():\n    content = pd.read_csv(transcript / 'content.csv')\n    documents.extend(content.loc[(content.speaker!='Operator') & (content.content.str.len() > 5), 'content'].tolist())\nlen(documents)\n22766\n```"]