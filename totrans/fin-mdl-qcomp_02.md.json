["```py\n\nimport numpy as np\nfrom scipy.stats import norm\nt = 1.0 # year\nK = 105 # Strike price\nr = 0.05 # Riskless short rate\nsigma = 0.25 # Volatility (stdev)\nS0 = 100 # Present price\n```", "```py\n\nI = 1000 # Samples\nz = np.random.standard_normal(I) # z_i values\nST = S0 * np.exp((r - 0.5 * sigma ** 2) * t + sigma * np.sqrt(t) * z)\n```", "```py\n\nmonths = 12 # months in a year\ndt = t/months\nS = np.zeros((months+1, I))\nS[0] = S0\nfor ti in range(1, months+1):\n    Z = np.random.normal(size=I)\n    S[ti] = S[ti-1]*np.exp((r-0.5*sigma**2)*dt+sigma*np.sqrt(dt)*Z)\n```", "```py\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.style.use('seaborn-v0_8')\nplt.figure(figsize=(10, 6))\nfor si in S.T:\n    plt.plot(range(len(si)), si)\nplt.xlabel(\"Months\", fontsize=14)\nplt.ylabel(\"Option price\", fontsize=14)\nplt.grid()\nplt.title(f\"{I} trajectories\", fontsize=16)\nplt.show()\n```", "```py\n\nplt.figure(figsize=(10, 6))\nplt.hist(S[-1], 100, density=True, facecolor='b', alpha=0.75)\nplt.xlabel(\"Option price at time t\", fontsize=14)\nplt.ylabel(\"Probabilities\", fontsize=14)\nplt.grid()\nplt.title(f\"Price distribution at time t\", fontsize=16)\nplt.show()\n```", "```py\n\nn     qubits = 5\n```", "```py\n\n# Price scaling\nscale = 100\nS = 0.067077 * scale # initial spot price (scaled)\nimport numpy as np\n# parameters for considered random distribution\nvol = 0.6  # volatility of 60%\nr = 0.02  # annual interest rate of 2%\nT = 40 / 365  # 40 days to maturity\n# resulting parameters for log-normal distribution\nmu = (r - 0.5 * vol**2) * T + np.log(S)\nsigma = vol * np.sqrt(T)\nmean = np.exp(mu + sigma**2 / 2)\nvariance = (np.exp(sigma**2) - 1) * np.exp(2 * mu + sigma**2)\nstddev = np.sqrt(variance)\n# lowest and highest value considered for the spot price; in between, an equidistant discretization is considered.\nlow = np.maximum(0, mean - 3 * stddev)\nhigh = scale\n```", "```py\n\nfrom qiskit_finance.circuit.library import LogNormalDistribution\ndist_circ = LogNormalDistribution(num_qubits, mu=mu, sigma=stddev**2, bounds=(low, high))\n```", "```py\n\ndist_circ.draw('mpl', fold=120)\n```", "```py\n\nimport matplotlib.pyplot as plt\n# plot probability distribution\nx = dist_circ.values / scale\ny = dist_circ.probabilities\nplt.bar(x, y, width=0.2)\nplt.xticks(x, rotation=90)\nplt.xlabel(\"Option price at time $t$\", size=15)\nplt.ylabel(\"Probability ($\\%$)\", size=15)\nplt.show()\n```", "```py\n\nfrom qiskit.circuit.library import LinearAmplitudeFunction\n# set the strike price (low << strike << high)\nstrike = S*1.05\n# and function parameters\nslopes = [0, 1]\noffsets = [0, 0]\n```", "```py\n\ncall_option = LinearAmplitudeFunction(\n    nqubits,\n    slopes,\n    offsets,\n    domain=(low, high),\n    image=(0, high - strike),\n    breakpoints=[low, strike],\n    rescaling_factor=0.05)\n```", "```py\n\nfrom qiskit import QuantumCircuit\ntotal_qubits = call_option     .num_qubits\neuropean_call = QuantumCircuit(total_qubits)\neuropean_call.append(dist_circ, range(num_qubits))\neuropean_call.append(call_option     , range(total_qubits))\n# draw the circuit\neuropean_call.draw('mpl')\n```", "```py\n\nfrom qiskit import Aer\nfrom qiskit.utils import QuantumInstance\nfrom qiskit.algorithms import IterativeAmplitudeEstimation, EstimationProblem\n# set target precision and confidence level\nepsilon = 0.01\nalpha = 0.05\nqi = QuantumInstance(Aer.get_backend(\"aer_simulator\"), shots=100)\nproblem = EstimationProblem(\n    state_preparation=european_call,\n    objective_qubits=[num_qubits],\n    post_processing=call_option     .post_processing,\n)\n# construct amplitude estimation\nae = IterativeAmplitudeEstimation(epsilon, alpha=alpha, quantum_instance=qi)\nresult = ae.estimate(problem)\nconf_int = np.array(result.confidence_interval_processed)\nprint(\"Estimated value:    \\t%.4f\" % (result.estimation_processed / scale))\nprint(\"Confidence interval:\\t[%.4f, %.4f]\" % tuple(conf_int/scale))\nWe would obtain the following prompt then:\nEstimated value:     0.0714\nConfidence interval: [-0.0267, 0.1695]\n```", "```py\n\nfrom qiskit_finance.applications.estimation import EuropeanCallPricing\neuropean_call_pricing = EuropeanCallPricing(\n    num_state_qubits=num_qubits,\n    strike_price=strike     ,\n    rescaling_factor=0.05,\n    bounds=(low, high),\n    uncertainty_model=dist_circ)\n```", "```py\n\nimport pandas as pd\ndataset = pd.read_csv(\"../../data/binance_data.csv\")\n# Lets pivot so that the date is the index and each assets presents its closing price\npivoted = dataset.pivot(index=\"Closing time\", columns=\"Asset\", values=\"Close\")\nassets = [\"BNBBTC\",\"ETHBTC\",\"LTCBTC\"]\n```", "```py\n\n     import numpy as np\n# We convert the Dataframe into a numpy array\ntraining_data = pivoted[assets].to_numpy()\n# Define minimal and maximal values for the training data\nbounds_min = np.percentile(training_data, 5, axis=0)\nbounds_max = np.percentile(training_data, 95, axis=0)\nbounds = []\nfor i, _ in enumerate(bounds_min):\n    bounds.append([bounds_min[i], bounds_max[i]])\n```", "```py\n\nfrom qiskit_machine_learning.datasets.dataset_helper import discretize_and_truncate\ndata_dim = [3, 3, 3]\n# Pre-processing, i.e., discretization of the data (gridding)\n(training_data, grid_data, grid_elements, prob_data) = discretize_and_truncate(\n    training_data,\n    np.asarray(bounds),\n    data_dim,\n    return_data_grid_elements=True,\n    return_prob=True,\n    prob_non_zero=True,\n)\n```", "```py\n\nimport matplotlib.pyplot as plt\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\nax1.hist(training_data[:, 0], bins=20)\nax1.set_title(f\"BNBBTC\")\nax1.set_xlabel(\"Values\")\nax1.set_ylabel(\"Counts\")\nax2.hist(training_data[:, 1], bins=20)\nax2.set_title(\"ETHBTC\")\nax2.set_xlabel(\"Values\")\nax2.set_ylabel(\"Counts\")\nax3.hist(training_data[:, 2], bins=20)\nax3.set_title(\"LTCBTC\")\nax3.set_xlabel(\"Values\")\nax3.set_ylabel(\"Counts\")\n```", "```py\n\nimport pennylane as qml\ndef generator(weights, wires, repetitions):\n    # Initial superposition for all possible states\n    for i in range(wires):\n        qml.Hadamard(wires=i)\n    k = 0 # carrying index\n    for i in range(wires):\n        qml.RY(weights[k], wires=i)\n        k += 1\n    # Repetition blocks\n    for _ in range(repetitions):\n        # Entangling block\n        for i in range(wires-1):\n            qml.CZ(wires=[i, i+1])\n        # Last CZ turning to zero\n        qml.CZ(wires=[wires-1, 0])\n        # RY rotations\n        for i in range(wires):\n            qml.RY(weights[k], wires=i)\n            k += 1\n    return qml.probs(wires=range(qubits))\n```", "```py\n\nimport torch\nqubits = np.sum(data_dim)\ndev = qml.device(\"default.qubit\", wires=qubits)\n# Also some parameters\nlayers = 1\nparams = [np.random.uniform(np.pi) for i in range(qubits + layers*qubits)]\ngen_circuit = qml.QNode(generator, dev, interface='torch', diff_method='best')\ndrawer = qml.draw_mpl(gen_circuit)\nprint(drawer(params, qubits, layers))\n```", "```py\n\nimport torch.nn as nn\nclass Discriminator(nn.Module):\n    def __init__(self, input_size):\n        super(Discriminator, self).__init__()\n        self.linear_input = nn.Linear(input_size, 20)\n        self.leaky_relu = nn.LeakyReLU(0.2)\n        self.linear20 = nn.Linear(20, 1)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        x = self.linear_input(input)\n        x = self.leaky_relu(x)\n        x = self.linear20(x)\n        x = self.sigmoid(x)\n        return x\n```", "```py\n\n# Generator loss function\ngen_loss_fun = nn.BCELoss()\n# Discriminator loss function\ndisc_loss_fun = nn.BCELoss()\n# Initialize generator and discriminator\ndiscriminator = Discriminator(len(data_dim))\nlr = 0.01  # learning rate\nb1 = 0.9  # first momentum parameter\nb2 = 0.999  # second momentum parameter\nnum_epochs = 50  # number of training epochs\n     from torch.optim import Adam\n# optimizer for the generator\noptimizer_gen = qml.AdamOptimizer(stepsize=lr, beta1=b1, beta2=b2)\n# optimizer for the discriminator\noptimizer_disc = Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\nfrom torch.utils.data import DataLoader\n# Define the training batch size\nbatch_size = 300\ndataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True)\n# Generator loss list\ngenerator_loss_values = []\n# Discriminator loss list\ndiscriminator_loss_values = []\n```", "```py\n\nfor epoch in range(num_epochs):\n    generator_loss_epoch = []\n    discriminator_loss_epoch = []\n    theta = torch.tensor(params, dtype=torch.float32)\n    for i, data in enumerate(dataloader):\n        # Adversarial ground truths\n        valid = torch.ones(data.size(0), 1)\n        fake = torch.zeros(data.size(0), 1)\n        # Generate a batch of data points\n        fake_samples = []\n        for _ in range(len(data)):\n            probabilities = gen_circuit(theta, qubits, layers)\n            gen_data = grid_elements[torch.argmax(probabilities)]\n            fake_samples.append(gen_data)\n        # Train Discriminator\n        optimizer_disc.zero_grad()\n        # Loss measures discriminator's ability to distinguish real from generated samples\n        inputs = data.to(torch.float32)\n        real_out = discriminator(inputs)\n        real_loss = disc_loss_fun(real_out, valid)\n        fake_input = torch.tensor(fake_samples, dtype=torch.float32)\n        fake_out = discriminator(fake_input)\n        fake_loss = disc_loss_fun(fake_out, fake)\n        discriminator_loss = (real_loss + fake_loss) / 2\n        discriminator_loss.backward(retain_graph=True)\n        optimizer_disc.step()\n        # Loss measures generator's ability to prepare good data samples\n        fake_input = torch.tensor(fake_samples, dtype=torch.float32)\n        fake_out = discriminator(fake_input)\n        generator_loss = gen_loss_fun(fake_out, valid)\n        returns, prev_cost = optimizer_gen.step_and_cost(gen_circuit, params, qubits, layers)\n        params = returns[0]\n        generator_loss_epoch.append(generator_loss.item())\n        discriminator_loss_epoch.append(discriminator_loss.item())\n    generator_loss_values.append(np.mean(generator_loss_epoch))\n    discriminator_loss_values.append(np.mean(discriminator_loss_epoch))\n```"]