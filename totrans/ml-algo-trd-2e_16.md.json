["```py\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nglove2word2vec(glove_input_file=glove_file, word2vec_output_file=w2v_file)\nmodel = KeyedVectors.load_word2vec_format(w2v_file, binary=False) \n```", "```py\naccuracy = model.wv.accuracy(analogies_path,\n                             restrict_vocab=300000,\n                             case_insensitive=True) \n```", "```py\ndef clean_doc(d):\n    doc = []\n    for sent in d.sents:\n        s = [t.text.lower() for t in sent if not\n        any([t.is_digit, not t.is_alpha, t.is_punct, t.is_space])]\n        if len(s) > 5 or len(sent) < 100:\n            doc.append(' '.join(s))\n    return doc\nnlp = English()\nsentencizer = nlp.create_pipe(\"sentencizer\")\nnlp.add_pipe(sentencizer)\nclean_articles = []\niter_articles = (article for article in articles)\nfor i, doc in enumerate(nlp.pipe(iter_articles, batch_size=100, n_process=8), 1):\n    clean_articles.extend(clean_doc(doc)) \n```", "```py\nsentences = LineSentence((data_path / f'articles_clean.txt').as_posix())\nphrases = Phrases(sentences=sentences,\n                          min_count=10,  # ignore terms with a lower count\n                          threshold=0.5,  # only phrases with higher score\n                          delimiter=b'_',  # how to join ngram tokens\n                          scoring='npmi')  # alternative: default\ngrams = Phraser(phrases)\nsentences = grams[sentences]\nwith (data_path / f'articles_ngrams.txt').open('w') as f:\n        for sentence in sentences:\n            f.write(' '.join(sentence) + '\\n') \n```", "```py\nSAMPLE_SIZE=.5\nsentences = file_path.read_text().split('\\n')\nwords = ' '.join(np.random.choice(sentences, size=int(SAMLE_SIZE* l en(sentences)), replace=False)).split() \n```", "```py\n    # Get (token, count) tuples for tokens meeting MIN_FREQ\n    MIN_FREQ = 10\n    token_counts = [t for t in Counter(words).most_common() if t[1] >= MIN_FREQ]\n    tokens, counts = list(zip(*token_counts))\n    # create id-token dicts & reverse dicts\n    id_to_token = pd.Series(tokens, index=range(1, len(tokens) + 1)).to_dict()\n    id_to_token.update({0: 'UNK'})\n    token_to_id = {t:i for i, t in id_to_token.items()}\n    data = [token_to_id.get(word, 0) for word in words] \n    ```", "```py\nSAMPLING_FACTOR =  1e-4\nsampling_table = make_sampling_table(vocab_size,\n                                     sampling_factor=SAMPLING_FACTOR) \n```", "```py\npairs, labels = skipgrams(sequence=data,\n                          vocabulary_size=vocab_size,\n                          window_size=WINDOW_SIZE,\n                          sampling_table=sampling_table,\n                          negative_samples=1.0,\n                          shuffle=True) \n```", "```py\npd.DataFrame({'target': target_word[:5],\n              'context': context_word[:5],\n              'label': labels[:5]})\n   target context label\n0   30867    2117     1\n1     196     359     1\n2   17960   32467     0\n3     314    1721     1\n4   28387    7811     0 \n```", "```py\ninput_target = Input((1,), name='target_input')\ninput_context = Input((1,), name='context_input') \n```", "```py\nembedding = Embedding(input_dim=vocab_size,\n                      output_dim=EMBEDDING_SIZE,\n                      input_length=1,\n                      name='embedding_layer')\ntarget = embedding(input_target)\ntarget = Reshape((EMBEDDING_SIZE, 1), name='target_embedding')(target)\ncontext = embedding(input_context)\ncontext = Reshape((EMBEDDING_SIZE, 1), name='context_embedding')(context) \n```", "```py\n# similarity measure\ndot_product = Dot(axes=1)([target, context])\ndot_product = Reshape((1,), name='similarity')(dot_product)\noutput = Dense(units=1, activation='sigmoid', name='output')(dot_product) \n```", "```py\nsentence_path = data_path / FILE_NAME\nsentences = LineSentence(str(sentence_path)) \n```", "```py\nmodel = Word2Vec(sentences,\n                 sg=1, # set to 1 for skip-gram; CBOW otherwise\n                 size=300,\n                 window=5,\n                 min_count=20,\n                 negative=15,\n                 workers=8,\n                 iter=EPOCHS,\n                 alpha=0.05) \n```", "```py\n# persist model\nmodel.save(str(gensim_path / 'word2vec.model'))\n# persist word vectors\nmodel.wv.save(str(gensim_path / 'word_vectors.bin')) \n```", "```py\nmodel.train(sentences, epochs=1, total_examples=model.corpus_count) \n```", "```py\nmost_sim = best_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=10) \n```", "```py\ncounter = Counter(sentence_path.read_text().split())\nmost_common = pd.DataFrame(counter.most_common(), columns=['token', 'count'])\nmost_common['p'] = np.log(most_common['count'])/np.log(most_common['count']).sum()similars = pd.DataFrame()\nfor token in np.random.choice(most_common.token, size=10, p=most_common.p):\n    similars[token] = [s[0] for s in best_model.wv.most_similar(token)] \n```", "```py\nwith pd.HDFStore(DATA_FOLDER / 'assets.h5') as store:\n    prices = store['quandl/wiki/prices'].adj_close\nsec = pd.read_csv('sec_path/filing_index.csv').rename(columns=str.lower)\nsec.date_filed = pd.to_datetime(sec.date_filed)\nsec = sec.loc[sec.ticker.isin(prices.columns), ['ticker', 'date_filed']]\nprice_data = []\nfor ticker, date in sec.values.tolist():\n    target = date + relativedelta(months=1)\n    s = prices.loc[date: target, ticker]\n    price_data.append(s.iloc[-1] / s.iloc[0] - 1)\ndf = pd.DataFrame(price_data,\n                  columns=['returns'],\n                  index=sec.index) \n```", "```py\nsentence_path = Path('data', 'ngrams', f'ngrams_2.txt')\nsentences = LineSentence(sentence_path) \n```", "```py\nmodel = Word2Vec(sentences,\n                 sg=1,          # 1=skip-gram; otherwise CBOW\n                 hs=0,          # hier. softmax if 1, neg. sampling if 0\n                 size=300,      # Vector dimensionality\n                 window=3,      # Max dist. btw target and context word\n                 min_count=50,  # Ignore words with lower frequency\n                 negative=10,   # noise word count for negative sampling\n                 workers=8,     # no threads\n                 iter=1,        # no epochs = iterations over corpus\n                 alpha=0.025,   # initial learning rate\n                 min_alpha=0.0001 # final learning rate\n                ) \n```", "```py\nsims=model.wv.most_similar(positive=['iphone'], restrict_vocab=15000)\n                  term   similarity\n0                 ipad    0.795460\n1              android    0.694014\n2           smartphone    0.665732 \n```", "```py\nmodel.wv.most_similar(positive=['france', 'london'],\n                      negative=['paris'],\n                      restrict_vocab=15000)\n             term  similarity\n0  united_kingdom    0.606630\n1         germany    0.585644\n2     netherlands    0.578868 \n```", "```py\ndf = pd.read_parquet('data_path / 'user_reviews.parquet').loc[:, ['stars', \n                                                                  'text']]\nstars = range(1, 6)\nsample = pd.concat([df[df.stars==s].sample(n=100000) for s in stars]) \n```", "```py\ntokenizer = RegexpTokenizer(r'\\w+')\nstopword_set = set(stopwords.words('english'))\ndef clean(review):\n    tokens = tokenizer.tokenize(review)\n    return ' '.join([t for t in tokens if t not in stopword_set])\nsample.text = sample.text.str.lower().apply(clean) \n```", "```py\nsample = pd.read_parquet('yelp_sample.parquet')\nsentences = []\nfor i, (stars, text) in df.iterrows():\n    sentences.append(TaggedDocument(words=text.split(), tags=[i])) \n```", "```py\nmodel = Doc2Vec(documents=sentences,\n                dm=1,           # 1=distributed memory, 0=dist.BOW\n                epochs=5,\n                size=300,       # vector size\n                window=5,       # max. distance betw. target and context\n                min_count=50,   # ignore tokens w. lower frequency\n                negative=5,     # negative training samples\n                dm_concat=0,    # 1=concatenate vectors, 0=sum\n                dbow_words=0,   # 1=train word vectors as well\n                workers=4)\nmodel.save((results_path / 'sample.model').as_posix()) \n```", "```py\nmodel.most_similar('good') \n```", "```py\ny = sample.stars.sub(1)\nX = np.zeros(shape=(len(y), size)) # size=300\nfor i in range(len(sample)):\n    X[i] = model.docvecs[i]\nX.shape\n(485825, 300) \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.2,\n                                                    random_state=42,\n                                                    stratify=y) \n```", "```py\nrf = RandomForestClassifier(n_jobs=-1, n_estimators=500)\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_test) \n```", "```py\ntrain_data = lgb.Dataset(data=X_train, label=y_train)\ntest_data = train_data.create_valid(X_test, label=y_test)\nparams = {'objective': 'multiclass',\n          'num_classes': 5}\nlgb_model = lgb.train(params=params,\n                      train_set=train_data,\n                      num_boost_round=5000,\n                      valid_sets=[train_data, test_data],\n                      early_stopping_rounds=25,\n                      verbose_eval=50)\n# generate multiclass predictions\nlgb_pred = np.argmax(lgb_model.predict(X_test), axis=1) \n```", "```py\nlr = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n                        class_weight='balanced')\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test) \n```"]