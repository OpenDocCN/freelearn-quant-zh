- en: Chapter 3. Utilizing Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When Snapchat first introduced a filter featuring a breakdancing hotdog, the
    stock price of the company surged. However, investors were less interested in
    the hotdog's handstand; what actually fascinated them was that Snapchat had successfully
    built a powerful form of computer vision technology.
  prefs: []
  type: TYPE_NORMAL
- en: The Snapchat app was now not only able to take pictures, but it was also able
    to find the surfaces within those pictures that a hotdog could breakdance on.
    Their app would then stick the hotdog there, something that could still be done
    when the user moved their phone, allowing the hotdog to keep dancing in the same
    spot.
  prefs: []
  type: TYPE_NORMAL
- en: While the dancing hotdog may be one of the sillier applications of computer
    vision, it successfully showed the world the potential of the technology. In a
    world full of cameras, from the billions of smartphones, security cameras, and
    satellites in use every day, to **Internet of Things** (**IoT**) devices, being
    able to interpret images yields great benefits for both consumers and producers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Computer vision allows us to both perceive and interpret the real world at
    scale. You can think of it like this: no analyst could ever look at millions of
    satellite images to mark mining sites and track their activity over time; it''s
    just not possible. Yet for computers, it''s not just a possibility; it''s something
    that''s a reality here and now.'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, something that’s being used in the real world now, by several firms,
    is retailers counting the number of cars in their parking lot in order to estimate
    what the sales of goods will be in a given period.
  prefs: []
  type: TYPE_NORMAL
- en: Another important application of computer vision can be seen in finance, specifically
    in the area of insurance. For instance, insurers might use drones to fly over
    roofs in order to spot issues before they become an expensive problem. This could
    extend to them using computer vision to inspect factories and equipment they insure.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at another case in the finance sector, banks needing to comply with
    **Know-Your-Customer** (**KYC**) rules are automating back-office processes and
    identity verification. In financial trading, computer vision can be applied to
    candlestick charts in order to find new patterns for technical analysis. We could
    dedicate a whole book to the practical applications of computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the building blocks of computer vision
    models. This will include a focus on the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization to prevent overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Momentum-based optimization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch normalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced architectures for computer vision beyond classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A note on libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we start, let''s have a look at all the different libraries we will
    be using in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Keras**: A high-level neural network library and an interface to TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow**: A dataflow programming and machine learning library that we
    use for GPU-accelerated computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scikit-learn**: A popular machine learning library with implementation of many
    classic algorithms as well as evaluation tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenCV**: An image processing library that can be used for rule-based augmentation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NumPy**: A library for handling matrices in Python.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seaborn**: A plotting library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tqdm**: A tool to monitor the progress of Python programs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's worth taking a minute to note that all of these libraries, except for OpenCV,
    can be installed via `pip`; for example, `pip install keras`.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV, however, will require a slightly more complex installation procedure.
    This is beyond the scope of this book, but the information is well documented
    online via OpenCV documentation, which you can view at the following URL: [https://docs.opencv.org/trunk/df/d65/tutorial_table_of_content_introduction.html](https://docs.opencv.org/trunk/df/d65/tutorial_table_of_content_introduction.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Alternately, it's worth noting that both Kaggle and Google Colab come with OpenCV
    preinstalled. To run the examples in this chapter, make sure you have OpenCV installed
    and can import with `import cv2`.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolutional Neural Networks**, **ConvNets**, or **CNNs** for short, are
    the driving engine behind computer vision. ConvNets allow us to work with larger
    images while still keeping the network at a reasonable size.'
  prefs: []
  type: TYPE_NORMAL
- en: The name Convolutional Neural Network comes from the mathematical operation that
    differentiates them from regular neural networks. Convolution is the mathematically
    correct term for sliding one matrix over another matrix. We'll explore in the
    next section, *Filters on MNIST*, why this is important for ConvNets, but also
    why this is not the best name in the world for them, and why ConvNets should,
    in reality, be called **filter nets**.
  prefs: []
  type: TYPE_NORMAL
- en: You may be asking, "but why filter nets?" The answer is simply because what
    makes them work is the fact that they use filters.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be working with the MNIST dataset, which is a collection
    of handwritten digits that has become a standard "Hello, World!" application for
    computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Filters on MNIST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What does a computer actually see when it sees an image? Well, the values of
    the pixels are stored as numbers in the computer. So, when the computer *sees*
    a black-and-white image of a seven, it actually sees something similar to the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filters on MNIST](img/B10354_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The number 7 from the MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: The preceding is an example from the MNIST dataset. The handwritten number in
    the image has been highlighted to make the figure seven visible for humans, but
    for the computer, the image is really just a collection of numbers. This means
    we can perform all kinds of mathematical operations on the image.
  prefs: []
  type: TYPE_NORMAL
- en: When detecting numbers, there are a few lower-level features that make a number.
    For example, in this handwritten figure 7, there's a combination of one vertical
    straight line, one horizontal line on the top, and one horizontal line through
    the middle. In contrast, a 9 is made up of four rounded lines that form a circle
    at the top and a straight, vertical line.
  prefs: []
  type: TYPE_NORMAL
- en: We're now able to present the central idea behind ConvNets. We can use small
    filters that can detect a certain kind of low-level feature, such as a vertical
    line, and then slide it over the entire image to detect all the vertical lines
    in the image.
  prefs: []
  type: TYPE_NORMAL
- en: The following screenshot shows a vertical line filter. To detect vertical lines
    in our image, we need to slide this 3x3 matrix filter over the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Filters on MNIST](img/B10354_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A vertical line filter
  prefs: []
  type: TYPE_NORMAL
- en: Using the MNIST dataset on the following page, we start in the top-left corner
    and slice out the top-left 3x3 grid of pixels, which in this case is all zeros.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then perform an element-wise multiplication of all the elements in the filter
    with all elements in the slice of the image. The nine products then get summed
    up, and bias is added. This value then forms the output of the filter and gets
    passed on as a new pixel to the next layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filters on MNIST](img/B10354_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As a result, the output of our vertical line filter will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filters on MNIST](img/B10354_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The output of a vertical line filter
  prefs: []
  type: TYPE_NORMAL
- en: Take a minute to notice that the vertical lines are visible while the horizontal
    lines are gone. Only a few artifacts remain. Also, notice how the filter captures
    the vertical line from one side.
  prefs: []
  type: TYPE_NORMAL
- en: Since it responds to high pixel values on the left and low pixel values on the
    right, only the right side of the output shows strong positive values. Meanwhile,
    the left side of the line actually shows negative values. This is not a big problem
    in practice as there are usually different filters for different kinds of lines
    and directions.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a second filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our vertical filter is working, but we've already noticed that we also need
    to filter our image for horizontal lines in order to detect a seven.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our horizontal line filter might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding a second filter](img/B10354_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A horizontal line filter
  prefs: []
  type: TYPE_NORMAL
- en: 'Using that example, we can now slide this filter over our image in the exact
    same way we did with the vertical filter, resulting in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding a second filter](img/B10354_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The output of the vertical line filter
  prefs: []
  type: TYPE_NORMAL
- en: 'See how this filter removes the vertical lines and pretty much only leaves
    the horizontal lines? The question now is what do we now pass onto the next layer? Well,
    we stack the outputs of both filters on top of each other, creating a three-dimensional
    cube:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding a second filter](img/B10354_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The MNIST convolution
  prefs: []
  type: TYPE_NORMAL
- en: By adding multiple convolutional layers, our ConvNet is able to extract ever
    more complex and semantic features.
  prefs: []
  type: TYPE_NORMAL
- en: Filters on color images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, our filter technique is not only limited to black-and-white images.
    In this section we're going to have a look at color images.
  prefs: []
  type: TYPE_NORMAL
- en: The majority of color images consist of three layers or channels, and this is
    commonly referred to as RGB, the initialism for the three layers. They are made
    up of one red channel, one blue channel, and one green channel. When these three
    channels are laid on top of each other, they add up to create the traditional
    color image that we know.
  prefs: []
  type: TYPE_NORMAL
- en: Taking that concept, an image is therefore not flat, but actually a cube, a
    three-dimensional matrix. Combining this idea with our objective, we want to apply
    a filter to the image, and apply it to all three channels at once. We will, therefore,
    perform an element-wise multiplication between two three-dimensional cubes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our 3x3 filter now has a depth of three and thus nine parameters, plus the
    bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filters on color images](img/B10354_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An example of a filter cube or convolutional kernel
  prefs: []
  type: TYPE_NORMAL
- en: This cube, which is referred to as a convolutional kernel, gets slid over the
    image just like the two-dimensional matrix did before. The element-wise products
    then again get summed up, the bias is added, and the outcome represents a pixel
    in the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: Filters always capture the whole depth of the previous layer. The filters are
    moved over the width and height of the image. Likewise, filters are not moved
    across the depth, that is, the different channels, of an image. In technical terms,
    weights, the numbers that make up the filters, are shared over width and height,
    but not over different channels.
  prefs: []
  type: TYPE_NORMAL
- en: The building blocks of ConvNets in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be building a simple ConvNet that can be used for classifying
    the MNIST characters, while at the same time, learning about the different pieces
    that make up modern ConvNets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can directly import the MNIST dataset from Keras by running the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Our dataset contains 60,000 28x28-pixel images. MNIST characters are black
    and white, so the data shape usually does not include channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will take a closer look at color channels later, but for now, let''s expand
    our data dimensions to show that we only have a one-color channel. We can achieve
    this by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: With the code being run, you can see that we now have a single color channel
    added.
  prefs: []
  type: TYPE_NORMAL
- en: Conv2D
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we come to the meat and potatoes of ConvNets: using a convolutional layer
    in Keras. Conv2D is the actual convolutional layer, with one Conv2D layer housing
    several filters, as can be seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: When creating a new Conv2D layer, we must specify the number of filters we want
    to use, and the size of each filter.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The size of the filter is also called `kernel_size`, as the individual filters
    are sometimes called kernels. If we only specify a single number as the kernel
    size, Keras will assume that our filters are squares. In this case, for example,
    our filter would be 3x3 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible, however, to specify non-square kernel sizes by passing a tuple
    to the `kernel_size` parameter. For example, we could choose to have a 3x4-pixel
    filter through `kernel_size = (3,4)`. However, this is very rare, and in the majority
    of cases, filters have a size of either 3x3 or 5x5\. Empirically, researchers
    have found that this is a size that yields good results.
  prefs: []
  type: TYPE_NORMAL
- en: Stride size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `strides` parameter specifies the step size, also called the stride size,
    with which the convolutional filter slides over the image, usually referred to
    as the feature map. In the vast majority of cases, filters move pixel by pixel,
    so their stride size is set to 1\. However, there are researchers that make more
    extensive use of larger stride sizes in order to reduce the spatial size of the
    feature map.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like with `kernel_size`, Keras assumes that we use the same stride size horizontally
    and vertically if we specify only one value, and in the vast majority of cases
    that is correct. However, if we want to use a stride size of one horizontally,
    but two vertically, we can pass a tuple to the parameter as follows: `strides=(1,2)`.
    As in the case of the filter size, this is rarely done.'
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we have to add `padding` to our convolutional layer. Padding adds zeros
    around our image. This can be done if we want to prevent our feature map from
    shrinking.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider a 5x5-pixel feature map and a 3x3 filter. The filter only fits
    on the feature map nine times, so we'll end up with a 3x3 output. This both reduces
    the amount of information that we can capture in the next feature map, and how
    much the outer pixels of the input feature map can contribute to the task. The
    filter never centers on them; it only goes over them once.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three options for padding: not using padding, known as "No" padding,
    "Same" padding and "Valid" padding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at each of the three paddings. First, No Padding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Padding](img/B10354_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Option 1: No padding'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we have Same Padding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Padding](img/B10354_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Option 2: Same padding'
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure the output has the same size as the input, we can use `same` padding.
    Keras will then add enough zeros around the input feature map so that we can preserve
    the size. The default padding setting, however, is `valid`. This padding does not
    preserve the feature map size, but only makes sure that the filter and stride size
    actually fit on the input feature map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Padding](img/B10354_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Option 3: Valid padding'
  prefs: []
  type: TYPE_NORMAL
- en: Input shape
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keras requires us to specify the input shape. However, this is only required
    for the first layer. For all the following layers, Keras will infer the input
    shape from the previous layer's output shape.
  prefs: []
  type: TYPE_NORMAL
- en: Simplified Conv2D notation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The preceding layer takes a 28x28x1 input and slides six filters with a 2x2 filter size
    over it, going pixel by pixel. A more common way to specify the same layer would
    be by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The number of filters (here `6`) and the filter size (here `3`) are set as positional
    arguments, while `strides` and `padding` default to `1` and `valid` respectively.
    If this was a layer deeper in the network, we wouldn't even have to specify the
    input shape.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU activation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolutional layers only perform a linear step. The numbers that make up the image
    get multiplied with the filter, which is a linear operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in order to approximate complex functions, we need to introduce non-linearity
    with an activation function. The most common activation function for computer
    vision is the Rectified Linear Units, or ReLU function, which we can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ReLU activation](img/B10354_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The ReLU activation function
  prefs: []
  type: TYPE_NORMAL
- en: 'The ReLU formula, which was used to produce the above chart, can be seen below:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ReLU(x) = max(x, 0)*'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the ReLU function returns the input if the input is positive.
    If it's not, then it returns zero. This very simple function has been shown to
    be quite useful, making gradient descent converge faster.
  prefs: []
  type: TYPE_NORMAL
- en: It is often argued that ReLU is faster because the derivative for all values
    above zero is just one, and it does not become very small as the derivative for
    some extreme values does, for example, with sigmoid or tanh.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU is also less computationally expensive than both sigmoid and tanh. It does
    not require any computationally expensive calculations, input values below zero
    are just set to zero, and the rest is outputted. Unfortunately, though, ReLU activations
    are a bit fragile and can "die."
  prefs: []
  type: TYPE_NORMAL
- en: When the gradient is very large and moves multiple weights towards a negative
    direction, then the derivative of ReLU will also always be zero, so the weights
    never get updated again. This might mean that a neuron never fires again. However,
    this can be mitigated through a smaller learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because ReLU is fast and computationally cheap, it has become the default activation
    function for many practitioners. To use the ReLU function in Keras, we can just
    name it as the desired activation function in the activation layer, by running
    this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: MaxPooling2D
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It's common practice to use a pooling layer after a number of convolutional
    layers. Pooling decreases the spatial size of the feature map, which in turn reduces
    the number of parameters needed in a neural network and thus reduces overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below, we can see an example of Max Pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![MaxPooling2D](img/B10354_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Max pooling
  prefs: []
  type: TYPE_NORMAL
- en: Max pooling returns the maximum element out of a pool. This is in contrast to
    the example average of `AveragePooling2D`, which returns the average of a pool.
    Max pooling often delivers superior results to average pooling, so it is the standard
    most practitioners use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Max pooling can be achieved by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: When using a max pooling layer in Keras, we have to specify the desired pool
    size. The most common value is a 2x2 pool. Just as with the `Conv2D` layer, we
    can also specify a stride size.
  prefs: []
  type: TYPE_NORMAL
- en: For pooling layers, the default stride size is `None`, in which case Keras sets
    the stride size to be the same as the pool size. In other words, pools are next
    to each other and don't overlap.
  prefs: []
  type: TYPE_NORMAL
- en: We can also specify padding, with `valid` being the default choice. However,
    specifying `same` padding for pooling layers is extremely rare since the point
    of a pooling layer is to reduce the spatial size of the feature map.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `MaxPooling2D` layer here takes 2x2-pixel pools next to each other with
    no overlap and returns the maximum element. A more common way of specifying the same
    layer is through the execution of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this case, both `strides` and `padding` are set to their defaults, `None`
    and `valid` respectively. There is usually no activation after a pooling layer
    since the pooling layer does not perform a linear step.
  prefs: []
  type: TYPE_NORMAL
- en: Flatten
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might have noticed that our feature maps are three dimensional while our
    desired output is a one-dimensional vector, containing the probability of each
    of the 10 classes. So, how do we get from 3D to 1D? Well, we `Flatten` our feature
    maps.
  prefs: []
  type: TYPE_NORMAL
- en: The `Flatten` operation works similar to NumPy's `flatten` operation. It takes
    in a batch of feature maps with dimensions `(batch_size, height, width, channels)`
    and returns a set of vectors with dimensions `(batch_size, height * width * channels)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It performs no computation and only reshapes the matrix. There are no hyperparameters
    to be set for this operation, as you can see in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Dense
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ConvNets usually consist of a feature extraction part, the convolutional layers,
    as well as a classification part. The classification part is made up out of the
    simple fully connected layers that we’ve already explored in [Chapter 1](ch01.xhtml
    "Chapter 1. Neural Networks and Gradient-Based Optimization"), *Neural Networks
    and Gradient-Based Optimization*, and [Chapter 2](ch02.xhtml "Chapter 2. Applying
    Machine Learning to Structured Data"), *Applying Machine Learning to Structured
    Data*.
  prefs: []
  type: TYPE_NORMAL
- en: To distinguish the plain layers from all other types of layers, we refer to
    them as `Dense` layers. In a dense layer, each input neuron is connected to an
    output neuron. We only have to specify the number of output neurons we would like,
    in this case, 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be done by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After the linear step of the dense layer, we can add a `softmax` activation
    for multi-class regression, just as we did in the first two chapters, by running
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Training MNIST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's now put all of these elements together so we can train a ConvNet on the
    MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we must specify the model, which we can do with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, you can see the general structure of a typical ConvNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The convolution and pooling layers are often used together in these blocks;
    you can find neural networks that repeat the `Conv2D`, `MaxPool2D` combination
    tens of times.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get an overview of our model with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Which will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In this summary, you can clearly see how the pooling layers reduce the size
    of the feature map. It's a little bit less obvious from the summary alone, but
    you can see how the output of the first `Conv2D` layer is 26x26 pixels, while
    the input images are 28x28 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: By using `valid` padding, `Conv2D` also reduces the size of the feature map,
    although only by a small amount. The same happens for the second `Conv2D` layer,
    which shrinks the feature map from 13x13 pixels to 11x11 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: You can also see how the first convolutional layer only has 60 parameters, while
    the `Dense` layer has 3,010, over 50 times as many parameters. Convolutional layers
    usually achieve surprising feats with very few parameters, which is why they are
    so popular. The total number of parameters in a network can often be significantly
    reduced by convolutional and pooling layers.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The MNIST dataset we are using comes preinstalled with Keras. When loading the
    data, make sure you have an internet connection if you want to use the dataset
    directly via Keras, as Keras has to download it first.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can import the dataset with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As explained at the beginning of the chapter, we want to reshape the dataset
    so that it can have a channel dimension as well. The dataset as it comes does
    not have a channel dimension yet, but this is something we can do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we add a channel dimension with NumPy, with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now there is a channel dimension, as we can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Compiling and training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous chapters, we have used one-hot encoded targets for multiclass
    regression. While we have reshaped the data, the targets are still in their original
    form. They are a flat vector containing the numerical data representation for
    each handwritten figure. Remember that we have 60,000 of these in the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Transforming targets through one-hot encoding is a frequent and annoying task,
    so Keras allows us to just specify a loss function that converts targets to one-hot
    on the fly. This loss function is called `sparse_categorical_crossentropy`.
  prefs: []
  type: TYPE_NORMAL
- en: It's the same as the categorical cross-entropy loss used in earlier chapters,
    the only difference is that this uses sparse, that is, not one-hot encoded, targets.
  prefs: []
  type: TYPE_NORMAL
- en: Just as before, you still need to make sure that your network output has as
    many dimensions as there are classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re now at a point where we can compile the model, which we can do with
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we are using an Adam optimizer. The exact workings of Adam are
    explained in the next section, *More bells and whistles for our neural network*,
    but for now, you can just think of it as a more sophisticated version of stochastic
    gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'When training, we can directly specify a validation set in Keras by running
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have successfully run that code, we''ll get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To better see what is going on, we can plot the progress of training with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Compiling and training](img/B10354_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The visualized output of validation and training accuracy
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding chart, the model achieves about 98% validation
    accuracy, which is pretty nice!
  prefs: []
  type: TYPE_NORMAL
- en: More bells and whistles for our neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a minute to look at some of the other elements of our neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Momentum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In previous chapters we've explained gradient descent in terms of someone trying
    to find the way down a mountain by just following the slope of the floor. Momentum
    can be explained with an analogy to physics, where a ball is rolling down the
    same hill. A small bump in the hill would not make the ball roll in a completely
    different direction. The ball already has some momentum, meaning that its movement
    gets influenced by its previous movement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of directly updating the model parameters with their gradient, we update
    them with the exponentially weighted moving average. We update our parameter with
    an outlier gradient, then we take the moving average, which will smoothen out
    outliers and capture the general direction of the gradient, as we can see in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Momentum](img/B10354_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How momentum smoothens gradient updates
  prefs: []
  type: TYPE_NORMAL
- en: 'The exponentially weighted moving average is a clever mathematical trick used
    to compute a moving average without having to memorize a set of previous values.
    The exponentially weighted average, *V*, of some value, ![Momentum](img/B10354_03_002.jpg),
    would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Momentum](img/B10354_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A beta value of 0.9 would mean that 90% of the mean would come from the previous
    moving average, ![Momentum](img/B10354_03_004.jpg), and 10% would come from the
    new value, ![Momentum](img/B10354_03_005.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: Using momentum makes learning more robust against gradient descent pitfalls
    such as outlier gradients, local minima, and saddle points.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can augment the standard stochastic gradient descent optimizer in Keras
    with momentum by setting a value for beta, which we do in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This little code snippet creates a stochastic gradient descent optimizer with
    a learning rate of 0.01 and a beta value of 0.9\. We can use it when we compile
    our model, as we''ll now do with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The Adam optimizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Back in 2015, Diederik P. Kingma and Jimmy Ba created the **Adam** (**Adaptive
    Momentum Estimation**) optimizer. This is another way to make gradient descent
    work more efficiently. Over the past few years, this method has shown very good
    results and has, therefore, become a standard choice for many practitioners. For
    example, we've used it with the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the Adam optimizer computes the exponentially weighted average of the
    gradients, just like a momentum optimizer does. It achieves this with the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Adam optimizer](img/B10354_03_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It then also computes the exponentially weighted average of the squared gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Adam optimizer](img/B10354_03_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It then updates the model parameters like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Adam optimizer](img/B10354_03_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here ![The Adam optimizer](img/B10354_03_009.jpg) is a very small number to
    avoid division by zero.
  prefs: []
  type: TYPE_NORMAL
- en: This division by the root of squared gradients reduces the update speed when
    gradients are very large. It also stabilizes learning as the learning algorithm
    does not get thrown off track by outliers as much.
  prefs: []
  type: TYPE_NORMAL
- en: Using Adam, we have a new hyperparameter. Instead of having just one momentum
    factor, ![The Adam optimizer](img/B10354_03_010.jpg), we now have two, ![The Adam
    optimizer](img/B10354_03_011.jpg) and ![The Adam optimizer](img/B10354_03_012.jpg).
    The recommended values for
  prefs: []
  type: TYPE_NORMAL
- en: '![The Adam optimizer](img/B10354_03_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and ![The Adam optimizer](img/B10354_03_014.jpg) are 0.9 and 0.999 respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use Adam in Keras like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As you have seen earlier in this chapter, we can also compile the model just
    by passing the `adam` string as an optimizer. In this case, Keras will create
    an Adam optimizer for us and choose the recommended values.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regularization is a technique used to avoid overfitting. Overfitting is when
    the model fits the training data too well, and as a result, it does not generalize
    well to either development or test data. You may see that overfitting is sometimes
    also referred to as "high variance," while underfitting, obtaining poor results
    on training, development, and test data, is referred to as "high bias."
  prefs: []
  type: TYPE_NORMAL
- en: In classical statistical learning, there is a lot of focus on the bias-variance
    tradeoff. The argument that is made is that a model that fits very well to the
    training set is likely to be overfitting and that some amount of underfitting
    (bias) has to be accepted in order to obtain good outcomes. In classical statistical
    learning, the hyperparameters that prevent overfitting also often prevent the
    training set fitting well.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization in neural networks, as it is presented here, is largely borrowed
    from classical learning algorithms. Yet, modern machine learning research is starting
    to embrace the concept of "orthogonality," the idea that different hyperparameters
    influence bias and variance.
  prefs: []
  type: TYPE_NORMAL
- en: By separating those hyperparameters, the bias-variance tradeoff can be broken,
    and we can find models that generalize well and deliver accurate predictions.
    However, so far these efforts have only yielded small rewards, as low-bias and
    low-variance models require large amounts of training data.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One popular technique to counter overfitting is L2 regularization. L2 regularization
    adds the sum of squared weights to the loss function. We can see an example of
    this in the formula below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![L2 regularization](img/B10354_03_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here *N* is the number of training examples and ![L2 regularization](img/B10354_03_016.jpg)
    is the regularization hyperparameter, which determines how much we want to regularize,
    with a common value being around 0.01.
  prefs: []
  type: TYPE_NORMAL
- en: Adding this regularization to the loss function means that high weights increase
    losses and the algorithm is incentivized to reduce the weights. Small weights,
    those around zero, mean that the neural network will rely less on them.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a regularized algorithm will rely less on every single feature and
    every single node activation, and instead will have a more holistic view, taking
    into account many features and activations. This will prevent the algorithm from overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'L1 regularization is very similar to L2 regularization, but instead of adding
    the sum of squares, it adds the sum of absolute values, as we can see in this
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![L1 regularization](img/B10354_03_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In practice, it is often a bit uncertain as to which of the two will work best,
    but the difference between the two is not very large.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization in Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Keras, regularizers that are applied to the weights are called **kernel_regularizer**,
    and regularizers that are applied to the bias are called **bias_regularizer**.
    You can also apply regularization directly to the activation of the nodes to prevent
    them from being activated very strongly with **activity_regularizer**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let''s add some L2 regularization to our network. To do this, we need
    to run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Setting `kernel_regularizer` as done in the first convolutional layer in Keras
    means regularizing weights. Setting `bias_regularizer` regularizes the bias, and setting
    `activity_regularizer` regularizes the output activations of a layer.
  prefs: []
  type: TYPE_NORMAL
- en: In this following example, the regularizers are set to be shown off, but here
    they actually harm the performance to our network. As you can see from the preceding
    training results, our network is not actually overfitting, so setting regularizers
    harms performance here, and as a result, the model underfits.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the following output, in this case, the model reaches about
    87% validation accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: You'll notice that the model achieves a higher accuracy on the validation than
    on the training set; this is a clear sign of underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the title of the 2014 paper by Srivastava et al gives away, *Dropout is
    A Simple Way to Prevent Neural Networks from Overfitting*. It achieves this by
    randomly removing nodes from the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B10354_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Schematic of the dropout method. From Srivastava et al, "Dropout: A Simple
    Way to Prevent Neural Networks from Overfitting," 2014'
  prefs: []
  type: TYPE_NORMAL
- en: With dropout, each node has a small probability of having its activation set
    to zero. This means that the learning algorithm can no longer rely heavily on
    single nodes, much like in L2 and L1 regularization. Dropout therefore also has
    a regularizing effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras, dropout is a new type of layer. It''s put after the activations you
    want to apply dropout to. It passes on activations, but sometimes it sets them
    to zero, achieving the same effect as a dropout in the cells directly. We can
    see this in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: A dropout value of 0.5 is considered a good choice if overfitting is a serious
    problem, while values that are over 0.5 are not very helpful, as the network would
    have too few values to work with. In this case, we chose a dropout value of 0.2,
    meaning that each cell has a 20% chance of being set to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that dropout is used after pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The low dropout value creates nice results for us, but again, the network does
    better on the validation set rather than the training set, a clear sign of underfitting
    taking place. Note that dropout is only applied at training time. When the model
    is used for predictions, dropout doesn't do anything.
  prefs: []
  type: TYPE_NORMAL
- en: Batchnorm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Batchnorm**, short for **batch** **normalization**, is a technique for "normalizing"
    input data to a layer batch-wise. Each batchnorm computes the mean and standard
    deviation of the data and applies a transformation so that the mean is zero and
    the standard deviation is one.'
  prefs: []
  type: TYPE_NORMAL
- en: This makes training easier because the loss surface becomes more "round." Different means
    and standard deviations along different input dimensions would mean that the network
    would have to learn a more complicated function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras, batchnorm is a new layer as well, as you can see in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Batchnorm often accelerates training by making it easier. You can see how the
    accuracy rate jumps up in the first epoch here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Batchnorm](img/B10354_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Training and validation accuracy of our MNIST classifier with batchnorm
  prefs: []
  type: TYPE_NORMAL
- en: Batchnorm also has a mildly regularizing effect. Extreme values are often overfitted
    to, and batchnorm reduces extreme values, similar to activity regularization.
    All this makes batchnorm an extremely popular tool in computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Working with big image datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Images tend to be big files. In fact, it's likely that you will not be able
    to fit your entire image dataset into your machine's RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need to load the images from disk "just in time" rather than loading
    them all in advance. In this section, we will be setting up an image data generator
    that loads images on the fly.
  prefs: []
  type: TYPE_NORMAL
- en: We'll be using a dataset of plant seedlings in this case. This was provided
    by Thomas Giselsson and others, 2017, via their publication, *A Public Image Database
    for Benchmark of Plant Seedling Classification Algorithms*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is available from the following link: [https://arxiv.org/abs/1711.05458](https://arxiv.org/abs/1711.05458).'
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering why we're looking at plants; after all, plant classifications
    are not a common problem that is faced in the finance sector. The simple answer
    is that this dataset lends itself to demonstrating many common computer vision
    techniques and is available under an open domain license; it's therefore a great
    training dataset for us to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Readers who wish to test their knowledge on a more relevant dataset should
    take a look at the *State Farm Distracted Driver* dataset as well as the *Planet:
    Understanding the Amazon from Space* dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The code and data for this section and the section on stacking pretrained models
    can be found and run here: [https://www.kaggle.com/jannesklaas/stacking-vgg](https://www.kaggle.com/jannesklaas/stacking-vgg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras comes with an image data generator that can load files from disk out
    of the box. To do this, you simply need to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: To obtain a generator reading from the files, we first have to specify the generator.
    In Keras, `ImageDataGenerator` offers a range of image augmentation tools, but
    in our example, we will only be making use of the rescaling function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rescaling multiplies all values in an image with a constant. For most common
    image formats, the color values range from 0 to 255, so we want to rescale by
    1/255\. We can achieve this by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This, however, is not yet the generator that loads the images for us. The `ImageDataGenerator`
    class offers a range of generators that can be created by calling functions on
    it.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain a generator loading file, we have to call `flow_from_directory`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then have to specify the directory Keras should use, the batch size we would
    like, in this case `32`, as well as the target size the images should be resized
    to, in this case 150x150 pixels. To do this, we can simply run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'How did Keras find the images and how does it know which classes the images
    belong to? The Keras generator expects the following folder structure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Root:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class 0
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: img
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: img
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: …
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Class 1
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: img
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: img
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: …
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Class 1
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: img
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Our dataset is already set up that way, and it's usually not hard to sort images
    to match the generator's expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Working with pretrained models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training large computer vision models is not only hard, but computationally
    expensive. Therefore, it's common to use models that were originally trained for another
    purpose and fine-tune them for a new purpose. This is an example of transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning aims to transfer the learning from one task to another task.
    As humans, we are very good at transferring what we have learned. When you see
    a dog that you have not seen before, you don't need to relearn everything about
    dogs for this particular dog; instead, you just transfer new learning to what
    you already knew about dogs. It's not economical to retrain a big network every
    time, as you'll often find that there are parts of the model that we can reuse.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will fine-tune VGG-16, originally trained on the ImageNet
    dataset. The ImageNet competition is an annual computer vision competition, and
    the ImageNet dataset consists of millions of images of real-world objects, from
    dogs to planes.
  prefs: []
  type: TYPE_NORMAL
- en: In the ImageNet competition, researchers compete to build the most accurate
    models. In fact, ImageNet has driven much of the progress in computer vision over
    the recent years, and the models built for ImageNet competitions are a popular
    basis to fine-tune models from.
  prefs: []
  type: TYPE_NORMAL
- en: VGG-16 is a model architecture developed by the visual geometry group at Oxford
    University. The model consists of a convolutional part and a classification part.
    We will only be using the convolutional part. In addition, we will be adding our
    own classification part that can classify plants.
  prefs: []
  type: TYPE_NORMAL
- en: 'VGG-16 can be downloaded via Keras by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'When downloading the data, we want to let Keras know that we don''t want to
    include the top part (the classification part); we also want to let Keras know
    the desired input shape. If we do not specify the input shape, the model will
    accept any image size, and it will not be possible to add `Dense` layers on top:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the VGG model is very large, with over 14.7 million trainable
    parameters. It also consists of both `Conv2D` and `MaxPooling2D` layers, both
    of which we've already learned about when working on the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'From this point, there are two different ways we can proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: Add layers and build a new model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess all the images through the pertained model and then train a new model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying VGG-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be adding layers on top of the VGG-16 model, and then
    from there, we will train the new, big model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do not want to retrain all those convolutional layers that have been trained
    already, however. So, we must first "freeze" all the layers in VGG-16, which we
    can do by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Keras downloads VGG as a functional API model. We will learn more about the
    functional API in [Chapter 6](ch06.xhtml "Chapter 6. Using Generative Models"),
    *Using Generative Models*, but for now, we just want to use the Sequential API,
    which allows us to stack layers through `model.add()`. We can convert a model
    with the functional API with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: As a result of running the code, we have now created a new model called `finetune`
    that works just like a normal Sequential model. We need to remember that converting
    models with the Sequential API only works if the model can actually be expressed
    in the Sequential API. Some more complex models cannot be converted.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result of everything we''ve just done, adding layers to our model is now
    simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The newly added layers are by default trainable, while the reused model socket
    is not. We can train this stacked model just as we would train any other model,
    on the data generator we defined in the previous section. This can be executed
    by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: After running this, the model manages to achieve a rate of about 75% validation
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Random image augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A general problem in machine learning is that no matter how much data we have,
    having more data will always be better, as it would increase the quality of our
    output while also preventing overfitting and allowing our model to deal with a
    larger variety of inputs. It's therefore common to apply random augmentation to
    images, for example, a rotation or a random crop.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to get a large number of different images out of one image, therefore
    reducing the chance that the model will overfit. For most image augmentation purposes,
    we can just use Keras' `ImageDataGenerator`.
  prefs: []
  type: TYPE_NORMAL
- en: More advanced augmentations can be done with the OpenCV library. However, focusing
    on this is outside the scope of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation with ImageDataGenerator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using an augmenting data generator, we only usually use it for training.
    The validation generator should not use the augmentation features because when
    we validate our model, we want to estimate how well it is doing on unseen, actual
    data, and not augmented data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is different from rule-based augmentation, where we try to create images
    that are easier to classify. For this reason, we need to create two `ImageDataGenerator`
    instances, one for training and one for validation. This can be done by running
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: This training data generator makes use of a few built-in augmentation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: There are more commands available in Keras. For a full list, you
    should refer to the Keras documentation at [https://keras.io/](https://keras.io/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following list, we''ve highlighted several commonly used commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rescale` scales the values in the image. We used it before and will also use
    it for validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rotation_range` is a range (0 to 180 degrees) in which to randomly rotate
    the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width_shift_range` and `height_shift_range` are ranges (relative to the image
    size, so here 20%) in which to randomly stretch images horizontally or vertically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shear_range` is a range (again, relative to the image) in which to randomly
    apply shear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zoom_range` is the range in which to randomly zoom into a picture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`horizontal_flip` specifies whether to randomly flip the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fill_mode` specifies how to fill empty spaces created by, for example, rotation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can check out what the generator does by running one image through it multiple
    times.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import the Keras image tools and specify an image path (this one
    was chosen at random). This can be done by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We then need to load the image and convert it to a NumPy array, which is achieved
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we have to add a batch size dimension to the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We then use the `ImageDataGenerator` instance we just created, but instead
    of using `flow_from_directory`, we''ll use `flow`, which allows us to pass the
    data directly into the generator. We then pass that one image we want to use,
    which we can do by running this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'In a loop, we then call `next` on our generator four times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Augmentation with ImageDataGenerator](img/B10354_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A few samples of the randomly modified image
  prefs: []
  type: TYPE_NORMAL
- en: The modularity tradeoff
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has shown that it is possible, and often useful, to aid a machine
    learning model with some rule-based system. You might also have noticed that the
    images in the dataset were all cropped to show only one plant.
  prefs: []
  type: TYPE_NORMAL
- en: While we could have built a model to locate and classify the plants for us,
    in addition to classifying it, we could have also built a system that would output
    the treatment a plant should directly receive. This begs the question of how modular
    we should make our systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'End-to-end deep learning was all the rage for several years. If given a huge
    amount of data, a deep learning model can learn what would otherwise have taken
    a system with many components much longer to learn. However, end-to-end deep learning
    does have several drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end deep learning needs huge amounts of data. Because models have so
    many parameters, a large amount of data is needed in order to avoid overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End-to-end deep learning is hard to debug. If you replace your entire system
    with one black box model, you have little hope of finding out why certain things
    happened.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some things are hard to learn but easy to write down as a code, especially sanity-check
    rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recently, researchers have begun to make their models more modular. A great
    example is Ha and Schmidthuber''s *World Models*, which can be read here: [https://worldmodels.github.io/](https://worldmodels.github.io/).
    In this, they''ve encoded visual information, made predictions about the future,
    and chosen actions with three different models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the practical side, we can take a look at Airbnb, who combine structural
    modeling with machine learning for their pricing engine. You can read more about
    it here: [https://medium.com/airbnb-engineering/learning-market-dynamics-for-optimal-pricing-97cffbcc53e3](https://medium.com/airbnb-engineering/learning-market-dynamics-for-optimal-pricing-97cffbcc53e3).
    Modelers knew that bookings roughly follow a Poisson Distribution and that there
    are also seasonal effects. So, Airbnb built a model to predict the parameters
    of the distribution and seasonality directly, rather than letting the model predict
    bookings directly.'
  prefs: []
  type: TYPE_NORMAL
- en: If you have a small amount of data, then your algorithm's performance needs
    to come from human insight. If some subtasks can be easily expressed in code,
    then it's usually better to express them in code. If you need explainability and
    want to see why certain choices were made, a modular setup with clearly interpretable
    intermediate outputs is a good choice. However, if a task is hard and you don't
    know exactly what subtasks it entails, and you have lots of data, then it's often
    better to use an end-to-end approach.
  prefs: []
  type: TYPE_NORMAL
- en: It's very rare to use a *pure* end-to-end approach. Images, for example, are
    always preprocessed from the camera chip, you never really work with raw data.
  prefs: []
  type: TYPE_NORMAL
- en: Being smart about dividing a task can boost performance and reduce risk.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision beyond classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen, there are many techniques that we can use to make our image
    classifier work better. These are techniques that you'll find used throughout
    this book, and not only for computer vision applications.
  prefs: []
  type: TYPE_NORMAL
- en: In this final section of the chapter, we will discuss some approaches that go
    beyond classifying images. These tasks often require more creative use of neural
    networks than what we've discussed throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this section, you don't need to worry too much about
    the details of the techniques presented, but instead look at how researchers were
    creative about using neural networks. We're taking this approach because you will
    often find that the tasks you are looking to solve require similar creativity.
  prefs: []
  type: TYPE_NORMAL
- en: Facial recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Facial recognition has many applications for retail institutions. For instance,
    if you're in the front office, you might want to automatically recognize your
    customer at an ATM, or alternatively, you might want to offer face-based security
    features, such as the iPhone offers. In the back office, however, you need to
    comply with KYC regulations, which require you to identify which customer you
    are working with.
  prefs: []
  type: TYPE_NORMAL
- en: On the surface, facial recognition looks like a classification task. You give
    an image of a face to the machine, and it will predict which person it is. The
    trouble is that you might have millions of customers, but only one or two pictures
    per customer.
  prefs: []
  type: TYPE_NORMAL
- en: On top of that, you'll likely be continuously getting new customers. You can't
    change your model every time you get a new customer, and a simple classification
    approach will fail if it has to choose between millions of classes with only one
    example for each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The creative insight here is that instead of classifying the customer''s face,
    you can see whether two images show the same face. You can see a visual representation
    of this idea in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Facial recognition](img/B10354_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Schematic of a Siamese network
  prefs: []
  type: TYPE_NORMAL
- en: To this end, you'll have to run the two images through first. A Siamese network
    is a class of neural network architecture that contains two or more identical
    subnetworks, both of which are identical and contain the same weights. In Keras,
    you can achieve such a setup by defining the layers first and then using them
    in both networks. The two networks then feed into a single classification layer,
    which determines whether the two images show the same face.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid running all of the customer images in our database through the entire
    Siamese network every time we want to recognize a face, it's common to save the
    final output of the Siamese network. The final output of the Siamese network for
    an image is called the face embedding. When we want to recognize a customer, we compare
    the embedding of the image of the customer's face with the embeddings stored in
    our database. We can do this with a single classification layer.
  prefs: []
  type: TYPE_NORMAL
- en: Storing facial embedding is very beneficial as it will save us a significant
    amount of computational cost, in addition to allowing for the clustering of faces.
    Faces will cluster together according to traits such as sex, age, and race. By
    only comparing an image to the images in the same cluster, we can save even more
    computational power and, as a result, get even faster recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to train Siamese networks. We can train them together with
    the classifier by creating pairs of matching and non-matching images and then
    using binary cross-entropy classification loss to train the entire model. However,
    another, and in many respects better, option is to train the model to generate
    face embeddings directly. This approach is described in Schroff, Kalenichenko,
    and Philbin''s 2015 paper, *FaceNet: A Unified Embedding for Face Recognition
    and Clustering*, which you can read here: [https://arxiv.org/abs/1503.03832](https://arxiv.org/abs/1503.03832).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to create triplets of images: one anchor image, one positive image
    showing the same face as the anchor image, and one negative image showing a different
    face than the anchor image. A triplet loss is used to make the distance between
    the anchor''s embedding and the positive''s embedding smaller, and the distance
    between the anchor and the negative larger.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Facial recognition](img/B10354_03_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here ![Facial recognition](img/B10354_03_020.jpg) is an anchor image, and ![Facial
    recognition](img/B10354_03_021.jpg) is the output of the Siamese network, the
    anchor image's embedding. The triplet loss is the Euclidean distance between the
    anchor and the positive minus the Euclidean distance between the anchor and the
    negative. A small constant, ![Facial recognition](img/B10354_03_022.jpg), is a
    margin enforced between positive and negative pairs. To reach zero loss, the difference
    between distances needs to be ![Facial recognition](img/B10354_03_023.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to understand that you can use a neural network to predict
    whether two items are semantically the same in order to get around large classification
    problems. You can train the Siamese model through some binary classification tasks
    but also by treating the outputs as embeddings and using a triplet loss. This
    insight extends to more than faces. If you wanted to compare time series to classify
    events, then you could use the exact same approach.
  prefs: []
  type: TYPE_NORMAL
- en: Bounding box prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The likelihood is that at some point, you''ll be interested in locating objects
    within images. For instance, say you are an insurance company that needs to inspect
    the roofs it insures. Getting people to climb on roofs to check them is expensive,
    so an alternative is to use satellite imagery. Having acquired the images, you
    now need to find the roofs in them, as we can see in the following screenshot.
    You can then crop out the roofs and send the roof images to your experts, who
    will check them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bounding box prediction](img/B10354_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: California homes with bounding boxes around their roofs
  prefs: []
  type: TYPE_NORMAL
- en: What you need are bounding box predictions. A bounding box predictor outputs
    the coordinates of several bounding boxes together with predictions for what object
    is shown in the box.
  prefs: []
  type: TYPE_NORMAL
- en: There are two approaches to obtaining such bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: A **Region-based Convolutional Neural Network** (**R-CNN**) reuses a classification
    model. It takes an image and slides the classification model over the image. The
    result is many classifications for different parts of the image. Using this feature
    map, a region proposal network performs a regression task to come up with bounding
    boxes and a classification network creates classifications for each bounding box.
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach has been refined, culminating in Ren and others'' 2016 paper,
    *Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks*,
    which is available at [https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497),
    but the basic concept of sliding a classifier over an image has remained the same.'
  prefs: []
  type: TYPE_NORMAL
- en: '**You Only Look Once** (**YOLO**), on the other hand, uses a single model consisting
    of only convolutional layers. It divides an image into a grid and predicts an
    object class for each grid cell. It then predicts several possible bounding boxes
    containing objects for each grid cell.'
  prefs: []
  type: TYPE_NORMAL
- en: For each bounding box, it regresses coordinates and both width and height values,
    as well as a confidence score that this bounding box actually contains an object.
    It then eliminates all bounding boxes with a too low confidence score or with
    a too large overlap with another, a more confident bounding box.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For a more detailed description, read Redmon and Farhadi''s 2016 paper, *YOLO9000:
    Better, Faster, Stronger*, available at [https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242).
    Further reading includes the 2018 paper, *YOLOv3: An Incremental Improvement*.
    This is available at [https://arxiv.org/abs/1804.027](https://arxiv.org/abs/1804.027).'
  prefs: []
  type: TYPE_NORMAL
- en: Both are well-written, tongue-in-cheek papers, that explain the YOLO concept
    in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of YOLO over an R-CNN is that it's much faster. Not having
    to slide a large classification model is much more efficient. However, an R-CNN's
    main advantage is that it is somewhat more accurate than a YOLO model. If your
    task requires real-time analysis, you should use YOLO; however, if you do not
    need real-time speed but just want the best accuracy, then using an R-CNN is the
    way to go.
  prefs: []
  type: TYPE_NORMAL
- en: Bounding box detection is often used as one of many processing steps. In the
    insurance case, the bounding box detector would crop out all roofs. The roof images
    can then be judged by a human expert, or by a separate deep learning model that
    classifies damaged roofs. Of course, you could train an object locator to distinguish
    between damaged and intact roofs directly, but in practice, this is usually not
    a good idea.
  prefs: []
  type: TYPE_NORMAL
- en: If you're interested in reading more about this, [Chapter 4](ch04.xhtml "Chapter 4. Understanding
    Time Series"), *Understanding Time Series*, has a great discussion on modularity.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fashion MNIST is a drop-in replacement for MNIST, but instead of handwritten
    digits, it is about classifying clothes. Try out the techniques we have used in
    this chapter on Fashion MNIST. How do they work together? What gives good results?
    You can find the dataset on Kaggle at [https://www.kaggle.com/zalando-research/fashionmnist](https://www.kaggle.com/zalando-research/fashionmnist).
  prefs: []
  type: TYPE_NORMAL
- en: 'Take on the whale recognition challenge and read the top kernels and discussion
    posts. The link can be found here: [https://www.kaggle.com/c/whale-categorization-playground](https://www.kaggle.com/c/whale-categorization-playground).
    The task of recognizing whales by their fluke is similar to recognizing humans
    by their face. There are good kernels showing off bounding boxes as well as Siamese
    networks. We have not covered all the technical tools needed to solve the task
    yet, so do not worry about the code in detail but instead focus on the concepts
    shown.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have seen the building blocks of computer vision models.
    We've learned about convolutional layers, and both the ReLU activation and regularization
    methods. You have also seen a number of ways to use neural networks creatively,
    such as with Siamese networks and bounding box predictors.
  prefs: []
  type: TYPE_NORMAL
- en: You have also successfully implemented and tested all these approaches on a simple
    benchmark task, the MNIST dataset. We scaled up our training and used a pretrained
    VGG model to classify thousands of plant images, before then using a Keras generator
    to load images from disk on the fly and customizing the VGG model to fit our new
    task.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned about the importance of image augmentation and the modularity
    tradeoff in building computer vision models. Many of these building blocks, such
    as convolutions, batchnorm, and dropout, are used in other areas beyond computer
    vision. They are fundamental tools that you will see outside of computer vision
    applications as well. By learning about them here, you have set yourself up to discover
    a wide range of possibilities in, for example, time series or generative models.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision has many applications in the financial industry, especially
    in back-office functions as well as alternative alpha generation. It is one application
    of modern machine learning that can translate into real value for many corporations
    today. An increasing number of firms incorporate image-based data sources in their decision
    making; you are now prepared to tackle such problems head-on.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of this chapter, we've seen that an entire pipeline is involved
    in a successful computer vision project, and working on the pipeline often has
    a similar or greater benefit as compared to working on the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will look at the most iconic and common form of financial
    data: time series. We will tackle the task of forecasting web traffic using more
    traditional statistical methods, such as **ARIMA** (short for **AutoRegressive
    Integrated Moving Average**), as well as modern neural network-based approaches.
    You will also learn about feature engineering with autocorrelation and Fourier
    transformations. Finally, you will learn how to compare and contrast different
    forecasting methods and build a high-quality forecasting system.'
  prefs: []
  type: TYPE_NORMAL
