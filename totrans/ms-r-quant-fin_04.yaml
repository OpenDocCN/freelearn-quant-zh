- en: Chapter 4. Big Data – Advanced Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will deal with one of the biggest challenges of high-performance
    financial analytics and data management; that is, how to handle large datasets
    efficiently and flawlessly in R.
  prefs: []
  type: TYPE_NORMAL
- en: Our main objective is to give a practical introduction on how to access and
    manage large datasets in R. This chapter does not focus on any particular financial
    theorem, but it aims to give practical, hands-on examples to researchers and professionals
    on how to implement computationally - intensive analyses and models that leverage
    large datasets in the R environment.
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of this chapter, we explained how to access data directly
    for multiple open sources. R offers various tools and options to load data into
    the R environment without any prior data-management requirements. This part of
    the chapter will guide you through practical examples on how to access data using
    the `Quandl` and `qualtmod` packages. The examples presented here will be a useful
    reference for the other chapters of this book. In the second part of this chapter,
    we will highlight the limitation of R to handle big data and show practical examples
    on how to load a large amount of data in R with the help of big memory and `ff`
    packages. We will also show how to perform essential statistical analyses, such
    as K-mean clustering and linear regression, using large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Getting data from open sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extraction of financial time series or cross-sectional data from open sources
    is one of the challenges of any academic analysis. While several years ago, the
    accessibility of public data for financial analysis was very limited, in recent
    years, more and more open access databases are available, providing huge opportunities
    for quantitative analysts in any field.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will present the `Quandl` and `quantmod` packages, two specific
    tools that can be used to seamlessly access and load financial data in the R environment.
    We will lead you through two examples to showcase how these tools can help financial
    analysts to integrate data directly from sources without any prior data management.
  prefs: []
  type: TYPE_NORMAL
- en: Quandl is an open source website for financial time series, indexing over millions
    of financial, economic, and social datasets from 500 sources. The `Quandl` package
    interacts directly with the `Quandl` API to offer data in a number of formats
    usable in R. Besides downloading data, users can also upload and edit their own
    data, as well as search in any of the data sources directly from R.upload and
    search for any data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first simple example, we will show you how to retrieve and plot exchange
    rate time series with Quandl in an easy way. Before we can access any data from
    Quandl, we need to install and load the `Quandl` package using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will download the currency exchange rates in EUR for USD, CHF, GBP, JPY,
    RUB, CAD, and AUD between January 01, 2005 and May 30, 2014\. The following command
    specifies how to select a particular time series and period for the analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As the next step, we will visualize the exchange rate evolution of four selected
    exchange rates, USD, GBP, CAD, and AUD, using the `matplot()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot displays the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting data from open sources](img/2078OT_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Exchange rate plot of USD, GBP, CAD, and AUD'
  prefs: []
  type: TYPE_NORMAL
- en: In the second example, we will demonstrate the usage of the `quantmod` package
    to access, load, and investigate data from open sources. One of the huge advantages
    of the quantmod package is that it works with a variety of sources and accesses
    data directly for Yahoo! Finance, Google Finance, **Federal Reserve Economic Data**
    (**FRED**), or the Oanda website.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will access the stock price information of BMW and analyze
    the performance of the car-manufacturing company since 2010:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'From the Web, we will obtain the price data of BMW stock from Yahoo! Finance
    for the given time period. The `quantmod` package provides an easy-to-use function,
    `getSymbols()`, to download data from local or remote sources. As the first argument
    of the function, we need to define the character vector by specifying the name
    of the symbol loaded. The second one specifies the environment where the object
    is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As the next step, we need to load the `BMW.DE` variable from the `bmw_stock`
    environment to a vector. With the help of the `head()` function, we can also show
    the first six rows of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `quantmod` package is also equipped with a finance charting ability. The
    `chartSeries()` function allows us to not only visualize but also interact with
    the charts. With its expanded functionality, we can also add a wide range of technical
    and trading indicators to a basic chart; this is a very useful functionality for
    technical analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we will add the Bollinger Bands using the `addBBands()` command
    and the MACD trend-following momentum indicator using the `addMACD()` command
    to get more insights on the stock price evolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot displays the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting data from open sources](img/2078OT_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: BMW stock price evolution with technical indicators'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will calculate the daily log return of the BMW stock for the given
    period. We would also like to investigate whether the returns have normal distribution.
    The following figure shows the daily log returns of the BMW stock in the form
    of a normal Q-Q plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot displays the output of the preceding code. It shows
    the daily log returns of the BMW stock in the form of a normal Q-Q plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting data from open sources](img/2078OT_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Q-Q Plot of the daily return of BMW'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to big data analysis in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data refers to the situations when volume, velocity, or a variety of data
    exceeds the abilities of our computation capacity to process, store, and analyze
    them. Big data analysis has to deal not only with large datasets but also with
    computationally intensive analyses, simulations, and models with many parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging large data samples can provide significant advantages in the field
    of quantitative finance; we can relax the assumption of linearity and normality,
    generate better perdition models, or identify low-frequency events.
  prefs: []
  type: TYPE_NORMAL
- en: However, the analysis of large datasets raises two challenges. First, most of
    the tools of quantitative analysis have limited capacity to handle massive data,
    and even simple calculations and data-management tasks can be challenging to perform.
    Second, even without the capacity limit, computation on large datasets may be
    extremely time consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Although R is a powerful and robust program with a rich set of statistical algorithms
    and capabilities, one of the biggest shortcomings is its limited potential to
    scale to large data sizes. The reason for this is that R requires the data that
    it operates on to be first loaded into memory. However, the operating system and
    system architecture can only access approximately 4 GB of memory. If the dataset
    reaches the RAM threshold of the computer, it can literally become impossible
    to work with on a standard computer with a standard algorithm. Sometimes, even
    small datasets can cause serious computation problems in R, as R has to store
    the biggest object created during the analysis process.
  prefs: []
  type: TYPE_NORMAL
- en: R, however, has a few packages to bridge the gap to provide efficient support
    for big data analysis. In this section, we will introduce two particular packages
    that can be useful tools to create, store, access, and manipulate massive data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will introduce the `bigmemory` package that is a widely used option
    for large-scale statistical computing. The package and its sister packages (`biganalytics`,
    `bigtabulate`, and `bigalgebra`) address two challenges in handling and analyzing
    massive datasets: data management and statistical analysis. The tools are able
    to implement massive matrices that do not fit in the R runtime environment and
    support their manipulation and exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: An alternative for the bigmemory package is the `ff` package. This package allows
    R users to handle large vectors and matrices and work with several large data
    files simultaneously. The big advantage of `ff` objects is that they behave as
    ordinary R vectors. However, the data is not stored in the memory; it is a resident
    on the disk.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will showcase how these packages can help R users overcome
    the limitations of R to cope with very large datasets. Although the datasets we
    use here are simple in size, they effectively shows the power of big data packages.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering on big data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data frames and matrices are easy-to-use objects in R, with typical manipulations
    that execute quickly on datasets with a reasonable size. However, problems can
    arise when the user needs to handle larger data sets. In this section, we will
    illustrate how the `bigmemory` and `biganalytics` packages can solve the problem
    of too large datasets, which is impossible to handle by data frames or data tables.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The latest updates of `bigmemory`, `biganalytics`, and `biglm` packages are
    not available on Windows at time of writing this chapter. The examples shown here
    assume that R Version 2.15.3 is the current state-of-the-art version of R for
    Windows.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will perform K-means clustering on large datasets.
    For illustrative purposes, we will use the Airline Origin and Destination Survey
    data of the U.S. Bureau of Transportation Statistics. The datasets contain the
    summary characteristics of more than 3 million domestic flights, including the
    itinerary fare, number of passengers, originating airport, roundtrip indicator,
    and miles flown, in a `csv` format.
  prefs: []
  type: TYPE_NORMAL
- en: Loading big matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reading dataset from `csv` files can be easily executed by the `read.csv()`
    file. However, when we have to handle larger datasets, the reading time of any
    file can become quite substantial. With some careful options, however, the data-loading
    functionality of R can be significantly improved.
  prefs: []
  type: TYPE_NORMAL
- en: One option is to specify correct types in `colClasses = argument` when loading
    data to R; this will result in a faster conversion of external data. Also, the
    `NULL` specification of columns that are not needed for the analysis can significantly
    decrease the time and memory consumed to load the data.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the dataset reaches the RAM threshold of the computer, we need to
    adopt more memory-efficient data-leading options. In the following example, we
    will show how the bigmemory package can handle this task.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we will install and load the required `bigmemory` and `biganalytics`
    packages to perform the K-means cluster analysis on big data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We used the `read.big.matrix` function to import the downloaded dataset in
    R from the local system. The function handles data not as a data frame but as
    matrix-like objects, which we need to turn into a matrix with the `as.matrix`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Big data K-means clustering analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The format of the big data K-means function in R is `bigkmeans` (*x*, *centers*),
    where *x* is a numeric dataset (big data matrix object), and centers is the number
    of clusters to extract. The function returns the cluster memberships, centroids,
    **within cluster sum of squares** (**WCSS**), and cluster sizes. The `bigkmeans()`
    function works either on regular R matrix objects or on `big.matrix` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will determine the number of clusters based on the percentage of variance
    explained by each cluster; therefore, we will plot the percentage of variance
    explained by the clusters versus the number of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot displays the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Big data K-means clustering analysis](img/2078OT_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Plot the within cluser sums of squares versus the number of clusters
    extracted'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sharp decrease from 1 to 3 clusters (with little decrease thereafter) suggests
    a three-cluster solution. Therefore, we will perform the big data K-means cluster
    analysis with three clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `bigkmeans()` function also works with ordinary matrix objects, offering
    a faster calculation than the `kmeans()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test this hypothesis, we will measure the average execution time of the
    `bigkmeans()` and `kmeans()` functions with different dataset sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot displays the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Big data K-means clustering analysis](img/2078OT_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Execution time of the kmeans() and bigkmeans() function according
    to the size of the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the average execution time of the two functions takes substantial
    time. The preceding figure, however, reveals that `bigkmeans()` works more efficiently
    with larger datasets than the `kmeans()` function, thus reducing the calculation
    time of R in the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Big data linear regression analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will illustrate how to load large datasets directly from
    a URL with the help of the `ff` package and how to interact with a `biglm` package
    to fit a general linear regression model to the datasets that are larger than
    the memory. The `biglm` package can effectively handle datasets even if they overload
    the RAM of the computer, as it loads data into memory in chunks. It processes
    the last chunk and updates the sufficient statistics required for the model. It
    then disposes the chunk and loads the next one. This process is repeated until
    all the data is processed in the calculation.
  prefs: []
  type: TYPE_NORMAL
- en: The following example examines the unemployment compensation amount as a linear
    function of a few social-economic data.
  prefs: []
  type: TYPE_NORMAL
- en: Loading big data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To perform a big data linear regression analysis, we first need to install
    and load the `ff` packages, which we will use to open large files in R, and the
    `biglm` package, which we will use to fit the linear regression model on our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: For the big data linear regression analysis, we used the Individual Income Tax
    ZIP Code Data provided by the U.S government agency, **Internal Revenue Service
    (IRS)**. ZIP code-level data shows selected income and tax items classified by
    the state, ZIP code, and income classes. We used the 2012 data of the database;
    this database is reasonable in size but allows us to highlight the functionality
    of the big data packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will directly load the required dataset into R from the URL with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have downloaded the data, we will use the `read.table.ffdf` function
    that reads the files into an `ffdf` object that is supported by the `ff` package.
    The `read.table.ffdf` function works very much like the `read.table` function.
    It also provides convenient options to read other file formats, such as `csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: After we have converted the dataset into an `ff` object, we will load the `biglm`
    package to perform the linear regression analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging the dataset of almost 1,67,000 observations along 77 different variables,
    we will investigate whether the location-level amount of unemployment compensation
    (defined as variable `A02300`) can be explained by the total salary and wages
    amount (A00200), the number of residents by income category (AGI_STUB), the number
    of dependents (the NUMDEP variable), and the number of married people (MARS2)
    in the given location.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a linear regression model on large datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the linear regression analysis, we will use the `biglm` function; therefore,
    before we specify our model, we need to load the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As the next step, we will define the formula and fit the model on our data.
    With the summary function, we can obtain the coefficients and the significance
    level of the variable of the fitted model. As the model output does not include
    the R-square value, we need to load the R-square value of the model with a separate
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can conclude from the regression model coefficient output that all the variables
    contribute significantly to the model. The independent variables explain 86.09
    percent of the total variance of the unemployment compensation amount, indicating
    a good fit of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we applied R to access data from open sources and perform various
    analyses on large datasets. The examples presented here aimed to be a practical
    guide to empirical researchers who handle a large amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: First, we introduced useful methods for open source data integration. R has
    powerful options to directly access data for financial analysis without any prior
    data-management requirement. Second, we discussed how to handle big data in an
    R environment. Although R has fundamental limitations in handling large datasets
    and performing computationally intensive analyses and simulations, we introduced
    specific tools and packages that can bridge this gap. We presented two examples
    on how to perform K-means clustering and how to fit linear regression models on
    big data. This is the last chapter of the first part in this book. Next we will
    look at FX derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Adler, D., Nenadic, O., Zucchini, W.,Gläser, C. (2007)**: The ff package:
    Handling Large Data Sets in R with Memory Mapped Pages of Binary Flat Files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enea, M. (2009)**: Fitting Linear Models and Generalized Linear Models with
    large data sets in R. In book of short papers, conference on "Statistical Methods
    for the analysis of large data-sets", Italian Statistical Society, Chieti-Pescara,
    23-25 September 2009, 411-414.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kane, M.,Emerson, JW., Weston (2010)**: The Bigmemory Project, Yale University'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kane, M.,Emerson, JW., Weston, S. (2013)**: Scalable Strategies for Computing
    with Massive Data. Journal of Statistical Software , Vol. 55, Issue 14'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lumley, T. (2009) biglm**: bounded memory linear and generalized linear models.
    R package version 0.7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
