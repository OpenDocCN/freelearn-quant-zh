["```py\nnlp = spacy.load('en')\nnlp.pipe_names\n['tagger', 'parser', 'ner']\n```", "```py\nsample_text = 'Apple is looking at buying U.K. startup for $1 billion'\ndoc = nlp(sample_text)\n```", "```py\npd.DataFrame([[t.text, t.lemma_, t.pos_, t.tag_, t.dep_, t.shape_, t.is_alpha, t.is_stop] for t in doc],\n             columns=['text', 'lemma', 'pos', 'tag', 'dep', 'shape', 'is_alpha', 'is_stop'])\n```", "```py\ndisplacy.render(doc, style='dep', options=options, jupyter=True)\n```", "```py\nspacy.explain(\"VBZ\")\nverb, 3rd person singular present\n```", "```py\nfiles = Path('..', 'data', 'bbc').glob('**/*.txt')\nbbc_articles = []\nfor i, file in enumerate(files):\n    _, _, _, topic, file_name = file.parts\n    with file.open(encoding='latin1') as f:\n        lines = f.readlines()\n        body = ' '.join([l.strip() for l in lines[1:]]).strip()\n        bbc_articles.append(body)\nlen(bbc_articles)\n2225\n```", "```py\ndoc = nlp(bbc_articles[0])\ntype(doc)\nspacy.tokens.doc.Doc\n```", "```py\nsentences = [s for s in doc.sents]\nsentences[:3]\n[Voting is under way for the annual Bloggies which recognize the best web blogs - online spaces where people publish their thoughts - of the year. ,\nNominations were announced on Sunday, but traffic to the official site was so heavy that the website was temporarily closed because of too many visitors.,\nWeblogs have been nominated in 30 categories, from the top regional blog, to the best-kept-secret blog.]\n```", "```py\nfor t in sentences[0]:\n    if t.ent_type_:\n        print('{} | {} | {}'.format(t.text, t.ent_type_, spacy.explain(t.ent_type_)))\nannual | DATE | Absolute or relative dates or periods\nthe | DATE | Absolute or relative dates or periods\nyear | DATE | Absolute or relative dates or periods\n```", "```py\nfrom textacy.extract import named_entities\nentities = [e.text for e in named_entities(doc)]\npd.Series(entities).value_counts()\nyear                          4\nUS                            2\nSouth-East Asia Earthquake    2\nannual                        2\nTsunami Blog                  2\n```", "```py\nfrom textacy.extract import ngrams\npd.Series([n.text for n in ngrams(doc, n=2, min_freq=2)]).value_counts()\nEast Asia          2\nAsia Earthquake    2\nTsunami Blog       2\nannual Bloggies    2\n```", "```py\niter_texts = (bbc_articles[i] for i in range(len(bbc_articles)))\nfor i, doc in enumerate(nlp.pipe(iter_texts, batch_size=50, n_threads=8)):\n      assert doc.is_parsed\n```", "```py\nmodel = {}\nfor language in ['en', 'es']:\n    model[language] = spacy.load(language)\n```", "```py\ntext = {}\npath = Path('../data/TED')\nfor language in ['en', 'es']:\n    file_name = path / 'TED2013_sample.{}'.format(language)\n    text[language] = file_name.read_text()\n```", "```py\nparsed, sentences = {}, {}\nfor language in ['en', 'es']:\n    parsed[language] = model[language](text[language])\n    sentences[language] = list(parsed[language].sents)\nprint('Sentences:', language, len(sentences[language]))\nSentences: en 19\nSentences: es 22\n```", "```py\npos = {}\nfor language in ['en', 'es']:\n    pos[language] = pd.DataFrame([[t.text, t.pos_, spacy.explain(t.pos_)] for t in sentences[language][0]],\n    columns=['Token', 'POS Tag', 'Meaning'])\npd.concat([pos['en'], pos['es']], axis=1).head()\n```", "```py\nfrom textblob import TextBlob\narticle = docs.sample(1).squeeze()\nparsed_body = TextBlob(article.body)\n```", "```py\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer('english')\n[(word, stemmer.stem(word)) for i, word in enumerate(parsed_body.words)\n    if word.lower() != stemmer.stem(parsed_body.words[i])]\n[('Andy', 'andi'),\n('faces', 'face'),\n('tenure', 'tenur'),\n('tries', 'tri'),\n('winning', 'win'),\n```", "```py\nparsed_body.sentiment\nSentiment(polarity=0.088031914893617, subjectivity=0.46456433637284694)\n```", "```py\nbinary_vectorizer = CountVectorizer(max_df=1.0,\n                                    min_df=1,\n                                    binary=True)\n\nbinary_dtm = binary_vectorizer.fit_transform(docs.body)\n<2225x29275 sparse matrix of type '<class 'numpy.int64'>'\n   with 445870 stored elements in Compressed Sparse Row format>\n```", "```py\nm = binary_dtm.todense() # pdist does not accept sparse format\npairwise_distances = pdist(m, metric='cosine')\nclosest = np.argmin(pairwise_distances) # index that minimizes distance\nrows, cols = np.triu_indices(n_docs) # get row-col indices\nrows[closest], cols[closest]\n(11, 75)\n```", "```py\nnlp = spacy.load('en')\ndef tokenizer(doc):\n    return [w.lemma_ for w in nlp(doc) \n                if not w.is_punct | w.is_space]\nvectorizer = CountVectorizer(tokenizer=tokenizer, binary=True)\ndoc_term_matrix = vectorizer.fit_transform(docs.body)\n```", "```py\nsample_docs = ['call you tomorrow',\n               'Call me a taxi',\n               'please call me... PLEASE!']\n```", "```py\nvectorizer = CountVectorizer()\ntf_dtm = vectorizer.fit_transform(sample_docs).todense()\ntokens = vectorizer.get_feature_names()\nterm_frequency = pd.DataFrame(data=tf_dtm,\n                             columns=tokens)\n\n  call  me  please  taxi  tomorrow  you\n0     1   0       0     0         1    1\n1     1   1       0     1         0    0\n2     1   1       2     0         0    0\n```", "```py\nvectorizer = CountVectorizer(binary=True)\ndf_dtm = vectorizer.fit_transform(sample_docs).todense().sum(axis=0)\ndocument_frequency = pd.DataFrame(data=df_dtm,\n                                  columns=tokens)\n   call  me  please  taxi  tomorrow  you\n0     3   2       1     1         1    1\n```", "```py\ntfidf = pd.DataFrame(data=tf_dtm/df_dtm, columns=tokens)\n   call   me  please  taxi  tomorrow  you\n0  0.33 0.00    0.00  0.00      1.00 1.00\n1  0.33 0.50    0.00  1.00      0.00 0.00\n2  0.33 0.50    2.00  0.00      0.00 0.00\n```", "```py\nvect = TfidfVectorizer(smooth_idf=True,\n                      norm='l2',  # squared weights sum to 1 by \n                                    document\n                      sublinear_tf=False,  # if True, use 1+log(tf)\n                      binary=False)\npd.DataFrame(vect.fit_transform(sample_docs).todense(),\n            columns=vect.get_feature_names())\n\n   call   me  please  taxi  tomorrow  you\n0  0.39 0.00    0.00  0.00      0.65 0.65\n1  0.43 0.55    0.00  0.72      0.00 0.00\n2  0.27 0.34    0.90  0.00      0.00 0.00\n```", "```py\nRangeIndex: 2225 entries, 0 to 2224\nData columns (total 3 columns):\ntopic 2225 non-null object\nheading 2225 non-null object\nbody 2225 non-null object\n```", "```py\ny = pd.factorize(docs.topic)[0] # create integer class values\nX = docs.body\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n```", "```py\nvectorizer = CountVectorizer()\nX_train_dtm = vectorizer.fit_transform(X_train)\nX_test_dtm = vectorizer.transform(X_test)\nX_train_dtm.shape, X_test_dtm.shape\n((1668, 25919), (557, 25919))\n```", "```py\nnb = MultinomialNB()\nnb.fit(X_train_dtm, y_train)\ny_pred_class = nb.predict(X_test_dtm)\n```", "```py\naccuracy_score(y_test, y_pred_class)\n0.97666068222621\n```", "```py\nvectorizer = CountVectorizer(min_df=.001, max_df=.8, stop_words='english')\ntrain_dtm = vectorizer.fit_transform(train.text)\n<1566668x934 sparse matrix of type '<class 'numpy.int64'>'\n   with 6332930 stored elements in Compressed Sparse Row format>\n```", "```py\nnb = MultinomialNB()\nnb.fit(train_dtm, train.polarity)\npredicted_polarity = nb.predict(test_dtm)\n```", "```py\naccuracy_score(test.polarity, y_pred_class)\n0.7768361581920904\n```", "```py\ntest['predicted'] = train.stars.mode().iloc[0]\naccuracy_score(test.stars, test.predicted)\n0.5196950594793454\n```", "```py\nnb = MultinomialNB()\nnb.fit(train_dtm,train.stars)\npredicted_stars = nb.predict(test_dtm)\n```", "```py\naccuracy_score(test.stars, predicted_stars)\n0.6465164206691094\n```", "```py\nlogreg = LogisticRegression(C=1e9)\nlogreg.fit(X=train_dtm, y=train.stars)\ny_pred_class = logreg.predict(test_dtm)\n```", "```py\naccuracy_score(test.stars, y_pred_class)\n0.7360498864740219\n```", "```py\ntrain_numeric = sparse.csr_matrix(train_dummies.astype(np.int8))\ntrain_dtm_numeric = sparse.hstack((train_dtm, train_numeric))\n```", "```py\nmulti_logreg = LogisticRegression(C=1e9, multi_class='multinomial', \n                                  solver='lbfgs')\nmulti_logreg.fit(train_dtm_numeric.astype(float), train.stars)\ny_pred_class = multi_logreg.predict(test_dtm_numeric.astype(float))\n```", "```py\naccuracy_score(test.stars, y_pred_class)\n0.7464488070176475\n```", "```py\nparam = {'objective':'multiclass', 'num_class': 5}\nbooster = lgb.train(params=param,\n                    train_set=lgb_train,\n                    num_boost_round=500,\n                    early_stopping_rounds=20,\n                    valid_sets=[lgb_train, lgb_test])\n```", "```py\ny_pred_class = booster.predict(test_dtm_numeric.astype(float))\naccuracy_score(test.stars, y_pred_class.argmax(1) + 1)\n0.738665855696524\n```"]