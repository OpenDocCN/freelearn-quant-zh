["```py\nclass OneStepTimeSeriesSplit:\n    \"\"\"Generates tuples of train_idx, test_idx pairs\n    Assumes the index contains a level labeled 'date'\"\"\"\n\n    def __init__(self, n_splits=3, test_period_length=1, shuffle=False):\n        self.n_splits = n_splits\n        self.test_period_length = test_period_length\n        self.shuffle = shuffle\n        self.test_end = n_splits * test_period_length\n\n    @staticmethod\n    def chunks(l, chunk_size):\n        for i in range(0, len(l), chunk_size):\n            yield l[i:i + chunk_size]\n\n    def split(self, X, y=None, groups=None):\n        unique_dates = (X.index\n                        .get_level_values('date')\n                        .unique()\n                        .sort_values(ascending=False)[:self.test_end])\n\n        dates = X.reset_index()[['date']]\n        for test_date in self.chunks(unique_dates, self.test_period_length):\n            train_idx = dates[dates.date < min(test_date)].index\n            test_idx = dates[dates.date.isin(test_date)].index\n            if self.shuffle:\n                np.random.shuffle(list(train_idx))\n            yield train_idx, test_idx\n```", "```py\nfrom sklearn.tree import DecisionTreeRegressor\n\n# configure regression tree\nregression_tree = DecisionTreeRegressor(criterion='mse', # default\n                                        max_depth=4,     # up to 4 splits\n                                        random_state=42)\n# Create training data\ny = data.returns\nX = data.drop('returns', axis=1)\nX2 = X.loc[:, ['t-1', 't-2']]\n\n# fit model\nregression_tree.fit(X=X2, y=y)\n\n# fit OLS model\nols_model = sm.OLS(endog=y, exog=sm.add_constant(X2)).fit()\n```", "```py\n# randomize train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n\n# configure & train tree learner\nclassifier = DecisionTreeClassifier(criterion='gini',\n                                    max_depth=5,\n                                    random_state=42)\nclassifier.fit(X=X_train, y=y_train)\n\n# Output:\nDecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n            splitter='best')\n```", "```py\ndot_data = export_graphviz(classifier,\n                           out_file=None, # opt. save to file and convert to png\n                           feature_names=X.columns,\n                           class_names=['Down', 'Up'],\n                           max_depth=3,\n                           filled=True,\n                           rounded=True,\n                           special_characters=True)\n\ngraphviz.Source(dot_data)\n```", "```py\ny_score = classifier.predict_proba(X=X_test)[:, 1] # only keep probabilities for pos. class\n```", "```py\nroc_auc_score(y_score=y_score, y_true=y_test)\n0.5941\n```", "```py\nclf = DecisionTreeClassifier(random_state=42)\nparam_grid = {'max_depth': range(10, 20),\n              'min_samples_leaf': [250, 500, 750],\n              'max_features': ['sqrt', 'auto']\n              }\n```", "```py\ngridsearch_clf = GridSearchCV(estimator=clf,\n                          param_grid=param_grid,\n                          scoring='roc_auc',\n                          n_jobs=-1,\n                          cv=cv,  # custom OneStepTimeSeriesSplit\n                          refit=True,\n                          return_train_score=True)\n\ngridsearch_clf.fit(X=X, y=y_binary)\n```", "```py\ngridsearch_clf.best_params_\n{'max_depth': 13, 'max_features': 'sqrt', 'min_samples_leaf': 500}\n\ngridsearch_clf.best_score_\n0.5855\n```", "```py\ndef get_leaves_count(tree):\n    t = tree.tree_\n    n = t.node_count\n    leaves = len([i for i in range(t.node_count) if t.children_left[i]== -1])\n    return leaves\n```", "```py\ntrain_scores, val_scores, leaves = {}, {}, {}\nfor max_depth in range(1, 26):\n    print(max_depth, end=' ', flush=True)\n    clf = DecisionTreeClassifier(criterion='gini', \n                                 max_depth=max_depth,\n                                 min_samples_leaf=500,\n                                 max_features='auto',\n                                 random_state=42)\n    train_scores[max_depth], val_scores[max_depth], leaves[max_depth] = [], [], []\n    for train_idx, test_idx in cv.split(X):\n        X_train, y_train,  = X.iloc[train_idx], y_binary.iloc[train_idx]\n        X_test, y_test = X.iloc[test_idx], y_binary.iloc[test_idx]\n        clf.fit(X=X_train, y=y_train)\n\n        train_pred = clf.predict_proba(X=X_train)[:, 1]\n        train_score = roc_auc_score(y_score=train_pred, y_true=y_train)\n        train_scores[max_depth].append(train_score)\n\n        test_pred = clf.predict_proba(X=X_test)[:, 1]\n        val_score = roc_auc_score(y_score=test_pred, y_true=y_test)\n        val_scores[max_depth].append(val_score)    \n        leaves[max_depth].append(get_leaves_count(clf))\n```", "```py\nnoise = .5  # noise relative to std(y)\nnoise = y.std() * noise_to_signal\n\nX_test = choice(x, size=test_size, replace=False)\n\nmax_depth = 10\nn_estimators=10\n\ntree = DecisionTreeRegressor(max_depth=max_depth)\nbagged_tree = BaggingRegressor(base_estimator=tree, n_estimators=n_estimators)\nlearners = {'Decision Tree': tree, 'Bagging Regressor': bagged_tree}\n\npredictions = {k: pd.DataFrame() for k, v in learners.items()}\nfor i in range(reps):\n    X_train = choice(x, train_size)\n    y_train = f(X_train) + normal(scale=noise, size=train_size)\n    for label, learner in learners.items():\n        learner.fit(X=X_train.reshape(-1, 1), y=y_train)\n        preds = pd.DataFrame({i: learner.predict(X_test.reshape(-1, 1))}, index=X_test)\n        predictions[label] = pd.concat([predictions[label], preds], axis=1)\n```", "```py\nrf_clf = RandomForestClassifier(n_estimators=10,\n                                criterion='gini',\n                                max_depth=None,\n                                min_samples_split=2,\n                                min_samples_leaf=1,\n                                min_weight_fraction_leaf=0.0,\n                                max_features='auto',\n                                max_leaf_nodes=None,\n                                min_impurity_decrease=0.0,\n                                min_impurity_split=None,\n                                bootstrap=True, oob_score=False,\n                                n_jobs=-1, random_state=42)\n```", "```py\ncv = OneStepTimeSeriesSplit(n_splits=10)\nclf = RandomForestClassifier(random_state=42, n_jobs=-1)\nparam_grid = {'n_estimators': [200, 400],\n              'max_depth': [10, 15, 20],\n              'min_samples_leaf': [50, 100]}\n```", "```py\ngridsearch_clf = GridSearchCV(estimator=clf,\n                          param_grid=param_grid,\n                          scoring='roc_auc',\n                          n_jobs=-1,\n                          cv=cv,\n                          refit=True,\n                          return_train_score=True,\n                          verbose=1)\n```", "```py\ngridsearch_clf.fit(X=X, y=y_binary)\n```", "```py\ngridsearch_clf.bestparams{'max_depth': 15,\n 'min_samples_leaf': 100,\n 'n_estimators': 400}\n```", "```py\ngridsearch_clf.bestscore_0.6013\n```"]