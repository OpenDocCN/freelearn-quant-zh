- en: Bayesian Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯机器学习
- en: In this chapter, we will introduce Bayesian approaches to machine learning,
    and how their different perspectives on uncertainty add value when developing
    and evaluating algorithmic trading strategies.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍贝叶斯方法在机器学习中的应用，以及它们对开发和评估算法交易策略时的不同不确定性视角的增值。
- en: Bayesian statistics allow us to quantify the uncertainty about future events
    and refine our estimates in a principled way as new information arrives. This
    dynamic approach adapts well to the evolving nature of financial markets. It is
    particularly useful when there is less relevant data and we require methods that
    systematically integrate prior knowledge or assumptions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯统计学使我们能够量化对未来事件的不确定性，并以原则性的方式在新信息到来时优化我们的估计。这种动态方法很好地适应了金融市场的发展性质。当存在较少相关数据且我们需要系统地整合先验知识或假设时，它特别有用。
- en: We will see that Bayesian approaches to machine learning allow for richer insights
    into the uncertainty around statistical metrics, parameter estimates, and predictions.
    The applications range from more granular risk management to dynamic updates of
    predictive models that incorporate changes in the market environment. The Black-Litterman
    approach to asset allocation (see [Chapter 5](1de6a332-69f8-4530-8d18-1007d0a3eb7e.xhtml), *Strategy
    Evaluation*, can be interpreted as a Bayesian model. It computes the expected
    return as an average of the market equilibrium and the investor's views, weighted
    by each asset's volatility, cross-asset correlations, and the confidence in each
    forecast.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到，贝叶斯方法使得对统计指标、参数估计和预测周围的不确定性有更丰富的见解。应用范围从更精细的风险管理到动态更新的预测模型，其中包含了市场环境的变化。资产配置的黑-利特曼方法（见[第五章](1de6a332-69f8-4530-8d18-1007d0a3eb7e.xhtml)，《策略评估》）可以解释为贝叶斯模型。它计算预期收益，作为市场均衡和投资者观点的加权平均值，每个资产的波动性，跨资产的相关性以及对每个预测的信心。
- en: 'More specifically, in this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中，我们将涵盖以下主题：
- en: How Bayesian statistics apply to machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯统计如何应用于机器学习
- en: How to use probabilistic programming with PyMC3
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用PyMC3进行概率编程
- en: How to define and train machine learning models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何定义和训练机器学习模型
- en: How to run state-of-the-art sampling methods to conduct approximate inference
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何运行最先进的抽样方法进行近似推断
- en: How to apply Bayesian machine learning to compute dynamic Sharpe ratios, build
    Bayesian classifiers, and estimate stochastic volatility
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何应用贝叶斯机器学习来计算动态夏普比率，构建贝叶斯分类器和估计随机波动性
- en: References, links to additional material, and the code examples for this chapter
    are in the corresponding directory of the GitHub repository. Please follow the
    installation instructions provided in [Chapter 1](d68f12f8-66fd-4857-b60e-399e5bbd9ea2.xhtml), *Machine
    Learning for Trading*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的参考文献、附加材料链接和代码示例位于GitHub存储库相应目录中。请按照[第一章](d68f12f8-66fd-4857-b60e-399e5bbd9ea2.xhtml)提供的安装说明进行操作，《交易的机器学习》。
- en: How Bayesian machine learning works
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯机器学习的工作原理
- en: Classical statistics is also called frequentist because it interprets probability
    as the relative frequency of an event over the long run, that is, after observing
    a large number of trials. In the context of probabilities, an event is a combination
    of one or more elementary outcomes of an experiment, such as any of six equal
    results in rolls of two dice or an asset price dropping by 10% or more on a given
    day.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 经典统计学也被称为频率派，因为它将概率解释为长期内事件的相对频率，即在观察了大量试验之后。在概率的背景下，一个事件是一个实验的一个或多个基本结果的组合，比如两个骰子掷出六个相等的结果中的任何一个，或者某个资产价格在某一天下跌10%或更多。
- en: Bayesian statistics, in contrast, views probability as a measure of the confidence
    or belief in the occurrence of an event. The Bayesian perspective of probability
    leaves more room for subjective views and, consequently, differences in opinions
    than the frequentist interpretation. This difference is most striking for events
    that do not happen often enough to arrive at an objective measure of long-term
    frequency.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯统计学相反，将概率视为事件发生的信心或信念的度量。贝叶斯概率的观点为主观观点留下了更多的空间，因此，与频率派解释相比，意见之间的差异更大。这种差异在很少发生的事件中最为显著，以至于无法得出客观的长期频率度量。
- en: Put differently, frequentist statistics assume that data is a random sample
    from a population and aims to identify the fixed parameters that generated the
    data. Bayesian statistics, in turn, take the data as given and considers the parameters
    to be random variables with a distribution that can be inferred from data. As
    a result, frequentist approaches require at least as many data points as there
    are parameters to be estimated. Bayesian approaches, on the other hand, are compatible
    with smaller datasets and are well-suited for online learning, one sample at a
    time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，频率学派统计假设数据是来自人群的随机样本，并旨在识别生成数据的固定参数。相反，贝叶斯统计将数据视为已知的，并认为参数是随机变量，其分布可以从数据中推断出来。因此，频率学派方法要求的数据点至少与要估计的参数一样多。另一方面，贝叶斯方法与较小的数据集兼容，并且非常适合逐个样本进行在线学习。
- en: The Bayesian view is very useful for many real-world events that are rare or
    unique, at least in important respects. Examples include the outcome of the next
    election or the question of whether the markets will crash within three months.
    In each case, there is both relevant historical data as well as unique circumstances
    that unfold as the event approaches.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯观点对于许多在某些重要方面罕见或独特的现实事件非常有用。例如，下次选举的结果或市场是否会在三个月内崩溃的问题。在每种情况下，既有相关的历史数据，又有随着事件临近而展开的独特情况。
- en: 'First, we will introduce Bayes'' theorem, which crystallizes the concept of
    updating beliefs by combining prior assumptions with new empirical evidence and
    comparing the resulting parameter estimates with their frequentist counterparts.
    We will then demonstrate two approaches to Bayesian statistical inference that
    produce insights into the posterior distribution of the latent, that is, unobserved
    parameters, such as their expected values, under different circumstances:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍贝叶斯定理，该定理通过将先验假设与新的经验证据相结合，并将得到的参数估计与频率学派的对应估计进行比较，以晶化通过更新信念来更新概念的过程。然后，我们将演示两种贝叶斯统计推断的方法，这些方法能够揭示潜在参数的后验分布，即未观察到的参数，在不同情况下的预期值等：
- en: Conjugate priors facilitate the updating process by providing a closed-form
    solution, but exact, analytical methods are not always available.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 共轭先验通过提供闭合形式的解决方案来促进更新过程，但确切的分析方法并不总是可用。
- en: Approximate inference simulates the distribution that results from combining
    assumptions and data and uses samples from this distribution to compute statistical
    insights.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 近似推断模拟了由假设和数据组合而成的分布，并使用该分布的样本来计算统计洞察。
- en: How to update assumptions from empirical evidence
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何从经验证据更新假设
- en: 'The theorem that Reverend Thomas Bayes came up with over 250 years ago uses
    fundamental probability theory to prescribe how probabilities or beliefs should
    change as relevant new information arrives. The following quote by – John Maynard
    Keynes captures the Bayesian mindset:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 牧师托马斯·贝叶斯在250多年前提出的定理利用基本的概率理论规定了概率或信念在相关新信息到达时应该如何变化。以下约翰·梅纳德·凯恩斯的引述体现了贝叶斯主义的思维方式：
- en: '"When the facts change, I change my mind. What do you do, sir?"'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: “当事实发生变化时，我改变我的想法。先生，你会怎么做？”
- en: It relies on the conditional and total probability and the chain rule; see the
    references on GitHub for reviews of these concepts.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 它依赖于条件概率和全概率以及链式法则；有关这些概念的评论，请参阅GitHub上的参考资料。
- en: The belief concerns a single or vector of parameters θ (also called hypotheses).
    Each parameter can be discrete or continuous. θ could be a one-dimensional statistic
    like the (discrete) mode of a categorical variable or a (continuous) mean, or
    a higher dimensional set of values like a covariance matrix or the weights of
    a deep neural network.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 信念涉及单个或一组参数 θ（也称为假设）。每个参数可以是离散的或连续的。θ 可以是一个一维统计量，比如（离散的）分类变量的模式，或者（连续的）均值，也可以是一个更高维度的值集，比如一个协方差矩阵或深度神经网络的权重。
- en: A key difference of frequentist statistics is that Bayesian assumptions are
    expressed as probability distributions rather than parameter values. Consequently,
    while frequentist inference focuses on point estimates, Bayesian inference yields
    probability distributions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 频率学派统计的一个关键区别在于，贝叶斯假设被表达为概率分布，而不是参数值。因此，虽然频率学派的推断关注点估计，贝叶斯推断则产生概率分布。
- en: 'Bayes'' Theorem updates the beliefs about the parameters of interest by computing
    the posterior probability distribution from the following inputs, as shown in
    the following diagram:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理通过计算从以下输入中得到的后验概率分布来更新对感兴趣参数的信念，如下图所示：
- en: The **prior** distribution indicates how likely we consider each possible hypothesis.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**先验**分布指示我们考虑每个可能的假设的可能性有多大。'
- en: The **likelihood ****function** outputs the probability of observing a dataset
    given certain values for the θ parameters.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**似然函数**输出在给定θ参数的某些值的情况下观察到数据集的概率。'
- en: 'The **evidence** measures how likely the observed data is given all possible
    hypotheses. Hence, it is the same for all parameter values and serves to normalize
    the numerator:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**证据**度量观察到的数据在所有可能的假设下的可能性。因此，它对所有参数值都是相同的，用于将分子标准化：'
- en: '![](img/cf94d4c2-95fd-4979-8996-50b006b89cba.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf94d4c2-95fd-4979-8996-50b006b89cba.png)'
- en: Bayes Theorem
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: The posterior is the product of prior and likelihood, divided by the evidence,
    and reflects the updated probability distribution of the hypotheses, taking into
    account both prior assumptions and the data. Viewed differently, the product of
    the prior and the likelihood results from applying the chain rule to factorize
    the joint distribution of data and parameters.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 后验是先验和似然的乘积，除以证据，反映了假设的更新概率分布，同时考虑了先前的假设和数据。从不同的角度看，先验和似然的乘积来自于将数据和参数的联合分布因子分解的链规则的应用。
- en: With higher-dimensional, continuous variables, the formulation becomes more
    complex and involves (multiple) integrals. An alternative formulation uses odds
    to express the posterior odds as the product of the prior odds times the likelihood
    ratio (see the references for more details).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高维、连续变量，制定变得更加复杂，涉及到（多个）积分。一种替代的制定方法使用赔率来表示后验赔率，作为先验赔率乘以似然比的乘积（有关更多细节，请参见参考资料）。
- en: Exact inference: Maximum a Posteriori estimation
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确推理：最大后验估计
- en: Practical applications of Bayes' rule to exactly compute posterior probabilities
    are quite limited because the computation of the evidence term in the denominator
    is quite challenging. The evidence reflects the probability of the observed data
    over all possible parameter values. It is also called the marginal likelihood
    because it requires *marginalizing out* the parameters' distribution by adding
    or integrating over their distribution. This is generally only possible in simple
    cases with a small number of discrete parameters that assume very few values.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 将贝叶斯规则的实际应用于准确计算后验概率的情况非常有限，因为计算分母中的证据项非常具有挑战性。证据反映了在所有可能的参数值上观察到的数据的概率。它也被称为边际似然，因为它需要通过添加或积分参数的分布来对参数的分布进行*边际化*。这通常只在具有少量假设值的少量离散参数的简单情况下才可能。
- en: '**Maximum a posteriori probability (MAP)** estimation leverages that the evidence
    is a constant factor that scales the posterior to meet the requirements for a
    probability distribution. Since the evidence does not depend on θ, the posterior
    distribution is proportional to the product of the likelihood and the prior. Hence,
    MAP estimation chooses the value of θ that maximizes the posterior given the observed
    data and the prior belief, that is, the mode of the posterior.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大后验概率（MAP）**估计利用了证据是一个常数因子，将后验缩放以满足概率分布的要求。由于证据不依赖于θ，后验分布与似然和先验的乘积成比例。因此，MAP估计选择使后验最大化的θ的值，考虑到观察到的数据和先验信念，即后验的模态。'
- en: The MAP approach contrasts with the **maximum likelihood estimation** (**MLE**)
    of parameters, which define a probability distribution. MLE picks the parameter
    value θ that maximizes the likelihood function for the observed training data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MAP方法与**最大似然估计**（**MLE**）不同，MLE定义了概率分布。MLE选择使观察到的训练数据的似然函数最大化的参数值θ。
- en: 'A look at the definitions highlights that MAP differs from MLE by including
    the prior distribution. In other words, unless the prior is a constant, the MAP
    estimate θ will differ from its MLE counterpart:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从定义的角度看，MAP与MLE的不同之处在于包括了先验分布。换句话说，除非先验是一个常数，否则MAP估计θ将与其MLE对应物不同：
- en: '![](img/33df27eb-e965-467f-a807-0403f6c49c92.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33df27eb-e965-467f-a807-0403f6c49c92.png)'
- en: '![](img/27ec9baf-4009-4439-a702-2681bfc8b8e9.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/27ec9baf-4009-4439-a702-2681bfc8b8e9.png)'
- en: The MLE solution tends to reflect the frequentist notion that probability estimates
    should reflect observed ratios. On the other hand, the impact of the prior on
    the MAP estimate often corresponds to adding data that reflects the prior assumptions
    to the MLE. For example, a strong prior that a coin is biased can be incorporated
    in the MLE context by adding skewed trial data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最大似然估计解往往反映了频率主义的概率估计应该反映观察到的比例的概念。另一方面，先验对MAP估计的影响通常相当于将反映先验假设的数据添加到MLE中。例如，一个强烈的先验，即硬币有偏的先验可以通过添加偏斜的试验数据来融入MLE背景。
- en: Prior distributions are a critical ingredient for Bayesian models. We will now
    introduce some convenient choices that facilitate analytical inference.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 先验分布是贝叶斯模型的重要组成部分。我们现在将介绍一些方便的选择，以便进行分析推断。
- en: How to select priors
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何选择先验
- en: The prior should reflect knowledge of the distribution of the parameters because
    it influences the MAP estimate. If a prior is not known with certainty, we need
    to make a choice, often from several reasonable options. In general, it is good
    practice to justify the prior and check for robustness by testing whether alternatives
    lead to the same conclusion.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 先验应反映参数分布的知识，因为它影响MAP估计。如果先验不确定，我们需要进行选择，通常从几个合理的选项中选择。一般来说，证明先验的合理性并通过测试替代是否得出相同结论是一个好的做法。
- en: 'There are several types of priors:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种类型的先验：
- en: '**Objective** priors maximize the impact of the data on the posterior. If the
    parameter distribution is unknown, we can select an uninformative prior like a
    uniform distribution, also called a flat prior, over a relevant range of parameter
    values.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客观**先验最大化数据对后验的影响。如果参数分布未知，我们可以选择一个无信息的先验，比如在参数值的相关范围内称为平坦先验的均匀分布。'
- en: In contrast, **subjective** priors aim to incorporate information that's external
    to the model into the estimate.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，**主观**先验旨在将模型外部的信息纳入估计中。
- en: An **empirical** prior combines Bayesian and frequentist methods and uses historical
    data to eliminate subjectivity, such as by estimating various moments to fit a
    standard distribution.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经验性**先验结合了贝叶斯和频率主义方法，利用历史数据消除主观性，例如通过估计各种时刻以适应标准分布。'
- en: In the context of a machine learning model, the prior can be viewed as a regularizer
    because it limits the values that the posterior can assume. Parameters that have
    zero prior probability, for example, are not part of the posterior distribution.
    Generally, more good data allows for stronger conclusions and reduces the influence
    of the prior.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型的背景下，先验可以被视为一种正则化器，因为它限制了后验可以假设的值。例如，具有零先验概率的参数不是后验分布的一部分。一般来说，更多的好数据可以得出更强的结论并减少先验的影响。
- en: How to keep inference simple – conjugate priors
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何保持推断简单 - 共轭先验
- en: A prior distribution is conjugate with respect to the likelihood when the resulting
    posterior is of the same type of distribution as the prior, except for different
    parameters. When both the prior and the likelihood are normally distributed, then
    the posterior is also normally distributed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当结果后验与先验具有相同类型的分布，只是参数不同时，先验分布与似然的共轭性。当先验和似然都是正态分布时，后验也是正态分布的。
- en: The conjugacy of the prior and likelihood implies a closed-form solution for
    the posterior that facilitates the update process and avoids the need to use numerical
    methods to approximate the posterior. Moreover, the resulting posterior can be
    used as prior for the next update step.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 先验和似然的共轭性暗示了后验的闭合形式解，从而便于更新过程并避免使用数值方法来近似后验。此外，由此产生的后验可以用作下一个更新步骤的先验。
- en: Let's illustrate this process using a binary classification example for stock
    price movements.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个股价波动的二元分类示例来说明这个过程。
- en: How to dynamically estimate the probabilities of asset price moves
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何动态估计资产价格波动的概率
- en: When the data consists of binary Bernoulli random variables with a certain success
    probability for a positive outcome, the number of successes in repeated trials
    follows a Binomial distribution. The conjugate prior is the Beta distribution
    with support over the interval [0, 1] and two shape parameters to model arbitrary
    prior distributions over the success probability. Hence, the posterior distribution
    is also a Beta distribution that we can derive by directly updating the parameters.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据由具有某种成功概率的二元伯努利随机变量组成时，重复试验中的成功次数遵循二项分布。共轭先验是支持区间[0, 1]上的Beta分布，并具有两个形状参数，用于对成功概率进行任意先验分布建模。因此，后验分布也是一个Beta分布，我们可以通过直接更新参数来得到。
- en: We will collect samples of different sizes of binarized daily S&P 500 returns,
    where the positive outcome is a price increase. Starting from an uninformative
    prior that allocates equal probability to each possible success probability in
    the interval [0, 1], we compute the posterior for different evidence samples.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将收集不同大小的二元化日度标准普尔500指数收益率样本，其中正面结果是价格上涨。从一个不含信息的先验开始，该先验将每个可能的成功概率在区间[0, 1]内分配相等的概率，我们计算不同证据样本的后验概率。
- en: 'The following code sample shows that the update consists of simply adding the
    observed numbers of success and failure to the parameters of the prior distribution
    to obtain the posterior:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码示例显示了更新只是简单地将观察到的成功和失败数量添加到先验分布的参数中以获得后验分布：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The resulting posterior distributions are plotted in the following graphs. They
    illustrate the evolution from a uniform prior that views all success probabilities
    as equally likely to an increasingly peaked distribution.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表中绘制了结果后验分布。它们说明了从将所有成功概率视为同等可能的均匀先验到越来越尖峰的分布的演变。
- en: 'After 500 samples, the probability is concentrated near the actual probability
    of a positive move at 54.7% from 2010 to 2017\. It also shows the small differences
    between MLE and MAP estimates, where the latter tends to be pulled slightly toward
    the expected value of the uniform prior, as shown in the following diagram:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 经过500个样本，概率集中在2010年至2017年间正面走势的实际概率约为54.7%。它还显示了MLE和MAP估计之间的小差异，后者倾向于稍微朝向均匀先验的期望值，如下图所示：
- en: '![](img/9875382b-377d-49ee-813a-beaab956a2d4.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9875382b-377d-49ee-813a-beaab956a2d4.png)'
- en: Posterior probabilities
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 后验概率
- en: In practice, the use of conjugate priors is limited to low-dimensional cases.
    In addition, the simplified MAP approach avoids computing the evidence term, but
    has several shortcomings even when it is available; it does not return a distribution
    so that we can derive a measure of uncertainty, or use it as a prior. Hence, we
    need to resort to approximates rather than exact inference using numerical methods
    and stochastic simulation, which we will introduce next.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，共轭先验的使用仅限于低维情况。此外，简化的MAP方法避免了计算证据项，但即使在其可用时也具有几个缺点；它不返回分布，因此我们无法推导出不确定性的度量，或将其用作先验。因此，我们需要采用数值方法和随机模拟而不是精确推理，我们将在下文介绍。
- en: Approximate inference: stochastic versus deterministic approaches
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 近似推理：随机与确定性方法
- en: For most models of practical relevance, it will not be possible to derive the
    exact posterior distribution analytically and compute the expected values for
    the latent parameters. The model may have too many parameters, or the posterior
    distribution may be too complex for an analytical solution. For continuous variables,
    the integrals may not have closed-form solutions, while the dimensionality of
    the space and the complexity of the integrand may prohibit numerical integration.
    For discrete variables, the marginalizations involve summing over all possible
    configurations of the hidden variables, and though this is always possible in
    principle, we often find in practice that there may be exponentially many hidden
    states so that exact calculation is prohibitively expensive.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数实际相关的模型，将无法通过分析方法推导出精确的后验分布并计算潜在参数的期望值。模型可能具有太多的参数，或者后验分布可能对于分析解而言过于复杂。对于连续变量，积分可能没有封闭形式的解，而空间的维数和被积函数的复杂性可能会阻止数值积分。对于离散变量，边缘化涉及对隐藏变量的所有可能配置求和，虽然原则上这总是可能的，但在实践中，我们经常发现可能存在指数多个隐藏状态，因此精确计算是非常昂贵的。
- en: 'Although for some applications the posterior distribution over unobserved parameters
    will be of interest, more often than not it is primarily required to evaluate
    expectations, for example, to make predictions. In such situations, we can rely
    on approximate inference:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic** techniques based on **Markov Chain Monte Carlo (MCMC)** sampling
    have popularized the use of Bayesian methods across many domains. They generally
    have the ability to converge to the exact result. In practice, sampling methods
    can be computationally demanding and are often limited to small-scale problems.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deterministic** methods, known as variational inference or variational Bayes,
    are based on analytical approximations to the posterior distribution and can scale
    well to large applications. They make simplified assumptions, for example, that
    the posterior factorizes in a particular way or it has a specific parametric form
    such as a Gaussian. Hence, they do not generate exact results and can be used
    as complements to sampling methods.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling-based stochastic inference
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sampling is about drawing samples, *X=(x[1], ..., x[n])*, from a given distribution,
    *p(x)*. Assuming the samples are independent, the law of large numbers ensures
    that for a growing number of samples, the fraction of a given instance, *x[i]*,
    in the sample (for the discrete case) corresponds to its probability, *p(x=x[i])*.
    In the continuous case, the analogous reasoning applies to a given region of the
    sample space. Hence, averages over samples can be used as unbiased estimators
    of the expected values of parameters of the distribution.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: A practical challenge consists in ensuring independent sampling because the
    distribution is unknown. Dependent samples may still be unbiased, but tend to
    increase the variance of the estimate so that more samples will be needed for
    an equally precise estimate as for independent samples.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Sampling from a multivariate distribution is computationally demanding as the
    number of states increases exponentially with the number of dimensions. Numerous
    algorithms facilitate the process (see references for an overview). Now, we will
    introduce a few popular variations of MCMC-based methods.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Markov chain Monte Carlo sampling
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Markov chain is a dynamic stochastic model that describes a random walk over
    a set of states, connected by transition probabilities. The Markov property stipulates
    that the process has no memory, and the next step only depends on the current
    state. In other words, it's conditional on the present, past, and future being
    independent, that is, information about past states does not help to predict the
    future beyond what we know from the present.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo methods rely on repeated random sampling to approximate results
    that may be deterministic, but that does not permit an analytic, exact solution.
    It was developed during the Manhattan Project to estimate energy at the atomic
    level and received its enduring code name to ensure secrecy.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Many algorithms apply the Monte Carlo method to a Markov Chain, and generally
    proceed as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 许多算法将蒙特卡洛方法应用于马尔可夫链，并通常按以下方式进行：
- en: Start at the current position.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从当前位置开始。
- en: Draw a new position from a proposal distribution.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从提议分布中抽取一个新的位置。
- en: 'Evaluate the probability of the new position in light of data and prior distributions:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在考虑数据和先验分布的情况下评估新位置的概率：
- en: If sufficiently likely, move to the new position
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果足够可能，移动到新的位置。
- en: Otherwise, remain at the current position
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，保持当前位置不变。
- en: Repeat from step 1.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从步骤1开始重复。
- en: After a given number of iterations, return all accepted positions.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过一定数量的迭代后，返回所有接受的位置。
- en: MCMC aims to identify and explore interesting regions of the posterior that
    concentrate on significant probability density. The memoryless process is said
    to converge when it consistently moves through nearby high probability states
    of the posterior where the acceptance rate increases. A key challenge is to balance
    the need for random exploration of the sample space with the risk of reducing
    the acceptance rate.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: MCMC旨在识别和探索后验的有趣区域，这些区域集中在显著的概率密度上。当它持续移动到后验的附近高概率状态时，无记忆的过程被认为是收敛的，其中接受率增加。一个关键挑战是平衡对样本空间的随机探索的需要和降低接受率的风险。
- en: The initial steps of this process are likely to be more reflective of the starting
    position than the posterior and are typically discarded as **burn-in** samples.
    A key MCMC property is that the process should forget about its initial position
    after a certain (but unknown) number of iterations.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程的初始步骤可能更反映出起始位置而不是后验，并且通常被丢弃为**burn-in**样本。 MCMC的一个关键特性是在一定数量的迭代后，过程应该忘记其初始位置。
- en: The remaining samples are called the trace of the process. Assuming convergence,
    the relative frequency of samples approximates the posterior and can be used to
    compute expected values based on the law of large numbers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的样本被称为过程的轨迹。假设收敛，则样本的相对频率近似于后验，可以根据大数定律计算期望值。
- en: As indicated previously, the precision of the estimate depends on the serial
    correlation of the samples collected by the random walk, each of which, by design,
    depends only on the previous state. Higher correlation limits the effective exploration
    of the posterior and needs to be subjected to diagnostic tests.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所指出的，估计的精度取决于随机游走收集的样本的串行相关性，每个样本设计上仅取决于前一个状态。更高的相关性限制了对后验的有效探索，并需要经过诊断测试。
- en: General techniques to design such a Markov chain include Gibbs sampling, the
    Metropolis-Hastings algorithm, and more recent Hamiltonian MCMC methods that tend
    to perform better.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 设计这样的马尔可夫链的一般技术包括Gibbs采样、Metropolis-Hastings算法和更近期的哈密顿MCMC方法，这些方法往往表现更好。
- en: Gibbs sampling
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gibbs采样
- en: Gibbs sampling simplifies multivariate sampling to a sequence of one-dimensional
    draws. From a starting point, it iteratively holds *n*-1 variables constant while
    sampling the *n^(th)* variable. It incorporates this sample and repeats.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Gibbs采样将多变量采样简化为一系列一维抽样。从一个起始点开始，它迭代地将 *n*-1 个变量保持不变，同时抽样第 *n* 个变量。它将这个样本合并并重复。
- en: The algorithm is very simple and easy to implement but produces highly correlated
    samples that slow down convergence. Its sequential nature also prevents parallelization.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法非常简单易实现，但产生高度相关的样本，导致收敛速度减慢。其顺序性也阻止了并行化。
- en: Metropolis-Hastings sampling
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Metropolis-Hastings采样
- en: The Metropolis-Hastings algorithm randomly proposes new locations based on its
    current state to effectively explore the sample space and reduce the correlation
    of samples relative to Gibbs sampling. To ensure that it samples from the posterior,
    it evaluates the proposal using the product of prior and likelihood, which is
    proportional to the posterior. It accepts with a probability that depends on the
    result, which is relative to the corresponding value for the current sample.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Metropolis-Hastings算法基于其当前状态随机提出新的位置，以有效地探索样本空间并相对于Gibbs采样减少样本的相关性。为了确保它从后验中采样，它使用先验和似然的乘积来评估提议，这与后验成比例。它根据结果接受的概率来接受，这与当前样本的相应值相关。
- en: A key benefit of the proposal evaluation method is that it works with a proportional
    evaluation rather than an exact evaluation of the posterior. However, it can take
    a long time to converge because the random movements that are not related to the
    posterior can reduce the acceptance rate so that a large number of steps produces
    only a small number of (potentially correlated) samples. The acceptance rate can
    be tuned by reducing the variance of the proposal distribution, but the resulting
    smaller steps imply less exploration.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Hamiltonian Monte Carlo – going NUTS
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hamiltonian Monte Carlo (HMC)** is a hybrid method that leverages the first-order
    derivative information of the gradient of the likelihood to propose new states
    for exploration and overcome some of the challenges of MCMC. In addition, it incorporates
    momentum to efficiently jump around the posterior. As a result, it converges faster
    to a high-dimensional target distribution than simpler random-walk Metropolis
    or Gibbs sampling.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: The No-U-Turn sampler is a self-tuning HMC extension that adaptively regulates
    the size and number of moves around the posterior before selecting a proposal.
    It works well on high-dimensional and complex posterior distributions and allows
    many complex models to be fit without specialized knowledge about the fitting
    algorithm itself. As we will see in the next section, it is the default sampler
    in PyMC3.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Variational Inference
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Variational Inference (VI)** is a machine learning method that approximates
    probability densities through optimization. In the Bayesian context, it approximates
    the posterior distribution as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Select a parametrized family of probability distributions
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the member of this family closest to the target, as measured by Kullback-Leibler
    divergence
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compared to MCMC, Variational Bayes tends to converge faster and scales to large
    data better. While MCMC approximates the posterior with samples from the chain
    that will eventually converge arbitrarily close to the target, variational algorithms
    approximate the posterior with the result of the optimization, which is not guaranteed
    to coincide with the target.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Variational Inference is better suited for large datasets and to quickly explore
    many models. In contrast, MCMC will deliver more accurate results on smaller datasets
    or when time and computational resources pose fewer constraints.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Automatic Differentiation Variational Inference (ADVI)
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The downside of Variational Inference is the need for model-specific derivations
    and the implementation of a tailored optimization routine that has slowed down
    widespread adoption.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: The recent **Automatic Differentiation Variational Inference (ADVI)** algorithm
    automates this process so that the user only specifies the model, expressed as
    a program, and ADVI automatically generates a corresponding variational algorithm
    (see references on GitHub for implementation details).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: We will see that PyMC3 supports various Variational Inference techniques, including
    ADVI.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到PyMC3支持各种变分推断技术，包括ADVI。
- en: Probabilistic programming with PyMC3
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyMC3进行概率编程
- en: Probabilistic programming provides a language to describe and fit probability
    distributions so that we can design, encode, and automatically estimate and evaluate
    complex models. It aims to abstract away some of the computational and analytical
    complexity to allow us to focus on the conceptually more straightforward and intuitive
    aspects of Bayesian reasoning and inference.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 概率编程提供了一种描述和拟合概率分布的语言，以便我们可以设计、编码和自动估计和评估复杂模型。它旨在抽象掉一些计算和分析复杂性，以使我们能够专注于贝叶斯推理和推断的概念上更为直观和简单的方面。
- en: The field has become quite dynamic since new languages emerged. Uber open sourced
    Pyro (based on PyTorch) and Google recently added a probability module to TensorFlow
    (see the resources linked on GitHub).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于新语言的出现，该领域变得非常动态。Uber开源了基于PyTorch的Pyro，并且Google最近为TensorFlow添加了一个概率模块（请参阅GitHub上链接的资源）。
- en: As a result, the practical relevance and use of Bayesian methods in machine
    learning will likely increase to generate insights into uncertainty and for use
    cases that require transparent rather than black-box models in particular.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，贝叶斯方法在机器学习中的实际相关性和使用可能会增加，以生成关于不确定性的洞见，特别是对于需要透明而不是黑盒模型的用例。
- en: In this section, we will introduce the popular PyMC3 library, which implements
    advanced MCMC sampling and Variational Inference for machine learning models using
    Python. Together with Stan, named after Stanislaw Ulam, who invented the Monte
    Carlo method, and developed by Andrew Gelman at Columbia University since 2012,
    it is the most popular probabilistic programming language.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍流行的PyMC3库，该库使用Python实现了高级MCMC采样和变分推断，用于机器学习模型。与Stan一起，以Monte Carlo方法的发明者Stanislaw
    Ulam命名，并由哥伦比亚大学的Andrew Gelman自2012年以来开发，它是最受欢迎的概率编程语言之一。
- en: Bayesian machine learning with Theano
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Theano的贝叶斯机器学习
- en: PyMC3 was released in January 2017 to add Hamiltonian MC methods to the Metropolis-Hastings
    sampler that's used in PyMC2 (released in 2012). PyMC3 uses Theano as its computational
    backend for dynamic C compilation and automatic differentiation. Theano is a matrix-focused
    and GPU-enabled optimization library that was developed at Yoshua Bengio's Montreal
    Institute for Learning Algorithms (MILA) and inspired TensorFlow. MILA recently
    ceased to further develop Theano due to the success of newer deep learning libraries
    (see Chapter 16 *Deep Learning* for details). PyMC4, which is planned for 2019,
    will use TensorFlow instead, with presumably limited impact on the API.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: PyMC3于2017年1月发布，以向PyMC2（2012年发布）中使用的Metropolis-Hastings采样器添加Hamiltonian MC方法。PyMC3使用Theano作为其计算后端，用于动态C编译和自动微分。Theano是一个以矩阵为重点且启用GPU的优化库，是由Yoshua
    Bengio的蒙特利尔机器学习算法研究所（MILA）开发的，并受到TensorFlow的启发。由于新的深度学习库的成功（有关详细信息，请参阅第16章《深度学习》），MILA最近停止进一步开发Theano。PyMC4计划于2019年使用TensorFlow，对API的影响可能有限。
- en: The PyMC3 workflow
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyMC3工作流程
- en: 'PyMC3 aims for intuitive and readable, yet powerful syntax that reflects how
    statisticians describe models. The modeling process generally follows these five
    steps:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: PyMC3的目标是直观且可读，但功能强大的语法，反映了统计学家描述模型的方式。建模过程通常遵循以下五个步骤：
- en: 'Encode a probability model by defining the following:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过定义以下内容来编码概率模型：
- en: The prior distributions that quantify knowledge and uncertainty about latent
    variables
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 量化关于潜变量的知识和不确定性的先验分布
- en: The likelihood function that conditions the parameters on observed data
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 条件参数在观察数据上的似然函数
- en: 'Analyze the posterior using one of the options described in the previous section:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一节中描述的选项之一分析后验：
- en: Obtain a point estimate using MAP inference
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MAP推断获得一个点估计
- en: Sample from the posterior using MCMC methods
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MCMC方法从后验中采样
- en: Approximate the posterior using variational Bayes.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用变分贝叶斯近似后验。
- en: Check your model using various diagnostic tools.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用各种诊断工具检查您的模型。
- en: Generate predictions.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成预测。
- en: The resulting model can be used for inference to gain detailed insights into
    parameter values as well as to predict outcomes for new data points.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的模型可用于推断，以获取参数值的详细洞察，以及预测新数据点的结果。
- en: We will illustrate this workflow using simple logistic regression (see the notebook
    bayesian_logistic_regression). Subsequently, we will use PyMC3 to compute and
    compare Bayesian Sharpe ratios, estimate dynamic pairs trading ratios, and implement
    Bayesian linear time series models.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Model definition – Bayesian logistic regression
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in [Chapter 6](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml), *Machine
    Learning Workflow*, logistic regression estimates a linear relationship between
    a set of features and a binary outcome, which is mediated by a sigmoid function
    to ensure that the model produces probabilities. The frequentist approach resulted
    in point estimates for the parameters that measure the influence of each feature
    on the probability that a data point belongs to the positive class, with confidence
    intervals based on assumptions about the parameter distribution.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, Bayesian logistic regression estimates the posterior distribution
    over the parameters itself. The posterior allows for more robust estimates of
    what is called a Bayesian credible interval for each parameter, with the benefit
    of more transparency about the model's uncertainty.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: A probabilistic program consists of observed and unobserved random variables
    (RVs). As we have discussed, we define the observed RVs via likelihood distributions
    and unobserved RVs via prior distributions. PyMC3 includes numerous probability
    distributions for this purpose.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: We will use a simple dataset that classifies 30,000 individuals by income using
    a threshold of $50K per year. This dataset will contain information on age, sex,
    hours worked, and years of education. Hence, we are modeling the probability that
    an individual earns more than $50K using these features.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'The PyMC3 library makes it very straightforward to perform approximate Bayesian
    inference for logistic regression. Logistic regression models the probability
    that individual *i* earns a high income based on *k* features, as outlined on
    the left-hand side of the following diagram:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0cfa2c56-e0a9-4395-9dec-4939160703ac.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'We will use the context manager `with` to define a `manual_logistic_model`
    that we can refer to later as a probabilistic model:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The random variables for the unobserved parameters for intercept and two features
    are expressed using uninformative priors that assume normal distributions with
    a mean of 0 and a standard deviation of 100.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The likelihood combines the parameters with the data according to the specification
    of the logistic regression.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The outcome is modeled as a Bernoulli RV with success probability given by
    the likelihood:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Visualization and plate notation
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `pm.model_to_graphviz(manual_logistic_model)` command produces the plate
    notation displayed in the preceding diagram on the right. It shows the unobserved
    parameters as light and the observed elements as dark circles. The rectangle indicates
    the number of repetitions of the observed model element implied by the data included
    in the model definition.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 命令 `pm.model_to_graphviz(manual_logistic_model)` 生成在右侧图中显示的 plate 符号。它显示未观察到的参数为浅色，观察到的元素为深色圆圈。矩形表示由模型定义中包含的数据暗示的观察模型元素的重复次数。
- en: The Generalized Linear Models module
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广义线性模型模块
- en: 'PyMC3 includes numerous common models so that we can usually leave the manual
    specification for custom applications. The following code defines the same logistic
    regression as a member of the **Generalized Linear Models** (**GLM**) family using
    the formula format inspired by the statistical language R that''s ported to Python
    by the `patsy` library:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: PyMC3 包含许多常见的模型，因此我们通常可以留下自定义应用程序的手动规范。以下代码使用受统计语言 R 启发的公式格式，并由 `patsy` 库移植到
    Python，将相同的逻辑回归定义为 **广义线性模型** (**GLM**) 家族的成员：
- en: '[PRE2]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: MAP inference
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MAP 推断
- en: 'We obtain point MAP estimates for the three parameters using the just defined
    model''s `.find_MAP()` method:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用刚刚定义的模型的 `.find_MAP()` 方法为三个参数获得点 MAP 估计值：
- en: '[PRE3]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: PyMC3 solves the optimization problem of finding the posterior point with the
    highest density using the quasi-Newton **Broyden-Fletcher-Goldfarb-Shanno (BFGS)**
    algorithm, but offers several alternatives, which are provided by the sciPy library.
    The result is virtually identical to the corresponding statsmodels estimate (see
    the notebook for more information).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: PyMC3 使用拟牛顿 **Broyden-Fletcher-Goldfarb-Shanno (BFGS)** 算法解决了找到具有最高密度的后验点的优化问题，但提供了几种替代方案，这些替代方案由
    sciPy 库提供。结果几乎与相应的 statsmodels 估计相同（有关更多信息，请参阅笔记本）。
- en: Approximate inference – MCMC
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 近似推断 - MCMC
- en: 'We will use a slightly more complicated model to illustrate Markov chain Monte
    Carlo inference:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用稍微复杂的模型来说明马尔可夫链蒙特卡洛推断：
- en: '[PRE4]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Patsy's function, `I()`, allows us to use regular Python expressions to create
    new variables on the fly. Here, we square `age` to capture the non-linear relationship
    that more experience adds less income later in life.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Patsy 的函数 `I()` 允许我们使用常规 Python 表达式动态创建新变量。在这里，我们将 `age` 平方以捕获更多经验在生活后期增加收入的非线性关系。
- en: Note that variables measured on very different scales can slow down the sampling
    process. Hence, we first apply sklearn's `scale()` function to standardize the
    `age`, `hours`, and `educ` variables.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，测量尺度非常不同的变量可能会减慢采样过程。因此，我们首先对 `age`、`hours` 和 `educ` 变量进行标准化，应用 sklearn
    的 `scale()` 函数。
- en: Once we have defined our model with the new formula, we are ready to perform
    inference to approximate the posterior distribution. MCMC sampling algorithms
    are available through the `pm.sample()` function.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们用新公式定义了我们的模型，我们就可以执行推断以近似后验分布。通过 `pm.sample()` 函数可用 MCMC 采样算法。
- en: By default, PyMC3 automatically selects the most efficient sampler and initializes
    the sampling process for efficient convergence. For a continuous model, PyMC3
    chooses the NUTS sampler that we discussed in the previous section. It also runs
    variational inference via ADVI to find good starting parameters for the sampler.
    One among several alternatives is to use the MAP estimate.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，PyMC3 自动选择最有效的采样器，并初始化采样过程以实现有效的收敛。对于连续模型，PyMC3 选择我们在前一节中讨论的 NUTS 采样器。它还通过
    ADVI 运行变分推断，以找到采样器的良好起始参数。其中一个选择是使用 MAP 估计。
- en: 'To see what convergence looks like, we first draw only `100` samples after
    tuning the sampler for `1000` iterations. This will be discarded afterwards. The
    sampling process can be parallelized for multiple chains using the `cores` argument
    (except when using GPU):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看收敛情况，我们首先在调整了采样器 `1000` 次迭代后仅绘制 `100` 个样本。此后将丢弃这些样本。采样过程可以使用 `cores` 参数并行化多个链（除非使用
    GPU）：
- en: '[PRE5]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The resulting trace contains the sampled values for each random variable. We
    can continue sampling by providing the trace of a prior run as input (see the
    notebook for more information).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 结果跟踪包含每个随机变量的采样值。我们可以通过提供先前运行的跟踪作为输入来继续采样（有关更多信息，请参阅笔记本）。
- en: Credible intervals
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 置信区间
- en: We can compute the credible intervals—the Bayesian counterpart of confidence
    intervals—as percentiles of the trace. The resulting boundaries reflect confidence
    about the range of the parameter value for a given probability threshold, as opposed
    to the number of times the parameter will be within this range for a large number
    of trials. The notebook illustrates computation and visualization.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Approximate inference – variational Bayes
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The interface for variational inference is very similar to the MCMC implementation.
    We just use the `fit()` function instead of the `sample()` function, with the
    option to include an early stopping `CheckParametersConvergence` callback if the
    distribution-fitting process converged up to a given tolerance:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can draw samples from the approximated distribution to obtain a trace object
    like we did previously for the MCMC sampler:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Inspection of the trace summary shows that the results are slightly less accurate.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Model diagnostics
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayesian model diagnostics includes validating that the sampling process has
    converged and consistently samples from high probability areas of the posterior,
    and confirming that the model represents the data well.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Convergence
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can visualize the samples over time and their distributions to check the
    quality of the results. The following charts show the posterior distributions
    after an initial 100 and an additional 100,000 samples, respectively, and illustrate
    how convergence implies that multiple chains identify the same distribution. The
    `pm.trace_plot()` function shows the evolution of the samples as well (see the
    notebook for more information):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0a30aea-9601-4882-b21c-b5463825d26d.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: Posterior distributions
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'PyMC3 produces various summary statistics for a sampler. These are available
    as individual functions in the stats module, or by providing a trace to the `pm.summary()`
    function:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **statsmodels** | **mean** | **sd** | **hpd_2.5** | **hpd_97.5** | **n_eff**
    | **Rhat** |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| Intercept | -1.97 | -1.97 | 0.04 | -2.04 | -1.89 | 69,492.17 | 1.00 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| sex[T. Male] | 1.20 | 1.20 | 0.04 | 1.12 | 1.28 | 72,374.10 | 1.00 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| age | 1.10 | 1.10 | 0.03 | 1.05 | 1.15 | 68,446.73 | 1.00 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| I(age ** 2) | -0.54 | -0.54 | 0.02 | -0.58 | -0.50 | 66,539.66 | 1.00 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| hours | 0.32 | 0.32 | 0.02 | 0.28 | 0.35 | 93,008.86 | 1.00 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| educ | 0.84 | 0.84 | 0.02 | 0.80 | 0.87 | 98,125.26 | 1.00 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: The preceding tables includes the (separately computed) statsmodels `logit`
    coefficients in the first column to show that, in this simple case, both models
    agree because the sample mean is very close to the coefficients.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: The remaining columns contain the **highest posterior density** (**HPD**) estimate
    for the minimum width credible interval, the Bayesian version of a confidence
    interval, which here is computed at the 95% level. The `n_eff` statistic summarizes
    the number of effective (not rejected) samples resulting from the ~100K draws.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: R-hat, also known as the Gelman-Rubin statistic, checks convergence by comparing
    the variance between chains to the variance within each chain. If the sampler
    converged, these variances should be identical, that is, the chains should look
    similar. Hence, the statistic should be near 1\. The `pm.forest_plot()` function
    also summarizes this statistic for the multiple chains (see the notebook for more
    information).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: R-hat，也称为 Gelman-Rubin 统计量，通过比较链之间的方差与每个链内的方差来检查收敛性。如果采样器收敛，则这些方差应该相同，即链应该看起来相似。因此，统计量应该接近1。`pm.forest_plot()`
    函数还为多个链总结了此统计量（有关更多信息，请参见笔记本）。
- en: 'For high-dimensional models with many variables, it becomes cumbersome to inspect
    numerous traces. When using NUTS, the energy plot helps to assess problems of
    convergence. It summarizes how efficiently the random process explores the posterior.
    The plot shows the energy and the energy transition matrix, which should be well-matched,
    as in the following example (see references for conceptual detail):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有许多变量的高维模型，检查大量轨迹变得繁琐。使用 NUTS 时，能量图有助于评估收敛问题。它总结了随机过程如何有效地探索后验分布。图表显示了能量和能量转移矩阵，它们应该是相匹配的，如下面的示例所示（有关概念细节，请参见参考资料）：
- en: '![](img/942e20a0-7f47-4a68-bec8-b9c441eb2998.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/942e20a0-7f47-4a68-bec8-b9c441eb2998.png)'
- en: Posterior Predictive Checks
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后验预测检查
- en: '**Posterior Predictive Checks** (**PPCs**) are very useful for examining how
    well a model fits the data. They do so by generating data from the model using
    parameters from draws from the posterior. We use the `pm.sample_ppc` function
    for this purpose and obtain *n* samples for each observation (the GLM module automatically
    names the outcome `''y''`):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**后验预测检查**（**PPCs**）非常有用，用于检查模型与数据的拟合程度。它们通过使用来自后验分布的参数生成模型数据来实现此目的。我们使用 `pm.sample_ppc`
    函数进行此操作，并为每个观测值获取 *n* 个样本（GLM 模块自动将结果命名为 `''y''`）：'
- en: '[PRE8]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can evaluate the in-sample fit using the auc score, for example, to compare
    different models:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 auc 分数来评估样本内拟合，例如，比较不同模型：
- en: '[PRE9]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Prediction
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测
- en: 'Predictions use Theano''s shared variables to replace the training data with
    test data before running posterior predictive checks. To facilitate visualization,
    we create a variable with a single predictor hours, create the train and test
    datasets, and convert the former to a shared variable. Note that we need to use
    numPy arrays and provide a list of column labels (see the notebook for details):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行后验预测检查之前，预测使用 Theano 的共享变量将训练数据替换为测试数据。为了方便可视化，我们创建一个带有单个预测器小时的变量，创建训练和测试数据集，并将前者转换为共享变量。请注意，我们需要使用
    numPy 数组并提供列标签列表（有关详细信息，请参见笔记本）：
- en: '[PRE10]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We then run the sampler as before, and apply the `pm.sample_ppc` function to
    the resulting trace after replacing the train with test data:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们像之前一样运行采样器，并在用测试数据替换训练数据后对结果的迹线应用 `pm.sample_ppc` 函数：
- en: '[PRE11]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The AUC score for this model with a single feature is 0.65\. The following
    plot shows the actual outcomes and uncertainty surrounding the predictions for
    each sampled predictor value:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 单特征模型的AUC分数为0.65。以下图表显示了每个采样预测器值的实际结果和预测周围的不确定性：
- en: '![](img/fdbd2eb0-564b-4cfa-a6db-79e5f0ff5838.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fdbd2eb0-564b-4cfa-a6db-79e5f0ff5838.png)'
- en: We will now illustrate how to apply Bayesian analysis to trading-related use
    cases.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将说明如何将贝叶斯分析应用于与交易相关的用例。
- en: Practical applications
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际应用
- en: There are numerous applications to Bayesian machine learning methods to investment.
    The transparency that probabilistic estimates create are naturally useful for
    risk management and performance evaluation. We will illustrate the computation
    and comparison of a metric like the Sharpe ratio. The GitHub repository also includes
    two notebooks referenced below that present the use of Bayesian ML for modeling
    linear time series and stochastic volatility.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯机器学习方法在投资领域有许多应用。概率估计产生的透明度对风险管理和绩效评估自然非常有用。我们将说明如何计算和比较诸如夏普比率之类的指标。GitHub
    仓库还包括下面引用的两个笔记本，展示了将贝叶斯 ML 用于建模线性时间序列和随机波动性的用法。
- en: These notebooks have been adapted from tutorials created at Quantopian where
    Thomas Wiecki leads data science and has significantly contributed to popularizing
    the use of Bayesian methods. The references also include a tutorial on using Bayesian
    ML to estimate pairs trading hedging ratios.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这些笔记本已经改编自 Quantopian 上创建的教程，Thomas Wiecki 领导数据科学，并且在推广贝叶斯方法的使用方面做出了重大贡献。参考资料还包括有关使用贝叶斯
    ML 估计配对交易套期保值比率的教程。
- en: Bayesian Sharpe ratio and performance comparison
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯夏普比率和表现比较
- en: In this section, we will illustrate how to define the Sharpe ratio as a probability
    model and compare the resulting posterior distributions for different return series.
    The Bayesian estimation for two groups provides complete distributions of credible
    values for the effect size, group means and their difference, standard deviations
    and their difference, and the normality of the data.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将说明如何将夏普比率定义为概率模型，并比较不同收益序列的后验分布。对两组的贝叶斯估计提供了完整的可信值分布，包括效应大小、组均值及其差异、标准差及其差异以及数据的正态性。
- en: Key use cases include the analysis of differences between alternative strategies,
    or between a strategy's in-sample return in relation to its out-of-sample return
    (see the `bayesian_sharpe_ratio` notebook for details). The Bayesian Sharpe ratio
    is also part of pyfolio's Bayesian tearsheet.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 主要用例包括分析替代策略之间的差异，或者分析策略的样本内收益与样本外收益之间的差异（详见`bayesian_sharpe_ratio`笔记本）。贝叶斯夏普比率也是pyfolio贝叶斯分析表的一部分。
- en: Model definition
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型定义
- en: To model the Sharpe ratio as a probabilistic model, we need the priors about
    the distribution of returns and the parameters that govern this distribution.
    The student t distribution exhibits fat tails that are relative to the normal
    distribution for low **degrees of freedom** (**df**), and is a reasonable choice
    to capture this aspect of returns.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将夏普比率建模为概率模型，我们需要关于收益分布和控制此分布的参数的先验。学生t分布相对于低**自由度**（**df**）的正态分布具有较厚的尾部，是捕捉收益这一方面的合理选择。
- en: 'Hence, we need to model the three parameters of this distribution, namely the
    mean and standard deviation of returns, and the degrees of freedom. We''ll assume
    normal and uniform distributions for the mean and the standard deviation, respectively,
    and an exponential distribution for the df with a sufficiently low expected value
    to ensure fat tails. Returns are based on these probabilistic inputs, and the
    annualized Sharpe ratio results from the standard computation, ignoring a risk-free
    rate (using daily returns):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要对这个分布的三个参数进行建模，即收益的均值和标准差，以及自由度。我们假设均值和标准差分别服从正态和均匀分布，并且自由度服从具有足够低期望值的指数分布，以确保有厚尾。收益基于这些概率输入，并且年化夏普比率是通过标准计算得出的，忽略了无风险利率（使用每日收益）：
- en: '[PRE12]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The notebook contains details on sampling and evaluating the Sharpe ratio for
    a single stock.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本包含有关对单个股票进行采样和评估夏普比率的详细信息。
- en: Performance comparison
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表现比较
- en: 'To compare the performance of two return series, we model each group''s Sharpe
    ratio separately and compute the effect size as the difference between the volatility-adjusted
    returns. Visualizing the traces reveals granular performance insights into the
    distributions of each metric, as illustrated by the following chart:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较两个收益序列的表现，我们分别对每个组的夏普比率建模，并将效应大小计算为波动率调整后收益之间的差异。通过可视化轨迹，可以深入了解每个指标的分布情况，如下图所示：
- en: '![](img/913b974b-938b-49b9-a005-f2b140943d2a.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/913b974b-938b-49b9-a005-f2b140943d2a.png)'
- en: Bayesian Linear Regression for Pairs Trading
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 用于配对交易的贝叶斯线性回归
- en: In the last chapter, we introduced pairs trading as a popular algorithmic trading
    strategy that relies on the cointegration of two or more assets. Given such assets,
    we need to estimate the hedging ratio to decide on the relative magnitude of long
    and short positions. A basic approach uses linear regression.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一章中，我们介绍了配对交易作为一种流行的算法交易策略，它依赖于两个或更多资产的协整性。给定这样的资产，我们需要估计对冲比率以决定多头和空头仓位的相对大小。基本方法使用线性回归。
- en: The `linear_regression` notebook illustrates how Bayesian linear regression
    tracks changes in the relationship between two assets over time.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`linear_regression`笔记本说明了贝叶斯线性回归如何跟踪两个资产之间随时间变化的关系。'
- en: Bayesian time series models
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯时间序列模型
- en: PyMC3 includes AR(p) models that allow us to gain similar insights into the
    parameter uncertainty, as for the previous models. The `bayesian_time_series`
    notebook illustrates a time series model for one or more lags.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: PyMC3包括允许我们对参数不确定性进行类似洞察的AR(p)模型，与先前的模型相同。`bayesian_time_series`笔记本说明了一个或多个滞后的时间序列模型。
- en: Stochastic volatility models
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机波动模型
- en: As discussed in the last chapter, asset prices have time-varying volatility.
    In some periods, returns are highly variable, while in others, they are very stable.
    Stochastic volatility models model this with a latent volatility variable, which
    is modeled as a stochastic process. The No-U-Turn sampler was introduced using
    such a model, and the `stochastic_volatility` notebook illustrates this use case.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上一章所讨论的，资产价格具有时变波动性。在某些时期，回报变动很大，而在其他时期则非常稳定。随机波动模型使用潜在波动性变量来建模，该变量被建模为随机过程。无
    U 转折采样器是使用这种模型引入的，并且`stochastic_volatility`笔记本展示了这种用法。
- en: Summary
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored Bayesian approaches to machine learning. We saw
    that they have several advantages, including the ability to encode prior knowledge
    or opinions, deeper insights into the uncertainty surrounding model estimates
    and predictions, and the suitability for online learning, where each training
    sample incrementally impacts the model's prediction.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了机器学习的贝叶斯方法。我们发现它们具有几个优点，包括能够编码先验知识或观点、更深入地了解模型估计和预测周围的不确定性，以及适用于在线学习，在这种情况下，每个训练样本逐渐影响模型的预测。
- en: We learned to apply the Bayesian workflow from model specification to estimation,
    diagnostics, and prediction using PyMC3 and explored several relevant applications.
    We will encounter more Bayesian models in [Chapter 14](beb6fa08-c790-47d5-82ef-f48a81dcf3d1.xhtml), *Topic
    Modeling* and in Chapter 19 on unsupervised deep learning where we will introduce
    variational autoencoders.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学会了从模型规范到估计、诊断和预测应用贝叶斯工作流程，使用 PyMC3 并探索了几个相关应用。我们将在[第 14 章](beb6fa08-c790-47d5-82ef-f48a81dcf3d1.xhtml)中遇到更多贝叶斯模型，*主题建模*，以及在第
    19 章中介绍无监督深度学习，在那里我们将介绍变分自动编码器。
- en: The next two chapter introduce tree-based, non-linear ensemble models, namely
    random forests and gradient boosting machines.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两章介绍基于树的、非线性的集成模型，即随机森林和梯度提升机。
