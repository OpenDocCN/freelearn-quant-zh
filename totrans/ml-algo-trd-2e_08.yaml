- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ML4T Workflow – From Model to Strategy Backtesting
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, it's time to **integrate the various building blocks** of the **machine
    learning for trading** (**ML4T**) workflow that we have so far discussed separately.
    The goal of this chapter is to present an end-to-end perspective of the process
    of designing, simulating, and evaluating a trading strategy driven by an ML algorithm.
    To this end, we will demonstrate in more detail how to backtest an ML-driven strategy
    in a historical market context using the Python libraries backtrader and Zipline.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The **ultimate objective of the ML4T workflow** is to gather evidence from
    historical data. This helps us decide whether to deploy a candidate strategy in
    a live market and put financial resources at risk. This process builds on the
    skills you developed in the previous chapters because it relies on your ability
    to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Work with a diverse set of data sources to engineer informative factors
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design ML models that generate predictive signals to inform your trading strategy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize the resulting portfolio from a risk-return perspective
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A realistic simulation of your strategy also needs to faithfully represent how
    security markets operate and how trades are executed. Therefore, the institutional
    details of exchanges, such as which order types are available and how prices are
    determined, also matter when you design a backtest or evaluate whether a backtesting
    engine includes the requisite features for accurate performance measurements.
    Finally, there are several methodological aspects that require attention to avoid
    biased results and false discoveries that will lead to poor investment decisions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, after working through this chapter, you will be able to:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Plan and implement end-to-end strategy backtesting
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand and avoid critical pitfalls when implementing backtests
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discuss the advantages and disadvantages of vectorized versus event-driven backtesting
    engines
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify and evaluate the key components of an event-driven backtester
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design and execute the ML4T workflow using data sources at both minute and daily
    frequencies, with ML models trained separately or as part of the backtest
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Zipline and backtrader
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: How to backtest an ML-driven strategy
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a nutshell, the ML4T workflow, illustrated in *Figure 8.1*, is about backtesting
    a trading strategy that leverages machine learning to generate trading signals,
    select and size positions, or optimize the execution of trades. It involves the
    following steps, with a specific investment universe and horizon in mind:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Source and prepare market, fundamental, and alternative data
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Engineer predictive alpha factors and features
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Design, tune, and evaluate ML models to generate trading signals
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide on trades based on these signals, for example, by applying rules
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Size individual positions in the portfolio context
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simulate the resulting trades triggered using historical market data
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate how the resulting positions would have performed
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B15439_08_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: The ML4T workflow'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: When we discussed the ML process in *Chapter 6*, *The Machine Learning Process*,
    we emphasized that the model's learning should generalize well to new applications.
    In other words, the predictions of an ML model trained on a given set of data
    should perform equally well when provided new input data. Similarly, the (relative)
    **backtest performance of a strategy should be indicative of future market performance**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Before we take a look at how backtesting engines run historical simulations,
    we need to review several methodological challenges. Failing to properly address
    them will render results unreliable and lead to poor decisions about the strategy's
    live implementation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Backtesting pitfalls and how to avoid them
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Backtesting simulates an algorithmic strategy based on historical data, with
    the goal of producing performance results that generalize to new market conditions.
    In addition to the generic uncertainty around predictions in the context of ever-changing
    markets, several implementation aspects can bias the results and increase the
    risk of mistaking in-sample performance for patterns that will hold out-of-sample.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: These aspects are under our control and include the selection and preparation
    of data, unrealistic assumptions about the trading environment, and the flawed
    application and interpretation of statistical tests. The risks of false backtest
    discoveries multiply with increasing computing power, bigger datasets, and more
    complex algorithms that facilitate the misidentification of apparent signals in
    a noisy sample.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will outline the most serious and common methodological
    mistakes. Please refer to the literature on multiple testing for further detail,
    in particular, a series of articles by Marcos Lopez de Prado collected in *Advances
    in Financial Machine Learning (2018)*. We will also introduce the deflated **Sharpe
    ratio** (**SR**), which illustrates how to adjust metrics that result from repeated
    trials when using the same set of financial data for your analysis.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data right
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data issues that undermine the validity of a backtest include **look-ahead bias**,
    **survivorship bias**, **outlier control**, as well as the **selection of the
    sample period**. We will address each of these in turn.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Look-ahead bias – use only point-in-time data
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the heart of an algorithmic strategy are trading rules that trigger actions
    based on data. Look-ahead bias emerges when we develop or evaluate trading rules
    **using historical information before it was known or available**. The resulting
    performance measures will be misleading and not representative of the future when
    data availability differs during live strategy execution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: A common cause of this bias is the failure to account for corrections or restatements
    of reported financials after their initial publication. Stock splits or reverse
    splits can also generate look-ahead bias. For example, when computing the earnings
    yield, **earnings-per-share** (**EPS**) data is usually reported on a quarterly
    basis, whereas market prices are available at a much higher frequency. Therefore,
    adjusted EPS and price data need to be synchronized, taking into account when
    the available data was, in fact, released to market participants.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: The **solution** involves the careful validation of the timestamps of all data
    that enters a backtest. We need to guarantee that conclusions are based only on
    point-in-time data that does not inadvertently include information from the future.
    High-quality data providers ensure that these criteria are met. When point-in-time
    data is not available, we need to make (conservative) assumptions about the lag
    in reporting.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Survivorship bias – track your historical universe
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Survivorship bias arises when the backtest data contains only securities that
    are currently active while **omitting assets that have disappeared** over time,
    due to, for example, bankruptcy, delisting, or acquisition. Securities that are
    no longer part of the investment universe often did not perform well, and failing
    to include these cases positively skew the backtest result.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The **solution**, naturally, is to verify that datasets include all securities
    available over time, as opposed to only those that are still available when running
    the test. In a way, this is another way of ensuring the data is truly point-in-time.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Outlier control – do not exclude realistic extremes
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data preparation typically includes some treatment of outliers such as winsorizing,
    or clipping, extreme values. The challenge is to **identify outliers that are
    truly not representative** of the period under analysis, as opposed to any extreme
    values that are an integral part of the market environment at that time. Many
    market models assume normally distributed data when extreme values are observed
    more frequently, as suggested by fat-tailed distributions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: The **solution** involves a careful analysis of outliers with respect to the
    probability of extreme values occurring and adjusting the strategy parameters
    to this reality.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Sample period – try to represent relevant future scenarios
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A backtest will not yield representative results that generalize to the future
    if the sample data does not **reflect the current (and likely future) environment**.
    A poorly chosen sample data might lack relevant market regime aspects, for example,
    in terms of volatility or volumes, fail to include enough data points, or contain
    too many or too few extreme historical events.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The **solution** involves using sample periods that include important market
    phenomena or generating synthetic data that reflects the relevant market characteristics.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Getting the simulation right
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Practical issues related to the implementation of the historical simulation
    include:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Failure to **mark to market** to accurately reflect market prices and account
    for drawdowns
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unrealistic assumptions** about the availability, cost, or market impact
    of trades'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorrect **timing of signals and trade execution**
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see how to identify and address each of these issues.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Mark-to-market performance – track risks over time
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A strategy needs to **meet investment objectives and constraints at all times**.
    If it performs well over the course of the backtest but leads to unacceptable
    losses or volatility over time, this will (obviously) not be practical. Portfolio
    managers need to track and report the value of their positions, called mark to
    market, on a regular basis and possibly in real time.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: The solution involves plotting performance over time or calculating (rolling)
    risk metrics, such as the **value at risk** (**VaR**) or the Sortino ratio.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Transaction costs – assume a realistic trading environment
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Markets do not permit the execution of all trades at all times or at the targeted
    price. A backtest that assumes **trades that may not actually be available** or
    would have occurred at less favorable terms will produce biased results.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Practical shortcomings include a strategy that assumes short sales when there
    may be no counterparty, or one that underestimates the market impact of trades
    (slippage) that are large or deal in less liquid assets, or the costs that arise
    due to broker fees.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The **solution** includes a limitation to a liquid universe and/or realistic
    parameter assumptions for trading and slippage costs. This also safeguards against
    misleading conclusions from unstable factor signals that decay fast and produce
    a high portfolio turnover.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Timing of decisions – properly sequence signals and trades
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to look-ahead bias, the simulation could make **unrealistic assumptions
    about when it receives and trades on signals**. For instance, signals may be computed
    from close prices when trades are only available at the next open, with possibly
    quite different prices. When we evaluate performance using the close price, the
    backtest results will not represent realistic future outcomes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The **solution** involves careful orchestration of the sequence of signal arrival,
    trade execution, and performance evaluation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Getting the statistics right
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most prominent challenge when backtesting validity, including published
    results, is the discovery of spurious patterns due to multiple testing. Selecting
    a strategy based on the tests of different candidates on the same data will bias
    the choice. This is because a positive outcome is more likely caused by the stochastic
    nature of the performance measure itself. In other words, the strategy overfits
    the test sample, producing deceptively positive results that are unlikely to generalize
    to future data that's encountered during live trading.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Hence, backtest performance is only informative if the number of trials is reported
    to allow for an assessment of the risk of selection bias. This is rarely the case
    in practical or academic research, inviting doubts about the validity of many
    published claims.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the risk of backtest overfitting does not only arise from running
    numerous tests but also affects strategies designed based on prior knowledge of
    what works and doesn't. Since the risks include the knowledge of backtests run
    by others on the same data, backtest-overfitting is very hard to avoid in practice.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Proposed **solutions** include prioritizing tests that can be justified using
    investment or economic theory, rather than arbitrary data-mining efforts. It also
    implies testing in a variety of contexts and scenarios, including possibly on
    synthetic data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The minimum backtest length and the deflated SR
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Marcos Lopez de Prado ([http://www.quantresearch.info/](http://www.quantresearch.info/))
    has published extensively on the risks of backtesting and how to detect or avoid
    it. This includes an online simulator of backtest-overfitting ([http://datagrid.lbl.gov/backtest/](http://datagrid.lbl.gov/backtest/),
    *Bailey, et al. 2015*).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Another result includes an estimate of the minimum length of the backtest period
    that an investor should require to avoid selecting a strategy that achieves a
    certain SR for a given number of in-sample trials, but has an expected out-of-sample
    SR of zero. The result implies that, for example, 2 years of daily backtesting
    data does not support conclusions about more than seven strategies. 5 years of
    data expands this number to 45 strategy variations. See *Bailey, Borwein, and
    Prado (2016)* for implementation details.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '*Bailey and Prado (2014)* also derived a deflated SR to compute the probability
    that the SR is statistically significant while controlling for the inflationary
    effect of multiple testing, non-normal returns, and shorter sample lengths. (See
    the `multiple_testing` subdirectory for the Python implementation of `deflated_sharpe_ratio.py`
    and references for the derivation of the related formulas.)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Optimal stopping for backtests
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to limiting backtests to strategies that can be justified on theoretical
    grounds as opposed to mere data-mining exercises, an important question is when
    to stop running additional tests.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: One way to answer this question relies on the solution to the **secretary problem**
    from the optimal stopping theory. This problem assumes we are selecting an applicant
    based on interview results and need to decide whether to hold an additional interview
    or choose the most recent candidate. In this context, the optimal rule is to always
    reject the first *n*/*e* candidates and then select the first candidate that surpasses
    all the previous options. Using this rule results in a 1/*e* probability of selecting
    the best candidate, irrespective of the size *n* of the candidate pool.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'Translating this rule directly to the backtest context produces the following
    **recommendation**: test a random sample of 1/*e* (roughly 37 percent) of reasonable
    strategies and record their performance. Then, continue with the tests until a
    strategy outperforms those tested before. This rule applies to tests of several
    alternatives, with the goal of choosing a near-best as soon as possible while
    minimizing the risk of a false positive. See the resources listed on GitHub for
    additional information.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: How a backtesting engine works
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Put simply, a backtesting engine iterates over historical prices (and other
    data), passes the current values to your algorithm, receives orders in return,
    and keeps track of the resulting positions and their value.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: In practice, there are numerous requirements for creating a realistic and robust
    simulation of the ML4T workflow that was depicted in *Figure 8.1* at the beginning
    of this chapter. The difference between vectorized and event-driven approaches
    illustrates how the faithful reproduction of the actual trading environment adds
    significant complexity.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized versus event-driven backtesting
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A vectorized backtest is the most basic way to evaluate a strategy. It simply
    multiplies a signal vector that represents the target position size with a vector
    of returns for the investment horizon to compute the period performance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Let's illustrate the vectorized approach using the daily return predictions
    that we created using ridge regression in the previous chapter. Using a few simple
    technical factors, we predicted the returns for the next day for the 100 stocks
    with the highest recent dollar trading volume (see *Chapter 7*, *Linear Models
    – From Risk Factors to Return Forecasts*, for details).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll transform the predictions into signals for a very simple strategy: on
    any given trading day, we will go long on the 10 highest positive predictions
    and go short on the lowest 10 negative predictions. If there are fewer positive
    or negative predictions, we''ll hold fewer long or short positions. The notebook
    `vectorized_backtest` contains the following code example, and the script `data.py`
    creates the input data stored in `backtest.h5`.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load the data for our strategy, as well as S&P 500 prices (which
    we convert into daily returns) to benchmark the performance:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The data combines daily return predictions and OHLCV market data for 253 distinct
    stocks over the 2014-17 period, with 100 equities for each day. Now, we can compute
    the daily forward returns and convert these and the predictions into wide format,
    with one ticker per column:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The next step is to select positive and negative predictions, rank them in
    descending and ascending fashion, and create long and short signals using an integer
    mask that identifies the top 10 on each side with identifies the predictions outside
    the top 10 with a one, and a zero:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can then multiply the binary DataFrames with the forward returns (using
    their negative inverse for the shorts) to get the daily performance of each position,
    assuming equal-sized investments. The daily average of these returns corresponds
    to the performance of equal-weighted long and short portfolios, and the sum reflects
    the overall return of a market-neutral long-short strategy:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When we compare the results, as shown in *Figure 8.2*, our strategy performed
    well compared to the S&P 500 for the first 2 years of the period – that is, until
    the benchmark catches up and our strategy underperforms during 2017.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy returns are also less volatile with a standard deviation of 0.002
    compared to 0.008 for the S&P 500; the correlation is low and negative at -0.093:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_02.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Vectorized backtest results'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'While this approach permits a quick back-of-the-envelope evaluation, it misses
    important features of a robust, realistic, and user-friendly backtest engine;
    for example:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: We need to manually align the timestamps of predictions and returns (using pandas' built-in
    capabilities) and do not have any safeguards against inadvertent look-ahead bias.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no explicit position sizing and representation of the trading process
    that accounts for costs and other market realities, or an accounting system that
    tracks positions and their performance.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is also no performance measurement other than what we compute after the
    fact, and risk management rules like stop-loss are difficult to simulate.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That's where event-driven backtesting comes in. An event-driven backtesting
    engine explicitly simulates the time dimension of the trading environment and
    imposes significantly more structure on the simulation. This includes the use
    of historical calendars that define when trades can be made and when quotes are
    available. The enforcement of timestamps also helps to avoid look-ahead bias and
    other implementation errors mentioned in the previous section (but there is no
    guarantee).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Generally, event-driven systems aim to capture the actions and constraints encountered
    by a strategy more closely and, ideally, can readily be converted into a live
    trading engine that submits actual orders.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Key implementation aspects
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The requirements for a realistic simulation may be met by a **single platform**
    that supports all steps of the process in an end-to-end fashion, or by **multiple
    tools** that each specialize in different aspects.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: For instance, you could handle the design and testing of ML models that generate
    signals using generic ML libraries like scikit-learn, or others that we will encounter
    in this book, and feed the model outputs into a separate backtesting engine. Alternatively,
    you could run the entire ML4T workflow end-to-end on a single platform like Quantopian
    or QuantConnect.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: The following sections highlight key items and implementation details that need
    to be addressed to put this process into action.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion – format, frequency, and timing
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step in the process concerns the sources of data. Traditionally, algorithmic
    trading strategies focused on market data, namely the OHLCV price and volume data
    that we discussed in *Chapter 2*, *Market and Fundamental Data – Sources and Techniques*.
    Today, data sources are more diverse and raise the question of how many different
    **storage formats and data types** to support, and whether to use a proprietary
    or custom format or rely on third-party or open source formats.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect is the **frequency of data sources** that can be used and whether
    sources at different frequencies can be combined. Common options in increasing
    order of computational complexity and memory and storage requirements include
    daily, minute, and tick frequency. Intermediate frequencies are also possible.
    Algorithmic strategies tend to perform better at higher frequencies, even though
    quantamental investors are gaining ground, as discussed in *Chapter 1*, *Machine
    Learning for Trading – From Idea to Execution*. Regardless, institutional investors
    will certainly require tick frequency.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Finally, data ingestion should also address **point-in-time constraints** to
    avoid look-ahead bias, as outlined in the previous section. The use of trading
    calendars helps limit data to legitimate dates and times; adjustments to reflect
    corporate actions like stock splits and dividends or restatements that impact
    prices revealed at specific times need to be made prior to ingestion.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Factor engineering – built-in factors versus libraries
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To facilitate the engineering of alpha factors for use in ML models, many backtesting
    engines include computational tools suitable for numerous standard transformations
    like moving averages and various technical indicators. A key advantage of **built-in
    factor engineering** is the easy conversion of the backtesting pipeline into a
    live trading engine that applies the same computations to the input data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: The **numerical Python libraries** (pandas, NumPy, TA-Lib) presented in *Chapter
    4*, *Financial Feature Engineering – How to Research Alpha Factors*, are an alternative
    to **pre-compute factors**. This can be efficient when the goal is to reuse factors
    in various backtests that amortize the computational cost.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: ML models, predictions, and signals
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned earlier, the ML workflow discussed in *Chapter 6*, *The Machine
    Learning Process*, can be embedded in an end-to-end platform that integrates the
    model design and evaluation part into the backtesting process. While convenient,
    this is also costly because model training becomes part of the backtest when the
    goal is perhaps to fine-tune trading rules.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Similar to factor engineering, you can decouple these aspects and design, train,
    and evaluate ML models using generic libraries for this purpose, and also provide
    the relevant predictions as inputs to the backtester. We will mostly use this
    approach in this book because it makes the exposition more concise and less repetitive.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Trading rules and execution
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A realistic strategy simulation requires a faithful representation of the trading
    environment. This includes access to relevant exchanges, the availability of the
    various order types discussed in *Chapter 2*, *Market and Fundamental Data – Sources
    and Techniques*, and the accounting for transaction costs. Costs include broker
    commissions, bid-ask spreads, and slippage, giving us the difference between the
    target execution price and the price that's eventually obtained. It is also important
    to ensure trades execute with delays that reflect liquidity and operating hours.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Performance evaluation
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, a backtesting platform needs to facilitate performance evaluation.
    It can provide standard metrics derived from its accounting of transactions, or
    provide an output of the metrics that can be used with a library like **pyfolio**
    that's suitable for this purpose.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In the next two sections, we will explore two of the most popular backtesting
    libraries, namely backtrader and Zipline.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: backtrader – a flexible tool for local backtests
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**backtrader** is a popular, flexible, and user-friendly Python library for
    local backtests with great documentation, developed since 2015 by Daniel Rodriguez.
    In addition to a large and active community of individual traders, there are several
    banks and trading houses that use backtrader to prototype and test new strategies
    before porting them to a production-ready platform using, for example, Java. You
    can also use backtrader for live trading with several brokers of your choice (see
    the backtrader documentation and *Chapter 23*, *Conclusions and Next Steps*).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: We'll first summarize the key concepts of backtrader to clarify the big picture
    of the backtesting workflow on this platform, and then demonstrate its usage for
    a strategy driven by ML predictions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts of backtrader's Cerebro architecture
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: backtrader's **Cerebro** (Spanish for "brain") architecture represents the key
    components of the backtesting workflow as (extensible) Python objects. These objects
    interact to facilitate processing input data and the computation of factors, formulate
    and execute a strategy, receive and execute orders, and track and measure performance.
    A Cerebro instance orchestrates the overall process from collecting inputs, executing
    the backtest bar by bar, and providing results.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The library uses conventions for these interactions that allow you to omit some
    detail and streamline the backtesting setup. I highly recommend browsing the documentation
    to dive deeper if you plan on using backtrader to develop your own strategies.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.3* outlines the key elements in the Cerebro architecture, and the
    following subsections summarize their most important functionalities:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_03.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: The backtrader Cerebro architecture'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Data feeds, lines, and indicators
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data feeds are the raw material for a strategy and contain information about
    individual securities, such as OHLCV market data with a timestamp for each observation,
    but you can customize the available fields. backtrader can ingest data from various
    sources, including CSV files and pandas DataFrames, and from online sources like
    Yahoo Finance. There are also extensions you can use to connect to online trading
    platforms like Interactive Brokers to ingest live data and execute transactions.
    The compatibility with DataFrame objects implies that you can load data from accessible
    by pandas, ranging from databases to HDF5 files. (See the demonstration in the
    *How to use backtrader in practice* section; also, see the *I/O* section of the
    pandas documentation.)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Once loaded, we add the data feeds to a Cerebro instance, which, in turn, makes
    it available to one or more strategies in the order received. Your strategy's
    trading logic can access each data feed by name (for example, the ticker) or sequence
    number and retrieve the current and past values of any field of the data feed.
    Each field is called a **line**.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: backtrader comes with over 130 common technical **indicators** that allow you
    to compute new values from lines or other indicators for each data feed to drive
    your strategy. You can also use standard Python **operations** to derive new values.
    Usage is fairly straightforward and well explained in the documentation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: From data and signals to trades – strategy
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Strategy** object contains your trading logic that places orders based
    on data feed information that the Cerebro instance presents at every bar during
    backtest execution. You can easily test variations by configuring a Strategy to
    accept arbitrary parameters that you define when adding an instance of your Strategy
    to your Cerebro.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: For every bar of a backtest, the Cerebro instance calls either the `.prenext()`
    or `.next()` method of your Strategy instance. The role of `.prenext()` is to
    address bars that do not yet have complete data for all feeds, for example, before
    there are enough periods to compute an indicator like a built-in moving average
    or if there is otherwise missing data. The default is to do nothing, but you can
    add trading logic of your choice or call `next()` if your main Strategy is designed
    to handle missing values (see the *How to use backtrader in practice* section).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: You can also use backtrader without defining an explicit Strategy and instead
    use a simplified Signals interface. The Strategy API gives you more control and
    flexibility, though; see the backtrader documentation for details on how to use
    the Signals API.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'A Strategy outputs orders: let''s see how backtrader handles these next.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Commissions instead of commission schemes
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once your Strategy has evaluated current and past data points at each bar, it
    needs to decide which orders to place. backtrader lets you create several standard
    **order** types that Cerebro passes to a Broker instance for execution and provides
    a notification of the result at each bar.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the Strategy methods `buy()` and `sell()` to place market, close,
    and limit orders, as well as stop and stop-limit orders. Execution works as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '**Market order**: Fills at the next open bar'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Close order**: Fills at the next close bar'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limit order**: Executes only if a price threshold is met (for example, only
    buy up to a certain price) during an (optional) period of validity'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop order**: Becomes a market order if the price reaches a given threshold'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop limit order**: Becomes a limit order once the stop is triggered'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, stop orders differ from limit orders because they cannot be seen
    by the market prior to the price trigger. backtrader also provides target orders
    that compute the required size, taking into account the current position to achieve
    a certain portfolio allocation in terms of the number of shares, the value of
    the position, or the percentage of portfolio value. Furthermore, there are **bracket
    orders** that combine, for a long order, a buy with two limit sell orders that
    activate as the buy executes. Should one of the sell orders fill or cancel, the
    other sell order also cancels.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: The **Broker** handles order execution, tracks the portfolio, cash value, and
    notifications and implements transaction costs like commission and slippage. The
    Broker may reject trades if there is not enough cash; it can be important to sequence
    buys and sells to ensure liquidity. backtrader also has a `cheat_on_open` feature
    that permits looking ahead to the next bar, to avoid rejected trades due to adverse
    price moves by the next bar. This feature will, of course, bias your results.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: In addition to **commission schemes** like a fixed or percentage amount of the
    absolute transaction value, you can implement your own logic, as demonstrated
    later, for a flat fee per share.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Making it all happen – Cerebro
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Cerebro control system synchronizes the data feeds based on the bars represented
    by their timestamp, and runs the trading logic and broker actions on an event-by-event
    basis accordingly. backtrader does not impose any restrictions on the frequency
    or the trading calendar and can use multiple time frames in parallel.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: It also vectorizes the calculation for indicators if it can preload source data.
    There are several options you can use to optimize operations from a memory perspective
    (see the Cerebro documentation for details).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: How to use backtrader in practice
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to demonstrate backtrader using the daily return predictions from
    the ridge regression from *Chapter 7*, *Linear Models – From Risk Factors to Return
    Forecasts*, as we did for the vectorized backtest earlier in this chapter. We
    will create the Cerebro instance, load the data, formulate and add the Strategy,
    run the backtest, and review the results.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自*第7章*，*线性模型 - 从风险因子到收益预测*中岭回归的每日收益预测来演示backtrader，就像我们在本章前面的向量化回测中所做的那样。我们将创建Cerebro实例，加载数据，制定和添加策略，运行回测，并审查结果。
- en: The notebook `backtesting_with_backtrader` contains the following code examples
    and some additional details.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`backtesting_with_backtrader`包含以下代码示例和一些额外的细节。
- en: How to load price and other data
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何加载价格和其他数据
- en: 'We need to ensure that we have price information for all the dates on which
    we would like to buy or sell stocks, not only for the days with predictions. To
    load data from a pandas DataFrame, we subclass backtrader''s `PandasData` class
    to define the fields that we will provide:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保我们具有我们想要买入或卖出股票的所有日期的价格信息，而不仅仅是预测日的价格信息。要从pandas DataFrame加载数据，我们子类化backtrader的`PandasData`类以定义我们将提供的字段：
- en: '[PRE4]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We then instantiate a `Cerebro` class and use the `SignalData` class to add
    one data feed for each ticker in our dataset that we load from HDF5:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们实例化一个`Cerebro`类，并使用`SignalData`类为我们从HDF5加载的数据集中的每个股票添加一个数据源：
- en: '[PRE5]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, we are ready to define our Strategy.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好定义我们的策略了。
- en: How to formulate the trading logic
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何制定交易逻辑
- en: 'Our `MLStrategy` subclasses backtrader''s `Strategy` class and defines parameters
    that we can use to modify its behavior. We also create a log file to create a
    record of the transactions:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`MLStrategy`子类化了backtrader的`Strategy`类，并定义了可用于修改其行为的参数。我们还创建了一个日志文件来记录交易：
- en: '[PRE6]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The core of the strategy resides in the `.next()` method. We go long/short
    on the `n_position` stocks with the highest positive/lowest negative forecast,
    as long as there are at least `min_positions` positions. We always sell any existing
    positions that do not appear in the new long and short lists and use `order_target_percent`
    to build equal-weights positions in the new targets (log statements are omitted
    to save some space):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 策略的核心位于`.next()`方法中。我们对最高正/最低负预测的`n_position`只进行多头/空头操作，只要至少有`min_positions`个仓位。我们始终卖出任何不在新多头和空头列表中的现有仓位，并使用`order_target_percent`在新目标中建立等权重仓位（为节省一些空间而省略了日志记录）：
- en: '[PRE7]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, we need to configure our `Cerebro` instance and add our `Strategy`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要配置我们的`Cerebro`实例并添加我们的`Strategy`。
- en: How to configure the Cerebro instance
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何配置Cerebro实例
- en: 'We use a custom commission scheme that assumes we pay a fixed amount of $0.02
    per share that we buy or sell:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个自定义的佣金方案，假设我们每买入或卖出一股就支付固定金额的$0.02：
- en: '[PRE8]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we define our starting cash amount and configure the broker accordingly:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义我们的起始现金金额并相应地配置经纪人：
- en: '[PRE9]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, all that''s missing is adding the `MLStrategy` to our `Cerebro` instance,
    providing parameters for the desired number of positions and the minimum number
    of long/shorts. We''ll also add a pyfolio analyzer so we can view the performance
    tearsheets we presented in *Chapter 5*, *Portfolio Optimization and Performance
    Evaluation*:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，唯一缺少的是将`MLStrategy`添加到我们的`Cerebro`实例中，提供所需仓位数和最小多头/空头仓位数的参数。我们还将添加一个pyfolio分析器，以便查看我们在*第5章*，*组合优化和绩效评估*中呈现的绩效图表：
- en: '[PRE10]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The backtest uses 869 trading days and takes around 45 seconds to run. The following
    figure shows the cumulative return and the evolution of the portfolio value, as
    well as the daily value of long and short positions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 回测使用了869个交易日，并花费大约45秒时间运行。以下图表显示了累积收益和组合价值的变化，以及多头和空头仓位的每日价值。
- en: Performance looks somewhat similar to the preceding vectorized test, with outperformance
    relative to the S&P 500 benchmark during the first half and poor performance thereafter.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 绩效看起来与前面的向量化测试有些相似，在前半段相对于标准普尔500指数的表现良好，在后半段表现不佳。
- en: 'The `backtesting_with_backtrader` notebook contains the complete pyfolio results:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`backtesting_with_backtrader`笔记本包含完整的pyfolio结果：'
- en: '![](img/B15439_08_04.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_08_04.png)'
- en: 'Figure 8.4: backtrader results'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4：backtrader结果
- en: backtrader summary and next steps
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: backtrader摘要和下一步
- en: backtrader is a very straightforward yet flexible and performant backtesting
    engine for local backtesting. You can load any dataset at the frequency you desire
    from a broad range of sources due to pandas compatibility. `Strategy` lets you
    define arbitrary trading logic; you just need to ensure you access the distinct
    data feeds as needed. It also integrates well with pyfolio for quick yet comprehensive
    performance evaluation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: In the demonstration, we applied our trading logic to predictions from a pre-trained
    model. We can also train a model during backtesting because we can access data
    prior to the current bar. Often, however, it is more efficient to decouple model
    training from strategy selection and avoid duplicating model training.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: One of the reasons for backtrader's popularity is the ability to use it for
    live trading with a broker of your choosing. The community is very lively, and
    code to connect to brokers or additional data sources, including for cryptocurrencies,
    is readily available online.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Zipline – scalable backtesting by Quantopian
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The backtesting engine Zipline powers Quantopian's online research, backtesting,
    and live (paper) trading platform. As a hedge fund, Quantopian aims to identify
    robust algorithms that outperform, subject to its risk management criteria. To
    this end, they use competitions to select the best strategies and allocate capital
    to share profits with the winners.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Quantopian first released Zipline in 2012 as version 0.5, and the latest version,
    1.3, dates from July 2018\. Zipline works well with its sister libraries Alphalens,
    pyfolio, and empyrical that we introduced in *Chapter 4,* *Financial Feature Engineering
    – How to Research Alpha Factors* and *Chapter 5,* *Portfolio Optimization and
    Performance Evaluation*, and integrates well with NumPy, pandas, and numeric libraries,
    but may not always support the latest version.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Zipline is designed to operate at the scale of thousands of securities, and
    each can be associated with a large number of indicators. It imposes more structure
    on the backtesting process than backtrader to ensure data quality by eliminating
    look-ahead bias, for example, and optimize computational efficiency while executing
    a backtest. We'll take a look at the key concepts and elements of the architecture,
    shown in *Figure 8.5,* before we demonstrate how to use Zipline to backtest ML-driven
    models on the data of your choice.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Calendars and the Pipeline for robust simulations
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Key features that contribute to the goals of scalability and reliability are
    data bundles that store OHLCV market data with on-the-fly adjustments for splits
    and dividends, trading calendars that reflect operating hours of exchanges around
    the world, and the powerful Pipeline API (see the following diagram). We will
    discuss their usage in the following sections to complement the brief Zipline
    introduction we gave in earlier chapters:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_05.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: The Zipline architecture'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Bundles – point-in-time data with on-the-fly adjustments
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The principal data store is a **bundle** that resides on disk in compressed,
    columnar bcolz format for efficient retrieval, combined with metadata stored in
    an SQLite database. Bundles are designed to contain only OHLCV data and are limited
    to daily and minute frequency. A great feature is that bundles store split and
    dividend information, and Zipline computes **point-in-time adjustments**, depending
    on the time period you pick for your backtest.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Zipline relies on the **TradingCalendar** library (also maintained by Quantopian)
    for operational details on exchanges around the world, such as time zone, market
    open and closing times, or holidays. Data sources have domains (for now, these
    are countries) and need to conform to the assigned exchange calendar. Quantopian
    is actively developing support for international securities, and these features
    may evolve.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: After installation, the command `zipline ingest -b bundle` lets you install
    the Quandl Wiki dataset (daily frequency) right away. The result ends up in the
    `.zipline` directory, which, by default, resides in your home folder. In addition,
    you can design your own bundles, as we'll see.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: In addition to bundles, you can provide OHCLV data to an algorithm as a pandas
    DataFrame or Panel. (Panel is recently deprecated, but Zipline is a few pandas
    versions behind.) However, bundles are more convenient and efficient.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'A shortcoming of bundles is that they do not let you store data other than
    price and volume information. However, two alternatives let you accomplish this:
    the `fetch_csv()` function downloads DataFrames from a URL and was designed for
    other Quandl data sources, for example, fundamentals. Zipline reasonably expects
    the data to refer to the same securities for which you have provided OHCLV data
    and aligns the bars accordingly. It''s very easy to patch the library to load
    a local CSV or HDF5 using pandas, and the GitHub repository provides some guidance
    on how to do so.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: In addition, `DataFrameLoader` and `BlazeLoader` permit you to feed additional
    attributes to a Pipeline (see the `DataFrameLoader` demo later in this chapter).
    `BlazeLoader` can interface with numerous sources, including databases. However,
    since the Pipeline API is limited to daily data, `fetch_csv()` will be critical
    to adding features at a minute frequency, as we will do in later chapters.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The Algorithm API – backtests on a schedule
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `TradingAlgorithm` class implements the Zipline Algorithm API and operates
    on `BarData` that has been aligned with a given trading calendar. After the initial
    setup, the backtest runs for a specified period and executes its trading logic
    as specific events occur. These events are driven by the daily or minutely trading
    frequency, but you can also schedule arbitrary functions to evaluate signals,
    place orders, and rebalance your portfolio, or log information about the ongoing
    simulation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: You can execute an algorithm from the command line, in a Jupyter Notebook, or
    by using the `run_algorithm()` method of the underlying `TradingAlgorithm` class.
    The algorithm requires an `initialize()` method that is called once when the simulation
    starts. It keeps state through a context dictionary and receives actionable information
    through a data variable containing point-in-time current and historical data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过命令行，在 Jupyter Notebook 中，或者使用底层 `TradingAlgorithm` 类的 `run_algorithm()`
    方法执行算法。该算法需要一个 `initialize()` 方法，在模拟开始时调用一次。它通过上下文字典保持状态，并通过包含即时当前和历史数据的数据变量接收可操作信息。
- en: You can add properties to the context dictionary, which is available to all
    other `TradingAlgorithm` methods, or register pipelines that perform more complex
    data processing, such as computing alpha factors and filtering securities accordingly.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以向上下文字典添加属性，该字典对所有其他 `TradingAlgorithm` 方法可用，或者注册执行更复杂数据处理的管道，例如计算 alpha 因子并相应地筛选证券。
- en: Algorithm execution occurs through optional methods that are either scheduled
    automatically by Zipline or at user-defined intervals. The method `before_trading_start()`
    is called daily before the market opens and primarily serves to identify a set
    of securities the algorithm may trade during the day. The method `handle_data()`
    is called at the given trading frequency, for example, every minute.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 算法执行通过 Zipline 自动安排的可选方法或用户定义的间隔发生。每天市场开盘前调用 `before_trading_start()` 方法，并主要用于识别算法可能在当天交易的一组证券。
    `handle_data()` 方法以给定的交易频率调用，例如，每分钟。
- en: Upon completion, the algorithm returns a DataFrame containing portfolio performance
    metrics if there were any trades, as well as user-defined metrics. As demonstrated
    in *Chapter 5,* *Portfolio Optimization and Performance Evaluation*, the output
    is compatible with pyfolio so that you can quickly create performance tearsheets.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，如果有任何交易，算法将返回包含投资组合绩效指标的 DataFrame，以及用户定义的指标。正如在*第五章，* *投资组合优化和绩效评估*中所演示的，输出与
    pyfolio 兼容，因此您可以快速创建绩效表。
- en: Known issues
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 已知问题
- en: Zipline currently requires the presence of Treasury curves and the S&P 500 returns
    for benchmarking ([https://github.com/quantopian/zipline/issues/2480](https://github.com/quantopian/zipline/issues/2480)).
    The latter relies on the IEX API, which now requires registration to obtain a
    key. It is easy to patch Zipline to circumvent this and download data from the
    Federal Reserve, for instance. The GitHub repository describes how to go about
    this. Alternatively, you can move the SPY returns provided in `zipline/resources/market_data/SPY_benchmark.csv`
    to your `.zipline` folder, which usually lives in your home directory, unless
    you changed its location.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Zipline 要求 Treasury 曲线和标普 500 指数收益率用于基准测试 ([https://github.com/quantopian/zipline/issues/2480](https://github.com/quantopian/zipline/issues/2480))。后者依赖于
    IEX API，现在需要注册才能获取密钥。很容易修补 Zipline 以规避此问题，并从联邦储备局下载数据，例如。GitHub 存储库描述了如何操作。另外，您可以将
    `zipline/resources/market_data/SPY_benchmark.csv` 中提供的 SPY 收益移动到您的 `.zipline`
    文件夹中，该文件夹通常位于您的主目录中，除非您更改了其位置。
- en: Live trading ([https://github.com/zipline-live/zipline](https://github.com/zipline-live/zipline))
    your own systems is only available with Interactive Brokers and is not fully supported
    by Quantopian.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 实时交易 ([https://github.com/zipline-live/zipline](https://github.com/zipline-live/zipline))
    您自己的系统仅适用于 Interactive Brokers，并且并不受 Quantopian 完全支持。
- en: Ingesting your own bundles with minute data
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用分钟数据摄取您自己的 bundle
- en: 'We will use the NASDAQ100 2013-17 sample provided by AlgoSeek that we introduced
    in *Chapter 2,* *Market and Fundamental Data – Sources and Techniques**,* to demonstrate
    how to write your own custom bundle. There are four steps:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 AlgoSeek 提供的 NASDAQ100 2013-17 样本，该样本在*第二章，* *市场和基本数据 - 来源和技术* 中介绍，来演示如何编写自己的自定义
    bundle。有四个步骤：
- en: Divide your OHCLV data into one file per ticker and store metadata and split
    and dividend adjustments.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的 OHCLV 数据分成一个文件 per ticker，并存储元数据以及拆分和股息调整。
- en: Write a script to pass the result to an `ingest()` function, which, in turn,
    takes care of writing the bundle to bcolz and SQLite format.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个脚本将结果传递给一个 `ingest()` 函数，该函数依次负责将 bundle 写入 bcolz 和 SQLite 格式。
- en: Register the bundle in an extension.py script that lives in your `.zipline`
    directory in your home folder, and symlink the data sources.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在位于您主目录的 `.zipline` 目录中的 extension.py 脚本中注册 bundle，并为数据源创建符号链接。
- en: For AlgoSeek data, we also provide a custom TradingCalendar because it includes
    trading activity outside NYSE market hours.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The directory `custom_bundles` contains the code examples for this section.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Getting your data ready to be bundled
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Chapter 2, Market and Fundamental Data – Sources and Techniques*, we parsed
    the daily files containing the AlgoSeek NASDAQ 100 OHLCV data to obtain a time
    series for each ticker. We will use this result because Zipline also stores each
    security individually.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we obtain equity metadata using the pandas DataReader `get_nasdaq_symbols()`
    function. Finally, since the Quandl Wiki data covers the NASDAQ 100 tickers for
    the relevant period, we extract the split and dividend adjustments from that bundle's
    SQLite database.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: The result is an HDF5 store containing price and volume data on some 135 tickers,
    as well as the corresponding meta and adjustment data. The script `algoseek_preprocessing.py`
    illustrates this process.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Writing your custom bundle ingest function
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Zipline documentation outlines the required parameters for an `ingest()`
    function, which kicks off the I/O process, but does not provide a lot of practical
    detail. The script `algoseek_1min_trades.py` shows how to get this part to work
    for minute data.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: There is a `load_equities()` function that provides the metadata, a `ticker_generator()`
    function that feeds symbols to a `data_generator()`, which, in turn, loads and
    format each symbol's market data, and an `algoseek_to_bundle()` function, which
    integrates all the pieces and returns the desired `ingest()` function.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Time zone alignment matters because Zipline translates all data series to UTC;
    we add US/Eastern time zone information to the OHCLV data and convert it to UTC.
    To facilitate execution, we create symlinks for this script and the `algoseek.h5`
    data in the `custom_data` folder in the `.zipline` directory, which we'll add
    to the `PATH` in the next step so Zipline can find this information.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Registering your bundle
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we can run `zipline ingest -b algoseek`, we need to register our custom
    bundle so Zipline knows what we are talking about. To this end, we'll add the
    following lines to an `extension.py` script in the `.zipline` file, which you
    may have to create first, alongside some inputs and settings (see the `extension.py`
    example).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'The registration itself is fairly straightforward but highlights a few important
    details. First, Zipline needs to be able to import the `algoseek_to_bundle()`
    function, so its location needs to be on the search path, for example, by using
    `sys.path.append()`. Second, we reference a custom calendar that we will create
    and register in the next step. Third, we need to inform Zipline that our trading
    days are longer than the default 6 and a half hours of NYSE days to avoid misalignments:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Creating and registering a custom TradingCalendar
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned previously, Quantopian also provides a TradingCalendar library
    to support trading around the world. The package contains numerous examples, and
    it is fairly straightforward to subclass one of the examples. Based on the NYSE
    calendar, we only need to override the open/close times and change the name:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We put the definition into `extension.py` and add the following registration:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: And now, we can refer to this trading calendar to ensure a backtest includes
    off-market hour activity.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: The Pipeline API – backtesting an ML signal
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Pipeline API facilitates the definition and computation of alpha factors
    for a cross-section of securities from historical data. Pipeline significantly
    improves efficiency because it optimizes computations over the entire backtest
    period, rather than tackling each event separately. In other words, it continues
    to follow an event-driven architecture but vectorizes the computation of factors
    where possible.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: A pipeline uses factors, filters, and classifiers classes to define computations
    that produce columns in a table with point-in-time values for a set of securities.
    Factors take one or more input arrays of historical bar data and produce one or
    more outputs for each security. There are numerous built-in factors, and you can
    also design your own `CustomFactor` computations.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts how loading the data using `DataFrameLoader`,
    computing the predictive `MLSignal` using the Pipeline API, and various scheduled
    activities integrate with the overall trading algorithm that''s executed via the
    `run_algorithm()` function. We''ll go over the details and the corresponding code
    in this section:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_06.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: ML signal backtest using Zipline''s Pipeline API'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: You need to register your pipeline with the `initialize()` method and execute
    it at each time step or on a custom schedule. Zipline provides numerous built-in
    computations, such as moving averages or Bollinger Bands, that can be used to
    quickly compute standard factors, but it also allows for the creation of custom
    factors, as we will illustrate next.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, the Pipeline API renders alpha factor research modular because
    it separates the alpha factor computation from the remainder of the algorithm,
    including the placement and execution of trade orders and the bookkeeping of portfolio
    holdings, values, and so on.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: We'll now illustrate how to load the lasso model daily return predictions, together
    with price data for our universe, into a pipeline and use a `CustomFactor` to
    select the top and bottom 10 predictions as long and short positions, respectively.
    The notebook `backtesting_with_zipline` contains the following code examples.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to combine the daily return predictions with the OHCLV data from
    our Quandl bundle, and then to go long on up to 10 equities with the highest predicted
    returns and short on those with the lowest predicted returns, requiring at least
    five stocks on either side, similar to the backtrader example above.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the DataFrameLoader for our Pipeline
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we load our predictions for the 2015-17 period and extract the Zipline
    IDs for the ~250 stocks in our universe during this period using the `bundle.asset_finder.lookup_symbols()`
    method, as shown in the following code:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To make the predictions available to the Pipeline API, we need to define a
    `Column` with a suitable data type for a `DataSet` with an appropriate `domain`,
    like so:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'While the bundle''s OHLCV data can rely on the built-in `USEquityPricingLoader`,
    we need to define our own `DataFrameLoader`, as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In fact, we need to slightly modify the Zipline library's source code to bypass
    the assumption that we will only load price data. To this end, we add a `custom_loader`
    parameter to the `run_algorithm` method and ensure that this loader is used when
    the pipeline needs one of SignalData's `Column` instances.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Creating a pipeline with a custom ML factor
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our pipeline is going to have two Boolean columns that identify the assets
    we would like to trade as long and short positions. To get there, we first define
    a `CustomFactor` called `MLSignal` that just receives the current return predictions.
    The motivation is to allow us to use some of the convenient `Factor` methods designed
    to rank and filter securities:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can set up our actual pipeline by instantiating `CustomFactor`, which
    requires no arguments other than the defaults provided. We combine its `top()`
    and `bottom()` methods with a filter to select the highest positive and lowest
    negative predictions:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The next step is to initialize our algorithm by defining a few context variables,
    setting transaction cost parameters, performing schedule rebalancing and logging,
    and attaching our pipeline:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Every day before the market opens, we run our pipeline to obtain the latest
    predictions:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After the market opens, we place orders for our long and short targets and
    close all other positions:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we are ready to execute our backtest and pass the results to pyfolio:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '*Figure 8.7* shows the plots for the strategy''s cumulative returns (left panel)
    and the rolling Sharpe ratio, which are comparable to the previous backtrader
    example.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'The backtest only takes around half the time, though:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_07.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: Zipline backtest results'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The notebook `backtesting_with_zipline` contains the full pyfolio tearsheet
    with additional metrics and plots.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: How to train a model during the backtest
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also integrate the model training into our backtest. You can find the
    code for the following end-to-end example of our ML4T workflow in the `ml4t_with_zipline`
    notebook:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_08.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: Flowchart of Zipline backtest with model training'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to roughly replicate the ridge regression daily return predictions
    we used earlier and generated in *Chapter 7,* *Linear Models – From Risk Factors
    to Return Forecasts*. We will, however, use a few additional pipeline factors
    to illustrate their usage. The principal new element is a `CustomFactor` that
    receives features and returns them as inputs to train a model and produce predictions.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the features – how to define pipeline factors
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To create a **pipeline factor**, we need one or more input variables, a `window_length`
    that indicates the number of most recent data points for each input and security,
    and the computation we want to conduct.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'A linear price trend that we estimate using linear regression (see *Chapter
    7,* *Linear Models – From Risk Factors to Return Forecasts*) works as follows:
    we use the 252 latest close prices to compute the regression coefficient on a
    linear time trend:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We will use 10 custom and built-in factors as features for our model to capture
    risk factors like momentum and volatility (see notebook `ml4t_with_zipline` for
    details). Next, we'll come up with a `CustomFactor` that trains our model.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: How to design a custom ML factor
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our `CustomFactor`, called `ML`, will have `StandardScaler` and a **stochastic
    gradient descent** (**SGD**) implementation of ridge regression as instance attributes,
    and we will train the model 3 days a week:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `compute` method generates predictions (addressing potential missing values),
    but first checks if the model should be trained:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `_train_model` method is the centerpiece of the puzzle. It shifts the returns
    and aligns the resulting forward returns with the factor features, removing missing
    values in the process. It scales the remaining data points and trains the linear
    `SGDRegressor`:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `make_ml_pipeline()` function preprocesses and combines the outcome, feature,
    and model parts into a pipeline with a column for predictions:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Tracking model performance during a backtest
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We obtain new predictions using the `before_trading_start()` function, which
    runs every morning before the market opens:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`evaluate_predictions` does exactly this: it tracks the past predictions of
    our model and evaluates them once returns for the relevant time horizon materialize
    (in our example, the next day):'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We also record the evaluation on a daily basis so we can review it after the
    backtest:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_09.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: Model out-of-sample performance'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plots summarize the backtest performance in terms of the cumulative
    returns and the rolling SR. The results have improved relative to the previous
    example (due to a different feature set), yet the model still underperforms the
    benchmark since mid-2016:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_10.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: Zipline backtest performance with model training'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Please see the notebook for additional details on how we define a universe,
    run the backtest, and rebalance and analyze the results using pyfolio.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Instead of how to use
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The notebook `ml4t_quantopian` contains an example of how to backtest a strategy
    that uses a simple ML model in the Quantopian research environment. The key benefit
    of using Zipline in the Quantopian cloud is access to many additional datasets,
    including fundamental and alternative data. See the notebook for more details
    on the various factors that we can derive in this context.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a much closer look at how backtesting works, what challenges
    there are, and how to manage them. We demonstrated how to use the two popular
    backtesting libraries, backtrader and Zipline.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, however, we walked through the end-to-end process of designing
    and testing an ML model, showed you how to implement trading logic that acts on
    the signals provided by the model's predictions, and saw how to conduct and evaluate
    backtests. Now, we are ready to continue exploring a much broader and more sophisticated
    array of ML models than the linear regressions we started with.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will cover how to incorporate the time dimension into our models.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
