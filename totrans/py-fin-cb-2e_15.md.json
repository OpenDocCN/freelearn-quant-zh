["```py\n    from fastai.tabular.all import *\n    from sklearn.model_selection import train_test_split\n    from chapter_15_utils import performance_evaluation_report_fastai\n    import pandas as pd \n    ```", "```py\n    df = pd.read_csv(\"../Datasets/credit_card_default.csv\",\n                     na_values=\"\") \n    ```", "```py\n    TARGET = \"default_payment_next_month\"\n    cat_features = list(df.select_dtypes(\"object\").columns)\n    num_features = list(df.select_dtypes(\"number\").columns)\n    num_features.remove(TARGET)\n    preprocessing = [FillMissing, Categorify, Normalize] \n    ```", "```py\n    splits = RandomSplitter(valid_pct=0.2, seed=42)(range_of(df))\n    splits \n    ```", "```py\n    ((#24000) [27362,16258,19716,9066,1258,23042,18939,24443,4328,4976...],\n     (#6000) [7542,10109,19114,5209,9270,15555,12970,10207,13694,1745...]) \n    ```", "```py\n    tabular_df = TabularPandas(\n        df,\n        procs=preprocessing,\n        cat_names=cat_features,\n        cont_names=num_features,\n        y_names=TARGET,\n        y_block=CategoryBlock(),\n        splits=splits\n    )\n    PREVIEW_COLS = [\"sex\", \"education\", \"marriage\",\n                    \"payment_status_sep\", \"age_na\", \"limit_bal\",\n                    \"age\", \"bill_statement_sep\"]\n    tabular_df.xs.iloc[:5][PREVIEW_COLS] \n    ```", "```py\n    data_loader = tabular_df.dataloaders(bs=64, drop_last=True)\n    data_loader.show_batch() \n    ```", "```py\n    recall = Recall()\n    precision = Precision()\n    learn = tabular_learner(\n        data_loader,\n        [500, 200],\n        metrics=[accuracy, recall, precision]\n    )\n    learn.model \n    ```", "```py\n    TabularModel(\n      (embeds): ModuleList(\n        (0): Embedding(3, 3)\n        (1): Embedding(5, 4)\n        (2): Embedding(4, 3)\n        (3): Embedding(11, 6)\n        (4): Embedding(11, 6)\n        (5): Embedding(11, 6)\n        (6): Embedding(11, 6)\n        (7): Embedding(10, 6)\n        (8): Embedding(10, 6)\n        (9): Embedding(3, 3)\n      )\n      (emb_drop): Dropout(p=0.0, inplace=False)\n      (bn_cont): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (layers): Sequential(\n        (0): LinBnDrop(\n          (0): Linear(in_features=63, out_features=500, bias=False)\n          (1): ReLU(inplace=True)\n          (2): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (1): LinBnDrop(\n          (0): Linear(in_features=500, out_features=200, bias=False)\n          (1): ReLU(inplace=True)\n          (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): LinBnDrop(\n          (0): Linear(in_features=200, out_features=2, bias=True)\n        )\n      )\n    ) \n    ```", "```py\n    learn.lr_find() \n    ```", "```py\n    SuggestedLRs(valley=0.0010000000474974513) \n    ```", "```py\n    learn.fit(n_epoch=25, lr=1e-3, wd=0.2) \n    ```", "```py\n    learn.recorder.plot_loss() \n    ```", "```py\n    valid_data_loader = learn.dls.test_dl(df.loc[list(splits[1])]) \n    ```", "```py\n    learn.validate(dl=valid_data_loader) \n    ```", "```py\n    (#4)[0.424113571643829,0.8248333334922,0.36228482003129,0.66237482117310] \n    ```", "```py\n    preds, y_true = learn.get_preds(dl=valid_data_loader) \n    ```", "```py\n    tensor([[0.8092, 0.1908],\n            [0.9339, 0.0661],\n            [0.8631, 0.1369],\n            ...,\n            [0.9249, 0.0751],\n            [0.8556, 0.1444],\n            [0.8670, 0.1330]]) \n    ```", "```py\n    preds.argmax(dim=-1) \n    ```", "```py\n    perf = performance_evaluation_report_fastai(\n        learn, valid_data_loader, show_plot=True\n    ) \n    ```", "```py\ntabular_df.valid.xs \n```", "```py\n    from sklearn.model_selection import StratifiedKFold\n    X = df.copy()\n    y = X.pop(TARGET)\n    strat_split = StratifiedKFold(\n        n_splits=5, shuffle=True, random_state=42\n    )\n    train_ind, test_ind = next(strat_split.split(X, y))\n    ind_splits = IndexSplitter(valid_idx=list(test_ind))(range_of(df))\n    tabular_df = TabularPandas(\n        df,\n        procs=preprocessing,\n        cat_names=cat_features,\n        cont_names=num_features,\n        y_names=TARGET,\n        y_block=CategoryBlock(),\n        splits=ind_splits\n    ) \n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.metrics import recall_score\n    from pytorch_tabnet.tab_model import TabNetClassifier\n    from pytorch_tabnet.metrics import Metric\n    import torch\n    import pandas as pd\n    import numpy as np \n    ```", "```py\n    df = pd.read_csv(\"../Datasets/credit_card_default.csv\",\n                     na_values=\"\") \n    ```", "```py\n    X = df.copy()\n    y = X.pop(\"default_payment_next_month\")\n    cat_features = list(X.select_dtypes(\"object\").columns)\n    num_features = list(X.select_dtypes(\"number\").columns) \n    ```", "```py\n    cat_dims = {}\n    for col in cat_features:\n        label_encoder = LabelEncoder()\n        X[col] = X[col].fillna(\"Missing\")\n        X[col] = label_encoder.fit_transform(X[col].values)\n        cat_dims[col] = len(label_encoder.classes_)\n    cat_dims \n    ```", "```py\n    {'sex': 3,\n     'education': 5,\n     'marriage': 4,\n     'payment_status_sep': 10,\n     'payment_status_aug': 10,\n     'payment_status_jul': 10,\n     'payment_status_jun': 10,\n     'payment_status_may': 9,\n     'payment_status_apr': 9} \n    ```", "```py\n    # create the initial split - training and temp\n    X_train, X_temp, y_train, y_temp = train_test_split(\n        X, y,\n        test_size=0.3,\n        stratify=y,\n        random_state=42\n    )\n    # create the valid and test sets\n    X_valid, X_test, y_valid, y_test = train_test_split(\n        X_temp, y_temp,\n        test_size=0.5,\n        stratify=y_temp,\n        random_state=42\n    ) \n    ```", "```py\n    for col in num_features:\n        imp_mean = X_train[col].mean()\n        X_train[col] = X_train[col].fillna(imp_mean)\n        X_valid[col] = X_valid[col].fillna(imp_mean)\n        X_test[col] = X_test[col].fillna(imp_mean) \n    ```", "```py\n    features = X.columns.to_list()\n    cat_ind = [features.index(feat) for feat in cat_features]\n    cat_dims = list(cat_dims.values()) \n    ```", "```py\n    class Recall(Metric):\n        def __init__(self):\n            self._name = \"recall\"\n            self._maximize = True\n        def __call__(self, y_true, y_score):\n            y_pred = np.argmax(y_score, axis=1)\n            return recall_score(y_true, y_pred) \n    ```", "```py\n    tabnet_params = {\n        \"cat_idxs\": cat_ind,\n        \"cat_dims\": cat_dims,\n        \"optimizer_fn\": torch.optim.Adam,\n        \"optimizer_params\": dict(lr=2e-2),\n        \"scheduler_params\": {\n            \"step_size\":20,\n            \"gamma\":0.9\n        },\n        \"scheduler_fn\": torch.optim.lr_scheduler.StepLR,\n        \"mask_type\": \"sparsemax\",\n        \"seed\": 42,\n    }\n    tabnet = TabNetClassifier(**tabnet_params) \n    ```", "```py\n    tabnet.fit(\n        X_train=X_train.values,\n        y_train=y_train.values,\n        eval_set=[\n            (X_train.values, y_train.values),\n            (X_valid.values, y_valid.values)\n        ],\n        eval_name=[\"train\", \"valid\"],\n        eval_metric=[\"auc\", Recall],\n        max_epochs=200,\n        patience=20,\n        batch_size=1024,\n        virtual_batch_size=128,\n        weights=1,\n    ) \n    ```", "```py\n    epoch 0  | loss: 0.69867 | train_auc: 0.61461 | train_recall: 0.3789  | valid_auc: 0.62232 | valid_recall: 0.37286 |  0:00:01s\n    epoch 1  | loss: 0.62342 | train_auc: 0.70538 | train_recall: 0.51539 | valid_auc: 0.69053 | valid_recall: 0.48744 |  0:00:02s\n    epoch 2  | loss: 0.59902 | train_auc: 0.71777 | train_recall: 0.51625 | valid_auc: 0.71667 | valid_recall: 0.48643 |  0:00:03s\n    epoch 3  | loss: 0.59629 | train_auc: 0.73428 | train_recall: 0.5268  | valid_auc: 0.72767 | valid_recall: 0.49447 |  0:00:04s\n    â€¦\n    epoch 42 | loss: 0.56028 | train_auc: 0.78509 | train_recall: 0.6028  | valid_auc: 0.76955 | valid_recall: 0.58191 |  0:00:47s\n    epoch 43 | loss: 0.56235 | train_auc: 0.7891  | train_recall: 0.55651 | valid_auc: 0.77126 | valid_recall: 0.5407  |  0:00:48s\n    Early stopping occurred at epoch 43 with best_epoch = 23 and best_valid_recall = 0.6191\n    Best weights from best epoch are automatically used! \n    ```", "```py\n    history_df = pd.DataFrame(tabnet.history.history) \n    ```", "```py\n    history_df[\"loss\"].plot(title=\"Loss over epochs\") \n    ```", "```py\n    y_pred = tabnet.predict(X_test.values)\n    print(f\"Best validation score: {tabnet.best_cost:.4f}\")\n    print(f\"Test set score: {recall_score(y_test, y_pred):.4f}\") \n    ```", "```py\n    Best validation score: 0.6191\n    Test set score: 0.6275 \n    ```", "```py\n    tabnet_feat_imp = pd.Series(tabnet.feature_importances_,\n                                index=X_train.columns)\n    (\n        tabnet_feat_imp\n        .nlargest(20)\n        .sort_values()\n        .plot(kind=\"barh\",\n              title=\"TabNet's feature importances\")\n    ) \n    ```", "```py\n    import pandas as pd\n    import torch\n    import yfinance as yf\n    from random import sample, seed\n    import pytorch_lightning as pl\n    from pytorch_lightning.callbacks import EarlyStopping\n    from pytorch_forecasting import DeepAR, TimeSeriesDataSet \n    ```", "```py\n    df = pd.read_html(\n        \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n    )\n    df = df[0]\n    seed(44)\n    sampled_tickers = sample(df[\"Symbol\"].to_list(), 100) \n    ```", "```py\n    raw_df = yf.download(sampled_tickers,\n                         start=\"2020-01-01\",\n                         end=\"2021-12-31\") \n    ```", "```py\n    df = raw_df[\"Adj Close\"]\n    df = df.loc[:, ~df.isna().any()]\n    selected_tickers = df.columns \n    ```", "```py\n    df = df.reset_index(drop=False) \n\n    df = ( \n        pd.melt(df, \n                id_vars=[\"Date\"], \n                value_vars=selected_tickers, \n                value_name=\"price\"\n        ).rename(columns={\"variable\": \"ticker\"}) \n    )\n    df[\"time_idx\"] = df.groupby(\"ticker\").cumcount() \n    df \n    ```", "```py\n    MAX_ENCODER_LENGTH = 40\n    MAX_PRED_LENGTH = 20\n    BATCH_SIZE = 128\n    MAX_EPOCHS = 30\n    training_cutoff = df[\"time_idx\"].max() - MAX_PRED_LENGTH \n    ```", "```py\n    train_set = TimeSeriesDataSet(\n        df[lambda x: x[\"time_idx\"] <= training_cutoff],\n        time_idx=\"time_idx\",\n        target=\"price\",\n        group_ids=[\"ticker\"],\n        time_varying_unknown_reals=[\"price\"],\n        max_encoder_length=MAX_ENCODER_LENGTH,\n        max_prediction_length=MAX_PRED_LENGTH,\n    )\n    valid_set = TimeSeriesDataSet.from_dataset(\n        train_set, df, min_prediction_idx=training_cutoff+1\n    ) \n    ```", "```py\n    train_dataloader = train_set.to_dataloader(\n        train=True, batch_size=BATCH_SIZE\n    )\n    valid_dataloader = valid_set.to_dataloader(\n        train=False, batch_size=BATCH_SIZE\n    ) \n    ```", "```py\n    pl.seed_everything(42)\n    deep_ar = DeepAR.from_dataset(\n        train_set,\n        learning_rate=1e-2,\n        hidden_size=30,\n        rnn_layers=4\n    )\n    trainer = pl.Trainer(gradient_clip_val=1e-1)\n    res = trainer.tuner.lr_find(\n        deep_ar,\n        train_dataloaders=train_dataloader,\n        val_dataloaders=valid_dataloader,\n        min_lr=1e-5,\n        max_lr=1e0,\n        early_stop_threshold=100,\n    )\n    fig = res.plot(show=True, suggest=True) \n    ```", "```py\n    pl.seed_everything(42)\n    deep_ar.hparams.learning_rate = res.suggestion()\n    early_stop_callback = EarlyStopping(\n        monitor=\"val_loss\",\n        min_delta=1e-4,\n        patience=10\n    )\n    trainer = pl.Trainer(\n        max_epochs=MAX_EPOCHS,\n        gradient_clip_val=0.1,\n        callbacks=[early_stop_callback]\n    )\n    trainer.fit(\n        deep_ar,\n        train_dataloaders=train_dataloader,\n        val_dataloaders=valid_dataloader,\n    ) \n    ```", "```py\n    best_model = DeepAR.load_from_checkpoint(\n        trainer.checkpoint_callback.best_model_path\n    ) \n    ```", "```py\n    raw_predictions, x = best_model.predict(\n        valid_dataloader,\n        mode=\"raw\",\n        return_x=True,\n        n_samples=100\n    )\n    tickers = valid_set.x_to_index(x)[\"ticker\"]\n    for idx in range(5):\n        best_model.plot_prediction(\n            x, raw_predictions, idx=idx, add_loss_to_title=True\n        )\n        plt.suptitle(f\"Ticker: {tickers.iloc[idx]}\") \n    ```", "```py\n    from pytorch_forecasting.metrics import MultivariateNormalDistributionLoss\n    import seaborn as sns\n    import numpy as np \n    ```", "```py\n    train_set = TimeSeriesDataSet(\n        df[lambda x: x[\"time_idx\"] <= training_cutoff],\n        time_idx=\"time_idx\",\n        target=\"price\",\n        group_ids=[\"ticker\"],\n        static_categoricals=[\"ticker\"],  \n        time_varying_unknown_reals=[\"price\"],\n        max_encoder_length=MAX_ENCODER_LENGTH,\n        max_prediction_length=MAX_PRED_LENGTH,\n    )\n    valid_set = TimeSeriesDataSet.from_dataset(\n        train_set, df, min_prediction_idx=training_cutoff+1\n    )\n    train_dataloader = train_set.to_dataloader(\n        train=True,\n        batch_size=BATCH_SIZE,\n        batch_sampler=\"synchronized\"\n    )\n    valid_dataloader = valid_set.to_dataloader(\n        train=False,\n        batch_size=BATCH_SIZE,\n        batch_sampler=\"synchronized\"\n    ) \n    ```", "```py\n    pl.seed_everything(42)\n    deep_var = DeepAR.from_dataset(\n        train_set,\n        learning_rate=1e-2,\n        hidden_size=30,\n        rnn_layers=4,\n        loss=MultivariateNormalDistributionLoss()\n    )\n    trainer = pl.Trainer(gradient_clip_val=1e-1)\n    res = trainer.tuner.lr_find(\n        deep_var,\n        train_dataloaders=train_dataloader,\n        val_dataloaders=valid_dataloader,\n        min_lr=1e-5,\n        max_lr=1e0,\n        early_stop_threshold=100,\n    ) \n    ```", "```py\n    pl.seed_everything(42)\n    deep_var.hparams.learning_rate = res.suggestion()\n    early_stop_callback = EarlyStopping(\n        monitor=\"val_loss\",\n        min_delta=1e-4,\n        patience=10\n    )\n    trainer = pl.Trainer(\n        max_epochs=MAX_EPOCHS,\n        gradient_clip_val=0.1,\n        callbacks=[early_stop_callback]\n    )\n    trainer.fit(\n        deep_var,\n        train_dataloaders=train_dataloader,\n        val_dataloaders=valid_dataloader,\n    ) \n    ```", "```py\n    best_model = DeepAR.load_from_checkpoint(\n        trainer.checkpoint_callback.best_model_path\n    ) \n    ```", "```py\n    preds = best_model.predict(valid_dataloader,\n                               mode=(\"raw\", \"prediction\"),\n                               n_samples=None)\n\n    cov_matrix = (\n        best_model\n        .loss\n        .map_x_to_distribution(preds)\n        .base_dist\n        .covariance_matrix\n        .mean(0)\n    )\n    cov_diag_mult = (\n        torch.diag(cov_matrix)[None] * torch.diag(cov_matrix)[None].T\n    )\n    corr_matrix = cov_matrix / torch.sqrt(cov_diag_mult) \n    ```", "```py\n    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    fif, ax = plt.subplots()\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n    sns.heatmap(\n        corr_matrix, mask=mask, cmap=cmap, \n        vmax=.3, center=0, square=True, \n        linewidths=.5, cbar_kws={\"shrink\": .5}\n    )\n    ax.set_title(\"Correlation matrix\") \n    ```", "```py\nplt.hist(corr_matrix[corr_matrix < 1].numpy()) \n```", "```py\n    import yfinance as yf\n    import pandas as pd\n    from neuralprophet import NeuralProphet\n    from neuralprophet.utils import set_random_seed \n    ```", "```py\n    df = yf.download(\"^GSPC\",\n                     start=\"2010-01-01\",\n                     end=\"2021-12-31\")\n    df = df[[\"Adj Close\"]].reset_index(drop=False)\n    df.columns = [\"ds\", \"y\"] \n    ```", "```py\n    TEST_LENGTH = 60\n    df_train = df.iloc[:-TEST_LENGTH]\n    df_test = df.iloc[-TEST_LENGTH:] \n    ```", "```py\n    set_random_seed(42)\n    model = NeuralProphet(changepoints_range=0.95)\n    metrics = model.fit(df_train, freq=\"B\")\n    (\n        metrics\n        .drop(columns=[\"RegLoss\"])\n        .plot(title=\"Evaluation metrics during training\",\n              subplots=True)\n    ) \n    ```", "```py\n    pred_df = model.predict(df)\n    pred_df.plot(x=\"ds\", y=[\"y\", \"yhat1\"],\n                 title=\"S&P 500 - forecast vs ground truth\") \n    ```", "```py\n    (\n        pred_df\n        .iloc[-TEST_LENGTH:]\n        .plot(x=\"ds\", y=[\"y\", \"yhat1\"],\n              title=\"S&P 500 - forecast vs ground truth\")\n    ) \n    ```", "```py\n    set_random_seed(42)\n    model = NeuralProphet(\n        changepoints_range=0.95,\n        n_lags=10,\n        ar_reg=1,\n    )\n    metrics = model.fit(df_train, freq=\"B\")\n    pred_df = model.predict(df)\n    pred_df.plot(x=\"ds\", y=[\"y\", \"yhat1\"],\n                 title=\"S&P 500 - forecast vs ground truth\") \n    ```", "```py\n    (\n        pred_df\n        .iloc[-TEST_LENGTH:]\n        .plot(x=\"ds\", y=[\"y\", \"yhat1\"],\n              title=\"S&P 500 - forecast vs ground truth\")\n    ) \n    ```", "```py\n    set_random_seed(42)\n    model = NeuralProphet(\n        changepoints_range=0.95,\n        n_lags=10,\n        ar_reg=1,\n        num_hidden_layers=3,\n        d_hidden=32,\n    )\n    metrics = model.fit(df_train, freq=\"B\")\n    pred_df = model.predict(df)\n    (\n        pred_df\n        .iloc[-TEST_LENGTH:]\n        .plot(x=\"ds\", y=[\"y\", \"yhat1\"],\n              title=\"S&P 500 - forecast vs ground truth\")\n    ) \n    ```", "```py\n    model.plot_components(model.predict(df_train)) \n    ```", "```py\nmodel.plot_parameters() \n```", "```py\nset_random_seed(42)\nmodel = NeuralProphet(\n    changepoints_range=0.95,\n    n_lags=10,\n    ar_reg=1,\n    num_hidden_layers=3,\n    d_hidden=32,\n)\nmodel = model.add_country_holidays(\n    \"US\", lower_window=-1, upper_window=1\n)\nmetrics = model.fit(df_train, freq=\"B\") \n```", "```py\nmodel = NeuralProphet(\n    n_lags=10,\n    n_forecasts=10,\n    ar_reg=1,\n    learning_rate=0.01\n)\nmetrics = model.fit(df_train, freq=\"B\")\npred_df = model.predict(df)\npred_df.tail() \n```", "```py\npred_df = model.predict(df, raw=True, decompose=False)\npred_df.tail() \n```", "```py\npred_df = model.predict(df_test)\nmodel.plot(pred_df)\nax = plt.gca()\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nax.set_title(\"10-day ahead multi-step forecast\") \n```", "```py\nmodel = model.highlight_nth_step_ahead_of_each_forecast(1)\nmodel.plot(pred_df)\nax = plt.gca()\nax.set_title(\"Step 1 of the 10-day ahead multi-step forecast\") \n```"]