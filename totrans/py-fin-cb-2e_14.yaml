- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Advanced Concepts for Machine Learning Projects
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习项目的高级概念
- en: In the previous chapter, we introduced a possible workflow for solving a real-life
    problem using machine learning. We went over the entire project, starting with
    cleaning the data, through training and tuning a model, and then lastly evaluating
    its performance. However, this is rarely the end of the project. In that project,
    we used a simple decision tree classifier, which most of the time can be used
    as a benchmark or minimum viable product (MVP). In this chapter, we cover a few
    more advanced concepts that can help with improving the value of the project and
    make it easier to adopt by the business stakeholders.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了解决现实问题的可能工作流程，使用机器学习从数据清洗、模型训练和调优，到最后评估其表现。然而，这通常并不是项目的终点。在那个项目中，我们使用了一个简单的决策树分类器，它通常可以作为基准或最小可行产品（MVP）。在本章中，我们将介绍一些更高级的概念，这些概念有助于提升项目的价值，并使其更容易被业务利益相关者采纳。
- en: 'After creating the MVP, which serves as a baseline, we would like to improve
    the model’s performance. While attempting to improve the model, we should also
    try to balance underfitting and overfitting. There are a few ways to do so, some
    of which include:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了最小可行产品（MVP）作为基准之后，我们希望提高模型的表现。在尝试改善模型时，我们还应该平衡欠拟合和过拟合。实现这一点有几种方法，其中包括：
- en: Gathering more data (observations)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集更多数据（观测数据）
- en: Adding more features—either by gathering additional data (for example, by using
    external data sources) or through feature engineering using currently available
    information
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多特征——无论是通过收集额外的数据（例如，使用外部数据源）还是通过特征工程使用当前可用的信息
- en: Using more complex models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更复杂的模型
- en: Selecting only the relevant features
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只选择相关特征
- en: Tuning the hyperparameters
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整超参数
- en: There is a common stereotype that data scientists spend 80% of their time on
    a project gathering and cleaning data while only 20% remains for the actual modeling.
    In line with the stereotype, adding more data might greatly improve a model’s
    performance, especially when dealing with imbalanced classes in a classification
    problem. But finding additional data (be it observations or features) is not always
    possible, or might simply be too complicated. Then, the other solution may be
    to use more complex models or to tune the hyperparameters to squeeze out some
    extra performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个常见的刻板印象认为，数据科学家在一个项目中花费80%的时间收集和清理数据，剩下的20%才用于实际的建模。按照这一刻板印象，增加更多数据可能会大大提高模型的表现，尤其是在处理分类问题中的不平衡类别时。但寻找额外的数据（无论是观测数据还是特征）并不总是可行，或者可能会变得非常复杂。那么，另一个解决方案可能是使用更复杂的模型，或者调整超参数以挤压出一些额外的性能。
- en: We start the chapter by presenting how to use more advanced classifiers, which
    are also based on decision trees. Some of them (XGBoost and LightGBM) are frequently
    used for winning machine learning competitions (such as those found on Kaggle).
    Additionally, we introduce the concept of stacking multiple machine learning models
    to further improve prediction performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们将介绍如何使用更先进的分类器，这些分类器同样基于决策树。其中一些（如XGBoost和LightGBM）在机器学习竞赛中常常被用来获胜（例如Kaggle上的竞赛）。此外，我们还介绍了堆叠多个机器学习模型的概念，以进一步提高预测性能。
- en: Another common real-life problem concerns dealing with imbalanced data, that
    is, when one class (such as default or fraud) is rarely observed in practice.
    This makes it especially difficult to train a model to accurately capture the
    minority class observations. We introduce a few common approaches to handling
    class imbalance and compare their performance on a credit card fraud dataset,
    in which the minority class corresponds to 0.17% of all the observations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的现实问题是处理不平衡数据，也就是当某一类别（如违约或欺诈）在实际中很少出现时。这使得训练一个准确捕捉少数类别观测值的模型变得特别困难。我们介绍了几种处理类别不平衡的常见方法，并在信用卡欺诈数据集上比较了它们的表现，其中少数类别仅占所有观测值的0.17%。
- en: Then, we also expand on hyperparameter tuning, which was explained in the previous
    chapter. Previously, we used either an exhaustive grid search or a randomized
    search, both of which are carried out in an uninformed manner. This means that
    there is no underlying logic in selecting the next set of hyperparameters to investigate.
    This time, we introduce Bayesian optimization, in which past attempts are used
    to select the next set of values to explore. This approach can significantly speed
    up the tuning phase of our projects.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们还扩展了超参数调优的内容，这在前一章中已有解释。之前，我们使用了穷尽的网格搜索或随机搜索，这两种方法都是在无信息的情况下进行的。这意味着在选择下一个要探索的超参数集时，并没有底层逻辑。这一次，我们介绍了贝叶斯优化方法，在这种方法中，过去的尝试被用来选择下一个要探索的超参数集。这种方法可以显著加速我们项目的调优阶段。
- en: In many industries (and finance especially) it is crucial to understand the
    logic behind a model’s prediction. For example, a bank might be legally obliged
    to provide actual reasons for declining a credit request, or it can try to limit
    its losses by predicting which customers are likely to default on a loan. To get
    a better understanding of the models, we explore various approaches to determining
    feature importance and model explainability. The latter is especially relevant
    when dealing with complex models, which are often considered to be black boxes,
    that is, unexplainable. We can additionally use those insights to select only
    the most relevant features, which can further improve the model’s performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多行业（尤其是金融行业），理解模型预测背后的逻辑至关重要。例如，银行可能在法律上被要求提供拒绝信用请求的实际理由，或者它可以通过预测哪些客户可能违约来尝试限制损失。为了更好地理解模型，我们探讨了确定特征重要性和模型可解释性的各种方法。后者在处理复杂模型时尤为重要，因为这些模型通常被认为是黑箱，即无法解释的。我们还可以利用这些见解，仅选择最相关的特征，这可以进一步提高模型的性能。
- en: 'In this chapter, we present the following recipes:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍以下几种方法：
- en: Exploring ensemble classifiers
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索集成分类器
- en: Exploring alternative approaches to encoding categorical features
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索编码分类特征的替代方法
- en: Investigating different approaches to handling imbalanced data
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探讨处理不平衡数据的不同方法
- en: Leveraging the wisdom of the crowds with stacked ensembles
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用群众智慧的堆叠集成模型
- en: Bayesian hyperparameter optimization
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯超参数优化
- en: Investigating feature importance
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探讨特征重要性
- en: Exploring feature selection techniques
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索特征选择技术
- en: Exploring explainable AI techniques
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索可解释的人工智能技术
- en: Exploring ensemble classifiers
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索集成分类器
- en: 'In *Chapter 13*, *Applied Machine Learning: Identifying Credit Default*, we
    learned how to build an entire machine learning pipeline, which contained both
    preprocessing steps (imputing missing values, encoding categorical features, and
    so on) and a machine learning model. Our task was to predict customer default,
    that is, their inability to repay their debts. We used a decision tree model as
    the classifier.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第13章*，*应用机器学习：识别信用违约*中，我们学习了如何构建一个完整的机器学习管道，其中包含预处理步骤（填补缺失值、编码分类特征等）和机器学习模型。我们的任务是预测客户违约，即无法偿还债务。我们使用了决策树模型作为分类器。
- en: Decision trees are considered simple models and one of their drawbacks is overfitting
    to the training data. They belong to the group of high-variance models, which
    means that a small change to the training data can greatly impact the tree’s structure
    and its predictions. To overcome those issues, they can be used as building blocks
    for more complex models. **Ensemble models** combine predictions of multiple base
    models (for example, decision trees) in order to improve the final model’s generalizability
    and robustness. This way, they transform the initial high-variance estimators
    into a low-variance aggregate estimator.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树被认为是简单模型，它们的一个缺点是对训练数据的过拟合。它们属于高方差模型，这意味着对训练数据的微小变化会极大地影响树的结构和预测结果。为了克服这些问题，决策树可以作为更复杂模型的构建块。**集成模型**通过结合多个基础模型（例如决策树）的预测，以提高最终模型的泛化能力和鲁棒性。这样，它们将最初的高方差估计器转变为低方差的综合估计器。
- en: 'On a high level, we could divide the ensemble models into two groups:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，我们可以将集成模型分为两组：
- en: '**Averaging methods**—several models are estimated independently and then their
    predictions are averaged. The underlying principle is that the combined model
    is better than a single one as its variance is reduced. Examples: Random Forest
    and Extremely Randomized Trees.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均法**—多个模型独立估计，然后将它们的预测结果平均。其基本原理是，组合模型比单一模型更好，因为其方差减少了。示例：随机森林和极度随机化树。'
- en: '**Boosting methods**—in this approach, multiple base estimators are built sequentially
    and each one tries to reduce the bias of the combined estimator. Again, the underlying
    assumption is that a combination of multiple weak models produces a powerful ensemble.
    Examples: Gradient Boosted Trees, XGBoost, LightGBM, and CatBoost.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升方法**—在这种方法中，多个基本估计器被顺序构建，每个估计器都试图减少组合估计器的偏差。其基本假设是，多个弱模型的组合会产生一个强大的集成模型。示例：梯度提升树、XGBoost、LightGBM
    和 CatBoost。'
- en: In this recipe, we use a selection of ensemble models to try to improve the
    performance of the decision tree approach. As those models are based on decision
    trees, the same principles about feature scaling (no explicit need for it) apply
    and we can reuse most of the previously created pipeline.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们使用了一些集成模型来尝试提升决策树方法的性能。由于这些模型基于决策树，关于特征缩放（不需要显式进行）的相同原则适用，因此我们可以重用之前创建的大部分管道。
- en: Getting ready
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we build on top of what we already established in the *Organizing
    the project with pipelines* recipe from the previous chapter, in which we created
    the default prediction pipeline, from loading the data to training the classifier.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们基于上一章的*通过管道组织项目*食谱中的内容，创建了默认的预测管道，从加载数据到训练分类器。
- en: In this recipe, we use the variant without the outlier removal procedure. We
    will be replacing the last step (the classifier) with more complex ensemble models.
    Additionally, we first fit the decision tree pipeline to the data to obtain the
    baseline model for performance comparison. For your convenience, we reiterate
    all the required steps in the notebook accompanying this chapter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们使用的是不包含异常值去除程序的变体。我们将用更复杂的集成模型替换最后一步（分类器）。此外，我们首先将决策树管道拟合到数据中，以获得基线模型用于性能比较。为了方便起见，我们在本章附带的笔记本中重申了所有必需的步骤。
- en: How to do it...
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Execute the following steps to train the ensemble classifiers:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以训练集成分类器：
- en: 'Import the libraries:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE0]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this chapter, we also use the already familiar `performance_evaluation_report`
    helper function.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本章中，我们还使用了已经熟悉的 `performance_evaluation_report` 辅助函数。
- en: 'Define and fit the Random Forest pipeline:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并拟合随机森林管道：
- en: '[PRE1]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The performance of the Random Forest can be summarized by the following plot:'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机森林的性能可以通过以下图表总结：
- en: '![](../Images/B18112_14_01.png)'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_01.png)'
- en: 'Figure 14.1: Performance evaluation of the Random Forest model'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.1：随机森林模型的性能评估
- en: 'Define and fit the Gradient Boosted Trees pipeline:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并拟合梯度提升树管道：
- en: '[PRE2]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The performance of the Gradient Boosted Trees can be summarized by the following
    plot:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 梯度提升树的性能可以通过以下图表总结：
- en: '![](../Images/B18112_14_02.png)'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_02.png)'
- en: 'Figure 14.2: Performance evaluation of the Gradient Boosted Trees model'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.2：梯度提升树模型的性能评估
- en: 'Define and fit an XGBoost pipeline:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并拟合 XGBoost 管道：
- en: '[PRE3]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The performance of the XGBoost can be summarized by the following plot:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: XGBoost 的性能可以通过以下图表总结：
- en: '![](../Images/B18112_14_03.png)'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_03.png)'
- en: 'Figure 14.3: Performance evaluation of the XGBoost model'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.3：XGBoost 模型的性能评估
- en: 'Define and fit the LightGBM pipeline:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并拟合 LightGBM 管道：
- en: '[PRE4]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The performance of the LightGBM can be summarized by the following plot:'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LightGBM 的性能可以通过以下图表总结：
- en: '![](../Images/B18112_14_04.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_04.png)'
- en: 'Figure 14.4: Performance evaluation of the LightGBM model'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4：LightGBM 模型的性能评估
- en: From the reports, it looks like the shapes of the ROC curve and the Precision-Recall
    curve were very similar for all the considered models. We will look at the scores
    of the models in the *There’s more…* section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从报告来看，所有考虑的模型的 ROC 曲线和精确率-召回率曲线形状非常相似。我们将在*更多内容…*部分查看各个模型的得分。
- en: How it works...
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: This recipe shows how easy it is to use different classifiers, as long as we
    want to use their default settings. In the first step, we imported the classifiers
    from their respective libraries.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例展示了使用不同分类器是多么简单，只要我们希望使用它们的默认设置。在第一步中，我们从各自的库中导入了分类器。
- en: In this recipe, we have used the `scikit-learn` API of libraries such as XGBoost
    or LightGBM. However, we could also use their native approaches to training models,
    which might require some additional effort, such as converting a `pandas` DataFrame
    to formats acceptable by those libraries. Using the native approaches can yield
    some extra benefits, for example, in terms of accessing certain hyperparameters
    or configuration settings.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了`scikit-learn` API，结合了像XGBoost或LightGBM这样的库。然而，我们也可以使用它们的原生方法来训练模型，这可能需要一些额外的工作，例如将`pandas`
    DataFrame转换为这些库接受的格式。使用原生方法可以带来一些额外的好处，例如可以访问某些超参数或配置设置。
- en: In *Steps 2* to *5*, we created a separate pipeline for each classifier. We
    combined the already established `ColumnTransformer` preprocessor with the corresponding
    classifier. Then, we fitted each pipeline to the training data and presented the
    performance evaluation report.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤2*到*步骤5*中，我们为每个分类器创建了一个独立的流水线。我们将已经建立的`ColumnTransformer`预处理器与相应的分类器结合在一起。然后，我们将每个流水线拟合到训练数据并展示了性能评估报告。
- en: Some of the considered ensemble models offer additional functionalities in the
    `fit` method (as opposed to setting hyperparameters when instantiating the class).
    For example, when using the `fit` method of LightGBM we can pass in the names/indices
    of categorical features. By doing so, the algorithm knows how to treat those features
    using its own approach, without the need for explicit one-hot encoding. Similarly,
    we could use a wide variety of available callbacks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一些考虑的集成模型在`fit`方法中提供了额外的功能（而不是在实例化类时设置超参数）。例如，使用LightGBM的`fit`方法时，我们可以传入分类特征的名称/索引。这样，算法就知道如何使用其自身的方法处理这些特征，而无需显式地进行独热编码。同样，我们还可以使用各种可用的回调函数。
- en: Thanks to modern Python libraries, fitting all the considered classifiers was
    extremely easy. We only had to replace the model’s class in the pipeline with
    another one. Keeping in mind how simple it is to experiment with different models,
    it is good to have at least a basic understanding of what those models do and
    what their strengths and weaknesses are. That is why below we provide a brief
    introduction to the considered algorithms.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了现代Python库，拟合所有考虑的分类器变得异常简单。我们只需将流水线中的模型类替换为另一个模型类。考虑到尝试不同模型是多么简单，理解这些模型的工作原理以及它们的优缺点是非常重要的。这就是为什么下面我们提供了对所考虑算法的简要介绍。
- en: Random Forest
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: '**Random Forest** is an example of an ensemble of models, that is, it trains
    multiple models (decision trees) and uses them to create predictions. In the case
    of a regression problem, it takes the average value of all the underlying trees.
    For classification it uses a majority vote. Random Forest offers more than just
    training many trees and aggregating their results.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**是一个集成模型的例子，即它训练多个模型（决策树），并利用这些模型来进行预测。在回归问题中，它取所有树的平均值；在分类问题中，它使用多数投票。随机森林不仅仅是训练多棵树并汇总它们的结果。'
- en: First, it uses **bagging** (bootstrap aggregation)—each tree is trained on a
    subset of all available observations. Those are drawn randomly with replacement,
    so—unless specified otherwise—the total number of observations used for each tree
    is the same as the total in the training set. Even though a single tree might
    have high variance with respect to a particular dataset (due to bagging), the
    forest will have lower variance overall, without increasing the bias. Additionally,
    this approach can also reduce the effect of any outliers in the data as they will
    not be used in all of the trees. To add even more randomness, each tree only considers
    a subset of all features to create each split. We can control that number using
    a dedicated hyperparameter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它使用**Bagging**（自助聚合法）——每棵树都在所有可用观测值的子集上进行训练。这些观测值是通过有放回的随机抽样得到的，因此——除非另行指定——每棵树使用的观测值总数与训练集中的总观测值数相同。即使单棵树可能会因为Bagging的原因在特定数据集上有较高的方差，但整个森林的方差将会较低，而不会增加偏差。此外，这种方法还可以减少数据中异常值的影响，因为它们不会出现在所有的树中。为了增加更多的随机性，每棵树只考虑所有特征的一个子集来创建每个分裂。我们可以使用一个专门的超参数来控制这个数字。
- en: Thanks to those two mechanisms, the trees in the forest are not correlated with
    each other and are built independently. The latter allows for the parallelization
    of the tree-building step.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了这两种机制，森林中的树彼此之间没有关联，并且是独立构建的。后者使得树构建步骤可以并行化。
- en: Random Forest provides a good trade-off between complexity and performance.
    Often—without any tuning—we can get much better performance than when using simpler
    algorithms, such as decision trees or linear/logistic regression. That is because
    Random Forest has a lower bias (due to its flexibility) and reduced variance (due
    to aggregating predictions of multiple models).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林提供了复杂度与性能之间的良好平衡。通常——即使没有任何调优——我们也能比使用更简单的算法（如决策树或线性/逻辑回归）获得更好的性能。这是因为随机森林具有较低的偏差（由于其灵活性）和较小的方差（由于聚合多个模型的预测）。
- en: Gradient Boosted Trees
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: '**Gradient Boosted Trees** is another type of ensemble model. The idea is to
    train many weak learners (shallow decision trees/stumps with high bias) and combine
    them to obtain a strong learner. In contrast to Random Forest, Gradient Boosted
    Trees is a sequential/iterative algorithm. In **boosting**, we start with the
    first weak learner, and each of the subsequent learners tries to learn from the
    mistakes of the previous ones. They do this by being fitted to the residuals (error
    terms) of the previous models.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升树**是另一种集成模型。其思想是训练许多弱学习器（具有高偏差的浅层决策树/树桩），并将它们组合起来以获得一个强学习器。与随机森林相比，梯度提升树是一个顺序/迭代算法。在**提升**中，我们从第一个弱学习器开始，每个后续的学习器都试图从前一个学习器的错误中学习。它们通过拟合前一个模型的残差（误差项）来实现这一点。'
- en: The reason why we create an ensemble of weak learners instead of strong learners
    is that in the case of the strong learners, the errors/mislabeled data points
    would most likely be the noise in the data, so the overall model would end up
    overfitting to the training data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一组弱学习器而不是强学习器的原因是，在强学习器的情况下，错误/标注错误的数据点很可能是数据中的噪音，因此整体模型最终会对训练数据发生过拟合。
- en: The term *gradient* comes from the fact that the trees are built using **gradient
    descent**, which is an optimization algorithm. Without going into too much detail,
    it uses the gradient (slope) of the loss function to minimize the overall loss
    and achieve the best performance. The loss function represents the difference
    between the actual and predicted values. In practice, to perform the gradient
    descent procedure in Gradient Boosted Trees, we add such a tree to the model that
    follows the gradient. In other words, such a tree reduces the value of the loss
    function.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*梯度*这个术语来源于树是使用**梯度下降**构建的，而梯度下降是一种优化算法。简而言之，它利用损失函数的梯度（斜率）来最小化整体损失并实现最佳性能。损失函数表示实际值和预测值之间的差异。实际上，为了在梯度提升树中执行梯度下降过程，我们会将这样一棵树添加到模型中，使其遵循梯度。换句话说，这样的树会降低损失函数的值。'
- en: 'We can describe the boosting procedure using the following steps:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下步骤描述提升过程：
- en: The process starts with a simple estimate (mean, median, and so on).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该过程从一个简单的估算开始（均值、中位数等）。
- en: A tree is fitted to the error of that prediction.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一棵树被拟合到该预测的误差。
- en: The prediction is adjusted using the tree’s prediction. However, it is not fully
    adjusted, but only to a certain degree (based on a learning rate hyperparameter).
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测通过使用树的预测值进行调整。然而，它并不会完全调整，而只是调整到一定程度（基于学习率超参数）。
- en: Another tree is fitted to the error of the updated prediction and the prediction
    is further adjusted as in the previous step.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一棵树被拟合到更新后的预测误差，并且预测值像之前的步骤那样进一步调整。
- en: The algorithm continues to iteratively reduce the error until a specified number
    of rounds (or another stopping criterion) is reached.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法会继续迭代地减少误差，直到达到指定的轮次（或其他停止准则）。
- en: The final prediction is the sum of the initial prediction and all the adjustments
    (predictions of the error weighted with the learning rate).
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终预测是初始预测与所有调整（按学习率加权的误差预测值）之和。
- en: In contrast to Random Forest, Gradient Boosted Trees use all available data
    to train the models. However, we can use random sampling without replacement for
    each tree by using the `subsample` hyperparameter. Then, we are dealing with **Stochastic
    Gradient Boosted Trees**. Additionally, similarly to Random Forest, we can make
    the trees consider only a subset of features when making a split.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林相比，梯度提升树使用所有可用数据来训练模型。然而，我们可以通过使用`subsample`超参数对每棵树进行不重复的随机采样。这样，我们就得到了**随机梯度提升树**。此外，类似于随机森林，我们可以让树在进行分裂时只考虑特征的一个子集。
- en: XGBoost
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost
- en: '**Extreme Gradient Boosting (XGBoost**) is an implementation of Gradient Boosted
    Trees that incorporates a series of improvements resulting in superior performance
    (both in terms of evaluation metrics and estimation time). Since being published,
    the algorithm has been successfully used to win many data science competitions.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**极端梯度提升（XGBoost）**是梯度提升树的一个实现，融合了一系列改进，从而提供了更优的性能（无论是在评估指标还是估计时间上）。自发布以来，该算法已成功用于赢得许多数据科学竞赛。'
- en: 'In this recipe, we only present a high-level overview of its distinguishable
    features. For a more detailed overview, please refer to the original paper (Chen
    *et al.* (2016)) or documentation. The key concepts of XGBoost are the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们仅提供了XGBoost一些可辨识特性的概述。欲了解更详细的概述，请参考原始论文（Chen *et al.* (2016)）或文档。XGBoost的关键概念如下：
- en: XGBoost combines a pre-sorted algorithm with a histogram-based algorithm to
    calculate the best splits. This tackles a significant inefficiency of Gradient
    Boosted Trees, namely that the algorithm considers the potential loss for all
    possible splits when creating a new branch (especially important when considering
    hundreds or thousands of features).
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost将预排序算法与基于直方图的算法相结合，用于计算最佳分裂。这解决了梯度提升树的一个重大低效问题，即在创建新分支时，算法需要考虑所有可能的分裂的潜在损失（特别是在考虑数百或数千个特征时，尤其重要）。
- en: The algorithm uses the Newton-Raphson method to approximate the loss function,
    which allows us to use a wider variety of loss functions.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法使用牛顿-拉夫森方法来近似损失函数，这使我们能够使用更广泛的损失函数。
- en: XGBoost has an extra randomization parameter to reduce the correlation between
    the trees.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost有一个额外的随机化参数，用于减少树与树之间的相关性。
- en: XGBoost combines Lasso (L1) and Ridge (L2) regularization to prevent overfitting.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost结合了Lasso（L1）和Ridge（L2）正则化，以防止过拟合。
- en: It offers a more efficient approach to tree pruning.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了一种更高效的树修剪方法。
- en: XGBoost has a feature called monotonic constraints—the algorithm sacrifices
    some accuracy and increases the training time to improve model interpretability.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost有一个名为单调约束的特性——该算法牺牲一些准确性并增加训练时间，以提高模型的可解释性。
- en: XGBoost does not take categorical features as input—we must use some kind of
    encoding for them.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost不接受类别特征作为输入——我们必须对它们进行某种编码。
- en: The algorithm can handle missing values in the data.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法可以处理数据中的缺失值。
- en: LightGBM
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LightGBM
- en: '**LightGBM**, released by Microsoft, is another competition-winning implementation
    of Gradient Boosted Trees. Thanks to some improvements, LightGBM results in a
    similar performance to XGBoost, but with faster training time. Key features include
    the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**LightGBM**由微软发布，是另一种赢得比赛的梯度提升树实现。由于一些改进，LightGBM的性能与XGBoost相似，但训练时间更快。其主要特点包括：'
- en: The difference in speed is caused by the approach to growing trees. In general,
    algorithms (such as XGBoost) use a level-wise (horizontal) approach. LightBGM,
    on the other hand, grows trees leaf-wise (vertically). The leaf-wise algorithm
    chooses the leaf with the maximum reduction in the loss function. Such algorithms
    tend to converge faster than the level-wise ones; however, they tend to be more
    prone to overfitting (especially with small datasets).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度的差异是由树的生长方式造成的。一般来说，算法（例如XGBoost）采用层级（水平）方式。另一方面，LightBGM采用叶子生长方式（垂直）。叶子生长算法选择具有最大损失函数减少的叶子。这类算法通常比层级方式更快收敛；然而，它们更容易过拟合（尤其是在小数据集上）。
- en: LightGBM employs a technique called **Gradient-based One-Side Sampling** (**GOSS**)
    to filter out the data instances used for finding the best split value. Intuitively,
    observations with small gradients are already well trained, while those with large
    gradients have more room for improvement. GOSS retains instances with large gradients
    and additionally samples randomly from observations with small gradients.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightGBM采用一种名为**基于梯度的单边采样**（**GOSS**）的技术，来过滤出用于寻找最佳切分值的数据实例。直观地说，梯度较小的观测值已经得到了较好的训练，而梯度较大的观测值还有更多改进空间。GOSS保留了梯度较大的实例，并且从梯度较小的观测值中随机采样。
- en: LightGBM uses **Exclusive Feature Bundling** (**EFB**) to take advantage of
    sparse datasets and bundles together features that are mutually exclusive (they
    never have values of zero at the same time). This leads to a reduction in the
    complexity (dimensionality) of the feature space.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightGBM使用**独特特征捆绑**（**EFB**）技术来利用稀疏数据集，并将互斥的特征（它们在同一时刻永远不会同时为零）捆绑在一起。这有助于减少特征空间的复杂性（维度）。
- en: The algorithm uses histogram-based methods to bucket continuous feature values
    into discrete bins in order to speed up training and reduce memory usage.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法使用基于直方图的方法将连续特征值分桶到离散的区间，以加速训练并减少内存使用。
- en: The leaf-wise algorithm was later added to XGBoost as well. To make use of it,
    we need to set `grow_policy` to `"lossguide"`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，叶节点算法也被添加到了XGBoost中。要使用它，我们需要将`grow_policy`设置为`"lossguide"`。
- en: There’s more...
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In this recipe, we showed how to use selected ensemble classifiers to try to
    improve our ability to predict customers’ likelihood of defaulting their loan.
    To make things even more interesting, these models have dozens of hyperparameters
    to tune, which can significantly increase (or decrease) their performance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了如何使用选定的集成分类器来提高我们预测客户违约贷款的能力。更有趣的是，这些模型有数十个超参数可以调整，这可能会显著提高（或降低）它们的性能。
- en: For brevity, we will not discuss the hyperparameter tuning of these models here.
    We refer you to the accompanying Jupyter notebook for a short introduction to
    tuning these models using a randomized grid search approach. Here, we only present
    a table containing the results. We can compare the performance of the models with
    default settings versus their tuned counterparts.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简便起见，我们将在这里不讨论这些模型的超参数调优。我们建议您查阅附带的Jupyter笔记本，里面简要介绍了如何使用随机网格搜索方法来调优这些模型。在这里，我们仅呈现一个包含结果的表格。我们可以比较默认设置下模型的性能与调优后的模型性能。
- en: '![](../Images/B18112_14_05.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_05.png)'
- en: 'Figure 14.5: Table comparing the performance of various classifiers'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5：比较不同分类器性能的表格
- en: For the models calibrated using the randomized search (including the `_rs` suffix
    in the name), we used 100 random sets of hyperparameters. As the considered problem
    deals with imbalanced data (the minority class is ~20%), we look at recall for
    performance evaluation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用随机搜索调整的模型（包括名称中带有`_rs`后缀的模型），我们使用了100组随机的超参数集。由于所考虑的问题涉及不平衡的数据（少数类约占20%），因此我们通过召回率来评估模型的性能。
- en: It seems that the basic decision tree achieved the best recall score on the
    test set. This came at the cost of much lower precision than the more advanced
    models. That is why the F1 score (a harmonic mean of precision and recall) is
    the lowest for the decision tree. We can see that the default LightGBM model achieved
    the best F1 score on the test set.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来基本的决策树在测试集上达到了最佳的召回率。这是以牺牲比更复杂模型低得多的精度为代价的。这就是为什么决策树的F1得分（精度与召回率的调和均值）最低的原因。我们可以看到，默认的LightGBM模型在测试集上达到了最佳的F1得分。
- en: The results by no means indicate that the more complex models are inferior—they
    might simply require more tuning or a different set of hyperparameters. For example,
    the ensemble models enforced the maximum depth of the tree (determined by the
    corresponding hyperparameter), while the decision tree had no such limit and it
    reached the depth of 37\. The more advanced the model, the more effort it requires
    to “get it right.”
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果并不意味着更复杂的模型就较差——它们可能只是需要更多的调优或者一组不同的超参数。例如，集成模型强制设定了树的最大深度（由相应的超参数决定），而决策树没有这样的限制，并且它的深度达到了37。模型越复杂，越需要更多的努力才能“做对”。
- en: 'There are many different ensemble classifiers available to experiment with.
    Some of the possibilities include:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的集成分类器可供实验。一些可能性包括：
- en: AdaBoost—the first boosting algorithm.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost——第一种提升算法。
- en: Extremely Randomized Trees—this algorithm offers improved randomness as compared
    to Random Forests. Similar to Random Forest, a random subset of features is considered
    when making a split. However, instead of looking for the most discriminative thresholds,
    the thresholds are drawn at random for each feature. Then, the best of these random
    thresholds is picked as the splitting rule. Such an approach usually allows us
    to reduce the variance of the model, while slightly increasing its bias.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极端随机树——该算法提供了比随机森林更强的随机性。与随机森林类似，在进行分裂时会考虑特征的随机子集。然而，与寻找最具区分性的阈值不同，每个特征的阈值是随机抽取的。然后，从这些随机阈值中选择最好的作为分裂规则。这种方法通常可以减少模型的方差，同时略微增加其偏差。
- en: CatBoost—another boosting algorithm (developed by Yandex) that puts a high emphasis
    on handling categorical features and achieving high performance with little hyperparameter
    tuning.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CatBoost——另一种提升算法（由Yandex开发），它特别强调处理分类特征并在少量超参数调整下实现高性能。
- en: NGBoost—at a very high level, this model introduces uncertainty estimation into
    the gradient boosting by using the natural gradient.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NGBoost——从非常高的层次看，这个模型通过使用自然梯度将不确定性估计引入梯度提升中。
- en: Histogram-based gradient boosting—a variant of gradient boosted trees available
    in `scikit-learn` and inspired by LightGBM. They accelerate the training procedure
    by discretizing (binning) the continuous features into a predetermined number
    of unique values.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于直方图的梯度提升——一种在 `scikit-learn` 中提供的梯度提升树变体，灵感来源于LightGBM。通过将连续特征离散化（分箱）为预定数量的唯一值，它们加速了训练过程。
- en: While some algorithms have introduced certain features first, the other popular
    implementations of gradient boosted trees often receive those as well. An example
    might be the histogram-based approach to discretizing continuous features. While
    it was introduced in LightGBM, it was later added to XGBoost as well. The same
    goes for the leaf-wise approach to growing trees.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些算法首先引入了某些特性，但其他流行的梯度提升树实现通常也会采用这些特性。例如，基于直方图的连续特征离散化方法。虽然它是在LightGBM中引入的，但后来也被添加到了XGBoost中。对于生长树的叶子方向方法也是如此。
- en: See also
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'We present additional resources on the algorithms mentioned in this recipe:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了更多关于本食谱中提到的算法的资源：
- en: 'Breiman, L. 2001\. “Random Forests.” *Machine Learning* 45(1): 5–32.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Breiman, L. 2001\. “随机森林。” *机器学习* 45(1): 5–32。'
- en: 'Chen, T., & Guestrin, C. 2016, August. Xgboost: A scalable tree boosting system.
    In *Proceedings of the 22nd international conference on knowledge discovery and
    data mining*, 785–794\. ACM.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen, T., & Guestrin, C. 2016年8月。Xgboost：一个可扩展的树提升系统。在 *第22届国际知识发现与数据挖掘会议论文集*，785–794。ACM。
- en: 'Duan, T., Anand, A., Ding, D. Y., Thai, K. K., Basu, S., Ng, A., & Schuler,
    A. 2020, November. Ngboost: Natural gradient boosting for probabilistic prediction.
    In *International Conference on Machine Learning*, 2690–2700\. PMLR.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan, T., Anand, A., Ding, D. Y., Thai, K. K., Basu, S., Ng, A., & Schuler,
    A. 2020年11月。Ngboost：用于概率预测的自然梯度提升。在 *国际机器学习会议*，2690–2700。PMLR。
- en: 'Freund, Y., & Schapire, R. E. 1996, July. Experiments with a new boosting algorithm.
    In *International Conference on Machine Learning*, 96: 148–156.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freund, Y., & Schapire, R. E. 1996年7月。关于一种新提升算法的实验。在 *国际机器学习会议*，96：148–156。
- en: Freund, Y., & Schapire, R. E. 1997\. “A decision-theoretic generalization of
    on-line learning and an application to boosting.” *Journal of Computer and System
    Sciences*, 55(1), 119–139.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freund, Y., & Schapire, R. E. 1997\. “在线学习的决策理论推广及其在提升中的应用。” *计算机与系统科学学报*，55(1)，119–139。
- en: 'Friedman, J. H. 2001\. “Greedy function approximation: a gradient boosting
    machine.” *Annals of Statistics*, 29(5): 1189–1232.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Friedman, J. H. 2001\. “贪婪函数逼近：一种梯度提升机。” *统计年鉴*，29(5): 1189–1232。'
- en: 'Friedman, J. H. 2002\. “Stochastic gradient boosting.” *Computational Statistics
    & Data Analysis*, 38(4): 367–378.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Friedman, J. H. 2002\. “随机梯度提升。” *计算统计与数据分析*，38(4): 367–378。'
- en: 'Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ... & Liu, T. Y.
    2017\. “Lightgbm: A highly efficient gradient boosting decision tree.” In *Neural
    Information Processing Systems*.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ... & Liu, T. Y. 2017\.
    “Lightgbm：一个高效的梯度提升决策树。” 在 *神经信息处理系统*。
- en: 'Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. 2018\.
    CatBoost: unbiased boosting with categorical features. In *Neural information
    Processing Systems*.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. 2018\.
    CatBoost：具有分类特征的无偏提升。在 *神经信息处理系统*。
- en: Exploring alternative approaches to encoding categorical features
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索编码分类特征的替代方法
- en: In the previous chapter, we introduced one-hot encoding as the standard solution
    for encoding categorical features so that they can be understood by ML algorithms.
    To recap, one-hot encoding converts categorical variables into several binary
    columns, where a value of 1 indicates that the row belongs to a certain category,
    and a value of 0 indicates otherwise.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们介绍了独热编码（one-hot encoding）作为编码分类特征的标准解决方案，使得机器学习算法能够理解这些特征。回顾一下，独热编码将分类变量转换为多个二进制列，其中值为1表示该行属于某个类别，值为0则表示不属于。
- en: The biggest drawback of that approach is the quickly expanding dimensionality
    of our dataset. For example, if we had a feature indicating from which of the
    US states the observation originates, one-hot encoding of this feature would result
    in the creation of 50 (or 49 if we dropped the reference value) new columns.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的最大缺点是数据集维度迅速扩展。例如，如果我们有一个特征表示观察数据来自美国的哪个州，那么对该特征进行独热编码将会创建50个新列（如果去掉参考值，则为49列）。
- en: 'Some other issues with one-hot encoding include:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用独热编码的其他问题包括：
- en: Creating that many Boolean features introduces sparsity to the dataset, which
    decision trees don’t handle well.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建这么多布尔特征会给数据集引入稀疏性，而决策树对此处理不佳。
- en: Decision trees’ splitting algorithm treats all the one-hot-encoded dummies as
    independent features. It means that when a tree makes a split using one of the
    dummy variables, the gain in purity per split is small. Thus, the tree is not
    likely to select one of the dummy variables closer to its root.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的分裂算法将所有的独热编码虚拟变量视为独立特征。这意味着当决策树使用其中一个虚拟变量进行分裂时，每次分裂的纯度增益较小。因此，决策树不太可能在接近根节点时选择某个虚拟变量。
- en: Connected to the previous point, continuous features will have higher feature
    importance than one-hot encoding dummy variables, as a single dummy can only bring
    a fraction of its respective categorical feature’s total information into the
    model.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与前一点相关，连续特征的特征重要性通常高于独热编码的虚拟变量，因为一个虚拟变量最多只能将其对应的分类特征的部分信息引入模型。
- en: Gradient boosted trees don’t handle high-cardinality features well, as the base
    learners have limited depth.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升树（Gradient Boosted Trees）不擅长处理高基数特征，因为基本学习器的深度有限。
- en: When dealing with a continuous variable, the splitting algorithm induces an
    ordering of the samples and can split that ordered list anywhere. A binary feature
    can only be split in one place, while a categorical feature with *k* unique categories
    can be split in ![](../Images/B18112_14_001.png) ways.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 处理连续变量时，分裂算法会对样本进行排序，并且可以在任何位置对这个排序后的列表进行分裂。而二进制特征只能在一个地方进行分裂，具有*k*个唯一类别的分类特征则可以有![](../Images/B18112_14_001.png)种分裂方式。
- en: 'We illustrate the advantage of the continuous features with an example. Assume
    that the splitting algorithm splits a continuous feature at a value of 10 into
    two groups: “below 10” and “10 and above.” In the next split, it can further split
    any of the two groups, for example, “below 6” and “6 and above.” That is not possible
    for a binary feature, as we can at most use it to split the groups once into “yes”
    or “no” groups. *Figure 14.6* illustrates potential differences between decision
    trees created with or without one-hot encoding.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个示例来说明连续特征的优势。假设分裂算法将一个连续特征在值为10的位置进行分裂，分成两组：“小于10”和“大于等于10”。在下一次分裂时，它可以进一步分裂这两组中的任意一组，例如，“小于6”和“大于等于6”。而对于二进制特征来说，这是不可能的，因为我们最多只能用它将数据分成“是”或“否”两组。*图14.6*展示了使用或不使用独热编码所创建的决策树之间可能的差异。
- en: '![](../Images/B18112_14_06.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_06.png)'
- en: 'Figure 14.6: Example of a dense decision tree without one-hot encoding (on
    the left) and a sparse decision tree with one-hot encoding (on the right)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：没有独热编码的密集决策树（左）和具有独热编码的稀疏决策树（右）示例
- en: Those drawbacks, among others, led to the development of a few alternative approaches
    to encoding categorical features. In this recipe, we introduce three of them.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这些缺点，以及其他一些因素，促使了几种替代分类特征编码方法的发展。在本节中，我们将介绍其中的三种方法。
- en: 'The first one is called **target encoding** (also known as mean encoding).
    In this approach, the following transformation is applied to a categorical feature,
    depending on the type of the target variable:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法称为**目标编码**（也叫均值编码）。在这种方法中，针对分类特征应用如下转换，具体取决于目标变量的类型：
- en: Categorical target—a feature is replaced with a blend of the posterior probability
    of the target given a certain category and the prior probability of the target
    over all the training data.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别目标——某个特征会被替换为在给定特定类别下目标的后验概率与所有训练数据中目标的先验概率的混合。
- en: Continuous target—a feature is replaced with a blend of the expected value of
    the target given a certain category and the expected value of the target over
    all the training data.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续目标——某个特征会被替换为在给定特定类别下目标的期望值与所有训练数据中目标的期望值的混合。
- en: In practice, the simplest scenario assumes that each category in the feature
    is replaced with the mean of the target value for that category. *Figure 14.7*
    illustrates this.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 实际应用中，最简单的情况假设每个特征中的类别都会被该类别目标值的均值替换。*图 14.7* 展示了这一点。
- en: '![](../Images/B18112_14_07.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_07.png)'
- en: 'Figure 14.7: Example of target encoding'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7：目标编码示例
- en: Target encoding results in a more direct representation of the relationship
    between the categorical feature and the target, while not adding any new columns.
    That is why it is a very popular technique in data science competitions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 目标编码能更直接地表示类别特征与目标之间的关系，同时不会添加任何新列。这也是它在数据科学竞赛中非常流行的原因。
- en: 'Unfortunately, it is not a silver bullet to encoding categorical features and
    comes with its disadvantages:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，它并不是编码类别特征的万灵药，并且带有一些缺点：
- en: The approach is very prone to overfitting. That is why it assumes blending/smoothing
    of the category mean with the global mean. We should be especially cautious when
    some categories are very infrequent.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法非常容易过拟合。因此，它假设类别均值与全局均值的混合/平滑。特别是当某些类别非常罕见时，我们应该特别小心。
- en: Connected to the risk of overfitting, we are effectively leaking target information
    into the features.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这与过拟合的风险相关，我们实际上是在将目标信息泄露到特征中。
- en: In practice, target encoding works quite well when we have high-cardinality
    features and are using some form of gradient boosted trees as our machine learning
    model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 实际应用中，当我们拥有高基数特征并且使用某种形式的梯度提升树作为机器学习模型时，目标编码效果相当好。
- en: The second approach we cover is called **Leave One Out Encoding** (**LOOE**)
    and it is very similar to target encoding. It attempts to reduce overfitting by
    excluding the current row’s target value when calculating the average of the category.
    This way, the algorithm avoids row-wise leakage. Another consequence of this approach
    is that the same category in multiple observations can have a different value
    in the encoded column. *Figure 14.8* illustrates this.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的第二种方法叫做**留一法编码**（**Leave One Out Encoding, LOOE**），它与目标编码非常相似。它通过在计算类别平均值时排除当前行的目标值来尝试减少过拟合。这样，算法就避免了按行泄露。这个方法的另一个结果是，相同类别在多个观察值中可以在编码列中具有不同的值。*图
    14.8* 展示了这一点。
- en: '![](../Images/B18112_14_08.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_08.png)'
- en: 'Figure 14.8: Example of Leave One Out Encoding'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8：留一法编码示例
- en: With LOOE, the ML model is exposed not only to the same value for each encoded
    category (as in target encoding) but to a range of values. That is why it should
    learn to generalize better.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LOOE，机器学习模型不仅会接触到每个编码类别的相同值（如目标编码中那样），还会接触到一系列值。这就是为什么它应该学会更好地泛化。
- en: The last of the considered encodings is called **Weight of Evidence** (**WoE**)
    encoding. This one is especially interesting, as it originates from the credit
    scoring world, where it was employed to improve the probability of default estimates.
    It was used to separate customers who defaulted on the loan from those who paid
    it back successfully.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后我们讨论的编码方法叫做**证据权重**（**Weight of Evidence, WoE**）编码。这种方法特别有趣，因为它起源于信用评分领域，在那里它被用来提高违约概率估算。它被用来区分违约客户与成功偿还贷款的客户。
- en: Weight of Evidence evolved from logistic regression. Another useful metric with
    the same origin as WoE is called **Information Value** (**IV**). It measures how
    much information a feature provides for the prediction. To put it a bit differently,
    it helps rank variables based on their importance in the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 证据权重（Weight of Evidence, WoE）源自逻辑回归。与 WoE 来源相同的另一个有用指标叫做**信息值**（**Information
    Value, IV**）。它衡量一个特征为预测提供了多少信息。换句话说，它帮助根据特征在模型中的重要性对变量进行排序。
- en: 'The weight of evidence indicates the predictive power of an independent variable
    in relation to the target. In other words, it measures how much the evidence supports
    or undermines a hypothesis. It is defined as the natural logarithm of the odds
    ratio:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 证据权重表示独立变量相对于目标变量的预测能力。换句话说，它衡量证据在多大程度上支持或削弱一个假设。它定义为赔率比的自然对数：
- en: '![](../Images/B18112_14_002.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_002.png)'
- en: '*Figure 14.9* illustrates the calculations.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14.9* 说明了计算过程。'
- en: '![](../Images/B18112_14_09.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_09.png)'
- en: 'Figure 14.9: Example of the WoE encoding'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9：WoE编码示例
- en: The fact that the encoding originates from credit scoring does not mean that
    it is only usable in such cases. We can generalize the good customers as the non-event
    or negative class, and the bad customers as the event or positive class. One of
    the restrictions of the approach is that, in contrast to the previous two, it
    can only be used with a binary categorical target.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管编码源于信用评分，但这并不意味着它只能在类似情况下使用。我们可以将优秀客户视为非事件或负类，而将差的客户视为事件或正类。该方法的一个限制是，与前两者不同，它只能用于二元类别目标。
- en: 'WoE was also historically used to encode categorical features as well. For
    example, in a credit scoring dataset, we could bin a continuous feature like age
    into discrete bins: 20–29, 30–39, 40–49, and so on, and only then calculate the
    WoE for those categories. The number of bins chosen for the encoding depends on
    the use case and the feature’s distribution.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: WoE（证据权重）在历史上也用于编码类别特征。例如，在信用评分数据集中，我们可以将连续特征如年龄分箱为离散区间：20-29岁、30-39岁、40-49岁，依此类推，然后计算这些类别的WoE。选择多少个区间用于编码，取决于具体应用和特征的分布情况。
- en: In this recipe, we show how to use those three encoders in practice using the
    default dataset we have already used before.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本篇教程中，我们将展示如何在实践中使用这三种编码器，使用我们之前已经用过的默认数据集。
- en: Getting ready
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: In this recipe, we use the pipeline we have used in the previous recipes. As
    the estimator, we use the Random Forest classifier. For your convenience, we reiterate
    all the required steps in the Jupyter notebook accompanying this chapter.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本篇教程中，我们使用之前教程中使用的管道。作为估算器，我们使用随机森林分类器。为了方便起见，我们在本章附带的Jupyter笔记本中重述了所有必要步骤。
- en: The Random Forest pipeline with one-hot encoded categorical features resulted
    in the test set’s recall of `0.3542`. We will try to improve upon this score with
    alternative approaches to encoding categorical features.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用独热编码的随机森林管道在测试集上的召回率为`0.3542`。我们将尝试通过其他编码方法来提高这个分数。
- en: How to do it…
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Execute the following steps to fit the ML pipelines with various categorical
    encoders:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以使用不同的类别编码器拟合机器学习管道：
- en: 'Import the libraries:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE5]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Fit the pipeline using target encoding:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用目标编码拟合管道：
- en: '[PRE6]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下图表：
- en: '![](../Images/B18112_14_10.png)'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_10.png)'
- en: 'Figure 14.10: Performance evaluation of the pipeline with target encoding'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.10：使用目标编码进行管道性能评估
- en: The recall obtained using this pipeline is equal to `0.3677`. This improves
    the score by slightly over 1 p.p.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用此管道获得的召回率为`0.3677`。这使得分数提高了略超过1个百分点。
- en: 'Fit the pipeline using Leave One Out Encoding:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用“留一编码”拟合管道：
- en: '[PRE7]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下图表：
- en: '![](../Images/B18112_14_11.png)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_11.png)'
- en: 'Figure 14.11: Performance evaluation of the pipeline with Leave One Out Encoding'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.11：使用“留一编码”进行管道性能评估
- en: The recall obtained using this pipeline is equal to `0.1462`, which is significantly
    worse than the target encoding approach.
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用此管道获得的召回率为`0.1462`，明显低于目标编码方法。
- en: 'Fit the pipeline using Weight of Evidence encoding:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用证据权重编码拟合管道：
- en: '[PRE8]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下图表：
- en: '![](../Images/B18112_14_12.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_12.png)'
- en: 'Figure 14.12: Performance evaluation of the pipeline with Weight of Evidence
    encoding'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.12：使用证据权重编码进行管道性能评估
- en: The recall obtained using this pipeline is equal to `0.3708`, which is a small
    improvement over target encoding.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此管道获得的召回率为`0.3708`，相较于目标编码有小幅提升。
- en: How it works…
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: First, we executed the code from the *Getting ready* section, that is, instantiated
    the pipeline with one-hot encoding and Random Forest as the classifier.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们执行了*准备工作*部分的代码，即实例化了一个使用独热编码和随机森林作为分类器的管道。
- en: After importing the libraries, we cloned the entire pipeline using the `clone`
    function. Then, we used the `set_params` method to replace the `OneHotEncoder`
    with `TargetEncoder`. Just as when tuning the hyperparameters of a pipeline, we
    had to use the same double underscore notation to access the particular element
    of the pipeline. The encoder was located under `preprocessor__categorical__cat_encoding`.
    Then, we fitted the pipeline using the `fit` method and printed the evaluation
    scores using the `performance_evaluation_report` helper function.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入库后，我们使用`clone`函数克隆了整个管道。然后，我们使用`set_params`方法将`OneHotEncoder`替换为`TargetEncoder`。正如调优管道的超参数时，我们必须使用相同的双下划线表示法来访问管道中的特定元素。编码器位于`preprocessor__categorical__cat_encoding`下。接着，我们使用`fit`方法拟合管道，并通过`performance_evaluation_report`辅助函数打印评估结果。
- en: 'As we have mentioned in the introduction, target encoding is prone to overfitting.
    That is why instead of simply replacing the categories with the corresponding
    averages, the algorithm is capable of blending the posterior probabilities with
    the prior probability (global average). We can control the blending with two hyperparameters:
    `min_samples_leaf` and `smoothing`.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在介绍中提到的，目标编码容易导致过拟合。这就是为什么算法不仅仅用相应的平均值替换类别，而是能够将后验概率与先验概率（全局平均）结合起来的原因。我们可以通过两个超参数来控制这种混合：`min_samples_leaf`
    和 `smoothing`。
- en: In *Steps 3* and *4*, we followed the very same steps as with target encoding,
    but we replaced the encoder with `LeaveOneOutEncoder` and `WOEEncoder` respectively.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 3*和*4*中，我们按照与目标编码相同的步骤操作，但分别将编码器替换为`LeaveOneOutEncoder`和`WOEEncoder`。
- en: Just as with target encoding, the other encoders use the target to build the
    encoding and are thus prone to overfitting. Fortunately, they also offer certain
    measures to prevent that from happening.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 和目标编码一样，其他编码器也使用目标来构建编码，因此也容易出现过拟合。幸运的是，它们也提供了一些防止过拟合的措施。
- en: In the case of LOOE, we can add normally distributed noise to the encodings
    in order to reduce overfitting. We can control the standard deviation of the Normal
    distribution used for generating the noise with the `sigma` argument. It is worth
    mentioning that the random noise is added to the training data only, and the transformation
    of the test set is not impacted. Just by adding the random noise to our pipeline
    (`sigma = 0.05`), we can improve the measured recall score from `0.1462` to around
    `0.35` (depending on random number generation).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在LOOE的情况下，我们可以向编码中添加正态分布噪声以减少过拟合。我们可以通过`sigma`参数控制用于生成噪声的正态分布的标准差。值得一提的是，随机噪声仅添加到训练数据中，测试集的转换不受影响。仅通过向我们的管道中添加随机噪声（`sigma
    = 0.05`），我们可以将测量的召回率从`0.1462`提高到大约`0.35`（具体取决于随机数生成）。
- en: Similarly, we can add random noise for the WoE encoder. We control the noise
    with the `randomized` (Boolean flag) and `sigma` (standard deviation of the Normal
    distribution) arguments. Additionally, there is the `regularization` argument,
    which prevents errors caused by division by zero.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以为WoE编码器添加随机噪声。我们通过`randomized`（布尔标志）和`sigma`（正态分布的标准差）参数来控制噪声。此外，还有`regularization`参数，它可以防止由于除零错误而导致的错误。
- en: There’s more…
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: Encoding categorical variables is a very broad area of active research, and
    every now and then new approaches to it are being published. Before changing the
    topic, we would also like to discuss a couple of related concepts.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 编码分类变量是一个非常广泛的活跃研究领域，不时会发布新的方法。在切换主题之前，我们还想讨论一些相关概念。
- en: Handling data leakage with k-fold target encoding
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用k折目标编码处理数据泄露
- en: We have already mentioned a few approaches to reducing the overfitting problem
    of the target encoder. A very popular solution among Kaggle practitioners is to
    use *k*-fold target encoding. The idea is similar to k-fold cross-validation and
    it allows us to use all the training data we have. We start by dividing the data
    into *k* folds—they can be stratified or purely random, depending on the use case.
    Then, we replace the observations present in the *l*-th fold with the target’s
    mean calculated using all the folds except the *l*-th one. This way, we are not
    leaking the target from the observations within the same fold.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到了一些减少目标编码器过拟合问题的方法。Kaggle 从业者中非常流行的一个解决方案是使用 *k* 折目标编码。这个想法类似于 k 折交叉验证，它允许我们使用所有可用的训练数据。我们首先将数据划分为
    *k* 个折叠——这些折叠可以是分层的，也可以是完全随机的，具体取决于应用场景。然后，我们用除第 *l* 个折叠之外的所有折叠计算出的目标均值来替换第 *l*
    个折叠中的观察值。这样，我们就避免了同一折叠中目标泄漏的问题。
- en: An inquisitive reader might have noticed that the LOOE is a special case of
    *k*-fold target encoding, in which *k* is equal to the number of observations
    in the training dataset.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 一位好奇的读者可能已经注意到，LOOE 是 *k* 折目标编码的一种特殊情况，其中 *k* 等于训练数据集中的观察数量。
- en: Even more encoders
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多编码器
- en: 'The `category_encoders` library offers almost 20 different encoding transformers
    for categorical features. Aside from the ones we have already mentioned, you might
    want to explore the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`category_encoders` 库提供了近 20 种不同的分类特征编码转换器。除了我们已经提到的编码器外，你还可以探索以下内容：'
- en: '**Ordinal encoding**—very similar to label encoding; however, it ensures that
    the encoding retains the ordinal nature of the feature. For example, the hierarchy
    of bad < neutral < good is preserved.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有序编码**——与标签编码非常相似；然而，它确保编码保留特征的有序性质。例如，坏 < 中立 < 好的层级关系被保留下来。'
- en: '**Count encoder** (frequency encoder)—each category of a feature is mapped
    to the number of observations belonging to that category.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计数编码器**（频率编码器）——将特征的每个类别映射到属于该类别的观察数。'
- en: '**Sum encoder**—compares the mean of the target for a given category to the
    overall average of the target.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总和编码器**——将给定类别的目标均值与目标的总体均值进行比较。'
- en: '**Helmert encoder**—compares the mean of a certain category to the mean of
    the subsequent levels. If we had categories [A, B, C], the algorithm would first
    compare A to B and C and then B to C alone. This kind of encoding is useful in
    situations in which the levels of the categorical feature are ordered, for example,
    from lowest to highest.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Helmert 编码器**——将某个类别的均值与后续级别的均值进行比较。如果我们有类别 [A, B, C]，算法会先比较 A 与 B 和 C，然后再比较
    B 与 C。此种编码在类别特征的级别有顺序的情况下非常有用，例如从低到高的顺序。'
- en: '**Backward difference encoder**—similar to the Helmert encoder, with the difference
    that it compares the mean of the current category to the mean of the previous
    one.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向差异编码器**——类似于 Helmert 编码器，不同之处在于它将当前类别的均值与前一个类别的均值进行比较。'
- en: '**M-estimate encoder**—a simplified version of the target encoder, which has
    only one tunable parameter (responsible for the strength of regularization).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**M-估计编码器**——目标编码器的简化版本，只有一个可调参数（负责正则化强度）。'
- en: '**James-Stein encoder**—a variant of target encoding that aims to improve theestimation
    of the category’s mean by shrinking it toward the central/global mean. Its single
    hyperparameter is responsible for the strength of shrinkage (this means the same
    as regularization in this context)—the bigger the value of the hyperparameter,
    the bigger the weight of the global mean (which might lead to underfitting). On
    the other hand, reducing the hyperparameter’s value might lead to overfitting.
    The best value is usually determined by cross-validation. The approach’s biggest
    disadvantage is that the James-Stein estimator is defined only for Normal distribution,
    which is not the case for any binary classification problem.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**James-Stein 编码器**——一种目标编码的变体，旨在通过将类别的均值收缩到中心/全局均值来提高估计精度。它的单一超参数控制着收缩的强度（在这个上下文中，这与正则化相同）——超参数值越大，全球均值的权重越大（这可能导致欠拟合）。另一方面，减少超参数值可能会导致过拟合。通常，最佳值是通过交叉验证来确定的。该方法的最大缺点是，James-Stein
    估计器仅适用于正态分布，而这并不适用于任何二元分类问题。'
- en: '**Binary encoder**—converts a category into binary digits and each one is provided
    a separate column. Thanks to this encoding, we generate far fewer columns than
    with OHE. To illustrate, for a categorical feature with 100 unique categories,
    binary encoding just needs to create 7 features, instead of 100 in the case of
    OHE.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二进制编码器**—将类别转换为二进制数字，每个数字都有一个单独的列。得益于这种编码方法，我们生成的列数远少于 OHE。例如，对于一个具有 100
    个独特类别的类别特征，二进制编码只需创建 7 个特征，而 OHE 则需要 100 个。'
- en: '**Hashing encoder**—uses a hashing function (often used in data encryption)
    to transform the categorical features. The outcome is similar to OHE, but with
    fewer features (we can control that with the encoder’s hyperparameters). It has
    two significant disadvantages. First, the encoding results in information loss,
    as the algorithm transforms the full set of available categories into fewer features.
    The second issue is called collision and it occurs as we are transforming a potentially
    high number of categories into a smaller set of features. Then, different categories
    could be represented by the same hash values.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**哈希编码器**—使用哈希函数（通常用于数据加密）来转换类别特征。其结果与 OHE 相似，但特征更少（我们可以通过编码器的超参数来控制这一点）。它有两个显著的缺点。首先，编码会导致信息丢失，因为算法将所有可用类别转换为更少的特征。第二个问题称为碰撞，发生在我们将潜在的大量类别转换为较小的特征集时。此时，不同的类别可能会被相同的哈希值表示。'
- en: '**Catboost encoder**—an improved variant of Leave One Out Encoding, which aims
    to overcome the issues of target leakage.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Catboost 编码器**—一种改进的 Leave One Out 编码变种，旨在克服目标泄漏问题。'
- en: See also
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Micci-Barreca, D. 2001\. “A preprocessing scheme for high-cardinality categorical
    attributes in classification and prediction problems.” *ACM SIGKDD Explorations
    Newsletter* 3(1): 27–32.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Micci-Barreca, D. 2001. “分类与预测问题中高基数类别属性的预处理方案。” *ACM SIGKDD Explorations Newsletter*
    3(1): 27–32.'
- en: Investigating different approaches to handling imbalanced data
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查处理不平衡数据的不同方法
- en: A very common issue when working with classification tasks is that of **class
    imbalance**, that is, when one class is highly outnumbered in comparison to the
    second one (this can also be extended to multi-class cases). In general, we are
    dealing with imbalance when the ratio of the two classes is not 1:1\. In some
    cases, a delicate imbalance is not that big of a problem, but there are industries/problems
    in which we can encounter ratios of 100:1, 1000:1, or even more extreme.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理分类任务时，一个非常常见的问题是**类别不平衡**，即当一个类别的样本数量远少于另一个类别时（这也可以扩展到多类别问题）。通常，当两个类别的比例不是
    1:1 时，我们就面临不平衡问题。在某些情况下，轻微的不平衡并不是大问题，但在一些行业或问题中，我们可能会遇到 100:1、1000:1 或甚至更极端的比例。
- en: Dealing with highly imbalanced classes can result in the poor performance of
    ML models. That is because most of the algorithms implicitly assume balanced distribution
    of classes. They do so by aiming to minimize the overall prediction error, to
    which the minority class by definition contributes very little. As a result, classifiers
    trained on imbalanced data are biased toward the majority class.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 处理高度不平衡的类别可能导致机器学习模型的性能较差。这是因为大多数算法隐式地假设类别分布是平衡的。它们通过旨在最小化总体预测误差来实现这一点，而根据定义，少数类对总体误差的贡献非常小。因此，在不平衡数据上训练的分类器会偏向多数类。
- en: One of the potential solutions to dealing with class imbalance is to resample
    the data. On a high level, we can either undersample the majority class, oversample
    the minority class, or combine the two approaches. However, that is just the general
    idea. There are many ways to approach resampling and we describe a few selected
    methods below.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 解决类别不平衡的潜在解决方案之一是对数据进行重采样。总体而言，我们可以对多数类进行欠采样，对少数类进行过采样，或者将这两种方法结合起来。然而，这只是一个大致的思路。实际上，有很多处理重采样的方法，下面我们描述了几种常见的方法。
- en: When working with resampling techniques, we only resample the training data!
    The test data stays intact.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用重采样技术时，我们只对训练数据进行重采样！测试数据保持不变。
- en: '![](../Images/B18112_14_13.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_13.png)'
- en: 'Figure 14.13: Undersampling of the majority class and oversampling of the minority
    class'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.13：多数类的欠采样与少数类的过采样
- en: The simplest approach to undersampling is called **random undersampling**. In
    this approach, weundersample the majority class, that is, draw random samples
    (by default, without replacement) from the majority class until the classes are
    balanced (with a ratio of 1:1 or any other desired ratio). The biggest issue of
    this method is the information loss caused by discarding vast amounts of data,
    often the majority of the entire training dataset. As a result, a model trained
    on undersampled data can achieve lower performance. Another possible implication
    is a biased classifier with an increased number of false positives, as the distribution
    of the training and test sets is not the same after resampling.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的欠采样方法称为**随机欠采样**。在这种方法中，我们对多数类进行欠采样，也就是说，从多数类中随机抽取样本（默认情况下，无放回抽样），直到类别平衡（比例为1:1或其他所需的比例）。该方法最大的问题是由于丢弃大量数据（通常是整个训练数据集的大部分）而导致信息丢失。因此，在欠采样数据上训练的模型可能会表现得较差。另一个可能的影响是分类器偏向，导致更多的假阳性，因为重采样后训练集和测试集的分布不一致。
- en: Analogically, the simplest approach to oversampling is called **random oversampling**.In
    this approach, we sample multiple times with replacement from the minority class,
    until the desired ratio is achieved. This method often outperforms random undersampling,
    as there is no information loss caused by discarding training data. However, random
    oversampling comes with the danger of overfitting, caused by replicating observations
    from the minority class.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，最简单的过采样方法称为**随机过采样**。在这种方法中，我们从少数类中进行多次有放回的抽样，直到达到期望的比例。该方法通常比随机欠采样效果更好，因为没有因丢弃训练数据而导致信息丢失。然而，随机过采样存在过拟合的风险，因为它通过复制少数类的观察值来增加数据。
- en: '**Synthetic Minority Oversampling Technique** (**SMOTE**) is a more advanced
    oversampling algorithm that creates new, synthetic observations from the minority
    class. This way, it overcomes the previously mentioned problem of overfitting.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**合成少数类过采样技术**（**SMOTE**）是一种更先进的过采样算法，它通过少数类创建新的合成观察值。通过这种方式，它克服了前面提到的过拟合问题。'
- en: To create the synthetic samples, the algorithm picks an observation from the
    minority class, identifies its *k-*nearest neighbors (using the *k*-NN algorithm),
    and then creates new observations on the lines connecting (interpolating) the
    observation to the nearest neighbors. Then, the process is repeated for other
    minority observations until the classes are balanced.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建合成样本，算法从少数类中选取一个观察值，识别其*k*-最近邻（使用*k*-NN算法），然后在连接（插值）观察值和最近邻的线段上创建新的观察值。然后，该过程会重复进行，直到其他少数类观察值的样本也得到平衡。
- en: Aside from reducing the problem of overfitting, SMOTE causes no loss of information,
    as it does not discard observations belonging to the majority class. However,
    SMOTE can accidentally introduce more noise to the data and cause overlapping
    of classes. This is because while creating the synthetic observations, it does
    not take into account the observations from the majority class. Additionally,
    the algorithm is not very effective for high-dimensional data (due to the curse
    of dimensionality). Lastly, the basic variant of SMOTE is only suitable for numerical
    features. However, SMOTE’s extensions (mentioned in the *There’s more…* section)
    can handle categorical features as well.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 除了减少过拟合问题外，SMOTE不会丢失任何信息，因为它不会丢弃属于多数类的观察值。然而，SMOTE可能会无意中向数据中引入更多噪声，并导致类别重叠。这是因为在创建合成观察值时，它没有考虑到多数类的观察值。此外，该算法在高维数据上效果不佳（由于维度灾难）。最后，SMOTE的基本变种仅适用于数值特征。然而，SMOTE的扩展（在*更多内容...*部分提到）可以处理分类特征。
- en: The last of the considered oversampling techniques is called **Adaptive Synthetic
    Sampling** (**ADASYN**) and it is a modification of the SMOTE algorithm. In ADASYN,
    the number of observations to be created for a certain minority point is determined
    by a density distribution (instead of a uniform weight for all points, as in SMOTE).
    This is how ADASYN’s adaptive nature enables it to generate more synthetic samples
    for observations that come from hard-to-learn neighborhoods. For example, a minority
    observation is hard to learn if there are many majority class observations with
    very similar feature values. It is easier to imagine that scenario in the case
    of only two features. Then, in a scatterplot, such a minority class observation
    might simply be surrounded by many of the majority class observations.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑的最后一种过采样技术叫做**自适应合成采样**（**ADASYN**），它是SMOTE算法的一种改进。在ADASYN中，为某个少数类点创建的观测值数量是由密度分布决定的（而不是像SMOTE中那样为所有点提供统一的权重）。这种自适应特性使得ADASYN能够为来自难以学习的邻域的观测值生成更多的合成样本。例如，如果存在许多与少数类观察值特征值非常相似的多数类观察值，则该少数类观察值会变得难以学习。我们可以通过仅考虑两个特征的情况来更容易地理解这种情况。在散点图中，这样的少数类观察值可能会被许多多数类观察值包围。
- en: 'There are two additional elements worth mentioning:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两个额外的要点值得提及：
- en: In contrast to SMOTE, the synthetic points are not limited to linear interpolation
    between two points. They can also lie on a plane created by three or more observations.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与SMOTE不同，合成点并不局限于两点之间的线性插值。它们还可以位于由三个或更多观测值创建的平面上。
- en: After creating the synthetic observations, the algorithm adds a small random
    noise to increase the variance, thus making the samples more realistic.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在创建合成观测值后，算法会加入少量随机噪声以增加方差，从而使得样本更加真实。
- en: 'Potential drawbacks of ADASYN include:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ADASYN的潜在缺点包括：
- en: A possible decrease in precision (more false positives) of the algorithm caused
    by its adaptability. This means that the algorithm might generate more observations
    in the areas with high numbers of observations from the majority class. Such synthetic
    data might be very similar to those majority class observations, potentially resulting
    in more false positives.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其自适应性，算法的精度可能会下降（产生更多的假阳性）。这意味着该算法可能会在具有大量多数类观测值的区域生成更多观测值。这些合成数据可能与多数类观测值非常相似，从而可能导致更多的假阳性。
- en: Struggling with sparsely distributed minority observations. Then, a neighborhood
    can contain only one or very few points.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理稀疏分布的少数类观测值时，某些邻域可能仅包含一个或极少数的点。
- en: Resampling is not the only potential solution to the problem of imbalanced classes.
    Another one is based on adjusting the class weights, thus putting more weight
    on the minority class. In the background, the class weights are incorporated into
    calculating the loss function. In practice, this means that misclassifying observations
    from the minority class increases the value of the loss function significantly
    more than in the case of misclassifying the observations from the majority class.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 重采样并不是解决类别不平衡问题的唯一潜在方案。另一个方法是基于调整类别权重，从而增加少数类的权重。在后台，类别权重会被纳入到损失函数的计算中。实际上，这意味着将少数类观测值分类错误会显著增加损失函数的值，而多数类观测值分类错误的影响则较小。
- en: In this recipe, we show an example of a credit card fraud problem, where the
    fraudulent class is observed in only 0.17% of the entire sample. In such cases,
    gathering more data (especially of the fraudulent class) might simply not be feasible,
    and we need to resort to other techniques that can help us in improving the models’
    performance.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们展示了一个信用卡欺诈问题的例子，其中欺诈类在整个样本中的比例仅为0.17%。在这种情况下，收集更多的数据（特别是欺诈类数据）可能根本不可行，我们需要依赖其他技术来帮助我们提升模型的性能。
- en: Getting ready
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Before proceeding to the coding part, we provide a brief description of the
    dataset selected for this exercise. You can download the dataset from Kaggle (link
    in the *See also* section).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入编码部分之前，我们简要描述了本次练习中选用的数据集。你可以从Kaggle下载该数据集（链接在*另请参见*部分）。
- en: The dataset contains information about credit card transactions made over a
    period of two days in September 2013 by European cardholders. Due to confidentiality,
    almost all features (28 out of 30) were anonymized by using **Principal Components
    Analysis** (**PCA**). The only two features with clear interpretation are `Time`
    (seconds elapsed between each transaction and the first one in the dataset) and
    `Amount` (the transaction’s amount).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含了2013年9月欧洲持卡人在两天内进行的信用卡交易信息。由于保密原因，几乎所有特征（28个中的30个）都通过使用**主成分分析**（**PCA**）进行了匿名化。唯一两个有明确解释的特征是`Time`（每笔交易与数据集中第一笔交易之间的秒数）和`Amount`（交易金额）。
- en: Lastly, the dataset is highly imbalanced and the positive class is observed
    in 0.173% of all transactions. To be precise, out of 284,807 transactions, 492
    were identified as fraudulent.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据集严重失衡，正类在所有交易中只占0.173%。准确地说，在284,807笔交易中，有492笔被识别为欺诈交易。
- en: How to do it...
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Execute the following steps to investigate different approaches to handling
    class imbalance:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以研究不同处理类别失衡的方法：
- en: 'Import the libraries:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE9]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Load and prepare data:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载和准备数据：
- en: '[PRE10]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using `y.value_counts(normalize=True)` we can confirm that the positive class
    is observed in 0.173% of the observations.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`y.value_counts(normalize=True)`我们可以确认正类在0.173%的观察值中出现。
- en: 'Scale the features using `RobustScaler`:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`RobustScaler`对特征进行缩放：
- en: '[PRE11]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Train the baseline model:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练基准模型：
- en: '[PRE12]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Undersample the training data and train a Random Forest classifier:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练数据进行欠采样，并训练一个随机森林分类器：
- en: '[PRE13]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After random undersampling, the ratio of the classes is as follows: `{0: 394,
    1: 394}`.'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '随机欠采样后，类别的比例如下：`{0: 394, 1: 394}`。'
- en: 'Oversample the training data and train a Random Forest classifier:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练数据进行过采样，并训练一个随机森林分类器：
- en: '[PRE14]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After random oversampling, the ratio of the classes is as follows: `{0: 227451,
    1: 227451}`.'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '随机过采样后，类别的比例如下：`{0: 227451, 1: 227451}`。'
- en: 'Oversample the training data using SMOTE:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SMOTE对训练数据进行过采样：
- en: '[PRE15]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After oversampling with SMOTE, the ratio of the classes is as follows: `{0:
    227451, 1: 227451}`.'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '使用SMOTE过采样后，类别的比例如下：`{0: 227451, 1: 227451}`。'
- en: 'Oversample the training data using ADASYN:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用ADASYN对训练数据进行过采样：
- en: '[PRE16]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After oversampling with ADASYN, the ratio of the classes is as follows: `{0:
    227451, 1: 227449}`.'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '使用ADASYN过采样后，类别的比例如下：`{0: 227451, 1: 227449}`。'
- en: 'Use sample weights in the Random Forest classifier:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在随机森林分类器中使用样本权重：
- en: '[PRE17]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Train the `BalancedRandomForestClassifier`:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练`BalancedRandomForestClassifier`：
- en: '[PRE18]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Train the `BalancedRandomForestClassifier` with balanced classes:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用平衡类别训练`BalancedRandomForestClassifier`：
- en: '[PRE19]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Combine the results in a DataFrame:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果合并到一个DataFrame中：
- en: '[PRE20]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Executing the snippet prints the following table:'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码片段将打印以下表格：
- en: '![](../Images/B18112_14_14.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_14.png)'
- en: 'Figure 14.14: Performance evaluation metrics of the various approaches to dealing
    with imbalanced data'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.14：处理失衡数据的各种方法的性能评估指标
- en: In *Figure 14.14* we can see the performance evaluation of various approaches
    we have tried in this recipe. As we are dealing with a highly imbalanced problem
    (the positive class accounts for 0.17% of all the observations), we can clearly
    observe the case of the **accuracy paradox**. Many models have an accuracy of
    ≈99.9%, but they still fail to detect fraudulent cases, which are the most important
    ones.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图14.14*中，我们可以看到我们在本食谱中尝试的各种方法的性能评估。由于我们面临的是一个严重失衡的问题（正类占所有观察值的0.17%），我们可以清楚地观察到**准确率悖论**的情况。许多模型的准确率约为99.9%，但它们仍然未能检测出欺诈案件，而欺诈案件才是最重要的。
- en: The accuracy paradox refers to a case in which inspecting accuracy as the evaluation
    metric creates the impression of having a very good classifier (a score of 90%,
    or even 99.9%), while in reality it simply reflects the distribution of the classes.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率悖论指的是当将准确率作为评估指标时，会给人留下一个非常好的分类器印象（如90%的得分，甚至是99.9%），而实际上它只是反映了类别的分布情况。
- en: Taking that into consideration, we compare the performance of the models using
    metrics that account for that. While looking at precision, the best performing
    approach is Random Forest with class weights. When considering recall as the most
    important metric, the best performing approach is either undersampling followed
    by a Random Forest model or a Balanced Random Forest model. In terms of the F1
    score, the best approach seems to be the vanilla Random Forest model.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我们使用了考虑类别不平衡的评估指标来比较模型的表现。在查看精确度时，表现最佳的方案是使用类别权重的随机森林。当将召回率作为最重要的评估指标时，表现最好的方法是先进行欠采样然后使用随机森林模型，或者使用平衡随机森林模型。在
    F1 分数方面，最好的方法似乎是原始的随机森林模型。
- en: It is also important to mention that no hyperparameter tuning was performed,
    which could potentially improve the performance of all of the approaches.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要指出的是，本实验中没有进行超参数调优，这可能会提升所有方法的性能。
- en: How it works...
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: After importing the libraries, we loaded the credit card fraud dataset from
    a CSV file. In the same step, we additionally dropped the `Time` feature, separated
    the target from the features using the `pop` method, and created an 80–20 stratified
    train-test split. It is crucial to remember to use stratification when dealing
    with imbalanced classes.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入库后，我们从 CSV 文件加载了信用卡欺诈数据集。在同一步骤中，我们额外删除了`Time`特征，使用`pop`方法将目标与特征分开，并创建了一个
    80-20 的分层训练-测试集拆分。在处理类别不平衡时，记得使用分层抽样非常重要。
- en: In this recipe, we only focused on working with imbalanced data. That is why
    we did not cover any EDA, feature engineering, and so on. As all the features
    were numerical, we did not have to carry out any special encoding.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们仅专注于处理不平衡数据。因此，我们没有涵盖任何探索性数据分析（EDA）、特征工程等内容。由于所有特征都是数值型的，我们不需要进行特殊的编码。
- en: The only preprocessing step we did was to scale all the features using `RobustScaler`.
    While Random Forest does not require explicit feature scaling, some of the rebalancing
    approaches use *k*-NN under the hood. And for such distance-based algorithms,
    the scale does matter. We fitted the scaler using only the training data and then
    transformed both the training and test sets.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的唯一预处理步骤是使用`RobustScaler`对所有特征进行缩放。虽然随机森林不需要显式的特征缩放，但一些重采样方法在底层使用了*k*-最近邻算法。而对于这种基于距离的算法，特征缩放是很重要的。我们只使用训练数据来拟合缩放器，然后对训练集和测试集进行转换。
- en: In *Step 4*, we fitted a vanilla Random Forest model, which we used as a benchmark
    for the more complex approaches.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们拟合了一个原始的随机森林模型，并将其作为更复杂方法的基准。
- en: In *Step 5*, we used the `RandomUnderSampler` class from the `imblearn` library
    to randomly undersample the majority class in order to match the size of the minority
    sample. Conveniently, classes from `imblearn` follow `scikit-learn`'s API style.
    That is why we had to first define the class with the arguments (we only set the
    `random_state`). Then, we applied the `fit_resample` method to obtain the undersampled
    data. We reused the Random Forest object to train the model on the undersampled
    data and stored the results for later comparison.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们使用了`imblearn`库中的`RandomUnderSampler`类，随机欠采样多数类以匹配少数类样本的大小。方便的是，`imblearn`中的类遵循了`scikit-learn`的
    API 风格。因此，我们首先定义了类及其参数（我们只设置了`random_state`）。然后，我们应用了`fit_resample`方法来获得欠采样的数据。我们重新使用了随机森林对象，基于欠采样数据训练模型，并存储了结果以供后续比较。
- en: '*Step 6* is analogical to *Step 5*, with the only difference being the use
    of the `RandomOverSampler` to randomly oversample the minority class in order
    to match the size of the majority class.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 6*与*步骤 5*类似，唯一的区别是使用`RandomOverSampler`来随机过采样少数类，以匹配多数类的样本大小。'
- en: In *Step 7* and *Step 8*, we applied the SMOTE and ADASYN variants of oversampling.
    As the `imblearn` library makes it very easy to apply different sampling methods,
    we will not go deeper into the description of the process.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 7*和*步骤 8*中，我们应用了 SMOTE 和 ADASYN 变体的过采样方法。由于`imblearn`库使得应用不同的采样方法变得非常简单，我们不会深入描述该过程。
- en: In all the mentioned resampling methods, we can actually specify the desired
    ratio between classes by passing a float to the `sampling_strategy` argument.
    The number represents the desired ratio of the number of observations in the minority
    class over the number of observations in the majority class.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有提到的重采样方法中，我们实际上可以通过向`sampling_strategy`参数传递一个浮动值来指定类别之间的期望比例。该数字表示少数类样本与多数类样本的观察数之比。
- en: In *Step 9*, instead of resampling the training data, we used the `class_weight`
    hyperparameter of the `RandomForestClassifier` to account for the class imbalance.
    By passing “`balanced`" , the algorithm automatically assigns weights inversely
    proportional to class frequencies in the training data.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤9*中，我们没有重采样训练数据，而是使用了`RandomForestClassifier`的`class_weight`超参数来解决类别不平衡问题。通过传递`"balanced"`，算法会自动分配与训练数据中类别频率成反比的权重。
- en: There are different possible approaches to using the `class_weight` hyperparameter.
    Passing `"balanced_subsample"` results in a similar weights assignment as in `"balanced"`;
    however, the weights are computed based on the bootstrap sample for every tree.
    Alternatively, we can pass a dictionary containing the desired weights. One way
    of determining the weights can be by using the `compute_class_weight` function
    from `sklearn.utils.class_weight`.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`class_weight`超参数有不同的可能方法。传递`"balanced_subsample"`将得到与`"balanced"`类似的权重分配；然而，权重是基于每棵树的自助样本计算的。或者，我们可以传递一个包含期望权重的字典。一种确定权重的方法是使用`sklearn.utils.class_weight`中的`compute_class_weight`函数。
- en: The `imblearn` library also features some modified versions of popular classifiers.
    In *Steps 10* and *11*, we used a modified Random Forest classifier, that is,
    **Balanced Random Forest**. The difference is that in Balanced Random Forest the
    algorithm randomly undersamples each bootstrapped sample to balance the classes.
    In practical terms, its API is virtually the same as in the vanilla `scikit-learn`
    implementation (including the tunable hyperparameters).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`imblearn`库还提供了一些流行分类器的修改版本。在*步骤10*和*步骤11*中，我们使用了修改过的随机森林分类器，即**平衡随机森林**。不同之处在于，在平衡随机森林中，算法会随机欠采样每个自助样本，以平衡类别。实际上，其API与普通的`scikit-learn`实现几乎相同（包括可调节的超参数）。'
- en: In the last step, we combined all the results into a single DataFrame and displayed
    the results.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们将所有结果合并成一个单独的DataFrame并展示了结果。
- en: There’s more...
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In this recipe, we presented only some of the available resampling methods.
    Below, we list a few more possibilities.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们仅介绍了一些可用的重采样方法。以下是更多的一些可能性。
- en: 'Undersampling:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 欠采样：
- en: '**NearMiss**—the name refers to a collection of undersampling approaches that
    are essentially heuristic rules based on the Nearest Neighbors algorithm. They
    base the selection of the observations from the majority class to keep on the
    distance between the observations from the majority and minority classes. The
    rest is removed in order to balance the classes. For example, the NearMiss-1 method
    selects observations from the majority class that have the smallest average distance
    to the three closest observations from the minority class.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NearMiss**—这个名称指的是一组欠采样方法，基本上是基于最近邻算法的启发式规则。它们基于从多数类和少数类的观察之间的距离来选择要保留的多数类观察。其余的则被删除，以实现类别平衡。例如，NearMiss-1方法选择那些与三个位于少数类的观察点距离最小的多数类观察。'
- en: '**Edited Nearest Neighbors**—this approach removes any majority class observation
    whose class is different from the class of at least two of its three nearest neighbors.
    The underlying idea is to remove the instances from the majority class that are
    near the boundary of classes.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编辑最近邻**—这种方法删除任何多数类的观察，该观察的类别与其三个最近邻中的至少两个的类别不同。其基本思想是删除那些位于类别边界附近的多数类实例。'
- en: '**Tomek links**—in this undersampling heuristic we first identify all the pairs
    of observations that are nearest to each other (they are the nearest neighbors)
    but belong to different classes. Such pairs are called Tomek links. Then, from
    those pairs, we remove the observations that belong to the majority class. The
    underlying idea is that by removing those observations from the Tomek link we
    increase the class separation.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tomek链接**—在这个欠采样启发式方法中，我们首先识别出所有最接近的观察对（它们是最近邻）且属于不同类别的对。这些对称为Tomek链接。然后，从这些对中，我们删除属于多数类的观察。其基本思想是通过从Tomek链接中删除这些观察，我们可以增加类别之间的分离度。'
- en: 'Oversampling:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 过采样：
- en: '**SMOTE-NC** (**Synthetic Minority Oversampling Technique for Nominal and Continuous**)—a
    variant of SMOTE suitable for a dataset containing both numerical and categorical
    features. The vanilla SMOTE can create illogical values for one-hot-encoded features.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SMOTE-NC**（**用于名义和连续特征的合成少数类过采样技术**）—SMOTE的变体，适用于包含数值和类别特征的数据集。普通的SMOTE可能会为独热编码特征创建不合逻辑的值。'
- en: '**Borderline SMOTE**—this variant of the SMOTE algorithm will create new, synthetic
    observations along the decision boundary between the two classes, as those are
    more prone to being misclassified.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边界SMOTE**——这种SMOTE算法的变种会在两个类别之间的决策边界上创建新的合成观察点，因为这些点更容易被错误分类。'
- en: '**SVM SMOTE**—a variant of SMOTE in which an SVM algorithm is used to indicate
    which observations to use for generating new synthetic observations.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SVM SMOTE**——SMOTE的一种变体，使用SVM算法来指示哪些观察点应被用于生成新的合成观察点。'
- en: '**K-means SMOTE**—in this approach, we first apply *k*-means clustering to
    identify clusters with a high proportion of minority class observations. Then,
    the vanilla SMOTE is applied to the selected clusters and each of those clusters
    will have new synthetic observations.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K-means SMOTE**——在这种方法中，我们首先应用*k*-均值聚类来识别具有大量少数类观察点的聚类。然后，将原始SMOTE应用于选定的聚类，每个聚类都会生成新的合成观察点。'
- en: Alternatively, we could combine the undersampling and oversampling approaches.
    The underlying idea is to first use an oversampling method to create duplicate
    or artificial observations and then use an undersampling method to reduce the
    noise or remove unnecessary observations.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以结合欠采样和过采样方法。其基本思想是，首先使用过采样方法创建重复或人工观察点，然后使用欠采样方法减少噪声或删除不必要的观察点。
- en: For example, we could first oversample the data with SMOTE and then undersample
    it using random undersampling. `imbalanced-learn` offers two combined resamplers—SMOTE
    followed by Tomek links or Edited Nearest Neighbours.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以先使用SMOTE对数据进行过采样，然后使用随机下采样进行欠采样。`imbalanced-learn`提供了两种组合重采样方法——SMOTE后接Tomek链接或编辑最近邻。
- en: 'In this recipe, we have only covered a small selection of the available approaches.
    Before changing topics, we wanted to mention some general notes on tackling problems
    with imbalanced classes:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们只涵盖了可用方法的一小部分。在切换话题之前，我们想提一些关于解决不平衡类问题的通用注意事项：
- en: Do not apply under/oversampling on the test set.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要在测试集上应用欠采样/过采样。
- en: For evaluating problems with imbalanced data, use metrics that account for class
    imbalance, such as precision, recall, F1 score, Cohen’s kappa, or the PR-AUC.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估不平衡数据问题时，使用考虑类不平衡的度量标准，例如精确度、召回率、F1分数、Cohen's kappa或PR-AUC。
- en: Use stratification when creating folds for cross-validation.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在创建交叉验证的折叠时使用分层采样。
- en: Introduce under-/oversampling during cross-validation, not before. Doing so
    before leads to overestimating the model’s performance!
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在交叉验证过程中引入欠采样/过采样，而不是之前。这样做会导致高估模型的性能！
- en: When creating pipelines with resampling using the `imbalanced-learn` library,
    we also need to use the `imbalanced-learn` variants of the pipeline. This is because
    the resamplers use the `fit_resample` method instead of the `fit_transform` required
    by `scikit-learn`'s pipelines.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用`imbalanced-learn`库创建具有重采样的管道时，我们还需要使用`imbalanced-learn`的管道变种。这是因为重采样器使用`fit_resample`方法，而不是`scikit-learn`管道所需的`fit_transform`方法。
- en: Consider framing the problem differently. For example, instead of a classification
    task, we could treat it as an anomaly detection problem. Then, we could use different
    techniques, for example, **isolation forest**.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑从不同的角度框架问题。例如，我们可以将任务视为一个异常检测问题，而不是分类任务。然后，我们可以使用不同的技术，例如**孤立森林**。
- en: Experiment with selecting a different probability threshold than the default
    50% to potentially tune the performance. Instead of rebalancing the dataset, we
    can use the model trained using the imbalanced dataset to plot the false positive
    and false negative rates as a function of the decision threshold. Then, we can
    choose the threshold that results in the performance that best suits our needs.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试选择不同于默认50%的概率阈值，以可能调优模型性能。我们可以使用使用不平衡数据集训练的模型绘制假阳性率和假阴性率与决策阈值的关系图，而不是重新平衡数据集。然后，我们可以选择一个在性能上最适合我们需求的阈值。
- en: We use the decision threshold to determine over which probability or score (a
    classifier’s output) we consider that the given observation belongs to the positive
    class. By default, that is 0.5.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用决策阈值来确定在哪个概率或得分（分类器的输出）上，我们认为给定的观察属于正类。默认情况下，这个值是0.5。
- en: See also
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'The dataset we have used in this recipe is available on Kaggle:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本食谱中使用的数据集可以在Kaggle上找到：
- en: '[https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)'
- en: 'Additional resources are available here:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 额外资源可在此处获取：
- en: 'Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. 2002\. “SMOTE:
    synthetic minority oversampling technique.” *Journal of artificial intelligence
    research* 16: 321–357.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. 2002. “SMOTE:
    合成少数类过采样技术。” *人工智能研究期刊* 16: 321–357.'
- en: 'Chawla, N. V. 2009\. “Data mining for imbalanced datasets: An overview.” *Data
    mining and knowledge discovery handbook*: 875–886.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chawla, N. V. 2009. “面向不平衡数据集的数据挖掘：概述。” *数据挖掘与知识发现手册*：875–886.
- en: 'Chen, C., Liaw, A., & Breiman, L. 2004\. “Using random forest to learn imbalanced
    data.” *University of California, Berkeley 110*: 1–12.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen, C., Liaw, A., & Breiman, L. 2004. “使用随机森林学习不平衡数据。” *加利福尼亚大学伯克利分校* 110:
    1–12.'
- en: Elor, Y., & Averbuch-Elor, H. 2022\. “To SMOTE, or not to SMOTE?.” *arXiv preprint
    arXiv:2201.08528*.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elor, Y., & Averbuch-Elor, H. 2022. “是使用SMOTE，还是不使用SMOTE？” *arXiv 预印本 arXiv:2201.08528*.
- en: 'Han, H., Wang, W. Y., & Mao, B. H. 2005, August. Borderline-SMOTE: a new over-sampling
    method in imbalanced data sets learning. In *International conference on intelligent
    computing*, 878–887\. Springer, Berlin, Heidelberg.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han, H., Wang, W. Y., & Mao, B. H. 2005年8月。边界-SMOTE：一种在不平衡数据集学习中的新过采样方法。在 *智能计算国际会议*，878–887.
    Springer，柏林，海德堡。
- en: 'He, H., Bai, Y., Garcia, E. A., & Li, S. 2008, June. ADASYN: Adaptive synthetic
    sampling approach for imbalanced learning. In *2008 IEEE international joint conference
    on neural networks (IEEE world congress on computational intelligence)*,1322–1328\.
    IEEE.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He, H., Bai, Y., Garcia, E. A., & Li, S. 2008年6月。ADASYN：用于不平衡学习的自适应合成采样方法。在
    *2008年IEEE国际神经网络联合会议（IEEE世界计算智能大会）*，1322–1328. IEEE.
- en: Le Borgne, Y.-A., Siblini, W., Lebichot, B., & Bontempi, G. 2022\. Reproducible
    Machine Learning for Credit Card Fraud Detection – Practical Handbook.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le Borgne, Y.-A., Siblini, W., Lebichot, B., & Bontempi, G. 2022. 可重复的机器学习在信用卡欺诈检测中的应用——实践手册。
- en: Liu, F. T., Ting, K. M., & Zhou, Z. H. 2008, December. Isolation forest. In
    *2008 Eighth Ieee International Conference On Data Mining*, 413–422\. IEEE.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu, F. T., Ting, K. M., & Zhou, Z. H. 2008年12月。隔离森林。在 *2008年第八届IEEE国际数据挖掘会议*，413–422.
    IEEE.
- en: 'Mani, I., & Zhang, I. 2003, August. kNN approach to unbalanced data distributions:
    a case study involving information extraction. In *Proceedings of workshop on
    learning from imbalanced datasets*, 126: 1–7\. ICML.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mani, I., & Zhang, I. 2003年8月。kNN方法用于不平衡数据分布：一个涉及信息提取的案例研究。在 *从不平衡数据集学习研讨会论文集*，126:
    1–7. ICML.'
- en: 'Nguyen, H. M., Cooper, E. W., & Kamei, K. 2009, November. Borderline over-sampling
    for imbalanced data classification. In *Proceedings: Fifth International Workshop
    on Computational Intelligence & Applications*, 2009(1): 24–29\. IEEE SMC Hiroshima
    Chapter.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nguyen, H. M., Cooper, E. W., & Kamei, K. 2009年11月。边界过采样用于不平衡数据分类。在 *计算智能与应用国际研讨会论文集*，2009(1):
    24–29. IEEE SMC 广岛分会。'
- en: Pozzolo, A.D.et al. 2015\. Calibrating Probability with Undersampling for Unbalanced
    Classification, *2015 IEEE Symposium Series on Computational Intelligence*.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pozzolo, A.D. 等. 2015. 使用欠采样进行概率校准以应对不平衡分类，*2015年IEEE计算智能学会年会*。
- en: 'Tomek, I. (1976). Two modifications of CNN, IEEE Transactions on Systems *Man
    and Communications*, 6: 769-772.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tomek, I. (1976). CNN的两种修改，IEEE系统 *人类与通信学报*，6: 769-772.'
- en: 'Wilson, D. L. (1972). “Asymptotic properties of nearest neighbor rules using
    edited data.” *IEEE Transactions on Systems, Man, and Cybernetics* 3: 408–421.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wilson, D. L. (1972). “使用编辑数据的最近邻规则的渐近性质。” *IEEE系统、人类与控制论学报* 3: 408–421.'
- en: Leveraging the wisdom of the crowds with stacked ensembles
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用集体智慧与堆叠集成
- en: '**Stacking** (stacked generalization) refers to a technique of creating ensembles
    of potentially heterogeneous machine learning models. The architecture of a stacking
    ensemble comprises at least two base models (known as level 0 models) and a meta-model
    (the level 1 model) that combines the predictions of the base models. The following
    figure illustrates an example with two base models.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆叠**（堆叠泛化）是指创建潜在异质的机器学习模型集成的一种技术。堆叠集成的架构包括至少两个基础模型（称为第0层模型）和一个元模型（第1层模型），后者将基础模型的预测进行组合。下图展示了一个包含两个基础模型的示例。'
- en: '![Diagram  Description automatically generated](../Images/B18112_14_15.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图示 描述自动生成](../Images/B18112_14_15.png)'
- en: 'Figure 14.15: High-level schema of a stacking ensemble with two base learners'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.15：具有两个基础学习器的堆叠集成的高级结构图
- en: The goal of stacking is to combine the capabilities of a range of well-performing
    models and obtain predictions that result in a potentially better performance
    than any single model in the ensemble. That is possible as the stacked ensemble
    tries to leverage the different strengths of the base models. Because of that,
    the base models should often be complex and diverse. For example, we could use
    linear models, decision trees, various kinds of ensembles, k-nearest neighbors,
    support vector machines, neural networks, and so on.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠的目标是将一系列表现良好的模型的能力结合起来，获得的预测结果有可能比集成中任何单一模型的性能更好。这是可能的，因为堆叠集成试图利用基础模型的不同优势。因此，基础模型通常应该是复杂和多样的。例如，我们可以使用线性模型、决策树、各种集成方法、k近邻、支持向量机、神经网络等等。
- en: Stacking can be a bit more difficult to understand than the previously covered
    ensemble methods (bagging, boosting, and so on) as there are at least a few variants
    of stacking when it comes to splitting data, handling potential overfitting, and
    data leakage. In this recipe, we follow the approach used in the `scikit-learn`
    library.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠可能比之前介绍的集成方法（如自助法、提升法等）更难理解，因为在分割数据、处理潜在的过拟合和数据泄漏时，堆叠有至少几种变体。在本食谱中，我们遵循`scikit-learn`库中使用的方法。
- en: The procedure used for creating a stacked ensemble can be described in three
    steps. We assume that we already have representative training and test datasets.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 创建堆叠集成的方法可以通过三个步骤来描述。我们假设已经有了代表性的训练集和测试集。
- en: '*Step 1*: Train level 0 models'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤1*：训练层次0模型'
- en: The essence of this step is that each of the level 0 models is trained on the
    full training dataset and then those models are used to generate predictions.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程的本质是，每个层次0模型都在完整的训练数据集上进行训练，然后这些模型被用来生成预测。
- en: Then, we have a few things to consider for our ensemble. First, we have to pick
    what kind of predictions we want to use. For a regression problem, this is straightforward
    as we do not have any choice. However, when working with a classification problem
    we can use the predicted class or the predicted probability/score.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要考虑一些关于集成的事项。首先，我们必须选择想要使用的预测类型。对于回归问题，这很简单，因为我们没有其他选择。然而，在处理分类问题时，我们可以使用预测的类别或预测的概率/分数。
- en: Second, we can either use only the predictions (whichever variant we picked
    before) as the features for the level 1 model or combine the original feature
    set with the predictions from the level 0 models. In practice, combining the features
    tends to work a bit better. Naturally, this heavily depends on the use case and
    the considered dataset.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们可以仅使用预测结果（无论选择了哪个变体）作为层次1模型的特征，或者将原始特征集与层次0模型的预测结果结合。在实践中，结合特征通常会效果更好。当然，这在很大程度上取决于使用场景和考虑的数据集。
- en: '*Step 2*: Train the level 1 model'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤2*：训练层次1模型'
- en: The level 1 model (or the meta-model) is often quite simple and ideally can
    provide a smooth interpretation of the predictions made by the level 0 models.
    That is why linear models are often selected for this task.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 层次1模型（或元模型）通常相当简单，理想情况下可以提供对层次0模型所做预测的平滑解释。这就是为什么线性模型通常被选用于此任务的原因。
- en: The term **blending** often refers to using a simple linear model as the level
    1 model. This is because the predictions of the level 1 model are then a weighted
    average (or blending) of the predictions made by the level 0 models.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**融合**一词通常指的是使用简单的线性模型作为层次1模型。这是因为层次1模型的预测是层次0模型预测的加权平均值（或融合）。'
- en: In this *step*, the level 1 model is trained using the features from the previous
    step (either only the predictions or combined with the initial set of features)
    and some cross-validation scheme. The latter is used to select the meta-model’s
    hyperparameters and/or the set of base models to consider for the ensemble.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个*步骤*中，层次1模型使用前一步的特征（可能仅是预测结果，或者与最初的特征集结合）以及某种交叉验证方案进行训练。后者用于选择元模型的超参数和/或考虑用于集成的基础模型集。
- en: '![](../Images/B18112_14_16.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_16.png)'
- en: 'Figure 14.16: Low-level schema of a stacking ensemble with two base learners'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.16：具有两个基础学习器的堆叠集成的低级结构图
- en: In `scikit-learn`'s approach to stacking, we assume that any of the base models
    could have a tendency to overfit, either due to the algorithm itself or due to
    some combination of its hyperparameters. But if that is the case, it should be
    offset by the other base models not suffering from the same problem. That is why
    cross-validation is applied to tune the meta-model and not the base models as
    well.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在`scikit-learn`的堆叠方法中，我们假设任何基础模型可能会过拟合，这可能是由于算法本身或其超参数的某种组合导致的。但如果确实如此，应该通过其他没有同样问题的基础模型来进行补偿。这就是为什么交叉验证应用于调优元模型，而不是基础模型。
- en: After the best hyperparameters/base learners are selected, the final estimator
    is trained on the full training dataset.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择最佳的超参数/基础学习器后，最终估计器将在整个训练数据集上进行训练。
- en: '*Step 3*: Make predictions on unseen data'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤3*：对未见过的数据进行预测'
- en: This step is the easiest one, as we are essentially fitting all the base models
    to the new observations to obtain the predictions, which are then used by the
    meta-model to create the stacked ensemble’s final predictions.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤是最简单的，因为我们本质上是将所有基础模型拟合到新的观测数据上，以获得预测结果，这些预测结果随后由元模型用于生成堆叠集成的最终预测。
- en: In this recipe, we create a stacked ensemble of models applied to the credit
    card fraud dataset.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们创建了一个堆叠模型集成，应用于信用卡欺诈数据集。
- en: How to do it...
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Execute the following steps to create a stacked ensemble:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以创建堆叠集成：
- en: 'Import the libraries:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE21]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Load and preprocess data:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并预处理数据：
- en: '[PRE22]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define a list of base models:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义基础模型列表：
- en: '[PRE23]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the accompanying Jupyter notebook, we specified the random state of all the
    models to which it is applicable. Here, we omitted that part for brevity.
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在随附的Jupyter笔记本中，我们指定了所有适用模型的随机状态。这里为了简洁起见，省略了这一部分。
- en: 'Train the selected models and calculate the recall using the test set:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练选定的模型并使用测试集计算召回率：
- en: '[PRE24]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Executing the snippet generates the following output:'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下输出：
- en: '[PRE25]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Out of the considered models, the Naive Bayes classifier achieved the best recall
    on the test set.
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在考虑的模型中，朴素贝叶斯分类器在测试集上达到了最佳的召回率。
- en: 'Define, fit, and evaluate the stacked ensemble:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义、拟合并评估堆叠集成：
- en: '[PRE26]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Executing the snippet generates the following output:'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下输出：
- en: '[PRE27]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Our stacked ensemble resulted in a worse score than the best of the individual
    models. However, we can try to further improve the ensemble. For example, we can
    allow the ensemble to use the initial features for the meta-model and replace
    the logistic regression meta-model with a Random Forest classifier.
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的堆叠集成的得分比最好的单个模型还要差。然而，我们可以尝试进一步改善集成。例如，我们可以允许集成使用初始特征作为元模型，并将逻辑回归元模型替换为随机森林分类器。
- en: 'Improve the stacking ensemble with additional features and a more complex meta-model:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用额外的特征和更复杂的元模型来改进堆叠集成：
- en: '[PRE28]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The second stacked ensemble achieved a recall score of `0.8571`, which is better
    than the best of the individual models.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个堆叠集成的召回率为`0.8571`，优于最好的单个模型。
- en: How it works...
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作...
- en: In *Step 1*, we imported the required libraries. Then, we loaded the credit
    card fraud dataset, separated the target from the features, dropped the `Time`
    feature, split the data into training and test sets (using a stratified split),
    and finally, scaled the data with `RobustScaler`. The transformation is not necessary
    for tree-based models, however; we use various classifiers (each with its own
    set of assumptions about the input data) as base models. For simplicity, we did
    not investigate different properties of the features, such as normality. Please
    refer to the previous recipe for more details on those processing steps.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤1*中，我们导入了所需的库。然后，我们加载了信用卡欺诈数据集，将目标变量与特征分开，删除了`Time`特征，将数据拆分为训练集和测试集（使用分层拆分），最后，使用`RobustScaler`对数据进行了缩放。尽管树模型不需要这种转换，但我们使用了各种分类器（每个分类器对输入数据有不同的假设）作为基础模型。为了简单起见，我们没有调查特征的不同属性，比如正态性。有关这些处理步骤的更多细节，请参阅之前的食谱。
- en: In *Step 3*, we defined a list of base learners for the stacked ensemble. We
    decided to use a few simple classifiers, such as a decision tree, a Naive Bayes
    classifier, a support vector classifier, and logistic regression. For brevity,
    we will not describe the properties of the selected classifiers here.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤3*中，我们定义了一组用于堆叠集成的基础学习器。我们决定使用几个简单的分类器，如决策树、朴素贝叶斯分类器、支持向量分类器和逻辑回归。为了简洁起见，我们这里不描述所选分类器的属性。
- en: When preparing a list of base learners, we can also provide the entire pipelines
    instead of just the estimators. This can come in handy when only some of the ML
    models require dedicated preprocessing of the features, such as scaling or encoding
    categorical variables.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备基础学习器列表时，我们还可以提供整个管道，而不仅仅是估计器。当只有某些机器学习模型需要专门处理特征预处理（如特征缩放或编码分类变量）时，这一点非常有用。
- en: In *Step 4*, we iterated over the list of classifiers, fitted each model (with
    its default settings) to the training data, and calculated the recall score using
    the test set. Additionally, if the estimator had an `n_jobs` parameter, we set
    it to `-1` to use all the available cores for computations. This way, we could
    speed up the model’s training, provided our machine has multiple cores/threads
    available. The goal of this step was to investigate the performance of the individual
    base models so that we could compare them to the stacked ensemble.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们遍历了分类器列表，使用默认设置将每个模型拟合到训练数据，并使用测试集计算召回率得分。此外，如果估计器具有`n_jobs`参数，我们将其设置为`-1`，以便使用所有可用核心进行计算。通过这种方式，我们可以加速模型的训练，前提是我们的机器有多个核心/线程可用。本步骤的目标是研究各个基础模型的性能，以便将它们与堆叠集成进行比较。
- en: In *Step 5*, we first defined the meta-model (logistic regression) and the 5-fold
    stratified cross-validation scheme. Then, we instantiated the `StackingClassifier`
    by providing the list of the base classifiers, together with the cross-validation
    scheme and the meta-model. In the `scikit-learn` implementation of stacking, the
    base learners are fitted using the entire training set. Then, in order to avoid
    overfitting and improve the model’s generalization, the meta-estimator uses the
    selected cross-validation scheme to train the model on the out-samples. To be
    precise, it uses `cross_val_predict` for this task.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们首先定义了元模型（逻辑回归）和5折分层交叉验证方案。然后，我们通过提供基础分类器列表、交叉验证方案和元模型来实例化`StackingClassifier`。在`scikit-learn`的堆叠实现中，基础学习器使用整个训练集进行拟合。然后，为了避免过拟合并提高模型的泛化能力，元估计器使用选定的交叉验证方案对模型进行训练，使用的是外样本。准确来说，它使用`cross_val_predict`来完成这项任务。
- en: A possible shortcoming of this approach is that applying cross-validation only
    to the meta-learner can result in overfitting of the base learners. Different
    libraries (mentioned in the *There’s more*… section) employ different approaches
    to cross-validation with stacked ensembles.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个可能缺点是，仅对元学习器应用交叉验证可能导致基础学习器的过拟合。不同的库（在*更多内容*部分中提到）采用不同的堆叠集成交叉验证方法。
- en: In the last step, we tried to improve the performance of the stacked ensemble
    by modifying its two characteristics. First, we changed the level 1 model from
    logistic regression to a Random Forest classifier. Second, we allowed the level
    1 model to use the features used by the level 0 base models. To do so, we set
    the `passthrough` argument to `True` while instantiating the `StackingClassifier`.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们尝试通过修改堆叠集成的两个特征来提高其性能。首先，我们将一级模型从逻辑回归更改为随机森林分类器。其次，我们允许一级模型使用由零级基础模型使用的特征。为此，我们在实例化`StackingClassifier`时，将`passthrough`参数设置为`True`。
- en: There’s more...
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'In order to get a better understanding of stacking, we can take a peek at the
    output of *Step 1*, which is the data being used to train the level 1 model. To
    get that data, we can use the `transform` method of a fitted `StackedClassifier`.
    Alternatively, we can use the familiar `fit_transform` method when the classifier
    was not fitted. In our case, we look into the stacked ensemble using both the
    predictions and original data as features:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解堆叠集成，我们可以查看*步骤 1*的输出，即用于训练一级模型的数据。为了获得这些数据，我们可以使用拟合后的`StackedClassifier`的`transform`方法。或者，当分类器没有拟合时，我们可以使用熟悉的`fit_transform`方法。在我们的案例中，我们查看堆叠集成，使用预测和原始数据作为特征：
- en: '[PRE29]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Executing the snippet generates the following table (abbreviated):'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这段代码会生成如下表格（简写版）：
- en: '![](../Images/B18112_14_17.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_17.png)'
- en: 'Figure 14.17: Preview of the input for the level 1 model in the stacking ensemble'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.17：堆叠集成中一级模型输入的预览
- en: We can see that the first four columns correspond to the predictions made by
    the base learners. Next to those, we can see the rest of the features, that is,
    those used by the base learners to generate their predictions.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到前四列对应于基础学习器做出的预测。在这些预测旁边，我们可以看到其余的特征，也就是基础学习器用于生成预测的特征。
- en: 'It is also worth mentioning that when using the `StackingClassifier` we can
    use various outputs of the base models as inputs for the level 1 model. For example,
    we can either use the predicted probabilities/scores or the predicted labels.
    Using the default settings of the `stack_method` argument, the classifier will
    try to use the following types of outputs (in that specific order): `predict_proba`,
    `decision_function`, and `predict`.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得一提的是，当使用`StackingClassifier`时，我们可以将基础模型的不同输出作为一级模型的输入。例如，我们可以使用预测的概率/得分或预测的标签。使用`stack_method`参数的默认设置，分类器会尝试使用以下类型的输出（按此特定顺序）：`predict_proba`、`decision_function`和`predict`。
- en: If we had used `stack_method="predict"`, we would have seen four columns of
    zeros and ones corresponding to the models’ class predictions (using the default
    decision threshold of 0.5).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`stack_method="predict"`，我们会看到四列零和一，对应于模型的类别预测（使用默认的0.5决策阈值）。
- en: 'In this recipe, we presented a simple example of a stacked ensemble. There
    are multiple ways in which we could try to further improve it. Some of the possible
    extensions include:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们展示了一个堆叠集成的简单示例。我们可以尝试进一步改进它的多种方式。一些可能的扩展包括：
- en: Adding more layers to the stacked ensemble
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向堆叠集成中添加更多层
- en: Using more diverse models, such as k-NN, boosted trees, neural networks, and
    so on
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更多样化的模型，例如k-NN、增强树、神经网络等
- en: Tuning the hyperparameters of the base classifiers and/or the meta-model
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整基础分类器和/或元模型的超参数
- en: The `ensemble` module of `scikit-learn` also contains a `VotingClassifier`,
    which can aggregate the predictions of multiple classifiers. `VotingClassifier`
    uses one of the two available voting schemes. The first one is `hard`, and it
    is simply the majority vote. The `soft` voting scheme uses the `argmax` of the
    sums of the predicted probabilities to predict the class label.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`的`ensemble`模块还包含一个`VotingClassifier`，它可以聚合多个分类器的预测结果。`VotingClassifier`使用两种可用的投票方案之一。第一种是`hard`投票，即简单的多数投票。`soft`投票方案使用预测概率的和的`argmax`来预测类别标签。'
- en: 'There are also other libraries providing stacking functionalities:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他库提供堆叠功能：
- en: '`vecstack`'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vecstack`'
- en: '`mlxtend`'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlxtend`'
- en: '`h2o`'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h2o`'
- en: These libraries also differ in the way they approach stacking, for example,
    how they split the data or how they handle potential overfitting and data leakage.
    Please refer to the respective documentation for more details.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这些库在堆叠方法上也有所不同，例如它们如何划分数据或如何处理潜在的过拟合和数据泄漏问题。有关更多详细信息，请参阅相应的文档。
- en: See also
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Additional resources are available here:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 其他资源可以在此处获得：
- en: 'Raschka, S. 2018\. “MLxtend: Providing machine learning and data science utilities
    and extensions to Python’s scientific computing stack.” *The Journal of Open Source
    Software* 3(24): 638.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Raschka, S. 2018. “MLxtend: 为Python的科学计算堆栈提供机器学习和数据科学工具及扩展。” *The Journal of
    Open Source Software* 3(24): 638。'
- en: 'Wolpert, D. H. 1992\. “Stacked generalization”. *Neural networks* 5(2): 241–259.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolpert, D. H. 1992. “堆叠泛化”。*Neural networks* 5(2): 241–259。'
- en: Bayesian hyperparameter optimization
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯超参数优化
- en: In the *Tuning hyperparameters using* *grid search and cross-validation* recipe
    in the previous chapter, we described how to use various flavors of grid search
    to find the best possible set of hyperparameters for our model. In this recipe,
    we introduce an alternative approach to finding the optimal set of hyperparameters,
    this time based on the Bayesian methodology.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章的*使用网格搜索和交叉验证调优超参数*配方中，我们描述了如何使用不同形式的网格搜索来找到模型的最佳超参数。在本配方中，我们介绍了一种基于贝叶斯方法找到最优超参数集的替代方法。
- en: The main motivation for the Bayesian approach is that both grid search and randomized
    search make uninformed choices, either through an exhaustive search over all combinations
    or through a random sample. This way, they spend a lot of time evaluating combinations
    that result in far from optimal performance, thus basically wasting time. That
    is why the Bayesian approach makes informed choices of the next set of hyperparameters
    to evaluate, this way reducing the time spent on finding the optimal set. One
    could say that the Bayesian methods try to limit the time spent evaluating the
    objective function by spending more time on selecting the hyperparameters to investigate,
    which in the end is computationally cheaper.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法的主要动机在于，无论是网格搜索还是随机化搜索，都会做出无知的选择，要么是通过对所有组合的穷举搜索，要么是通过随机抽样。这样，它们会花费大量时间评估那些远未达到最佳性能的组合，从而基本上浪费了时间。这就是为什么贝叶斯方法会根据已知信息选择下一个需要评估的超参数集合，从而减少寻找最佳集合所花费的时间。可以说，贝叶斯方法通过在选择要研究的超参数时投入更多时间，从而限制了评估目标函数所花费的时间，最终在计算上更加高效。
- en: A formalization of the Bayesian approach is **Sequential Model-Based** **Optimization**
    (**SMBO**). On a very high level, SMBO uses a surrogate model together with an
    acquisition function to iteratively (hence “sequential”) select the most promising
    hyperparameters in the search space in order to approximate the actual objective
    function.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法的一个形式化是**基于序列模型的优化**（**SMBO**）。从非常高层次看，SMBO利用替代模型和获取函数，通过迭代（因此称为“序列”）选择搜索空间中最有前景的超参数，以逼近实际的目标函数。
- en: In the context of Bayesian HPO, the true objective function is often the cross-validation
    error of a trained machine learning model. It can be computationally very expensive
    and can take hours (or even days) to calculate. That is why in SMBO we create
    a **surrogate model**, which is a probability model of the objective function
    built using its past evaluations. It maps the input values (hyperparameters) to
    a probability of a score on the true objective function. Hence, we can think of
    it as an approximation of the true objective function. In the approach we follow
    (the one used by the `hyperopt` library), the surrogate model is created using
    the **Tree-Structured** **Parzen Estimator** (**TPE**). Other possibilities include
    Gaussian processes or Random Forest regression.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯超参数优化（HPO）的背景下，真实目标函数通常是已训练机器学习模型的交叉验证误差。计算这些目标函数可能非常昂贵，可能需要数小时（甚至数天）才能计算完成。这就是为什么在SMBO中我们创建了**替代模型**，它是一个基于历史评估构建的目标函数的概率模型。它将输入值（超参数）映射到真实目标函数的得分概率。因此，我们可以将其视为对真实目标函数的近似。在我们采用的方法中（即`hyperopt`库所使用的方法），替代模型是通过**树状****帕尔岑估计器**（**TPE**）构建的。其他可能的选择包括高斯过程或随机森林回归。
- en: In each iteration, we first fit the surrogate model to all observations of the
    target function we made so far. Then, we apply the acquisition function (such
    as **Expected Improvement**) to determine the next set of hyperparameters based
    on their expected utility. Intuitively, this approach uses the history of past
    evaluations to make the best possible selection for the next iteration. Values
    close to the ones that performed well in the past are more likely to improve the
    overall performance than those that historically performed poorly. The acquisition
    function also defines a balance between the exploration of new areas in the hyperparameter
    space and the exploitation of the areas that are already known to provide favorable
    results.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们首先将替代模型拟合到迄今为止对目标函数的所有观察数据。然后，我们应用获取函数（例如**期望改进**）来根据超参数的预期效用确定下一组超参数。从直观上讲，这种方法利用过去评估的历史数据，为下一次迭代做出最佳选择。与过去表现良好的值接近的超参数，较可能提升整体性能，而那些历史上表现不佳的值则不太可能带来改进。获取函数还在超参数空间的探索新领域和利用已知能提供良好结果的领域之间定义了一种平衡。
- en: 'The simplified steps of Bayesian optimization are:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化的简化步骤如下：
- en: Create the surrogate model of the true objective function.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建真实目标函数的替代模型。
- en: Find a set of hyperparameters that performs best on the surrogate.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到在替代模型中表现最好的超参数集合。
- en: Use that set to evaluate the true objective function.
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用该集合评估真实目标函数。
- en: Update the surrogate, using the results from evaluating the true objective.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用评估真实目标的结果更新替代模型。
- en: Repeat *Steps 2–4*, until reaching the stop criterion (the specified maximum
    number of iterations or amount of time).
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤 2-4*，直到达到停止准则（指定的最大迭代次数或时间量）。
- en: From these steps, we see that the longer the algorithm runs, the closer the
    surrogate function approximates the true objective function. That is because with
    each iteration it is updated based on the evaluation of the true objective function,
    and thus with each run it is a bit “less wrong.”
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些步骤中可以看出，算法运行的时间越长，代理函数就越接近真实目标函数。这是因为每次迭代都会根据真实目标函数的评估来更新，因此每次运行时都会“少一些错误”。
- en: 'As we have already mentioned, the biggest advantage of Bayesian HPO is that
    it decreases the time spent searching for the optimal set of parameters. That
    is especially significant when the number of parameters is high and evaluating
    the true objective is computationally expensive. However, it also comes with a
    few possible shortcomings:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经提到的，贝叶斯超参数优化的最大优势是它减少了寻找最优参数集的时间。这一点在参数数量较多且评估真实目标计算代价高的情况下尤其重要。然而，它也有一些可能的缺点：
- en: Some steps of the SMBO procedure cannot be executed in parallel, as the algorithm
    selects the set of hyperparameters sequentially based on past results.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMBO 程序的一些步骤无法并行执行，因为算法会根据过去的结果顺序选择一组超参数。
- en: Choosing a proper distribution/scale for the hyperparameters can be tricky.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为超参数选择合适的分布/尺度可能会很棘手。
- en: Exploration versus exploitation bias—when the algorithm finds a local optimum,
    it might concentrate on hyperparameter values around it, instead of exploring
    potential new values located far away in the search space. Randomized search is
    not troubled by this issue, as it does not concentrate on any values.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索与开发的偏差——当算法找到局部最优解时，它可能会集中在该解附近的超参数值上，而不是探索在搜索空间中远离它的潜在新值。随机搜索不会遇到这个问题，因为它不会集中于任何值。
- en: The values of hyperparameters are selected independently. For example, in Gradient
    Boosted Trees, it is recommended to jointly consider the learning rate and the
    number of estimators, in order to avoid overfitting and reduce computation time.
    TPE would not be able to discover this relationship. In cases where we know about
    such a relation, we can partially overcome this problem by using different choices
    to define the search space.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数的值是独立选择的。例如，在梯度提升树中，建议联合考虑学习率和估计器的数量，以避免过拟合并减少计算时间。TPE 无法发现这种关系。在我们知道有这种关系的情况下，可以通过使用不同的选择来定义搜索空间，从而部分解决这个问题。
- en: In this brief introduction, we presented a high-level overview of the methodology.
    However, there is much more ground to cover in terms of surrogate models, acquisition
    functions, and so on. That is why we refer to a list of papers in the *See also*
    section for a more in-depth explanation.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在这简短的介绍中，我们提供了该方法论的高层次概述。然而，关于代理模型、获取函数等方面还有很多内容需要涵盖。因此，我们在*另见*部分参考了更多论文，以便进行更深入的解释。
- en: In this recipe, we use the Bayesian hyperparameter optimization to tune a LightGBM
    model. We chose this model as it provides a very good balance between performance
    and training time. We will be using the already familiar credit card fraud dataset,
    which is a highly imbalanced dataset.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们使用贝叶斯超参数优化来调整 LightGBM 模型。我们选择这个模型，因为它在性能和训练时间之间提供了非常好的平衡。我们将使用已经熟悉的信用卡欺诈数据集，这是一个高度不平衡的数据集。
- en: How to do it...
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Execute the following steps to run Bayesian hyperparameter optimization of
    a LightGBM model:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以运行 LightGBM 模型的贝叶斯超参数优化：
- en: 'Load the libraries:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载库：
- en: '[PRE30]'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define parameters for later use:'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义后续使用的参数：
- en: '[PRE31]'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Load and prepare the data:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并准备数据：
- en: '[PRE32]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Train the benchmark LightGBM model with the default hyperparameters:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认超参数训练基准 LightGBM 模型：
- en: '[PRE33]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下图形：
- en: '![](../Images/B18112_14_18.png)'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_18.png)'
- en: 'Figure 14.18: Performance evaluation of the benchmark LightGBM model'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.18：基准 LightGBM 模型的性能评估
- en: Additionally, we learned that the benchmark’s recall score on the test set is
    equal to `0.4286`.
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，我们了解到基准模型在测试集上的召回率得分为 `0.4286`。
- en: 'Define the objective function:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义目标函数：
- en: '[PRE34]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Define the search space:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义搜索空间：
- en: '[PRE35]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can generate a single draw from the sample space using the `sample` function:'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以使用 `sample` 函数从样本空间中生成一个单一的抽样：
- en: '[PRE36]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Executing the snippet prints the following dictionary:'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段将打印以下字典：
- en: '[PRE37]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Find the best hyperparameters using Bayesian HPO:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用贝叶斯HPO寻找最佳超参数：
- en: '[PRE38]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Inspect the best set of hyperparameters:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查最佳超参数集：
- en: '[PRE39]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Executing the snippet prints the list of the best hyperparameters:'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段将打印出最佳超参数的列表：
- en: '[PRE40]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Fit a new model using the best hyperparameters:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最佳超参数拟合新模型：
- en: '[PRE41]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Evaluate the fitted model on the test set:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估拟合的模型：
- en: '[PRE42]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段将生成以下图表：
- en: '![](../Images/B18112_14_19.png)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_19.png)'
- en: 'Figure 14.19: Performance evaluation of the tuned LightGBM model'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.19：调优后的LightGBM模型的性能评估
- en: We can see that the tuned model achieved better performance on the test set.
    To make it more concrete, its recall score was `0.8980`, as compared to the benchmark
    value of `0.4286`.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，调优后的模型在测试集上表现更好。为了更具体地说明，它的召回率得分为`0.8980`，而基准值为`0.4286`。
- en: How it works...
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'After loading the required libraries, we defined a set of parameters that we
    used in this recipe: the number of folds for cross-validation, the maximum number
    of iterations in the optimization procedure, the random state, and the metric
    used for optimization.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 加载所需库后，我们定义了一组在本配方中使用的参数：交叉验证的折数、优化过程中的最大迭代次数、随机状态和用于优化的指标。
- en: In *Step 3*, we imported the dataset and created the training and test sets.
    We described a few preprocessing steps in previous recipes, so please refer to
    those for more information. Then, we trained a benchmark LightGBM model using
    the default hyperparameters.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3步*中，我们导入了数据集并创建了训练集和测试集。我们在之前的配方中描述了一些预处理步骤，更多信息请参考这些内容。接着，我们使用默认超参数训练了基准的LightGBM模型。
- en: While using LightGBM, we can actually define a few random seeds. There are separate
    ones used for bagging and selecting a subset of features for each tree. Also,
    there is a `deterministic` flag that we can specify. To make the results fully
    reproducible, we should also make sure those additional settings are correctly
    specified.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LightGBM时，我们实际上可以定义几个随机种子。每个树的装袋和特征子集选择都有各自的种子。此外，还有一个`deterministic`标志，我们可以指定它。为了使结果完全可重复，我们还应该确保这些额外的设置被正确指定。
- en: 'In *Step 5*, we defined the true objective function (the one for which the
    Bayesian optimization will create a surrogate). The function takes the set of
    hyperparameters as inputs and uses stratified 5-fold cross-validation to calculate
    the loss value to be minimized. In the case of fraud detection, we want to detect
    as much fraud as possible, even if it means creating more false positives. That
    is why we selected recall as the metric of interest. As the optimizer will minimize
    the function, we multiplied it by -1 to create a maximization problem. The function
    must return either a single value (the loss) or a dictionary with at least two
    key-value pairs:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5步*中，我们定义了真实目标函数（贝叶斯优化将为其创建代理函数）。该函数将超参数集作为输入，并使用分层的5折交叉验证计算要最小化的损失值。在欺诈检测的情况下，我们希望尽可能多地检测到欺诈，即使这意味着产生更多的假阳性。因此，我们选择召回率作为关注的指标。由于优化器将最小化该函数，我们将其乘以-1，以将问题转化为最大化问题。该函数必须返回一个单一值（损失）或一个包含至少两个键值对的字典：
- en: '`loss`—The value of the true objective function.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`—真实目标函数的值。'
- en: '`status`—An indicator that the loss value was calculated correctly. It can
    be either `STATUS_OK` or `STATUS_FAIL`.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`status`—指示损失值是否正确计算的指标。它可以是`STATUS_OK`或`STATUS_FAIL`。'
- en: Additionally, we returned the set of hyperparameters used for evaluating the
    objective function. We will get back to it in the *There’s more…* section.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还返回了用于评估目标函数的超参数集。在*更多内容…*部分我们将回到这一点。
- en: We used the `cross_val_score` function to calculate the validation score. However,
    there are cases in which we might want to manually iterate over the folds created
    with `StratifiedKFold`. One such case would be to access more functionalities
    of the native API of LightGBM, for example, early stopping.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`cross_val_score`函数计算验证得分。然而，在某些情况下，我们可能希望手动遍历由`StratifiedKFold`创建的折。例如，我们希望访问LightGBM原生API的更多功能，例如早期停止。
- en: 'In *Step 6*, we defined the hyperparameter grid. The search space is defined
    as a dictionary, but in comparison to the spaces defined for `GridSearchCV`, we
    used `hyperopt`''s built-in functions, such as the following:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第6步*中，我们定义了超参数网格。搜索空间被定义为一个字典，但与为`GridSearchCV`定义的空间相比，我们使用了`hyperopt`的内置函数，例如以下内容：
- en: '`hp.choice(label,` `list)`—returns one of the indicated options.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.choice(label,` `list)`—返回所指定选项中的一个。'
- en: '`hp.uniform(label,` `lower_value,` `upper_value)`—the uniform distribution
    between two values.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.uniform(label,` `lower_value,` `upper_value)`—在两个值之间的均匀分布。'
- en: '`hp.quniform(label,` `low,` `high,` `q)`—the quantized (or discrete) uniform
    distribution between two values. In practice, it means that we obtain uniformly
    distributed, evenly spaced (determined by `q`) integers.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.quniform(label,` `low,` `high,` `q)`—在两个值之间的量化（或离散）均匀分布。实际上，这意味着我们得到的是均匀分布、间隔均匀（由
    `q` 确定）的整数。'
- en: '`hp.loguniform(label,` `low,` `high)`—the logarithm of the returned value is
    uniformly distributed. In other words, the returned numbers are evenly distributed
    on a logarithmic scale. Such a distribution is useful for exploring values that
    vary over several orders of magnitude. For example, when tuning the learning rate
    we would like to test values such as 0.001, 0.01, 0.1, and 1, instead of a uniformly
    distributed set between 0 and 1.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.loguniform(label,` `low,` `high)`—返回值的对数是均匀分布的。换句话说，返回的数字在对数尺度上是均匀分布的。这种分布对于探索跨越多个数量级变化的值非常有用。例如，在调节学习率时，我们希望测试像
    0.001、0.01、0.1 和 1 这样的值，而不是在 0 和 1 之间均匀分布的值集。'
- en: '`hp.randint(label,` `upper_value)`—returns a random integer in the range `[0,
    upper_value)`.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.randint(label,` `upper_value)`—返回一个范围在 `[0, upper_value)` 之间的随机整数。'
- en: Bear in mind that in this setup we had to define the names (denoted as `label`
    in the snippets above) of the hyperparameters twice. Additionally, in some cases,
    we wanted to force the values to be integers using `scope.int`.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在这个设置中，我们必须将超参数的名称（上面代码片段中的 `label`）定义两次。此外，在某些情况下，我们希望强制值为整数，可以使用 `scope.int`。
- en: In *Step 7*, we ran the Bayesian optimization to find the best set of hyperparameters.
    First, we defined the `Trials` object, which was used for storing the history
    of the search. We could even use it to resume a search or expand an already finished
    one, that is, increase the number of iterations using the already stored history.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 7* 中，我们运行了贝叶斯优化，以寻找最佳的超参数集。首先，我们定义了 `Trials` 对象，用于存储搜索的历史记录。我们甚至可以使用它来恢复搜索或扩展已经完成的搜索，也就是说，通过使用已经存储的历史记录来增加迭代次数。
- en: Second, we ran the optimization by passing the objective function, the search
    space, the surrogate model, the maximum number of iterations, and the `trials`
    object for storing the history. For more details on tuning the TPE algorithm,
    please refer to `hyperopt`'s documentation. Additionally, we set the value of
    `rstate`, which is `hyperopt`'s equivalent of `random_state`. We can easily store
    the `trials` object in a pickle file for later use. To do so, we can use the `pickle.dump`
    and `pickle.load` functions.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们通过传递目标函数、搜索空间、代理模型、最大迭代次数和 `trials` 对象（用于存储历史记录）来运行优化。有关调节 TPE 算法的更多细节，请参阅
    `hyperopt` 的文档。此外，我们还设置了 `rstate` 的值，它是 `hyperopt` 中相当于 `random_state` 的设置。我们可以轻松地将
    `trials` 对象存储到 pickle 文件中，以便以后使用。为此，我们可以使用 `pickle.dump` 和 `pickle.load` 函数。
- en: After running the Bayesian HPO, the `trials` object contains a lot of interesting
    and useful information. We can find the best set of hyperparameters under `trials.best_trial`,
    while `trials.results` contains all the explored sets of hyperparameters. We will
    be using this information in the *There’s more…* section.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 运行贝叶斯优化（Bayesian HPO）后，`trials` 对象包含了许多有趣且有用的信息。我们可以通过 `trials.best_trial` 找到最佳的超参数集，而
    `trials.results` 则包含了所有探索过的超参数集。我们将在 *还有更多内容…* 部分使用这些信息。
- en: In *Step 8*, we inspected the best set of hyperparameters. Instead of just printing
    the dictionary, we had to use the `space_eval` function. This is because just
    by printing the dictionary we will see the indices of any categorical features
    instead of their names. As an example, by printing the `best_set` dictionary we
    could potentially see a `0` instead of `'gbdt'` for the `boosting_type` hyperparameter.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 8* 中，我们检查了最佳的超参数集。我们不仅仅是打印字典，而是必须使用 `space_eval` 函数。这是因为仅打印字典时，我们会看到任何分类特征的索引，而不是它们的名称。例如，打印
    `best_set` 字典时，我们可能会看到 `0`，而不是 `boosting_type` 超参数中的 `'gbdt'`。
- en: In the last two steps, we trained a LightGBM classifier using the identified
    hyperparameters and evaluated its performance on the test set.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后两步中，我们使用确定的超参数训练了一个 LightGBM 分类器，并在测试集上评估了它的性能。
- en: There’s more...
  id: totrans-483
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: There are still quite a lot of interesting and useful things to mention about
    Bayesian hyperparameter optimization. We try to present those in the following
    subsections. For brevity’s sake, we do not present all the code here. For the
    complete code walk-through, please refer to the Jupyter notebook available in
    the book’s GitHub repository.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有很多有趣且有用的内容需要提及关于贝叶斯超参数优化的内容。我们尝试在以下小节中进行介绍。为了简洁起见，我们不在此展示所有代码。如需完整的代码示例，请参考书籍
    GitHub 仓库中的 Jupyter notebook。
- en: Conditional hyperparameter spaces
  id: totrans-485
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 条件超参数空间
- en: Conditional hyperparameter spaces can be useful when we would like to experiment
    with different machine learning models, each of those coming with completely separate
    hyperparameters. Alternatively, some hyperparameters are simply not compatible
    with others, and this should be accounted for while tuning the model.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 条件超参数空间在我们想尝试不同的机器学习模型时非常有用，每个模型都有完全不同的超参数。或者，有些超参数彼此之间根本不兼容，在调优模型时需要考虑这一点。
- en: 'In the case of LightGBM, an example could be the following pair: `boosting_type`
    and `subsample`/`subsample_freq`. The boosting type `"goss"` is not compatible
    with subsampling, that is, selecting only a subsample of the training observations
    for each iteration. That is why we would like to set `subsample` to 1 when we
    are using GOSS, but tune it otherwise. `subsample_freq` is a complementary hyperparameter
    that determines how often (every *n-*th iteration) we should use subsampling.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LightGBM，一个例子可能是以下的超参数组合：`boosting_type` 和 `subsample`/`subsample_freq`。提升类型
    `"goss"` 与子采样不兼容，也就是说，在每次迭代中仅选择一部分训练样本进行训练。这就是为什么在使用 GOSS 时，我们希望将 `subsample`
    设置为 1，但在其他情况下进行调优。`subsample_freq` 是一个补充性超参数，决定我们在每 *n-* 次迭代中应使用多少频率的子采样。
- en: 'We define a conditional search space using `hp.choice` in the following snippet:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下代码片段中使用 `hp.choice` 定义了一个条件搜索空间：
- en: '[PRE43]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'And an example of a draw from this space looks as follows:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是从该空间中提取的一个示例：
- en: '[PRE44]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'There is one more step that we need to take before being able to use such a
    draw for our Bayesian HPO. As the search space is initially nested, we have to
    assign the drawn samples to the top-level key in the dictionary. We do so in the
    following snippet:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够使用这种抽取值进行贝叶斯超参数优化之前，还有一步需要完成。由于搜索空间最初是嵌套的，我们需要将抽取的样本分配到字典中的顶层键。我们可以通过以下代码片段来实现：
- en: '[PRE45]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The `get` method extracts the value of the requested key from the dictionary
    or returns the default value if the requested key does not exist.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '`get` 方法从字典中提取所请求键的值，如果请求的键不存在，则返回默认值。'
- en: 'Executing the snippet returns a properly formatted dictionary:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 执行该代码片段会返回一个格式正确的字典：
- en: '[PRE46]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Lastly, we should place the code cleaning up the dictionary in the objective
    function, which we then pass to the optimization routine.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应该将清理字典的代码放入目标函数中，然后将其传递给优化过程。
- en: In the Jupyter notebook, we have also tuned the LightGBM with the conditional
    search space. It achieved a recall score of `0.8980` on the test set, which is
    the same score as the model tuned without the conditional search space.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter notebook 中，我们还使用条件搜索空间对 LightGBM 进行了调优。它在测试集上达到了 `0.8980` 的召回率，和没有使用条件搜索空间的模型得分相同。
- en: '![](../Images/B18112_14_20.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_20.png)'
- en: 'Figure 14.20: Performance evaluation of the LightGBM model tuned with the conditional
    search space'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.20：使用条件搜索空间调优后的 LightGBM 模型的性能评估
- en: A deep dive into the explored hyperparameters
  id: totrans-501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深入探索已探索的超参数
- en: 'We have mentioned that `hyperopt` offers a wide range of distributions from
    which we could sample. It will be much easier to understand when we actually see
    what the distributions look like. First, we inspect the distribution of the learning
    rate. We have specified it as:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到过，`hyperopt` 提供了多种分布供我们进行采样。当我们实际看到这些分布的样子时，理解起来会更加容易。首先，我们检查学习率的分布。我们将其指定为：
- en: '[PRE47]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In the following figure, we can see a **kernel density estimate** (**KDE**)
    plot of 10,000 random draws from the log-uniform distribution of the learning
    rate.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到从学习率的对数均匀分布中抽取的 10,000 个随机值的 **核密度估计**（**KDE**）图。
- en: '![](../Images/B18112_14_21.png)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_21.png)'
- en: 'Figure 14.21: Distribution of the learning rate'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.21：学习率的分布
- en: As intended, we can see that the distribution puts more weight on observations
    from several orders of magnitude.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们可以看到分布在几个数量级的观测值上赋予了更多的权重。
- en: 'The next distribution worth inspecting is the quantized uniform distribution
    that we have used for the `min_child_samples` hyperparameter. We defined it as:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个值得检查的分布是我们为`min_child_samples`超参数使用的量化均匀分布。我们将其定义为：
- en: '[PRE48]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the following figure, we can see that the distribution reflects the assumptions
    we set for it, that is, the evenly spaced integers are uniformly distributed.
    In our case, we sampled every fifth integer. To keep the plot readable, we only
    displayed the first 20 bars. But the full distribution goes to 500, just as we
    have specified.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到分布反映了我们为其设置的假设，即均匀分布的整数是均匀分布的。在我们的例子中，我们每次采样一个间隔为5的整数。为了保持图表的可读性，我们只显示了前20个条形图。但完整的分布范围为500，正如我们所指定的那样。
- en: '![](../Images/B18112_14_22.png)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_22.png)'
- en: 'Figure 14.22: Distribution of the min_child_samples hyperparameter'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.22：`min_child_samples`超参数的分布
- en: So far, we have only looked at the information available in the search space.
    However, we can also derive much more information from the `Trials` object, which
    stores the entire history of the Bayesian HPO procedure, that is, which hyperparameters
    were explored and what the resulting score was.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只查看了搜索空间中可用的信息。然而，我们还可以从`Trials`对象中推导出更多信息，它存储了整个贝叶斯HPO过程的历史记录，即探索了哪些超参数，以及相应的得分是多少。
- en: 'For this part, we use the `Trials` object containing the search history, using
    the search space without the conditional `boosting_type` tuning. In order to easily
    explore that data, we prepare a DataFrame containing the required information
    per iteration: the hyperparameters and the value of the loss function. We can
    extract the information from `trials.results`. This is the reason why we additionally
    passed the `params` object to the final dictionary while defining the `objective`
    function.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们使用了包含搜索历史的`Trials`对象，使用不包含条件`boosting_type`调优的搜索空间。为了便于探索这些数据，我们准备了一个包含每次迭代所需信息的DataFrame：超参数和损失函数的值。我们可以从`trials.results`中提取这些信息。这也是我们在定义`objective`函数时额外传递`params`对象到最终字典中的原因。
- en: 'Initially, the hyperparameters are stored in one column as a dictionary. We
    can use the `json_normalize` function to break them up into separate columns:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，超参数作为字典存储在一列中。我们可以使用`json_normalize`函数将其拆分为单独的列：
- en: '[PRE49]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Executing the snippet prints the following table:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 执行该代码段后，会打印出以下表格：
- en: '![](../Images/B18112_14_23.png)'
  id: totrans-518
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_23.png)'
- en: 'Figure 14.23: A snippet of the DataFrame containing all the explored hyperparameter
    combinations and their corresponding losses'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.23：包含所有探索的超参数组合及其对应损失的DataFrame片段
- en: For brevity, we only printed a few of the available columns. Using this information,
    we can further explore the optimization that resulted in the best set of hyperparameters.
    For example, we can see that the best score was achieved in the 151st iteration
    (the first row of the DataFrame has an index of `150` and indices in Python start
    with `0`).
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们只打印了几个可用的列。利用这些信息，我们可以进一步探索优化过程，以便找到最佳的超参数组合。例如，我们可以看到最佳得分是在第151次迭代时取得的（DataFrame的第一行索引为`150`，而Python的索引从`0`开始）。
- en: 'In the next figure, we have plotted the two distributions of the `colsample_bytree`
    hyperparameter: the one we defined as the prior for sampling, and the one that
    was actually sampled during the Bayesian optimization. Additionally, we plotted
    the evolution of the hyperparameter over iterations and added a regression line
    to indicate the direction of change.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们绘制了`colsample_bytree`超参数的两种分布：一种是我们定义的用于采样的先验分布，另一种是在贝叶斯优化过程中实际采样的分布。此外，我们还绘制了超参数随迭代的变化，并添加了回归线以指示变化的方向。
- en: In the left plot, we can see that the posterior distribution of `colsample_bytree`
    was concentrated toward the right side, indicating the higher range of considered
    values. By inspecting the KDE plots it seems that there is a non-zero density
    for values above 1, which should not be allowed.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧图中，我们可以看到`colsample_bytree`的后验分布集中在右侧，表明考虑的值处于较高范围。通过检查KDE图，我们发现对于大于1的值似乎存在非零密度，而这些值是不允许的。
- en: This is just the artifact from using the plotting method; in the `Trials` object
    we can confirm that not a single value above 1.0 was sampled during the optimization.
    In the right plot, the values of `colsample_bytree` seem to be scattered all over
    the allowed range. By looking at the regression line, it seems that there is a
    somewhat increasing trend.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_24.png)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.24: Distribution of the colsample_bytree hyperparameter'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we can look at the evolution of the loss over iterations. The loss represents
    the negative of the average recall score (from a 5-fold cross-validation on the
    training set). The lowest value (corresponding to maximum average recall) of `-0.90`
    occurred in the 151st iteration. With a few exceptions, the loss is quite stable
    in the `-0.75` to `-0.85` range.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_25.png)'
  id: totrans-527
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.25: The evolution of the loss (average recall) over iterations. The
    best iteration is marked with a star'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: Other popular libraries for hyperparameter optimization
  id: totrans-529
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`hyperopt` is one of the most popular Python libraries for hyperparameter optimization.
    However, it is definitely not the only one. Below you can find a list of popular
    alternatives:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: '`optuna`—a library offering vast hyperparameter tuning capabilities, including
    exhaustive Grid Search, Random Search, Bayesian HPO, and evolutionary algorithms.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-optimize`—a library offering the `BayesSearchCV` class, which is a
    Bayesian drop-in replacement for `scikit-learn`''s `GridSearchCV`.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hyperopt-sklearn`—a spin-off library of `hyperopt` offering model selection
    among machine learning algorithms from `scikit-learn`. It allows you to search
    for the best option among preprocessing steps and ML models, thus covering the
    entire scope of ML pipelines. The library covers almost all classifiers/regressors/preprocessing
    transformers available in `scikit-learn`.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ray[tune]`—Ray is an open-source, general-purpose distributed computing framework.
    We can use its `tune` module to run distributed hyperparameter tuning. It is also
    possible to combine `tune`''s distributed computing capabilities with other well-established
    libraries such as `hyperopt` or `optuna`.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Tpot`—TPOT is an AutoML tool that optimizes ML pipelines using genetic programming.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bayesian-optimization`—a library offering general-purpose Bayesian global
    optimization with Gaussian processes.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`smac`—SMAC is a general tool for optimizing the parameters of arbitrary algorithms,
    including hyperparameter optimization of ML models.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-538
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Additional resources are available here:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: 'Bergstra, J. S., Bardenet, R., Bengio, Y., & Kégl, B. 2011\. Algorithms for
    hyper-parameter optimization. In *Advances in Neural Information Processing Systems*:
    2546–2554.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bergstra, J., Yamins, D., & Cox, D. D. 2013, June. Hyperopt: A Python library
    for optimizing the hyperparameters of machine learning algorithms. In *Proceedings
    of the 12th Python in science conference:* 13–20.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bergstra, J., Yamins, D., Cox, D. D. 2013\. Making a Science of Model Search:
    Hyperparameter Optimization in *Hundreds of Dimensions for Vision Architectures.
    Proc. of the 30th International Conference on Machine Learning* (ICML 2013).'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bergstra, J., Yamins, D., Cox, D. D. 2013\. 使模型搜索成为科学：视觉架构的*数百维度的超参数优化*。第30届国际机器学习大会（ICML
    2013）论文集。
- en: Claesen, M., & De Moor, B. 2015\. “Hyperparameter search in machine learning.”
    *arXiv preprint arXiv:1502.02127*.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Claesen, M., & De Moor, B. 2015\. “机器学习中的超参数搜索。” *arXiv预印本 arXiv:1502.02127*。
- en: 'Falkner, S., Klein, A., & Hutter, F. 2018, July. BOHB: Robust and efficient
    hyperparameter optimization at scale. In *International Conference on Machine
    Learning*: 1437–1446\. PMLR.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falkner, S., Klein, A., & Hutter, F. 2018年7月。BOHB：大规模的稳健高效的超参数优化。在 *国际机器学习大会*：1437–1446。PMLR。
- en: 'Hutter, F., Kotthoff, L., & Vanschoren, J. 2019\. *Automated machine learning:
    methods, systems, challenges*: 219\. Springer Nature.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hutter, F., Kotthoff, L., & Vanschoren, J. 2019\. *自动化机器学习：方法、系统、挑战*：219。Springer
    Nature。
- en: 'Klein, A., Falkner, S., Bartels, S., Hennig, P., & Hutter, F. 2017, April.
    Fast Bayesian optimization of machine learning hyperparameters on large datasets.
    In *Artificial intelligence and statistics*: 528–536\. PMLR.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Klein, A., Falkner, S., Bartels, S., Hennig, P., & Hutter, F. 2017年4月。大规模数据集上的机器学习超参数的快速贝叶斯优化。在
    *人工智能与统计学*：528–536。PMLR。
- en: 'Komer B., Bergstra J., & Eliasmith C. 2014\. “Hyperopt-Sklearn: automatic hyperparameter
    configuration for Scikit-learn” Proc. SciPy.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Komer B., Bergstra J., & Eliasmith C. 2014\. “Hyperopt-Sklearn：Scikit-learn的自动化超参数配置”
    Proc. SciPy。
- en: 'Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B., &
    Talwalkar, A. 2018\. Massively parallel hyperparameter tuning: [https://doi.org/10.48550/arXiv.1810.05934](https://doi.org/10.48550/arXiv.1810.05934)'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B., &
    Talwalkar, A. 2018\. 大规模并行超参数调优： [https://doi.org/10.48550/arXiv.1810.05934](https://doi.org/10.48550/arXiv.1810.05934)
- en: 'Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & De Freitas, N. 2015\.
    *Taking the human out of the loop: A review of Bayesian optimization*. Proceedings
    of the *IEEE*, 104(1): 148–175.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & De Freitas, N. 2015\.
    *从循环中去除人类：贝叶斯优化综述*。*IEEE*会议录，104(1)：148–175。
- en: 'Snoek, J., Larochelle, H., & Adams, R. P. 2012\. Practical Bayesian optimization
    of machine learning algorithms. *Advances in Neural Information Processing Systems*:
    *25*.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snoek, J., Larochelle, H., & Adams, R. P. 2012\. 机器学习算法的实用贝叶斯优化。*神经信息处理系统进展*：*25*。
- en: Investigating feature importance
  id: totrans-551
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查特征重要性
- en: We have already spent quite some time creating the entire pipeline and tuning
    the models to achieve better performance. However, what is equally—or in some
    cases even more—important is the model’s interpretability. That means not only
    giving an accurate prediction but also being able to explain the whybehind it.
    For example, we can look into the case of customer churn. Knowing what the actual
    predictors of the customers leaving are might be helpful in improving the overall
    service and potentially making them stay longer.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经花费了相当多的时间来创建整个管道并调优模型，以实现更好的性能。然而，同样重要——甚至在某些情况下更重要——的是模型的可解释性。这意味着不仅要给出准确的预测，还需要能够解释其背后的原因。例如，我们可以查看客户流失的案例。了解客户离开的实际预测因素可能有助于改善整体服务，并有可能让他们停留更长时间。
- en: In a financial setting, banks often use machine learning in order to predict
    a customer’s ability to repay credit or a loan. In many cases, they are obliged
    to justify their reasoning, that is, if they decline a credit application, they
    need to know exactly why this customer’s application was not approved. In the
    case of very complicated models, this might be hard, or even impossible.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融环境中，银行通常使用机器学习来预测客户偿还信用或贷款的能力。在许多情况下，他们必须为自己的推理提供正当理由，即如果他们拒绝了一份信用申请，他们需要确切知道为什么这位客户的申请没有被批准。对于非常复杂的模型来说，这可能是困难的，甚至是不可能的。
- en: 'We can benefit in multiple ways by knowing the importance of our features:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 通过了解特征的重要性，我们可以从多个方面受益：
- en: By understanding the model’s logic, we can theoretically verify its correctness
    (if a sensible feature is a good predictor), but also try to improve the model
    by focusing only on the important variables.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过理解模型的逻辑，我们可以理论上验证其正确性（如果某个合理的特征是一个好的预测因素），同时也能通过只关注重要变量来尝试改进模型。
- en: We can use the feature importances to only keep the *x* most important features
    (contributing to a specified percentage of total importance), which can not only
    lead to better performance by removing potential noise but also to a shorter training
    time.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用特征重要性来保留*前x*个最重要的特征（这些特征贡献了指定百分比的总重要性），这不仅能通过去除潜在的噪音提高性能，还能缩短训练时间。
- en: In some real-life cases, it makes sense to sacrifice some accuracy (or any other
    performance metric) for the sake of interpretability.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一些现实案例中，为了可解释性，牺牲一些准确性（或其他任何性能指标）是合理的。
- en: It is also important to be aware that the more accurate (in terms of a specified
    performance metric) the model is, the more reliable the feature importances are.
    That is why we investigate the importance of the features after tuning the models.
    Please note that we should also account for overfitting, as an overfitted model
    will not return reliable feature importances.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 同时需要注意的是，模型的准确性（就指定的性能指标而言）越高，特征重要性就越可靠。这就是为什么我们在调整模型后会调查特征的重要性。请注意，我们还应考虑过拟合，因为过拟合的模型不会返回可靠的特征重要性。
- en: In this recipe, we show how to calculate the feature importance on an example
    of a Random Forest classifier. However, most of the methods are model-agnostic.
    In other cases, there are often equivalent approaches (such as in the case of
    XGBoost and LightGBM). We mention some of those in the *There’s more…* section.
    We briefly present the three selected methods of calculating feature importance.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们展示了如何在随机森林分类器的示例中计算特征重要性。然而，大多数方法都是与模型无关的。在其他情况下，通常也有等效的方法（例如在XGBoost和LightGBM的情况下）。我们会在*更多内容...*部分提到其中的一些方法。我们简要介绍了计算特征重要性的三种选定方法。
- en: '**Mean Decrease in Impurity** (**MDI**): The default feature importance used
    by Random Forest (in `scikit-learn`), also known as the Gini importance. As we
    know, decision trees use a metric of impurity (Gini index/entropy/MSE) to create
    the best splits while growing. When training a decision tree, we can compute how
    much each feature contributes to decreasing the weighted impurity. To calculate
    the feature importance for the entire forest, the algorithm averages the decrease
    in impurity over all the trees.'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '**不纯度的平均减少**（**MDI**）：这是随机森林（在`scikit-learn`中使用的默认特征重要性），也称为基尼重要性。正如我们所知，决策树使用一种不纯度度量（基尼指数/熵/MSE）来创建最佳分裂。在训练决策树时，我们可以计算每个特征在减少加权不纯度方面的贡献。为了计算整个森林的特征重要性，算法会计算所有树的不纯度减少的平均值。'
- en: While working with impurity-based metrics, we should focus on the ranking of
    the variables (relative values) rather than the absolute values of the feature
    importances (which are also normalized to add up to 1).
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用基于不纯度的指标时，我们应该关注变量的排名（相对值），而不是特征重要性的绝对值（这些值也已归一化，使其总和为1）。
- en: 'Here are the advantages of this approach:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这种方法的优点：
- en: Fast calculation
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速计算
- en: Easy to retrieve
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于获取
- en: 'Here are the disadvantages of this approach:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这种方法的缺点：
- en: Biased—It tends to inflate the importance of continuous (numerical) features
    or high-cardinality categorical variables. This can sometimes lead to absurd cases,
    whereby an additional random variable (unrelated to the problem at hand) scores
    high in the feature importance ranking.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏倚—它倾向于高估连续（数值型）特征或高卡方类别变量的重要性。这有时会导致荒谬的情况，其中一个额外的随机变量（与当前问题无关）在特征重要性排名中得分很高。
- en: Impurity-based importances are calculated on the basis of the training set and
    do not reflect the model’s ability to generalize to unseen data.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于不纯度的特征重要性是基于训练集计算的，并不能反映模型对未见数据的泛化能力。
- en: '**Drop-column feature importance**:The idea behind this approach is very simple.
    We compare a model with all the features to a model with one of the features dropped
    for training and inference. We repeat this process for all the features.'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '**丢列特征重要性**：这种方法背后的理念非常简单。我们将一个包含所有特征的模型与一个去掉某个特征的模型进行比较，进行训练和推断。我们对所有特征都重复这个过程。'
- en: 'Here is the advantage of this approach:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这种方法的优点：
- en: Often considered the most accurate/reliable measure of feature importance
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常被认为是最准确/最可靠的特征重要性度量
- en: 'Here is the disadvantage of this approach:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这种方法的缺点：
- en: Potentially highest computation cost caused by retraining the model for each
    variant of the dataset
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于对每个数据集变体进行重新训练，可能会导致最高的计算成本
- en: '**Permutation feature importance**:This approach directly measures feature
    importance by observing how random reshuffling of each predictor influences the
    model’s performance. The permutation procedure breaks the relationship between
    the feature and the target. Hence, the drop in the model’s performance is indicative
    of how much the model is dependent on a particular feature. If the decrease in
    the performance after reshuffling a feature is small, then it was not a very important
    feature in the first place. Conversely, if the decrease in performance is significant,
    the feature can be considered an important one for the model.'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps of the algorithm are:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: Train the baseline model and record the score of interest.
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly permute (reshuffle) the values of one of the features, then use the
    entire dataset (with one reshuffled feature) to obtain predictions and record
    the score. The feature importance is the difference between the baseline score
    and the one from the permuted dataset.
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the second step for all features.
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For evaluating the performance, we can either use the training data or the validation/test
    set. Using one of the latter two has the additional benefit of gaining insights
    into the model’s ability to generalize. For example, features that turn out to
    be important on the training set but not on the validation set might actually
    cause the model to overfit. For more discussion about the topic, please refer
    to the *Interpretable Machine Learning* book (referenced in the *See also* section).
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the advantages of this approach:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: Model-agnostic
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reasonably efficient—no need to retrain the model at every step
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshuffling preserves the distribution of the variables
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the disadvantages of this approach:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: Computationally more expensive than the default feature importances
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is likely to produce unreliable importances when features are highly correlated
    (see Strobl *et al.* for a detailed explanation)
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will explore the feature importance using the credit card
    default dataset we have already explored in the *Exploring ensemble classifiers*
    recipe.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-587
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we use the fitted Random Forest pipeline (called `rf_pipeline`)
    from the *Exploring ensemble classifiers* recipe. Please refer to this step in
    the Jupyter notebook to see all the initial steps not included here to avoid repetition.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-589
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Execute the following steps to evaluate the feature importance of a Random
    Forest model:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-592
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Extract the classifier and preprocessor from the fitted pipeline:'
  id: totrans-593
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-594
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Recover feature names from the preprocessing transformer and transform the
    training/test sets:'
  id: totrans-595
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-596
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Extract the MDI feature importance and calculate the cumulative importance:'
  id: totrans-597
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-598
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Define a function for plotting the top *x* features in terms of their importance:'
  id: totrans-599
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We use the function as follows:'
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-602
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_26.png)'
  id: totrans-604
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.26: Top 10 most important features using the MDI metric'
  id: totrans-605
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The most important features are categorical features indicating the payment
    status from July and September. After four of those, we can see continuous features
    such as `limit_balance`, `age`, various bill statements, and previous payments.
  id: totrans-606
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the cumulative importance of the features:'
  id: totrans-607
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-609
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_27.png)'
  id: totrans-610
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.27: Cumulative MDI importance'
  id: totrans-611
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The top 10 features account for 86.23% of the total importance, while the top
    17 features account for 95% of the total importance.
  id: totrans-612
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate and plot permutation importance using the training set:'
  id: totrans-613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-614
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_28.png)'
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.28: Top 10 most important features according to permutation importance
    calculated on the training set'
  id: totrans-617
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can see that the set of the most important features was reshuffled in comparison
    to the MDI importance. The most important now is `payment_status_sep_Unknown`,
    which is an undefined label (not assigned a clear meaning in the original paper)
    in the `payment_status_sep` categorical feature. We can also see that `age` is
    not among the top 10 most important features determined using this approach.
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate and plot permutation importance using the test set:'
  id: totrans-619
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-620
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_29.png)'
  id: totrans-622
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.29: Top 10 most important features according to permutation importance
    calculated on the test set'
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Looking at the figures, we can state that the same four features were selected
    as the most important ones using the training and test sets. The other ones were
    slightly reshuffled.
  id: totrans-624
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If we notice that the feature importances calculated using the training and
    test sets are significantly different, we should investigate whether the model
    is overfitted. To solve that, we might want to apply some form of regularization.
    In this case, we could try increasing the value of the `min_samples_leaf` hyperparameter.
  id: totrans-625
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a function for calculating the drop-column feature importance:'
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-627
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'There are two things worth mentioning here:'
  id: totrans-628
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We fixed the `random_state`, as we are specifically interested in performance
    changes caused by removing a feature. Hence, we are controlling the source of
    variability during the estimation procedure.
  id: totrans-629
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In this implementation, we use the training data for evaluation. We leave it
    as an exercise for the reader to modify the function to accept additional objects
    for evaluation.
  id: totrans-630
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calculate and plot the drop-column feature importance:'
  id: totrans-631
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'First, plot the top 10 most important features:'
  id: totrans-633
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-635
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_30.png)'
  id: totrans-636
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.30: Top 10 most important features according to drop-column feature
    importance'
  id: totrans-637
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using the drop-column feature importance (evaluated on the training data), the
    most important feature was `payment_status_sep_Unknown`. The same feature was
    identified as the most important one using permutation feature importance.
  id: totrans-638
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, plot the 10 least important features:'
  id: totrans-639
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-640
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-641
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](../Images/B18112_14_31.png)'
  id: totrans-642
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.31: The 10 least important features according to drop-column feature
    importance'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
- en: In the case of drop-column feature importance, negative importance indicates
    that removing a given feature from the model actually improves the performance.
    That is true as long as the considered metric treats higher values as better.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
- en: We can use these results to remove features that have negative importance and
    thus potentially improve the model’s performance and/or reduce the training time.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-646
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Step 1*, we imported the required libraries. Then, we extracted the classifier
    and the `ColumnTransformer` preprocessor from the pipeline. In this recipe, we
    worked with a tuned Random Forest classifier (using the hyperparameters determined
    in the *Exploring ensemble classifiers* recipe).
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 3*, we first extracted the column names from the preprocessor using
    the `get_feature_names_out` method. Then, we prepared the training and test sets
    by applying the preprocessor’s transformations.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we extracted the MDI feature importances using the `feature_importances_`
    attribute of the fitted Random Forest classifier. The values were automatically
    normalized so that they added up to `1`. Additionally, we calculated the cumulative
    feature importance.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, we defined a helper function to plot the most/least important features
    and plotted the top 10 most important features, calculated using the mean decrease
    in impurity.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 6*, we plotted the cumulative importance of all the features. Using
    this plot, we could decide if we wanted to reduce the number of features in the
    model to account for a certain percentage of total importance. By doing so, we
    could potentially decrease the model’s training time.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 7*, we calculated the permutation feature importance using the `permutation_importance`
    function available in `scikit-learn`. We decided to use recall as the scoring
    metric and set the `n_repeats` argument to `25`, so the algorithm reshuffled each
    feature `25` times. The output of the procedure is a dictionary containing three
    elements: the raw feature importances, the average value per feature, and the
    corresponding standard deviation. Additionally, while using `permutation_importance`
    we can evaluate multiple metrics at once by providing a list of selected metrics.'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: We decided to use the `scikit-learn` implementation of permutation feature importance.
    However, there are alternative options available, for example, in the `rfpimp`
    or `eli5` libraries. The former also contains the drop-column feature importance.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 8*, we calculated and evaluated the permutation feature importance,
    this time using the test set.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: 'We have mentioned in the introduction that permutation importance can return
    unreliable scores when our dataset has correlated features, that is, the importance
    score will be spread across the correlated features. We could try the following
    approaches to overcome this issue:'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: Permute groups of correlated features together. `rfpimp` offers such functionality
    in the `importances` function.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could use hierarchical clustering on the features’ Spearman’s rank correlations,
    pick a threshold, and then only keep a single feature from each of the identified
    clusters.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Step 9*, we defined a function for calculating the drop-column feature importance.
    First, we trained and evaluated the baseline model using all features. As the
    scoring metric, we chose recall. Then, we used the `clone` function of `scikit-learn`
    to create a copy of the model with the exact same specification as the baseline
    one. We then iteratively trained the model on a dataset without one feature, calculated
    the selected evaluation metric, and stored the difference in scores.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 10*, we applied the drop-column feature importance function and plotted
    the results, both the most and least important features.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  id: totrans-660
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have mentioned that the default feature importance of `scikit-learn`'s Random
    Forest is the MDI/Gini importance. It is also worth mentioning that the popular
    boosting algorithms (which we mentioned in the *Exploring ensemble classifiers*
    recipe) also adapted the `feature_importances_` attribute of the fitted model.
    However, they use different metrics of feature importance, depending on the algorithm.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: 'For XGBoost, we have the following possibilities:'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: '`weight`—measures the number of times a feature is used to split the data across
    all trees. Similar to the Gini importance, however, it does not take into account
    the number of samples.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gain`—measures the average gain of the feature when it is used in trees. Intuitively
    we can think of it as the Gini importance measure, where Gini impurity is replaced
    by the objective of the gradient boosting model.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cover`—measures the average coverage of the feature when it is used in trees.
    Coverage is defined as the number of samples affected by the split.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `cover` method can overcome one of the potential issues of the `weight`
    approach—simply counting the number of splits may be misleading, as some splits
    might affect just a few observations, and are therefore not really relevant.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: 'For LightGBM, we have the following possibilities:'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: '`split`—measures the number of times the feature is used in a model'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gain`—measures the total gains of splits that use the feature'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-670
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Additional resources are available here:'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: 'Altmann, A., Toloşi, L., Sander, O., & Lengauer, T. 2010\. “Permutation importance:
    a corrected feature importance measure.” *Bioinformatics*, 26(10): 1340–1347.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Louppe, G. 2014\. “Understanding random forests: From theory to practice.”
    *arXiv preprint arXiv:1407.7502*.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Molnar, C. 2020\. *Interpretable Machine Learning:* [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. 2009\. *The
    elements of statistical learning: data mining, inference, and prediction*, 2:
    1–758\. New York: Springer.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hooker, G., Mentch, L., & Zhou, S. 2021\. “Unrestricted permutation forces
    extrapolation: variable importance requires at least one more model, or there
    is no free variable importance.” *Statistics and Computing*, 31(6): 1–16.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parr, T., Turgutlu, K., Csiszar, C., & Howard, J. 2018\. Beware default random
    forest importances. March 26, 2018\. [https://explained.ai/rf-importance/](https://explained.ai/rf-importance/).
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Strobl, C., Boulesteix, A. L., Kneib, T., Augustin, T., & Zeileis, A. 2008\.
    “Conditional variable importance for random forests.” *BMC Bioinformatics*, 9(1):
    307.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Strobl, C., Boulesteix, A. L., Zeileis, A., & Hothorn, T. 2007\. “Bias in random
    forest variable importance measures: Illustrations, sources and a solution.” *BMC
    bioinformatics*, 8(1): 1–21.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring feature selection techniques
  id: totrans-680
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we saw how to evaluate the importance of features used
    for training ML models. We can use that knowledge to carry out feature selection,
    that is, keeping only the most relevant features and discarding the rest.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection is a crucial part of any machine learning project. First,
    it allows us to remove features that are either completely irrelevant or are not
    contributing much to a model’s predictive capabilities. This can benefit us in
    multiple ways. Probably the most important benefit is that such unimportant features
    can actually negatively impact the performance of our model as they introduce
    noise and contribute to overfitting. As we have already established—*garbage in,
    garbage out*. Additionally, fewer features can often be translated into a shorter
    training time and help us avoid the curse of dimensionality.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: Second, we should follow Occam’s razor and keep our models simple and explainable.
    When we have a moderate number of features, it is easier to explain what is actually
    happening in the model. This can be crucial for the ML project’s adoption by the
    stakeholders.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already established the *why* of feature selection. Now it is time
    to explore the *how*. On a high level, feature selection methods can be grouped
    into three categories:'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: '**Filter methods**—a generic set of univariate methods that specify a certain
    statistical measure and then filter the features based on it. This group does
    not incorporate any specific ML algorithm, hence it is characterized by (usually)
    lower computation time and is less prone to overfitting. A potential drawback
    of this group is that the methods evaluate the relationship between the target
    and each of the features individually. This can lead to them overlooking important
    relationships between the features. Examples include correlation, chi-squared
    test, **analysis of variance** (**ANOVA**), information gain, variance thresholding,
    and so on.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wrapper methods**—this group of approaches considers feature selection a
    search problem, that is, it uses certain procedures to repeatedly evaluate a specific
    ML model with a different set of features to find the optimal set. It is characterized
    by the highest computational costs and the highest possibility of overfitting.
    Examples include forward selection, backward elimination, stepwise selection,
    recursive feature elimination, and so on.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedded methods**—this set of methods uses ML algorithms that have built-in
    feature selection, for example, Lasso with its regularization or Random Forest.
    By using these implicit feature selection methods, the algorithms try to prevent
    overfitting. In terms of computational complexity, this method is usually somewhere
    between the filter and wrapper groups.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will apply a selection of feature selection methods to the
    credit card fraud dataset. We believe it provides a good example, especially given
    a lot of the features are anonymized and we do not know the exact meaning behind
    them. Hence, it is also likely that some of them do not really contribute much
    to the model’s performance.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-689
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will be using the credit card fraud dataset that we introduced
    in the *Investigating different approaches to handling imbalanced data* recipe.
    For convenience, we have included all the necessary preparation steps in this
    section from the accompanying Jupyter notebook.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting challenge to applying feature selection methods would be
    BNP Paribas Cardif Claims Management (the dataset is available at Kaggle—a link
    is provided in the *See also* section). Similar to the dataset used in this recipe,
    it contains 131 anonymized features.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-692
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Execute the following steps to experiment with various feature selection methods:'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  id: totrans-694
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-695
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Train the benchmark model:'
  id: totrans-696
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-697
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Executing the snippet generates the following output:'
  id: totrans-698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-699
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Looking at the recall scores, the model is clearly overfitted to the training
    data. Normally, we should try to address this. However, to keep the exercise simple
    we assume that the model is good enough to proceed.
  id: totrans-700
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the best features using Mutual Information:'
  id: totrans-701
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-702
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Using the next snippet, we plot the results:'
  id: totrans-703
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-704
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-705
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_32.png)'
  id: totrans-706
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.32: Performance of the model depending on the number of selected
    features. Features are selected using the Mutual Information criterion'
  id: totrans-707
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By inspecting the figure, we can see that we achieved the best recall score
    on the test set using `8`, `9`, `10`, and `12` features. As simplicity is desired,
    we decided to choose `8`. Using the following snippet, we extract the names of
    the 8 most important features:'
  id: totrans-708
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-709
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Executing the snippet returns the following output:'
  id: totrans-710
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-711
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Select the best features using MDI feature importance, retrain the model, and
    evaluate its performance:'
  id: totrans-712
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-713
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Executing the snippet generates the following output:'
  id: totrans-714
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-715
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Using the following snippet, we extract the threshold used for feature selection
    and the most relevant features:'
  id: totrans-716
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-717
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This generates the following output:'
  id: totrans-718
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-719
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The threshold value corresponds to the average feature importance of the RF
    model.
  id: totrans-720
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using a loop similar to the one in *Step 3*, we can generate a bar chart showing
    the model’s performance depending on the number of features kept in the model.
    We iteratively select the top *k* features based on the MDI. To avoid repetition,
    we do not include the code here (it is available in the accompanying Jupyter notebook).
    By analyzing the figure, we can see that the model achieved the best score with
    `10` features, which is more than in the previous approach.
  id: totrans-721
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_33.png)'
  id: totrans-722
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.33: Performance of the model depending on the number of selected
    features. Features are selected using the Mean Decrease in Impurity feature importance'
  id: totrans-723
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the best 10 features using Recursive Feature Elimination:'
  id: totrans-724
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-725
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'In order to avoid repetition, we present the most important features and the
    accompanying scores without the code, as it is almost identical to what we have
    covered in the previous steps:'
  id: totrans-726
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-727
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Select the best features using Recursive Feature Elimination with cross-validation:'
  id: totrans-728
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-729
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Below we present the outcome of the feature selection:'
  id: totrans-730
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-731
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'This approach resulted in the selection of `16` features. Overall, `6` features
    appeared in each of the considered approaches: `V10`, `V11`, `V12`, `V14`, `V16`,
    and `V17`.'
  id: totrans-732
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Additionally, using the following snippet we can visualize the cross-validation
    scores, that is, what the average recall of the `5` folds was for each of the
    considered numbers of retained features. We had to add `5` to the index of the
    DataFrame, as we chose to retain a minimum of `5` features in the `RFECV` procedure:'
  id: totrans-733
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-734
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-735
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_34.png)'
  id: totrans-736
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.34: Average CV score for each step of the RFE procedure'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the figure confirms that the highest average recall was obtained
    using 16 features.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
- en: While evaluating the benefits of feature selection, we should consider two scenarios.
    In the more obvious one, the performance of the model improves when we remove
    some of the features. This does not need any further explanation. The second scenario
    is more interesting. After removing features, we can end up with a very similar
    performance to the initial one or slightly worse. However, this does not necessarily
    mean that we have failed. Consider a case in which we removed ~60% of the features
    while keeping the same performance. This could already be a major improvement
    that—depending on the dataset and model—can potentially reduce the training time
    by hours or days. Additionally, such a model would be easier to interpret.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-740
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After importing the required libraries, we trained a benchmark Random Forest
    classifier and printed the recall score from the training and test sets.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 3*, we applied the first of the considered feature selection approaches.
    It was an example of the univariate *filter* category of feature selection techniques.
    As the statistical criterion, we used the Mutual Information score. To calculate
    the metric, we used the `mutual_info_classif` function from `scikit-learn`, which
    is capable of working with a categorical target and numerical features only. Hence,
    any categorical features need to be appropriately encoded beforehand. Fortunately,
    we only have continuous numerical features in this dataset.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
- en: The **Mutual Information** (**MI**)score of two random variables is a measure
    of the mutual dependence between those variables. When the score is equal to zero,
    the two variables are independent. The higher the score, the higher the dependency
    between the variables. In general, calculating the MI requires knowledge of the
    probability distributions of each of the features, which we do not usually know.
    That is why the `scikit-learn` implementation uses a nonparametric approximation
    based on k-Nearest Neighbors distances. One of the advantages of using MI is that
    it can capture nonlinear relationships between the features.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
- en: Next, we combined the MI criterion with the `SelectKBest` class, which allows
    us to select the *k* best features determined by an arbitrary metric. Using this
    approach, we almost never know upfront how many features we would like to keep.
    Hence, we iterated over all the possible values (from `2` to `29`, where the latter
    is the total number of features in the dataset). The `SelectKBest` class employs
    the familiar `fit`/`transform` approach. Within each iteration, we fitted the
    class to the training data (both features and the target are required for this
    step) and then transformed the training and test sets. The transformation resulted
    in keeping only the *k* most important features according to the MI criterion.
    Then, we once again fitted the Random Forest classifier using only the selected
    features and recorded the relevant recall scores.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn` allows us to easily use different metrics together with the
    `SelectKBest` class. For example, we could use the following scoring functions:'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
- en: '`f_classif`—the ANOVA F-value estimating the degree of linear dependency between
    two variables. The F statistic is calculated as the ratio of between-group variability
    to the within-group variability. In this case, the group is simply the class of
    the target. A potential drawback of this method is that it only accounts for linear
    relationships.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chi2`—the chi-squared statistics. This metric is only suitable for non-negative
    features such as Booleans or frequencies, or more generally, for categorical features.
    Intuitively, it evaluates if a feature is independent of the target. If that is
    the case, it is also uninformative when it comes to classifying the observations.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aside from selecting the *k* best features, the `feature_selection` module of
    `scikit-learn` also offers classes that allow choosing features based on the percentile
    of the highest scores, a false positive rate test, an estimated false discovery
    rate, or a family-wise error rate.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we explored an example of the *embedded* feature selection techniques.
    In this group, feature selection is performed as part of the model building phase.
    We used the `SelectFromModel` class to select the best features based on the model’s
    built-in feature importance metric (in this case, the MDI feature importance).
    When instantiating the class, we can provide the `threshold` argument to determine
    the threshold used to select the most relevant features. Features with weights/coefficients
    above that threshold would be kept in the model. We can also use the `"mean"`
    (default one) and `"median"` keywords to use the mean/median values of all feature
    importances as the threshold. We can also combine those keywords with scaling
    factors, for example, `"1.5*mean"`. Using the `max_features` argument, we can
    determine the maximum number of features we allow to be selected.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
- en: The `SelectFromModel` class works with any estimator that has either the `feature_importances_`
    (for example, Random Forest, XGBoost, LightGBM, and so on) or `coef_` (for example,
    Linear Regression, Logistic Regression, and Lasso) attribute.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
- en: In this step, we demonstrated two approaches to recovering the selected features.
    The first one is the `get_support` method, which returns a list with Boolean flags
    indicating whether the given feature was selected. The second one is the `get_feature_names_out`
    method, which directly returns the names of the selected features. While fitting
    the Random Forest classifier, we manually selected the columns of the training
    dataset. However, we could have also used the `transform` method of the fitted
    `SelectFromModel` class to automatically extract only the relevant features as
    a `numpy` array.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, we used an example of the *wrapper* methods. **Recursive Feature
    Elimination** (**RFE**)is an algorithm that recursively trains an ML model, calculates
    the feature importances (via `coef_` or `feature_importances_`), and drops the
    least important feature or features.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
- en: The process starts by training the model using all the features. Then, the least
    important feature or features are pruned from the dataset. Next, the model is
    trained again with the reduced feature set, and the least important features are
    again eliminated. The process is repeated until it reaches the desired number
    of features. While instantiating the `RFE` class, we provided the Random Forest
    estimator together with the number of features to select. Additionally, we could
    provide the `step` argument, which determined how many features to eliminate during
    each iteration.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: RFE can be a computationally expensive algorithm to run, especially with a large
    feature set and cross-validation. Hence, it might be a good idea to apply some
    other feature selection technique before using RFE. For example, we could use
    the filtering approach and remove some of the correlated features.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
- en: As we have mentioned before, we rarely know the optimal number of features upfront.
    That is why in *Step 6* we try to account for that drawback. By combining RFE
    with cross-validation, we can automatically determine the optimal number of features
    to keep using the RFE procedure. To do so, we used the `RFECV` class and provided
    some additional inputs. We had to specify the cross-validation scheme (5-fold
    stratified CV, as we are dealing with an imbalanced dataset), the scoring metric
    (recall), and the minimum number of features to retain. For the last argument,
    we arbitrarily chose 5.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, to explore the CV scores in more depth, we accessed the cross-validation
    scores for each fold using the `cv_results_` attribute of the fitted `RFECV` class.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-757
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some of the other available approaches
  id: totrans-758
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have already mentioned quite a few univariate filter methods. Some other
    notable ones include:'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
- en: '**Variance thresholding**—this method simply removes features with variance
    lower than a specified threshold. Thus, it can be used to remove constant and
    quasi-constant features. The latter ones are those that have very little variability
    as almost all the values are identical. By definition, this method does not look
    at the target value, only at the features.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correlation-based**—there are multiple ways to measure correlation, hence
    we will only focus on the general logic of this approach. First, we determine
    the correlation between the features and the target. We can choose a threshold
    above which we want to keep the features for modeling.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we should also consider removing features that are highly correlated among
    themselves. We should identify such groups and then leave only one feature from
    each of the groups in our dataset. Alternatively, we could use the **Variance
    Inflation Factor** (**VIF**) to determine multicollinearity and drop features
    based on high VIF values. VIF is available in `statsmodels`.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
- en: We did not consider using correlation as a criterion in this recipe, as the
    features in the credit card fraud dataset are the outcomes of PCA. Hence, by definition
    they are orthogonal, that is, uncorrelated.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
- en: There are also multivariate filter methods available. For example, **Maximum
    Relevance Minimum Redundancy** (**MRMR**) is a family of algorithms that attempts
    to identify a subset of features that have high relevance with respect to the
    target variable, while having a small redundancy with each other.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
- en: 'We could also explore the following *wrapper* techniques:'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward feature selection**—we start with no features. We test each of the
    features separately and see which one most improves the model. We add that feature
    to our feature set. Then, we sequentially train models with a second feature added.
    Similarly, at this step, we again test all the remaining features individually.
    We select the best one and add it to the selected pool. We continue adding features
    one at a time until we reach a stopping criterion (max number of features or no
    further improvement). Traditionally, the feature to be added was based on the
    features’ p-values. However, modern libraries use the improvement on a cross-validated
    metric of choice as the selection criterion.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backward feature selection**—similar to the previous approach, but we start
    with all the features in our set and sequentially remove one feature at a time
    until there is no further improvement (or all features are statistically significant).
    This method differs from RFE as it does not use the coefficients or feature importances
    to select the features to be removed. Instead, it optimizes for the performance
    improvement measured by the difference in the cross-validated score.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exhaustive feature selection**—simply speaking, in this brute-force approach
    we try all the possible combinations of the features. Naturally, this is the most
    computationally expensive of the wrapper techniques, as the number of feature
    combinations to be tested grows exponentially with the number of features. For
    example, if we had 3 features, we would have to test 7 combinations. Assume we
    have features `a`, `b`, and `c`. We would have to test the following combinations:
    `[a, b, c, ab, ac, bc, abc]`.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stepwise selection**—a hybrid approach combining forward and backward feature
    selection. The process starts with zero features and adds them one by one using
    the lowest significant p-value. At each addition step, the procedure also checks
    if any of the current features are statistically insignificant. If that is the
    case, they are dropped from the feature set and the algorithm continues to the
    next addition step. The procedure allows the final model to have only statistically
    significant features.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two approaches are implemented in `scikit-learn`. Alternatively, you
    can find all four of them in the `mlxtend` library.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
- en: 'We should also mention a few things to keep in mind about the *wrapper* techniques
    presented above:'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
- en: The optimal number of features depends on the ML algorithm.
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to their iterative nature, they are able to detect certain interactions
    between the features.
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These methods usually provide the best performing subset of features for a given
    ML algorithm.
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They come at the highest computational cost, as they operate greedily and retrain
    the model multiple times.
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As the last wrapper method, we will mention the **Boruta** algorithm. Without
    going into too much detail, it creates a set of shadow features (permuted duplicates
    of the original features) and selects features using a simple heuristic: a feature
    is useful if it is doing better than the best of the randomized features. The
    entire process is repeated multiple times before the algorithm returns the best
    set of features. The algorithm is compatible with ML models from the `ensemble`
    module of `scikit-learn` and algorithms such as XGBoost and LightGBM. For more
    details on the algorithm, please refer to the paper mentioned in the *See also*
    section. The Boruta algorithm is implemented in the `boruta` library.'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, it is worth mentioning that we can also combine multiple feature selection
    approaches to improve their reliability. For example, we could select features
    using a few approaches and then ultimately select the ones that appeared in all
    or most of them.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
- en: Combining feature selection and hyperparameter tuning
  id: totrans-778
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have already established, we do not know the optimal number of features
    to keep in advance. Hence, we might want to combine feature selection with hyperparameter
    tuning and treat the number of features to keep as another hyperparameter.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily do so using `pipelines` and `GridSearchCV` from `scikit-learn`
    :'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-781
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Executing the snippet returns the best set of hyperparameters:'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-783
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: When combining filter feature selection methods with cross-validation, we should
    do the filtering within the cross-validation procedure. Otherwise, we are selecting
    the features using all the available observations and introducing bias.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
- en: One thing to keep in mind is that the features selected within various folds
    of the cross-validation can be different. Let’s consider an example of a `5`-fold
    cross-validation procedure that keeps `3` features. It can happen that in some
    of the `5` cross-validation rounds, the `3` selected features might not overlap.
    However, they should not be too different, as we assume that the overall patterns
    in the data and the distribution of the features are very similar across folds.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-786
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Additional references on the topic:'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
- en: 'Bommert, A., Sun, X., Bischl, B., Rahnenführer, J., & Lang, M. 2020\. “Benchmark
    for filter methods for feature selection in high-dimensional classification data.”
    *Computational Statistics & Data Analysis*, 143: 106839.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding, C., & Peng, H. 2005\. “Minimum redundancy feature selection from microarray
    gene expression data.” *Journal of bioinformatics and computational biology*,
    3(2): 185–205.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kira, K., & Rendell, L. A. 1992\. A practical approach to feature selection.
    In *Machine learning proceedings*, *1992*: 249–256\. Morgan Kaufmann.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kira, K., & Rendell, L. A. 1992, July. The feature selection problem: Traditional
    methods and a new algorithm. In Aaai, 2(1992a): 129-134.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuhn, M., & Johnson, K. 2019\. *Feature engineering and selection: A practical
    approach for predictive models*. CRC Press.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kursa M., & Rudnicki W. Sep. 2010\. “Feature Selection with the Boruta Package”
    *Journal of Statistical Software*, 36(11): 1-13.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Urbanowicz, RJ., et al. 2018\. “Relief-based feature selection: Introduction
    and review.” *Journal of biomedical informatics*, 85: 189–203.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu, L., & Liu, H. 2003\. Feature selection for high-dimensional data: A fast
    correlation-based filter solution. In *Proceedings of the 20th international conference
    on machine learning (ICML-03)*: 856–863.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao, Z., Anand, R., & Wang, M. 2019, October. Maximum relevance and minimum
    redundancy feature selection methods for a marketing machine learning platform.
    In *2019 IEEE international conference on data science and advanced analytics
    (DSAA)*: 442–452\. IEEE.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the additional dataset mentioned in the *Getting ready* section
    here:'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/competitions/bnp-paribas-cardif-claims-management](https://www.kaggle.com/competitions/bnp-paribas-cardif-claims-management
    )'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring explainable AI techniques
  id: totrans-799
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In one of the previous recipes, we looked into feature importance as one of
    the means of getting a better understanding of how the models work under the hood.
    While this might be quite a simple task in the case of linear regression, it gets
    increasingly difficult with the complexity of the models.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
- en: One of the big trends in the ML/DL field is **explainable AI** (**XAI**). It
    refers to various techniques that allow us to better understand the predictions
    of black box models. While the current XAI approaches will not turn a black box
    model into a fully interpretable one (or a white box), they will definitely help
    us better understand why the model returns certain predictions for a given set
    of features.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the benefits of having explainable AI models are as follows:'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
- en: Builds trust in the model—if the model’s reasoning (via its explanation) matches
    common sense or the beliefs of human experts, it can strengthen the trust in the
    model’s predictions
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facilitates the model’s or project’s adoption by business stakeholders
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gives insights useful for human decision-making by providing reasoning for the
    model’s decision process
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makes debugging easier
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can steer the direction of future data gathering or feature engineering
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before mentioning the particular XAI techniques, it is worth clarifying the
    difference between interpretability and explainability. **Interpretability** can
    be considered a stronger version of explainability. It offers a causality-based
    explanation of a model’s predictions. On the other hand, **explainability** is
    used to make sense of the predictions made by black box models, which cannot be
    interpretable. In particular, XAI techniques can be used to explain what is going
    on in the model’s prediction process, but they are unable to causally prove why
    a certain prediction has been made.
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we cover three XAI techniques. See the *There’s more…* section
    for a reference to more of the available approaches.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
- en: The first technique is called **Individual Conditional Expectation** (**ICE**)
    and it is a local and model-agnostic approach to explainability. The *local* part
    refers to the fact that this technique describes the impact of feature(s) at the
    observation level. ICE is most frequently presented in a plot and depicts how
    an observation’s prediction changes as a result of a change in a given feature’s
    value.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
- en: To obtain the ICE values for a single observation in our dataset and one of
    its features, we have to create multiple copies of that observation. In all of
    them, we keep the values of other features (except the considered one) constant,
    while replacing the value of the feature of interest with the values from a grid.
    Most commonly, the grid consists of all the distinct values of that feature in
    the entire dataset (for all observations). Then, we use the (black box) model
    to make predictions for each of the modified copies of the original observation.
    Those predictions are plotted as the ICE curve.
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to calculate and intuitive to understand what the curves represent.
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ICE can uncover heterogeneous relationships, that is, when a feature has a different
    direction of impact on the target, depending on the intervals of the explored
    feature’s values.
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
- en: We can meaningfully display only one feature at a time.
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotting many ICE curves (for multiple observations) can make the plot overcrowded
    and hard to interpret.
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ICE assumes independence of features—when features are correlated, some points
    in the curve might actually be invalid data points (either very unlikely or simply
    impossible) according to the joint feature distribution.
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second approach is called the **Partial Dependence Plot** (**PDP**) and
    is heavily connected to ICE. It is also a model-agnostic method; however, it is
    a global one. It means that PDP describes the impact of feature(s) on the target
    in the context of the entire dataset.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
- en: PDP presents the marginal effect of a feature on the prediction. Intuitively,
    we can think of partial dependence as a mapping of the expected response of the
    target as a function of the feature of interest. It can also show whether the
    relationship between the feature and the target is linear or nonlinear. In terms
    of calculating the PDP, it is simply the average of all the ICE curves.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
- en: Similar to ICE, it is easy to calculate and intuitive to understand what the
    curves represent.
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the feature of interest is not correlated with other features, the PDP then
    perfectly represents how the selected feature impacts the prediction (on average).
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The calculation for the PDPs has a causal interpretation (within the model)—by
    observing the changes in prediction caused by the changes to one of the features,
    we analyze the causal relationship between the two.
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
- en: PDPs also assume the independence of features.
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PDPs can obscure heterogenous relationships created by interactions. For example,
    we could observe a linear relationship between the target and a certain feature.
    However, the ICE curves might show that there are exceptions to that pattern,
    for example, where the target remains constant in some ranges of the feature.
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PDPs can be used to analyze, at most, two features at a time.
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last of the XAI techniques we cover in this recipe is called **SHapley Additive
    exPlanations** (**SHAP**). It is a model-agnostic framework for explaining predictions
    using a combination of game theory and local explanations.
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
- en: The exact methodology and calculations involved in this method are outside of
    the scope of this book. We can briefly mention that **Shapley values** are a method
    used in game theory that involves a fair distribution of both gains and costs
    to players cooperating in a game. As each player contributes differently to the
    coalition, the Shapley value makes sure that each participant gets a fair share,
    depending on how much they contributed.
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
- en: 'We could compare it to the ML setting, in which features are the players, the
    cooperative game is creating the ML model’s prediction, and the payoff is the
    difference between the average prediction of the instance minus the average prediction
    of all instances. Hence, the interpretation of a Shapley value for a certain feature
    is as follows: the value of the feature contributed *x* to the prediction of this
    observation, compared to the average prediction for the dataset.'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
- en: Having covered the Shapley values, it is time to explain what SHAP is. It is
    an approach to explaining the outputs of any ML/DL model. SHAP combines optimal
    credit allocation with local explanations, using Shapley values (originating from
    game theory) and their extensions.
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
- en: 'SHAP offers the following:'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
- en: It is a computationally efficient and theoretically robust method of calculating
    Shapley values for ML models (ideally having trained the model only once).
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KernelSHAP—an alternative, kernel-based estimation method for estimating Shapley
    values. It was inspired by local surrogate models.
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TreeSHAP—an efficient estimation method for tree-based models.
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various global interpretation methods based on aggregations of Shapley values.
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get a better understanding of SHAP, it is recommended to also get familiar
    with LIME. Please refer to the *There’s more…* section for a brief description.
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
- en: Shapley values have a solid theoretical background (axioms of efficiency, symmetry,
    dummy, and additivity). Lundberg *et al.* (2017) explain minor discrepancies between
    those axioms in the context of Shapley values and their counterpart properties
    of the SHAP values, that is, local accuracy, missingness, and consistency.
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks to the efficiency property, SHAP might be the only framework in which
    the prediction is fairly distributed among the feature values.
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP offers global interpretability—it shows feature importance, feature dependence,
    interactions, and an indication of whether a certain feature has a positive or
    negative impact on the model’s predictions.
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP offers local interpretability—while many techniques only focus on aggregate
    explainability, we can calculate SHAP values for each individual prediction to
    learn how features contribute to that particular prediction.
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP can be used to explain a large variety of models, including linear models,
    tree-based models, and neural networks.
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TreeSHAP (the fast implementation for tree-based models) makes it feasible to
    use the approach for real-life use cases.
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
- en: Computation time—the number of possible combinations of the features increases
    exponentially with the number of considered features, which in turn increases
    the time of calculating SHAP values. That is why we have to revert to approximations.
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to permutation feature importance, SHAP values are sensitive to high
    correlations among features. If that is the case, the impact of such features
    on the model score can be split among those features in an arbitrary way, leading
    us to believe that they are less important than if their impacts remained undivided.
    Also, correlated features might result in using unrealistic/impossible combinations
    of features.
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As Shapley values do not offer a prediction model (such as in the case of LIME),
    they cannot be used to make statements about how a change in the inputs corresponds
    to a change in the prediction. For example, we cannot state that “if the value
    of feature *Y* was higher by 50 units, then the predicted probability would increase
    by 1 percentage point.”
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KernelSHAP is slow and, similarly to other permutation-based interpretation
    methods, ignores dependencies between features.
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-851
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will be using the credit card fraud dataset that we introduced
    in the *Investigating different approaches to handling imbalanced data* recipe.
    For convenience, we have included all the necessary preparation steps in this
    section of the accompanying Jupyter notebook.
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-853
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Execute the following steps to investigate various approaches to explaining
    the predictions of an XGBoost model trained on the credit card fraud dataset:'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  id: totrans-855
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-856
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Train the ML model:'
  id: totrans-857
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-858
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Executing the snippet generates the following output:'
  id: totrans-859
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-860
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: We can conclude that the model is overfitted to the training data and ideally
    we should try to fix that by, for example, using stronger regularization while
    training the XGBoost model. To keep the exercise concise, we assume that the model
    is good to go for further analysis.
  id: totrans-861
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similarly to investigating feature importance, we should first make sure that
    the model has satisfactory performance on the validation/test set before we start
    explaining its predictions.
  id: totrans-862
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the ICE curves:'
  id: totrans-863
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-864
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-865
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_35.png)'
  id: totrans-866
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.35: The ICE plot of the V4 feature, created using 5,000 random samples
    from the training data'
  id: totrans-867
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Figure 14.35* presents the ICE curves for the `V4` feature, calculated using
    `5,000` random observations from the training data. In the plot, we can see that
    the vast majority of the observations are located around `0`, while a few of the
    curves show quite a significant change in predicted probability.'
  id: totrans-868
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The black marks at the bottom of the plot indicate the percentiles of the feature
    values. By default, the ICE plot and PDP are constrained to the 5th and 95th percentiles
    of the feature values; however, we can change this using the `percentiles` argument.
  id: totrans-869
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A potential issue with the ICE curves is that it might be hard to see if the
    curves differ between observations, as they start at different predictions. A
    solution would be to center the curves at a certain point and display only the
    difference in the prediction compared to that point.
  id: totrans-870
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the centered ICE curves:'
  id: totrans-871
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-872
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-873
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_36.png)'
  id: totrans-874
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.36: The centered ICE plot of the V4 feature, created using 5,000
    random samples from the training data'
  id: totrans-875
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The interpretation of the centered ICE curves is only slightly different. Instead
    of looking at the impact of changing the value of a feature on the prediction,
    we look at the relative change in the prediction, as compared to the average prediction.
    This way, it is easier to analyze the direction of the change in the predicted
    value.
  id: totrans-876
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate the Partial Dependence Plot:'
  id: totrans-877
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-878
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-879
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_37.png)'
  id: totrans-880
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.37: The Partial Dependence Plot of the V4 feature, prepared using
    the training data'
  id: totrans-881
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By analyzing the plot, on average there seems to be a very small increase in
    the predicted probability with the increase of the `V4` feature.
  id: totrans-882
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similar to the ICE curves, we can also center the PDP.
  id: totrans-883
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To get some further insights, we can generate the PDP together with the ICE
    curves. We can do so using the following snippet:'
  id: totrans-884
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-885
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-886
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_38.png)'
  id: totrans-887
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.38: The Partial Dependence Plot of the V4 feature (prepared using
    the training data), together with the ICE curves'
  id: totrans-888
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As we can see, the partial dependence (PD) line is almost horizontal at 0\.
    Because of the differences in scale (please refer to *Figure 14.37*), the PD line
    is virtually meaningless in such a plot. To make the plot more readable or easier
    to interpret, we could try restricting the range of thy a-axis using the `plt.ylim`
    function. This way, we would focus on the area with the majority of the ICE curves,
    while neglecting the few ones that are far away from the bulk of the curves. However,
    we should keep in mind that those outlier curves are also important for the analysis.
  id: totrans-889
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate the individual PDPs of two features and a joint one:'
  id: totrans-890
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-891
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-892
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_39.png)'
  id: totrans-893
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.39: The centered Partial Dependence Plot of the V4 and V8 features,
    individually and jointly'
  id: totrans-894
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By jointly plotting the PDPs of two features, we are able to visualize the interactions
    among them. By looking at *Figure 14.39* we could draw a conclusion that the `V4`
    feature is more important, as most of the lines visible in the rightmost plot
    are perpendicular to the `V4` axis and parallel to the `V8` axis. However, there
    is some shift in the decision lines determined by the `V8` feature, for example,
    around the `0.25` value.
  id: totrans-895
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Instantiate an explainer and calculate the SHAP values:'
  id: totrans-896
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-897
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: The `shap_values` object is a `284807` by `29` `numpy` array containing the
    calculated SHAP values.
  id: totrans-898
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate the SHAP summary plot:'
  id: totrans-899
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-900
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-901
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_40.png)'
  id: totrans-902
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.40: The summary plot calculated using SHAP values'
  id: totrans-903
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When looking at the summary plot, we should be aware of the following:'
  id: totrans-904
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Features are sorted by the sum of the SHAP value magnitudes (absolute values)
    across all observations.
  id: totrans-905
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The color of the points shows if that feature had a high or low value for that
    observation.
  id: totrans-906
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The horizontal location on the plot shows whether the effect of that feature’s
    value resulted in a higher or lower prediction.
  id: totrans-907
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, the plots display the `20` most important features. We can adjust
    that using the `max_display` argument.
  id: totrans-908
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Overlapping points are jittered in the *y* axis direction. Hence, we can get
    a sense of the distribution of the SHAP values per feature.
  id: totrans-909
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An advantage of this type of plot over other feature importance metrics (for
    example, permutation importance) is that it contains more information that can
    help with understanding the global feature importance. For example, let’s assume
    that a feature is of medium importance. Using this plot, we could see if that
    medium importance corresponds to the feature values having a large effect on the
    prediction for a few observations, but in general no effect. Or maybe it had a
    medium-sized effect on all predictions.
  id: totrans-910
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Having discussed the overall considerations, let’s mention a few observations
    from *Figure 14.40*:'
  id: totrans-911
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Overall, high values of the `V4` feature (the most important one) contributed
    to higher predictions, while lower values resulted in lower predictions (observation
    being less likely to be a fraudulent one).
  id: totrans-912
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The overall effect of the `V14` feature on the prediction was negative, but
    for quite a few observations with a low value of that feature, it resulted in
    a higher prediction.
  id: totrans-913
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alternatively, we can present the same information using a bar chart. Then,
    we focus on the aggregate feature importance, while ignoring the insights into
    feature effects:'
  id: totrans-914
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-915
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-916
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_41.png)'
  id: totrans-917
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.41: The summary plot (bar chart) calculated using the SHAP values'
  id: totrans-918
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Naturally, the order of the features (their importance) is the same as in *Figure
    14.40*. We could use this plot as an alternative to the permutation feature importance.
    However, we should then keep in mind the underlying differences. Permutation feature
    importance is based on the decrease in model performance (measured using a metric
    of choice), while SHAP is based on the magnitude of feature attributions.
  id: totrans-919
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can get an even more concise representation of the summary chart using the
    following command: `shap.plots.bar(explainer_x)`.'
  id: totrans-920
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Locate an observation belonging to the positive and negative classes:'
  id: totrans-921
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-922
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Explain those observations:'
  id: totrans-923
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-924
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-925
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_42.png)'
  id: totrans-926
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.42: An (abbreviated) force plot explaining an observation belonging
    to the negative class'
  id: totrans-927
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In a nutshell, the force plot shows how features contribute to pushing the prediction
    from the base value (average prediction) to the actual prediction. As the plot
    contained much more information and it was too wide to fit the page, we only present
    the most relevant part. Please refer to the accompanying Jupyter notebook to inspect
    the full plot.
  id: totrans-928
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Below are some of the observations we can make based on *Figure 14.42*:'
  id: totrans-929
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The base value (*-8.589*) is the average prediction of the entire dataset.
  id: totrans-930
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f(x) = -13.37* is the prediction of this observation.'
  id: totrans-931
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We can interpret the arrows as the impact of given features on the prediction.
    The red arrows indicate an increase in the prediction. The blue arrows indicate
    a decrease in the prediction. The size of the arrows corresponds to the magnitude
    of the feature’s effect. The values by the feature names show the feature values.
  id: totrans-932
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If we subtract the total length of the red arrows from the total length of the
    blue arrows, we will get the distance from the base value to the final prediction.
  id: totrans-933
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As such, we can see that the biggest contributor to the decrease in the prediction
    (compared to the average prediction) was feature `V14`'s value of -*0.3112*.
  id: totrans-934
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We then follow the same step for the positive observation:'
  id: totrans-935
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-936
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-937
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_43.png)'
  id: totrans-938
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.43: An (abbreviated) force plot explaining an observation belonging
    to the positive class'
  id: totrans-939
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compared to *Figure 14.42*, we can clearly see how outbalanced the blue features
    (negatively impacting the prediction, labeled *lower*) are compared to the red
    ones (labeled *higher*). We can also see that both figures have the same base
    value, as this is the dataset’s average predicted value.
  id: totrans-940
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a waterfall plot for the positive observation:'
  id: totrans-941
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-942
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-943
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_44.png)'
  id: totrans-944
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14.44: A waterfall plot explaining an observation from the positive
    class'
  id: totrans-945
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Inspecting *Figure 14.44* reveals many similarities to *Figure 14.43*, as both
    plots are explaining the very same observation using a slightly different visualization.
    Hence, most of the insights on interpreting the waterfall plot are the same as
    for the force plot. Some nuances include:'
  id: totrans-946
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The bottom of the plot starts at the baseline value (the model’s average prediction).
    Then, each row shows the positive or negative contribution of each feature that
    leads to the model’s final prediction for that particular observation.
  id: totrans-947
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP explains XGBoost classifiers in terms of their margin output. This means
    that the units on the *x* axis are log-odds units. A negative value implies probabilities
    lower than `0.5` that the observation was a fraudulent one.
  id: totrans-948
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The least impactful features are collapsed into a joint term. We can control
    that using the `max_display` argument of the function.
  id: totrans-949
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create a dependence plot of the `V4` feature:'
  id: totrans-950
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-951
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-952
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/B18112_14_45.png)'
  id: totrans-953
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.45: A dependence plot visualizing the dependence between the V4 and
    V12 features'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
- en: 'Some things to know about a dependence plot:'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
- en: It is potentially the simplest global interpretation plot.
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This type of plot is an alternative to Partial Dependence Plots. While PDPs
    show the average effects, the SHAP dependence plot additionally shows the variance
    on the *y* axis. Hence it contains information about the distribution of effects.
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The plot presents the feature’s value (*x* axis) vs. the SHAP value of that
    feature (*y* axis) across all the observations in the dataset. Each dot represents
    a single observation.
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given we are explaining an XGBoost classification model, the unit of the *y*
    axis is the log odds of being a fraudulent case.
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The color corresponds to a second feature that may have an interaction effect
    with the feature we specified. It is automatically selected by the `shap` library.
    The documentation states that if an interaction effect is present between the
    two features, it will show up as a distinct vertical pattern of coloring. In other
    words, we should look out for clear vertical spreads between colors for the same
    values on the *x* axis.
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To complete the analysis, we can mention a potential conclusion from *Figure
    14.45*. Unfortunately, it will not be quite intuitive, as the features were anonymized.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s look at observations with the value of feature `V4` around
    5\. For those samples, observations with lower values of feature `V12` are more
    likely to be fraudulent than the observations with higher values of the `V12`
    feature.
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-963
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After importing the libraries, we trained an XGBoost model to detect credit
    card fraud.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 3*, we plotted the ICE curves using `PartialDependenceDisplay` class.
    We had to provide the fitted model, the dataset (we used the training set), and
    the feature(s) of interest. Additionally, we provided the `subsample` argument,
    which specified the number of random observations from the dataset for which the
    ICE curves were plotted. As the dataset has over *200,000* observations, we arbitrarily
    chose *5,000* as a manageable number of curves to be plotted.
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
- en: We have mentioned that the grid used for calculating the ICE curves most frequently
    consists of all the unique values available in the dataset. `scikit-learn` by
    default creates an equally spaced grid, covering the range between the extreme
    values of the feature. We can customize the grid’s density using the `grid_resolution`
    argument.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
- en: 'The `from_estimator` method of `PartialDependenceDisplay` also accepts the
    `kind` argument, which can take the following values:'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
- en: '`kind="individual"`—the method will plot the ICE curves.'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kind="average"`—the method will display the Partial Dependence Plot.'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kind="both"`—the method will display both the PDP and ICE curves.'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Step 4*, we plotted the same ICE curves; however, we centered them at the
    origin. We did so by setting the `centered` argument to `True`. This effectively
    subtracts the average target value from the target vector and centers the target
    value at `0`.
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, we plotted the Partial Dependence Plot, also using the `PartialDependenceDisplay.from_estimator`.
    As the PDP is the default value, we did not have to specify the `kind` argument.
    We also showed the outcome of plotting both the PDP and ICE curves in the same
    figure. As plotting the two-way PDP takes quite a bit of time, we sampled (without
    replacement) *20,000* observations from the training set.
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
- en: One thing to keep in mind about `PartialDependenceDisplay` is that it treats
    categorical features as numeric.
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
- en: Partial Dependence Plots are also available in the `pdpbox` library.
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 6*, we created a more complex figure using the same functionality of
    `PartialDependenceDisplay`. In one figure, we plotted the individual PD plots
    of two features (`V4` and `V8`), and their joint (also called two-way) PD plot.
    To obtain the last one, we had to provide the two features of interest as a tuple.
    By specifying `features=["V4", "V8", ("V4", "V8")]`, we indicated that we wanted
    to plot two individual PD plots and then a joint one for the two features. Naturally,
    there is no need to plot all `3` plots in the same figure. We could have used
    `features=[("V4", "V8")]` to create just the joint PDP.
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting angle to explore would be to overlay two Partial Dependence
    Plots, calculated for the same feature but using different ML models. Then we
    could compare if the expected impact on the prediction is similar across different
    models.
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
- en: 'We have focused on plotting the ICE curves and the Partial Dependence line.
    However, we can also calculate those values without automatically plotting them.
    To do so, we can use the `partial_dependence` function. It returns a dictionary
    containing `3` elements: the values that create the evaluated grid, the predictions
    for all the points in the grid for all samples in the dataset (used for ICE curves),
    and the averaged values of the predictions for each point in the grid (used for
    the PDP).'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 7*, we instantiated the `explainer` object, which is the primary class
    used to explain any ML/DL model using the `shap` library. To be more precise,
    we used the `TreeExplainer` class, as we were trying to explain an XGBoost model,
    that is, a tree-based model. Then, we calculated the SHAP values using the `shap_values`
    method of the instantiated `explainer`. To explain the model’s predictions, we
    used the entire dataset. At this point, we could have also decided to use the
    training or validation/test sets.
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
- en: By definition, SHAP values are very complicated to compute (an NP-hard class
    problem). However, thanks to the simplicity of linear models, we can read the
    SHAP values from a partial dependence plot. Please refer to `shap`'s documentation
    for more information on this topic.
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 8*, we started with global explanation approaches. We generated two
    variants of a summary plot using the `shap.summary_plot` function. The first one
    was a density scatterplot of SHAP values for each of the features. It combines
    the overall feature importance with feature effects. We can use that information
    to evaluate the impact each feature has on the model’s predictions (also on the
    observation level).
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
- en: The second one was a bar chart, showing the average of the absolute SHAP values
    across the entire dataset. In both cases, we can use the plot to infer the feature
    importance calculated using SHAP values; however, the first plot provides additional
    information. To generate this plot, we had to additionally pass `plot_type="bar"`
    while calling the `shap.summary_plot` function.
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
- en: After looking at the global explanations, we wanted to look into local ones.
    To make the analysis more interesting, we wanted to present the explanations for
    observations belonging to both the negative and positive classes. That is why
    in *Step 9* we identified the indices of such observations.
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 10*, we used `shap.force_plot` to explain observation-level predictions
    of both observations. While calling the function, we had to provide three inputs:'
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
- en: The baseline value (the average prediction for the entire dataset), which is
    available in the explainer object (`explainer.expected_value`)
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SHAP values for the particular observation
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The feature values of the particular observation
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Step 11*, we also created an observation-level plot explaining the predictions;
    however, we used a slightly different representation. We created a waterfall plot
    (using the `shap.plots.waterfall` function) to explain the positive observation.
    The only thing worth mentioning is that the function expects a single row of an
    `Explanation` object as input.
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
- en: In the last step, we created a SHAP dependence plot (a global-level explanation)
    using the `shap.dependence_plot` function. We had to provide the feature of interest,
    the SHAP values, and the feature values. As the considered feature, we selected
    the `V4` one as it was identified as the most important one by the summary plot.
    The second feature (`V12`) was determined automatically by the library.
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-989
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we have only provided a glimpse of the field of XAI. The field
    is constantly growing, as explainable methods are becoming more and more important
    for practitioners and businesses.
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
- en: Another popular XAI technique is called LIME, which stands for **Local Interpretable
    Model-Agnostic Explanations**. It is an observation-level approach used for explaining
    the predictions of any model in an interpretable and faithful manner. To obtain
    the explanations, LIME locally approximates the selected hard-to-explain model
    with an interpretable one (such as linear models with regularization). The interpretable
    models are trained on small perturbations (with additional noise) of the original
    observations, thus providing a good local approximation.
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
- en: '**Treeinterpreter** is another observation-level XAI method useful for explaining
    Random Forest models. The idea is to use the underlying trees to explain how each
    feature contributes to the end result. The prediction is defined as the sum of
    each feature’s contributions and the average given by the initial node that is
    based on the entire training set. Using this approach, we can observe how the
    value of the prediction changes along the prediction path within the decision
    tree (after every split), combined with the information on which features caused
    the split, that is, a change in prediction.'
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, there are many more available approaches, for example:'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
- en: Ceteris-paribus profiles
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Break-down plots
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accumulated Local Effects (ALE)
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global surrogate models
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counterfactual explanations
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anchors
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We recommend investigating the following Python libraries focusing on AI explainability:'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
- en: '`shapash`—compiles various visualizations from SHAP/LIME as an interactive
    dashboard in the form of a web app.'
  id: totrans-1001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`explainerdashboard`—prepares a dashboard web app that explains `scikit-learn`-compatible
    ML models. The dashboard covers model performance, feature importance, feature
    contributions to individual predictions, a “what if” analysis, PDPs, SHAP values,
    visualization of individual decision trees, and more.'
  id: totrans-1002
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dalex`—the library covers various XAI methods, including variable importance,
    PDPs and ALE plots, breakdown and SHAP waterfall plots, and more.'
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interpret`—the InterpretML library was created by Microsoft. It covers popular
    explanation methods of black-box models (such as PDPs, SHAP, LIME, and so on)
    and allows you to train so-called glass-box models, which are interpretable. For
    example, `ExplainableBoostingClassifier` is designed to be fully interpretable,
    but at the same time provides similar accuracy to the state-of-the-art algorithms.'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eli5`—an explainability library that provides various global and local explanations.
    It also covers text explanation (powered by LIME) and permutation feature importance.'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alibi`—a library focusing on model inspection and interpretation. It covers
    approaches such as anchors explanations, integrated gradients, counterfactual
    examples, the Contrastive Explanation Method, and accumulated local effects.'
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-1007
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Additional resources are available here:'
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
- en: 'Biecek, P., & Burzykowski, T. 2021\. *Explanatory model analysis: Explore,
    explain and examine predictive models*. Chapman and Hall/CRC.'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Friedman, J. H. 2001\. “Greedy function approximation: a gradient boosting
    machine.” *Annals of Statistics*: 1189–1232.'
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goldstein, A., Kapelner, A., Bleich, J., & Pitkin, E. 2015\. “Peeking inside
    the black box: Visualizing statistical learning with plots of individual conditional
    expectation.” *Journal of Computational and Graphical Statistics*, 24(1): 44–65.'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. 2009\. *The
    Elements of Statistical Learning: Data Mining, Inference, and Prediction*, 2:
    1–758). New York: Springer.'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lundberg, S. M., Erion, G., Chen, H., DeGrave, A., Prutkin, J. M., Nair, B.,
    ... & Lee, S. I. 2020\. “From local explanations to global understanding with
    explainable AI for trees.” *Nature Machine Intelligence*, 2(1): 56–67.'
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lundberg, S. M., Erion, G. G., & Lee, S. I. 2018\. “Consistent individualized
    feature attribution for tree ensembles.” *arXiv preprint arXiv:1802.03888*.
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lundberg, S. M., & Lee, S. I. 2017\. A unified approach to interpreting model
    predictions. *Advances in Neural Information Processing Systems*, *30*.
  id: totrans-1015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Molnar, C. 2020\. *Interpretable machine learning.* [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/).
  id: totrans-1016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ribeiro, M.T., Singh, S., & Guestrin, C. 2016\. “Why should I trust you?: Explaining
    the predictions of any classifier.” Proceedings of the *22nd ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining*. ACM.'
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saabas, A. *Interpreting random forests*. [http://blog.datadive.net/interpreting-random-forests/](http://blog.datadive.net/interpreting-random-forests/).
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1019
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered a wide variety of useful concepts that can
    help with improving almost any ML or DL project. We started by exploring more
    complex classifiers (which also have their corresponding variants for regression
    problems), considering alternative approaches to encoding categorical features,
    creating stacked ensembles, and looking into possible solutions to class imbalance.
    We also showed how to use the Bayesian approach to hyperparameter tuning, in order
    to find an optimal set of hyperparameters faster than using the more popular yet
    uninformed grid search approaches.
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
- en: We have also dived into the topic of feature importance and AI explainability.
    This way, we can better understand what is happening in the so-called black box
    models. This is crucial not only for the people working on the ML/DL project but
    also for any business stakeholders. Additionally, we can combine those insights
    with feature selection techniques to potentially further improve a model’s performance
    or reduce its training time.
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, the data science field is constantly growing and more and more useful
    tools are becoming available every day. We cannot cover all of them, but below
    you can find a short list of libraries/tools that you might find useful in your
    projects:'
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
- en: '`DagsHub`—a platform similar to GitHub, but tailor-made for data scientists
    and machine learning practitioners. By integrating powerful open-source tools
    such as Git, DVC, MLFlow, and Label Studio and doing the DevOps heavy lifting
    for its users, you can easily build, manage and scale your ML project - all in
    one place.'
  id: totrans-1023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deepchecks`—an open-source Python library for testing ML/DL models and data.
    We can use the library for various testing and validation needs throughout our
    projects; for example, we can verify our data’s integrity, inspect the features’
    and target’s distributions, confirm valid data splits, and evaluate the performance
    of our models.'
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DVC`—an open-source version control system for ML projects. Using **DVC**
    (**data version control**), we can store the information about different versions
    of our data (be it tabular, images, or something else) and models in Git, while
    storing the actual data elsewhere (cloud storage like AWS, GCS, Google Drive,
    and so on). Using DVC, we can also create reproducible data pipelines, while storing
    the intermediate versions of the datasets along the way. And to make using it
    easier, DVC uses the same syntax as Git.'
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLFlow`—an open-source platform for managing the ML life cycle. It covers
    aspects such as experimentation, reproducibility, deployment, and model registry.'
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nannyML`—an open-source Python library for post-deployment data science. We
    can use it to identify data drift (a change in the distribution of the features
    between the data used for training a model and inference in production) or to
    estimate the model’s performance in the absence of ground truth. The latter one
    can be especially interesting for projects in which the ground truth becomes available
    after a long period of time, for example, a loan default within multiple months
    from the moment of making the prediction.'
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pycaret`—an open-source, low-code Python library that automates a lot of the
    components of ML workflows. For example, we can train and tune dozens of machine
    learning models for a classification or regression task using as little as a few
    lines of code. It also contains separate modules for anomaly detection or time
    series forecasting.'
  id: totrans-1028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
