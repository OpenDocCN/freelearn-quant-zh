- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Advanced Concepts for Machine Learning Projects
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习项目的高级概念
- en: In the previous chapter, we introduced a possible workflow for solving a real-life
    problem using machine learning. We went over the entire project, starting with
    cleaning the data, through training and tuning a model, and then lastly evaluating
    its performance. However, this is rarely the end of the project. In that project,
    we used a simple decision tree classifier, which most of the time can be used
    as a benchmark or minimum viable product (MVP). In this chapter, we cover a few
    more advanced concepts that can help with improving the value of the project and
    make it easier to adopt by the business stakeholders.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了解决现实问题的可能工作流程，使用机器学习从数据清洗、模型训练和调优，到最后评估其表现。然而，这通常并不是项目的终点。在那个项目中，我们使用了一个简单的决策树分类器，它通常可以作为基准或最小可行产品（MVP）。在本章中，我们将介绍一些更高级的概念，这些概念有助于提升项目的价值，并使其更容易被业务利益相关者采纳。
- en: 'After creating the MVP, which serves as a baseline, we would like to improve
    the model’s performance. While attempting to improve the model, we should also
    try to balance underfitting and overfitting. There are a few ways to do so, some
    of which include:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了最小可行产品（MVP）作为基准之后，我们希望提高模型的表现。在尝试改善模型时，我们还应该平衡欠拟合和过拟合。实现这一点有几种方法，其中包括：
- en: Gathering more data (observations)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集更多数据（观测数据）
- en: Adding more features—either by gathering additional data (for example, by using
    external data sources) or through feature engineering using currently available
    information
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多特征——无论是通过收集额外的数据（例如，使用外部数据源）还是通过特征工程使用当前可用的信息
- en: Using more complex models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更复杂的模型
- en: Selecting only the relevant features
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只选择相关特征
- en: Tuning the hyperparameters
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整超参数
- en: There is a common stereotype that data scientists spend 80% of their time on
    a project gathering and cleaning data while only 20% remains for the actual modeling.
    In line with the stereotype, adding more data might greatly improve a model’s
    performance, especially when dealing with imbalanced classes in a classification
    problem. But finding additional data (be it observations or features) is not always
    possible, or might simply be too complicated. Then, the other solution may be
    to use more complex models or to tune the hyperparameters to squeeze out some
    extra performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个常见的刻板印象认为，数据科学家在一个项目中花费80%的时间收集和清理数据，剩下的20%才用于实际的建模。按照这一刻板印象，增加更多数据可能会大大提高模型的表现，尤其是在处理分类问题中的不平衡类别时。但寻找额外的数据（无论是观测数据还是特征）并不总是可行，或者可能会变得非常复杂。那么，另一个解决方案可能是使用更复杂的模型，或者调整超参数以挤压出一些额外的性能。
- en: We start the chapter by presenting how to use more advanced classifiers, which
    are also based on decision trees. Some of them (XGBoost and LightGBM) are frequently
    used for winning machine learning competitions (such as those found on Kaggle).
    Additionally, we introduce the concept of stacking multiple machine learning models
    to further improve prediction performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们将介绍如何使用更先进的分类器，这些分类器同样基于决策树。其中一些（如XGBoost和LightGBM）在机器学习竞赛中常常被用来获胜（例如Kaggle上的竞赛）。此外，我们还介绍了堆叠多个机器学习模型的概念，以进一步提高预测性能。
- en: Another common real-life problem concerns dealing with imbalanced data, that
    is, when one class (such as default or fraud) is rarely observed in practice.
    This makes it especially difficult to train a model to accurately capture the
    minority class observations. We introduce a few common approaches to handling
    class imbalance and compare their performance on a credit card fraud dataset,
    in which the minority class corresponds to 0.17% of all the observations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的现实问题是处理不平衡数据，也就是当某一类别（如违约或欺诈）在实际中很少出现时。这使得训练一个准确捕捉少数类别观测值的模型变得特别困难。我们介绍了几种处理类别不平衡的常见方法，并在信用卡欺诈数据集上比较了它们的表现，其中少数类别仅占所有观测值的0.17%。
- en: Then, we also expand on hyperparameter tuning, which was explained in the previous
    chapter. Previously, we used either an exhaustive grid search or a randomized
    search, both of which are carried out in an uninformed manner. This means that
    there is no underlying logic in selecting the next set of hyperparameters to investigate.
    This time, we introduce Bayesian optimization, in which past attempts are used
    to select the next set of values to explore. This approach can significantly speed
    up the tuning phase of our projects.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们还扩展了超参数调优的内容，这在前一章中已有解释。之前，我们使用了穷尽的网格搜索或随机搜索，这两种方法都是在无信息的情况下进行的。这意味着在选择下一个要探索的超参数集时，并没有底层逻辑。这一次，我们介绍了贝叶斯优化方法，在这种方法中，过去的尝试被用来选择下一个要探索的超参数集。这种方法可以显著加速我们项目的调优阶段。
- en: In many industries (and finance especially) it is crucial to understand the
    logic behind a model’s prediction. For example, a bank might be legally obliged
    to provide actual reasons for declining a credit request, or it can try to limit
    its losses by predicting which customers are likely to default on a loan. To get
    a better understanding of the models, we explore various approaches to determining
    feature importance and model explainability. The latter is especially relevant
    when dealing with complex models, which are often considered to be black boxes,
    that is, unexplainable. We can additionally use those insights to select only
    the most relevant features, which can further improve the model’s performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多行业（尤其是金融行业），理解模型预测背后的逻辑至关重要。例如，银行可能在法律上被要求提供拒绝信用请求的实际理由，或者它可以通过预测哪些客户可能违约来尝试限制损失。为了更好地理解模型，我们探讨了确定特征重要性和模型可解释性的各种方法。后者在处理复杂模型时尤为重要，因为这些模型通常被认为是黑箱，即无法解释的。我们还可以利用这些见解，仅选择最相关的特征，这可以进一步提高模型的性能。
- en: 'In this chapter, we present the following recipes:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍以下几种方法：
- en: Exploring ensemble classifiers
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索集成分类器
- en: Exploring alternative approaches to encoding categorical features
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索编码分类特征的替代方法
- en: Investigating different approaches to handling imbalanced data
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探讨处理不平衡数据的不同方法
- en: Leveraging the wisdom of the crowds with stacked ensembles
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用群众智慧的堆叠集成模型
- en: Bayesian hyperparameter optimization
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯超参数优化
- en: Investigating feature importance
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探讨特征重要性
- en: Exploring feature selection techniques
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索特征选择技术
- en: Exploring explainable AI techniques
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索可解释的人工智能技术
- en: Exploring ensemble classifiers
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索集成分类器
- en: 'In *Chapter 13*, *Applied Machine Learning: Identifying Credit Default*, we
    learned how to build an entire machine learning pipeline, which contained both
    preprocessing steps (imputing missing values, encoding categorical features, and
    so on) and a machine learning model. Our task was to predict customer default,
    that is, their inability to repay their debts. We used a decision tree model as
    the classifier.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第13章*，*应用机器学习：识别信用违约*中，我们学习了如何构建一个完整的机器学习管道，其中包含预处理步骤（填补缺失值、编码分类特征等）和机器学习模型。我们的任务是预测客户违约，即无法偿还债务。我们使用了决策树模型作为分类器。
- en: Decision trees are considered simple models and one of their drawbacks is overfitting
    to the training data. They belong to the group of high-variance models, which
    means that a small change to the training data can greatly impact the tree’s structure
    and its predictions. To overcome those issues, they can be used as building blocks
    for more complex models. **Ensemble models** combine predictions of multiple base
    models (for example, decision trees) in order to improve the final model’s generalizability
    and robustness. This way, they transform the initial high-variance estimators
    into a low-variance aggregate estimator.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树被认为是简单模型，它们的一个缺点是对训练数据的过拟合。它们属于高方差模型，这意味着对训练数据的微小变化会极大地影响树的结构和预测结果。为了克服这些问题，决策树可以作为更复杂模型的构建块。**集成模型**通过结合多个基础模型（例如决策树）的预测，以提高最终模型的泛化能力和鲁棒性。这样，它们将最初的高方差估计器转变为低方差的综合估计器。
- en: 'On a high level, we could divide the ensemble models into two groups:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，我们可以将集成模型分为两组：
- en: '**Averaging methods**—several models are estimated independently and then their
    predictions are averaged. The underlying principle is that the combined model
    is better than a single one as its variance is reduced. Examples: Random Forest
    and Extremely Randomized Trees.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均法**—多个模型独立估计，然后将它们的预测结果平均。其基本原理是，组合模型比单一模型更好，因为其方差减少了。示例：随机森林和极度随机化树。'
- en: '**Boosting methods**—in this approach, multiple base estimators are built sequentially
    and each one tries to reduce the bias of the combined estimator. Again, the underlying
    assumption is that a combination of multiple weak models produces a powerful ensemble.
    Examples: Gradient Boosted Trees, XGBoost, LightGBM, and CatBoost.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升方法**—在这种方法中，多个基本估计器被顺序构建，每个估计器都试图减少组合估计器的偏差。其基本假设是，多个弱模型的组合会产生一个强大的集成模型。示例：梯度提升树、XGBoost、LightGBM
    和 CatBoost。'
- en: In this recipe, we use a selection of ensemble models to try to improve the
    performance of the decision tree approach. As those models are based on decision
    trees, the same principles about feature scaling (no explicit need for it) apply
    and we can reuse most of the previously created pipeline.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们使用了一些集成模型来尝试提升决策树方法的性能。由于这些模型基于决策树，关于特征缩放（不需要显式进行）的相同原则适用，因此我们可以重用之前创建的大部分管道。
- en: Getting ready
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we build on top of what we already established in the *Organizing
    the project with pipelines* recipe from the previous chapter, in which we created
    the default prediction pipeline, from loading the data to training the classifier.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们基于上一章的*通过管道组织项目*食谱中的内容，创建了默认的预测管道，从加载数据到训练分类器。
- en: In this recipe, we use the variant without the outlier removal procedure. We
    will be replacing the last step (the classifier) with more complex ensemble models.
    Additionally, we first fit the decision tree pipeline to the data to obtain the
    baseline model for performance comparison. For your convenience, we reiterate
    all the required steps in the notebook accompanying this chapter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们使用的是不包含异常值去除程序的变体。我们将用更复杂的集成模型替换最后一步（分类器）。此外，我们首先将决策树管道拟合到数据中，以获得基线模型用于性能比较。为了方便起见，我们在本章附带的笔记本中重申了所有必需的步骤。
- en: How to do it...
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Execute the following steps to train the ensemble classifiers:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以训练集成分类器：
- en: 'Import the libraries:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE0]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this chapter, we also use the already familiar `performance_evaluation_report`
    helper function.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本章中，我们还使用了已经熟悉的 `performance_evaluation_report` 辅助函数。
- en: 'Define and fit the Random Forest pipeline:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并拟合随机森林管道：
- en: '[PRE1]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The performance of the Random Forest can be summarized by the following plot:'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机森林的性能可以通过以下图表总结：
- en: '![](../Images/B18112_14_01.png)'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_01.png)'
- en: 'Figure 14.1: Performance evaluation of the Random Forest model'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.1：随机森林模型的性能评估
- en: 'Define and fit the Gradient Boosted Trees pipeline:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并拟合梯度提升树管道：
- en: '[PRE2]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The performance of the Gradient Boosted Trees can be summarized by the following
    plot:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 梯度提升树的性能可以通过以下图表总结：
- en: '![](../Images/B18112_14_02.png)'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_02.png)'
- en: 'Figure 14.2: Performance evaluation of the Gradient Boosted Trees model'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.2：梯度提升树模型的性能评估
- en: 'Define and fit an XGBoost pipeline:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并拟合 XGBoost 管道：
- en: '[PRE3]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The performance of the XGBoost can be summarized by the following plot:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: XGBoost 的性能可以通过以下图表总结：
- en: '![](../Images/B18112_14_03.png)'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_03.png)'
- en: 'Figure 14.3: Performance evaluation of the XGBoost model'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.3：XGBoost 模型的性能评估
- en: 'Define and fit the LightGBM pipeline:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并拟合 LightGBM 管道：
- en: '[PRE4]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The performance of the LightGBM can be summarized by the following plot:'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LightGBM 的性能可以通过以下图表总结：
- en: '![](../Images/B18112_14_04.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_04.png)'
- en: 'Figure 14.4: Performance evaluation of the LightGBM model'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4：LightGBM 模型的性能评估
- en: From the reports, it looks like the shapes of the ROC curve and the Precision-Recall
    curve were very similar for all the considered models. We will look at the scores
    of the models in the *There’s more…* section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从报告来看，所有考虑的模型的 ROC 曲线和精确率-召回率曲线形状非常相似。我们将在*更多内容…*部分查看各个模型的得分。
- en: How it works...
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: This recipe shows how easy it is to use different classifiers, as long as we
    want to use their default settings. In the first step, we imported the classifiers
    from their respective libraries.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例展示了使用不同分类器是多么简单，只要我们希望使用它们的默认设置。在第一步中，我们从各自的库中导入了分类器。
- en: In this recipe, we have used the `scikit-learn` API of libraries such as XGBoost
    or LightGBM. However, we could also use their native approaches to training models,
    which might require some additional effort, such as converting a `pandas` DataFrame
    to formats acceptable by those libraries. Using the native approaches can yield
    some extra benefits, for example, in terms of accessing certain hyperparameters
    or configuration settings.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了`scikit-learn` API，结合了像XGBoost或LightGBM这样的库。然而，我们也可以使用它们的原生方法来训练模型，这可能需要一些额外的工作，例如将`pandas`
    DataFrame转换为这些库接受的格式。使用原生方法可以带来一些额外的好处，例如可以访问某些超参数或配置设置。
- en: In *Steps 2* to *5*, we created a separate pipeline for each classifier. We
    combined the already established `ColumnTransformer` preprocessor with the corresponding
    classifier. Then, we fitted each pipeline to the training data and presented the
    performance evaluation report.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤2*到*步骤5*中，我们为每个分类器创建了一个独立的流水线。我们将已经建立的`ColumnTransformer`预处理器与相应的分类器结合在一起。然后，我们将每个流水线拟合到训练数据并展示了性能评估报告。
- en: Some of the considered ensemble models offer additional functionalities in the
    `fit` method (as opposed to setting hyperparameters when instantiating the class).
    For example, when using the `fit` method of LightGBM we can pass in the names/indices
    of categorical features. By doing so, the algorithm knows how to treat those features
    using its own approach, without the need for explicit one-hot encoding. Similarly,
    we could use a wide variety of available callbacks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一些考虑的集成模型在`fit`方法中提供了额外的功能（而不是在实例化类时设置超参数）。例如，使用LightGBM的`fit`方法时，我们可以传入分类特征的名称/索引。这样，算法就知道如何使用其自身的方法处理这些特征，而无需显式地进行独热编码。同样，我们还可以使用各种可用的回调函数。
- en: Thanks to modern Python libraries, fitting all the considered classifiers was
    extremely easy. We only had to replace the model’s class in the pipeline with
    another one. Keeping in mind how simple it is to experiment with different models,
    it is good to have at least a basic understanding of what those models do and
    what their strengths and weaknesses are. That is why below we provide a brief
    introduction to the considered algorithms.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了现代Python库，拟合所有考虑的分类器变得异常简单。我们只需将流水线中的模型类替换为另一个模型类。考虑到尝试不同模型是多么简单，理解这些模型的工作原理以及它们的优缺点是非常重要的。这就是为什么下面我们提供了对所考虑算法的简要介绍。
- en: Random Forest
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: '**Random Forest** is an example of an ensemble of models, that is, it trains
    multiple models (decision trees) and uses them to create predictions. In the case
    of a regression problem, it takes the average value of all the underlying trees.
    For classification it uses a majority vote. Random Forest offers more than just
    training many trees and aggregating their results.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**是一个集成模型的例子，即它训练多个模型（决策树），并利用这些模型来进行预测。在回归问题中，它取所有树的平均值；在分类问题中，它使用多数投票。随机森林不仅仅是训练多棵树并汇总它们的结果。'
- en: First, it uses **bagging** (bootstrap aggregation)—each tree is trained on a
    subset of all available observations. Those are drawn randomly with replacement,
    so—unless specified otherwise—the total number of observations used for each tree
    is the same as the total in the training set. Even though a single tree might
    have high variance with respect to a particular dataset (due to bagging), the
    forest will have lower variance overall, without increasing the bias. Additionally,
    this approach can also reduce the effect of any outliers in the data as they will
    not be used in all of the trees. To add even more randomness, each tree only considers
    a subset of all features to create each split. We can control that number using
    a dedicated hyperparameter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它使用**Bagging**（自助聚合法）——每棵树都在所有可用观测值的子集上进行训练。这些观测值是通过有放回的随机抽样得到的，因此——除非另行指定——每棵树使用的观测值总数与训练集中的总观测值数相同。即使单棵树可能会因为Bagging的原因在特定数据集上有较高的方差，但整个森林的方差将会较低，而不会增加偏差。此外，这种方法还可以减少数据中异常值的影响，因为它们不会出现在所有的树中。为了增加更多的随机性，每棵树只考虑所有特征的一个子集来创建每个分裂。我们可以使用一个专门的超参数来控制这个数字。
- en: Thanks to those two mechanisms, the trees in the forest are not correlated with
    each other and are built independently. The latter allows for the parallelization
    of the tree-building step.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了这两种机制，森林中的树彼此之间没有关联，并且是独立构建的。后者使得树构建步骤可以并行化。
- en: Random Forest provides a good trade-off between complexity and performance.
    Often—without any tuning—we can get much better performance than when using simpler
    algorithms, such as decision trees or linear/logistic regression. That is because
    Random Forest has a lower bias (due to its flexibility) and reduced variance (due
    to aggregating predictions of multiple models).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林提供了复杂度与性能之间的良好平衡。通常——即使没有任何调优——我们也能比使用更简单的算法（如决策树或线性/逻辑回归）获得更好的性能。这是因为随机森林具有较低的偏差（由于其灵活性）和较小的方差（由于聚合多个模型的预测）。
- en: Gradient Boosted Trees
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: '**Gradient Boosted Trees** is another type of ensemble model. The idea is to
    train many weak learners (shallow decision trees/stumps with high bias) and combine
    them to obtain a strong learner. In contrast to Random Forest, Gradient Boosted
    Trees is a sequential/iterative algorithm. In **boosting**, we start with the
    first weak learner, and each of the subsequent learners tries to learn from the
    mistakes of the previous ones. They do this by being fitted to the residuals (error
    terms) of the previous models.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升树**是另一种集成模型。其思想是训练许多弱学习器（具有高偏差的浅层决策树/树桩），并将它们组合起来以获得一个强学习器。与随机森林相比，梯度提升树是一个顺序/迭代算法。在**提升**中，我们从第一个弱学习器开始，每个后续的学习器都试图从前一个学习器的错误中学习。它们通过拟合前一个模型的残差（误差项）来实现这一点。'
- en: The reason why we create an ensemble of weak learners instead of strong learners
    is that in the case of the strong learners, the errors/mislabeled data points
    would most likely be the noise in the data, so the overall model would end up
    overfitting to the training data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一组弱学习器而不是强学习器的原因是，在强学习器的情况下，错误/标注错误的数据点很可能是数据中的噪音，因此整体模型最终会对训练数据发生过拟合。
- en: The term *gradient* comes from the fact that the trees are built using **gradient
    descent**, which is an optimization algorithm. Without going into too much detail,
    it uses the gradient (slope) of the loss function to minimize the overall loss
    and achieve the best performance. The loss function represents the difference
    between the actual and predicted values. In practice, to perform the gradient
    descent procedure in Gradient Boosted Trees, we add such a tree to the model that
    follows the gradient. In other words, such a tree reduces the value of the loss
    function.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*梯度*这个术语来源于树是使用**梯度下降**构建的，而梯度下降是一种优化算法。简而言之，它利用损失函数的梯度（斜率）来最小化整体损失并实现最佳性能。损失函数表示实际值和预测值之间的差异。实际上，为了在梯度提升树中执行梯度下降过程，我们会将这样一棵树添加到模型中，使其遵循梯度。换句话说，这样的树会降低损失函数的值。'
- en: 'We can describe the boosting procedure using the following steps:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下步骤描述提升过程：
- en: The process starts with a simple estimate (mean, median, and so on).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该过程从一个简单的估算开始（均值、中位数等）。
- en: A tree is fitted to the error of that prediction.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一棵树被拟合到该预测的误差。
- en: The prediction is adjusted using the tree’s prediction. However, it is not fully
    adjusted, but only to a certain degree (based on a learning rate hyperparameter).
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测通过使用树的预测值进行调整。然而，它并不会完全调整，而只是调整到一定程度（基于学习率超参数）。
- en: Another tree is fitted to the error of the updated prediction and the prediction
    is further adjusted as in the previous step.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一棵树被拟合到更新后的预测误差，并且预测值像之前的步骤那样进一步调整。
- en: The algorithm continues to iteratively reduce the error until a specified number
    of rounds (or another stopping criterion) is reached.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法会继续迭代地减少误差，直到达到指定的轮次（或其他停止准则）。
- en: The final prediction is the sum of the initial prediction and all the adjustments
    (predictions of the error weighted with the learning rate).
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终预测是初始预测与所有调整（按学习率加权的误差预测值）之和。
- en: In contrast to Random Forest, Gradient Boosted Trees use all available data
    to train the models. However, we can use random sampling without replacement for
    each tree by using the `subsample` hyperparameter. Then, we are dealing with **Stochastic
    Gradient Boosted Trees**. Additionally, similarly to Random Forest, we can make
    the trees consider only a subset of features when making a split.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林相比，梯度提升树使用所有可用数据来训练模型。然而，我们可以通过使用`subsample`超参数对每棵树进行不重复的随机采样。这样，我们就得到了**随机梯度提升树**。此外，类似于随机森林，我们可以让树在进行分裂时只考虑特征的一个子集。
- en: XGBoost
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost
- en: '**Extreme Gradient Boosting (XGBoost**) is an implementation of Gradient Boosted
    Trees that incorporates a series of improvements resulting in superior performance
    (both in terms of evaluation metrics and estimation time). Since being published,
    the algorithm has been successfully used to win many data science competitions.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**极端梯度提升（XGBoost）**是梯度提升树的一个实现，融合了一系列改进，从而提供了更优的性能（无论是在评估指标还是估计时间上）。自发布以来，该算法已成功用于赢得许多数据科学竞赛。'
- en: 'In this recipe, we only present a high-level overview of its distinguishable
    features. For a more detailed overview, please refer to the original paper (Chen
    *et al.* (2016)) or documentation. The key concepts of XGBoost are the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们仅提供了XGBoost一些可辨识特性的概述。欲了解更详细的概述，请参考原始论文（Chen *et al.* (2016)）或文档。XGBoost的关键概念如下：
- en: XGBoost combines a pre-sorted algorithm with a histogram-based algorithm to
    calculate the best splits. This tackles a significant inefficiency of Gradient
    Boosted Trees, namely that the algorithm considers the potential loss for all
    possible splits when creating a new branch (especially important when considering
    hundreds or thousands of features).
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost将预排序算法与基于直方图的算法相结合，用于计算最佳分裂。这解决了梯度提升树的一个重大低效问题，即在创建新分支时，算法需要考虑所有可能的分裂的潜在损失（特别是在考虑数百或数千个特征时，尤其重要）。
- en: The algorithm uses the Newton-Raphson method to approximate the loss function,
    which allows us to use a wider variety of loss functions.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法使用牛顿-拉夫森方法来近似损失函数，这使我们能够使用更广泛的损失函数。
- en: XGBoost has an extra randomization parameter to reduce the correlation between
    the trees.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost有一个额外的随机化参数，用于减少树与树之间的相关性。
- en: XGBoost combines Lasso (L1) and Ridge (L2) regularization to prevent overfitting.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost结合了Lasso（L1）和Ridge（L2）正则化，以防止过拟合。
- en: It offers a more efficient approach to tree pruning.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了一种更高效的树修剪方法。
- en: XGBoost has a feature called monotonic constraints—the algorithm sacrifices
    some accuracy and increases the training time to improve model interpretability.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost有一个名为单调约束的特性——该算法牺牲一些准确性并增加训练时间，以提高模型的可解释性。
- en: XGBoost does not take categorical features as input—we must use some kind of
    encoding for them.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost不接受类别特征作为输入——我们必须对它们进行某种编码。
- en: The algorithm can handle missing values in the data.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法可以处理数据中的缺失值。
- en: LightGBM
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LightGBM
- en: '**LightGBM**, released by Microsoft, is another competition-winning implementation
    of Gradient Boosted Trees. Thanks to some improvements, LightGBM results in a
    similar performance to XGBoost, but with faster training time. Key features include
    the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**LightGBM**由微软发布，是另一种赢得比赛的梯度提升树实现。由于一些改进，LightGBM的性能与XGBoost相似，但训练时间更快。其主要特点包括：'
- en: The difference in speed is caused by the approach to growing trees. In general,
    algorithms (such as XGBoost) use a level-wise (horizontal) approach. LightBGM,
    on the other hand, grows trees leaf-wise (vertically). The leaf-wise algorithm
    chooses the leaf with the maximum reduction in the loss function. Such algorithms
    tend to converge faster than the level-wise ones; however, they tend to be more
    prone to overfitting (especially with small datasets).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度的差异是由树的生长方式造成的。一般来说，算法（例如XGBoost）采用层级（水平）方式。另一方面，LightBGM采用叶子生长方式（垂直）。叶子生长算法选择具有最大损失函数减少的叶子。这类算法通常比层级方式更快收敛；然而，它们更容易过拟合（尤其是在小数据集上）。
- en: LightGBM employs a technique called **Gradient-based One-Side Sampling** (**GOSS**)
    to filter out the data instances used for finding the best split value. Intuitively,
    observations with small gradients are already well trained, while those with large
    gradients have more room for improvement. GOSS retains instances with large gradients
    and additionally samples randomly from observations with small gradients.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightGBM采用一种名为**基于梯度的单边采样**（**GOSS**）的技术，来过滤出用于寻找最佳切分值的数据实例。直观地说，梯度较小的观测值已经得到了较好的训练，而梯度较大的观测值还有更多改进空间。GOSS保留了梯度较大的实例，并且从梯度较小的观测值中随机采样。
- en: LightGBM uses **Exclusive Feature Bundling** (**EFB**) to take advantage of
    sparse datasets and bundles together features that are mutually exclusive (they
    never have values of zero at the same time). This leads to a reduction in the
    complexity (dimensionality) of the feature space.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightGBM使用**独特特征捆绑**（**EFB**）技术来利用稀疏数据集，并将互斥的特征（它们在同一时刻永远不会同时为零）捆绑在一起。这有助于减少特征空间的复杂性（维度）。
- en: The algorithm uses histogram-based methods to bucket continuous feature values
    into discrete bins in order to speed up training and reduce memory usage.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法使用基于直方图的方法将连续特征值分桶到离散的区间，以加速训练并减少内存使用。
- en: The leaf-wise algorithm was later added to XGBoost as well. To make use of it,
    we need to set `grow_policy` to `"lossguide"`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，叶节点算法也被添加到了XGBoost中。要使用它，我们需要将`grow_policy`设置为`"lossguide"`。
- en: There’s more...
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In this recipe, we showed how to use selected ensemble classifiers to try to
    improve our ability to predict customers’ likelihood of defaulting their loan.
    To make things even more interesting, these models have dozens of hyperparameters
    to tune, which can significantly increase (or decrease) their performance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了如何使用选定的集成分类器来提高我们预测客户违约贷款的能力。更有趣的是，这些模型有数十个超参数可以调整，这可能会显著提高（或降低）它们的性能。
- en: For brevity, we will not discuss the hyperparameter tuning of these models here.
    We refer you to the accompanying Jupyter notebook for a short introduction to
    tuning these models using a randomized grid search approach. Here, we only present
    a table containing the results. We can compare the performance of the models with
    default settings versus their tuned counterparts.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简便起见，我们将在这里不讨论这些模型的超参数调优。我们建议您查阅附带的Jupyter笔记本，里面简要介绍了如何使用随机网格搜索方法来调优这些模型。在这里，我们仅呈现一个包含结果的表格。我们可以比较默认设置下模型的性能与调优后的模型性能。
- en: '![](../Images/B18112_14_05.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_05.png)'
- en: 'Figure 14.5: Table comparing the performance of various classifiers'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5：比较不同分类器性能的表格
- en: For the models calibrated using the randomized search (including the `_rs` suffix
    in the name), we used 100 random sets of hyperparameters. As the considered problem
    deals with imbalanced data (the minority class is ~20%), we look at recall for
    performance evaluation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用随机搜索调整的模型（包括名称中带有`_rs`后缀的模型），我们使用了100组随机的超参数集。由于所考虑的问题涉及不平衡的数据（少数类约占20%），因此我们通过召回率来评估模型的性能。
- en: It seems that the basic decision tree achieved the best recall score on the
    test set. This came at the cost of much lower precision than the more advanced
    models. That is why the F1 score (a harmonic mean of precision and recall) is
    the lowest for the decision tree. We can see that the default LightGBM model achieved
    the best F1 score on the test set.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来基本的决策树在测试集上达到了最佳的召回率。这是以牺牲比更复杂模型低得多的精度为代价的。这就是为什么决策树的F1得分（精度与召回率的调和均值）最低的原因。我们可以看到，默认的LightGBM模型在测试集上达到了最佳的F1得分。
- en: The results by no means indicate that the more complex models are inferior—they
    might simply require more tuning or a different set of hyperparameters. For example,
    the ensemble models enforced the maximum depth of the tree (determined by the
    corresponding hyperparameter), while the decision tree had no such limit and it
    reached the depth of 37\. The more advanced the model, the more effort it requires
    to “get it right.”
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果并不意味着更复杂的模型就较差——它们可能只是需要更多的调优或者一组不同的超参数。例如，集成模型强制设定了树的最大深度（由相应的超参数决定），而决策树没有这样的限制，并且它的深度达到了37。模型越复杂，越需要更多的努力才能“做对”。
- en: 'There are many different ensemble classifiers available to experiment with.
    Some of the possibilities include:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的集成分类器可供实验。一些可能性包括：
- en: AdaBoost—the first boosting algorithm.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost——第一种提升算法。
- en: Extremely Randomized Trees—this algorithm offers improved randomness as compared
    to Random Forests. Similar to Random Forest, a random subset of features is considered
    when making a split. However, instead of looking for the most discriminative thresholds,
    the thresholds are drawn at random for each feature. Then, the best of these random
    thresholds is picked as the splitting rule. Such an approach usually allows us
    to reduce the variance of the model, while slightly increasing its bias.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极端随机树——该算法提供了比随机森林更强的随机性。与随机森林类似，在进行分裂时会考虑特征的随机子集。然而，与寻找最具区分性的阈值不同，每个特征的阈值是随机抽取的。然后，从这些随机阈值中选择最好的作为分裂规则。这种方法通常可以减少模型的方差，同时略微增加其偏差。
- en: CatBoost—another boosting algorithm (developed by Yandex) that puts a high emphasis
    on handling categorical features and achieving high performance with little hyperparameter
    tuning.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CatBoost——另一种提升算法（由Yandex开发），它特别强调处理分类特征并在少量超参数调整下实现高性能。
- en: NGBoost—at a very high level, this model introduces uncertainty estimation into
    the gradient boosting by using the natural gradient.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NGBoost——从非常高的层次看，这个模型通过使用自然梯度将不确定性估计引入梯度提升中。
- en: Histogram-based gradient boosting—a variant of gradient boosted trees available
    in `scikit-learn` and inspired by LightGBM. They accelerate the training procedure
    by discretizing (binning) the continuous features into a predetermined number
    of unique values.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于直方图的梯度提升——一种在 `scikit-learn` 中提供的梯度提升树变体，灵感来源于LightGBM。通过将连续特征离散化（分箱）为预定数量的唯一值，它们加速了训练过程。
- en: While some algorithms have introduced certain features first, the other popular
    implementations of gradient boosted trees often receive those as well. An example
    might be the histogram-based approach to discretizing continuous features. While
    it was introduced in LightGBM, it was later added to XGBoost as well. The same
    goes for the leaf-wise approach to growing trees.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些算法首先引入了某些特性，但其他流行的梯度提升树实现通常也会采用这些特性。例如，基于直方图的连续特征离散化方法。虽然它是在LightGBM中引入的，但后来也被添加到了XGBoost中。对于生长树的叶子方向方法也是如此。
- en: See also
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'We present additional resources on the algorithms mentioned in this recipe:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了更多关于本食谱中提到的算法的资源：
- en: 'Breiman, L. 2001\. “Random Forests.” *Machine Learning* 45(1): 5–32.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Breiman, L. 2001\. “随机森林。” *机器学习* 45(1): 5–32。'
- en: 'Chen, T., & Guestrin, C. 2016, August. Xgboost: A scalable tree boosting system.
    In *Proceedings of the 22nd international conference on knowledge discovery and
    data mining*, 785–794\. ACM.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen, T., & Guestrin, C. 2016年8月。Xgboost：一个可扩展的树提升系统。在 *第22届国际知识发现与数据挖掘会议论文集*，785–794。ACM。
- en: 'Duan, T., Anand, A., Ding, D. Y., Thai, K. K., Basu, S., Ng, A., & Schuler,
    A. 2020, November. Ngboost: Natural gradient boosting for probabilistic prediction.
    In *International Conference on Machine Learning*, 2690–2700\. PMLR.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan, T., Anand, A., Ding, D. Y., Thai, K. K., Basu, S., Ng, A., & Schuler,
    A. 2020年11月。Ngboost：用于概率预测的自然梯度提升。在 *国际机器学习会议*，2690–2700。PMLR。
- en: 'Freund, Y., & Schapire, R. E. 1996, July. Experiments with a new boosting algorithm.
    In *International Conference on Machine Learning*, 96: 148–156.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freund, Y., & Schapire, R. E. 1996年7月。关于一种新提升算法的实验。在 *国际机器学习会议*，96：148–156。
- en: Freund, Y., & Schapire, R. E. 1997\. “A decision-theoretic generalization of
    on-line learning and an application to boosting.” *Journal of Computer and System
    Sciences*, 55(1), 119–139.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freund, Y., & Schapire, R. E. 1997\. “在线学习的决策理论推广及其在提升中的应用。” *计算机与系统科学学报*，55(1)，119–139。
- en: 'Friedman, J. H. 2001\. “Greedy function approximation: a gradient boosting
    machine.” *Annals of Statistics*, 29(5): 1189–1232.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Friedman, J. H. 2001\. “贪婪函数逼近：一种梯度提升机。” *统计年鉴*，29(5): 1189–1232。'
- en: 'Friedman, J. H. 2002\. “Stochastic gradient boosting.” *Computational Statistics
    & Data Analysis*, 38(4): 367–378.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Friedman, J. H. 2002\. “随机梯度提升。” *计算统计与数据分析*，38(4): 367–378。'
- en: 'Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ... & Liu, T. Y.
    2017\. “Lightgbm: A highly efficient gradient boosting decision tree.” In *Neural
    Information Processing Systems*.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ... & Liu, T. Y. 2017\.
    “Lightgbm：一个高效的梯度提升决策树。” 在 *神经信息处理系统*。
- en: 'Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. 2018\.
    CatBoost: unbiased boosting with categorical features. In *Neural information
    Processing Systems*.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. 2018\.
    CatBoost：具有分类特征的无偏提升。在 *神经信息处理系统*。
- en: Exploring alternative approaches to encoding categorical features
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索编码分类特征的替代方法
- en: In the previous chapter, we introduced one-hot encoding as the standard solution
    for encoding categorical features so that they can be understood by ML algorithms.
    To recap, one-hot encoding converts categorical variables into several binary
    columns, where a value of 1 indicates that the row belongs to a certain category,
    and a value of 0 indicates otherwise.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们介绍了独热编码（one-hot encoding）作为编码分类特征的标准解决方案，使得机器学习算法能够理解这些特征。回顾一下，独热编码将分类变量转换为多个二进制列，其中值为1表示该行属于某个类别，值为0则表示不属于。
- en: The biggest drawback of that approach is the quickly expanding dimensionality
    of our dataset. For example, if we had a feature indicating from which of the
    US states the observation originates, one-hot encoding of this feature would result
    in the creation of 50 (or 49 if we dropped the reference value) new columns.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的最大缺点是数据集维度迅速扩展。例如，如果我们有一个特征表示观察数据来自美国的哪个州，那么对该特征进行独热编码将会创建50个新列（如果去掉参考值，则为49列）。
- en: 'Some other issues with one-hot encoding include:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用独热编码的其他问题包括：
- en: Creating that many Boolean features introduces sparsity to the dataset, which
    decision trees don’t handle well.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建这么多布尔特征会给数据集引入稀疏性，而决策树对此处理不佳。
- en: Decision trees’ splitting algorithm treats all the one-hot-encoded dummies as
    independent features. It means that when a tree makes a split using one of the
    dummy variables, the gain in purity per split is small. Thus, the tree is not
    likely to select one of the dummy variables closer to its root.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的分裂算法将所有的独热编码虚拟变量视为独立特征。这意味着当决策树使用其中一个虚拟变量进行分裂时，每次分裂的纯度增益较小。因此，决策树不太可能在接近根节点时选择某个虚拟变量。
- en: Connected to the previous point, continuous features will have higher feature
    importance than one-hot encoding dummy variables, as a single dummy can only bring
    a fraction of its respective categorical feature’s total information into the
    model.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与前一点相关，连续特征的特征重要性通常高于独热编码的虚拟变量，因为一个虚拟变量最多只能将其对应的分类特征的部分信息引入模型。
- en: Gradient boosted trees don’t handle high-cardinality features well, as the base
    learners have limited depth.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升树（Gradient Boosted Trees）不擅长处理高基数特征，因为基本学习器的深度有限。
- en: When dealing with a continuous variable, the splitting algorithm induces an
    ordering of the samples and can split that ordered list anywhere. A binary feature
    can only be split in one place, while a categorical feature with *k* unique categories
    can be split in ![](../Images/B18112_14_001.png) ways.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 处理连续变量时，分裂算法会对样本进行排序，并且可以在任何位置对这个排序后的列表进行分裂。而二进制特征只能在一个地方进行分裂，具有*k*个唯一类别的分类特征则可以有![](../Images/B18112_14_001.png)种分裂方式。
- en: 'We illustrate the advantage of the continuous features with an example. Assume
    that the splitting algorithm splits a continuous feature at a value of 10 into
    two groups: “below 10” and “10 and above.” In the next split, it can further split
    any of the two groups, for example, “below 6” and “6 and above.” That is not possible
    for a binary feature, as we can at most use it to split the groups once into “yes”
    or “no” groups. *Figure 14.6* illustrates potential differences between decision
    trees created with or without one-hot encoding.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个示例来说明连续特征的优势。假设分裂算法将一个连续特征在值为10的位置进行分裂，分成两组：“小于10”和“大于等于10”。在下一次分裂时，它可以进一步分裂这两组中的任意一组，例如，“小于6”和“大于等于6”。而对于二进制特征来说，这是不可能的，因为我们最多只能用它将数据分成“是”或“否”两组。*图14.6*展示了使用或不使用独热编码所创建的决策树之间可能的差异。
- en: '![](../Images/B18112_14_06.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_06.png)'
- en: 'Figure 14.6: Example of a dense decision tree without one-hot encoding (on
    the left) and a sparse decision tree with one-hot encoding (on the right)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：没有独热编码的密集决策树（左）和具有独热编码的稀疏决策树（右）示例
- en: Those drawbacks, among others, led to the development of a few alternative approaches
    to encoding categorical features. In this recipe, we introduce three of them.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这些缺点，以及其他一些因素，促使了几种替代分类特征编码方法的发展。在本节中，我们将介绍其中的三种方法。
- en: 'The first one is called **target encoding** (also known as mean encoding).
    In this approach, the following transformation is applied to a categorical feature,
    depending on the type of the target variable:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法称为**目标编码**（也叫均值编码）。在这种方法中，针对分类特征应用如下转换，具体取决于目标变量的类型：
- en: Categorical target—a feature is replaced with a blend of the posterior probability
    of the target given a certain category and the prior probability of the target
    over all the training data.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别目标——某个特征会被替换为在给定特定类别下目标的后验概率与所有训练数据中目标的先验概率的混合。
- en: Continuous target—a feature is replaced with a blend of the expected value of
    the target given a certain category and the expected value of the target over
    all the training data.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续目标——某个特征会被替换为在给定特定类别下目标的期望值与所有训练数据中目标的期望值的混合。
- en: In practice, the simplest scenario assumes that each category in the feature
    is replaced with the mean of the target value for that category. *Figure 14.7*
    illustrates this.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 实际应用中，最简单的情况假设每个特征中的类别都会被该类别目标值的均值替换。*图 14.7* 展示了这一点。
- en: '![](../Images/B18112_14_07.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_07.png)'
- en: 'Figure 14.7: Example of target encoding'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7：目标编码示例
- en: Target encoding results in a more direct representation of the relationship
    between the categorical feature and the target, while not adding any new columns.
    That is why it is a very popular technique in data science competitions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 目标编码能更直接地表示类别特征与目标之间的关系，同时不会添加任何新列。这也是它在数据科学竞赛中非常流行的原因。
- en: 'Unfortunately, it is not a silver bullet to encoding categorical features and
    comes with its disadvantages:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，它并不是编码类别特征的万灵药，并且带有一些缺点：
- en: The approach is very prone to overfitting. That is why it assumes blending/smoothing
    of the category mean with the global mean. We should be especially cautious when
    some categories are very infrequent.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法非常容易过拟合。因此，它假设类别均值与全局均值的混合/平滑。特别是当某些类别非常罕见时，我们应该特别小心。
- en: Connected to the risk of overfitting, we are effectively leaking target information
    into the features.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这与过拟合的风险相关，我们实际上是在将目标信息泄露到特征中。
- en: In practice, target encoding works quite well when we have high-cardinality
    features and are using some form of gradient boosted trees as our machine learning
    model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 实际应用中，当我们拥有高基数特征并且使用某种形式的梯度提升树作为机器学习模型时，目标编码效果相当好。
- en: The second approach we cover is called **Leave One Out Encoding** (**LOOE**)
    and it is very similar to target encoding. It attempts to reduce overfitting by
    excluding the current row’s target value when calculating the average of the category.
    This way, the algorithm avoids row-wise leakage. Another consequence of this approach
    is that the same category in multiple observations can have a different value
    in the encoded column. *Figure 14.8* illustrates this.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的第二种方法叫做**留一法编码**（**Leave One Out Encoding, LOOE**），它与目标编码非常相似。它通过在计算类别平均值时排除当前行的目标值来尝试减少过拟合。这样，算法就避免了按行泄露。这个方法的另一个结果是，相同类别在多个观察值中可以在编码列中具有不同的值。*图
    14.8* 展示了这一点。
- en: '![](../Images/B18112_14_08.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_08.png)'
- en: 'Figure 14.8: Example of Leave One Out Encoding'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8：留一法编码示例
- en: With LOOE, the ML model is exposed not only to the same value for each encoded
    category (as in target encoding) but to a range of values. That is why it should
    learn to generalize better.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LOOE，机器学习模型不仅会接触到每个编码类别的相同值（如目标编码中那样），还会接触到一系列值。这就是为什么它应该学会更好地泛化。
- en: The last of the considered encodings is called **Weight of Evidence** (**WoE**)
    encoding. This one is especially interesting, as it originates from the credit
    scoring world, where it was employed to improve the probability of default estimates.
    It was used to separate customers who defaulted on the loan from those who paid
    it back successfully.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后我们讨论的编码方法叫做**证据权重**（**Weight of Evidence, WoE**）编码。这种方法特别有趣，因为它起源于信用评分领域，在那里它被用来提高违约概率估算。它被用来区分违约客户与成功偿还贷款的客户。
- en: Weight of Evidence evolved from logistic regression. Another useful metric with
    the same origin as WoE is called **Information Value** (**IV**). It measures how
    much information a feature provides for the prediction. To put it a bit differently,
    it helps rank variables based on their importance in the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 证据权重（Weight of Evidence, WoE）源自逻辑回归。与 WoE 来源相同的另一个有用指标叫做**信息值**（**Information
    Value, IV**）。它衡量一个特征为预测提供了多少信息。换句话说，它帮助根据特征在模型中的重要性对变量进行排序。
- en: 'The weight of evidence indicates the predictive power of an independent variable
    in relation to the target. In other words, it measures how much the evidence supports
    or undermines a hypothesis. It is defined as the natural logarithm of the odds
    ratio:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 证据权重表示独立变量相对于目标变量的预测能力。换句话说，它衡量证据在多大程度上支持或削弱一个假设。它定义为赔率比的自然对数：
- en: '![](../Images/B18112_14_002.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_002.png)'
- en: '*Figure 14.9* illustrates the calculations.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14.9* 说明了计算过程。'
- en: '![](../Images/B18112_14_09.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_09.png)'
- en: 'Figure 14.9: Example of the WoE encoding'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9：WoE编码示例
- en: The fact that the encoding originates from credit scoring does not mean that
    it is only usable in such cases. We can generalize the good customers as the non-event
    or negative class, and the bad customers as the event or positive class. One of
    the restrictions of the approach is that, in contrast to the previous two, it
    can only be used with a binary categorical target.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管编码源于信用评分，但这并不意味着它只能在类似情况下使用。我们可以将优秀客户视为非事件或负类，而将差的客户视为事件或正类。该方法的一个限制是，与前两者不同，它只能用于二元类别目标。
- en: 'WoE was also historically used to encode categorical features as well. For
    example, in a credit scoring dataset, we could bin a continuous feature like age
    into discrete bins: 20–29, 30–39, 40–49, and so on, and only then calculate the
    WoE for those categories. The number of bins chosen for the encoding depends on
    the use case and the feature’s distribution.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: WoE（证据权重）在历史上也用于编码类别特征。例如，在信用评分数据集中，我们可以将连续特征如年龄分箱为离散区间：20-29岁、30-39岁、40-49岁，依此类推，然后计算这些类别的WoE。选择多少个区间用于编码，取决于具体应用和特征的分布情况。
- en: In this recipe, we show how to use those three encoders in practice using the
    default dataset we have already used before.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本篇教程中，我们将展示如何在实践中使用这三种编码器，使用我们之前已经用过的默认数据集。
- en: Getting ready
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: In this recipe, we use the pipeline we have used in the previous recipes. As
    the estimator, we use the Random Forest classifier. For your convenience, we reiterate
    all the required steps in the Jupyter notebook accompanying this chapter.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本篇教程中，我们使用之前教程中使用的管道。作为估算器，我们使用随机森林分类器。为了方便起见，我们在本章附带的Jupyter笔记本中重述了所有必要步骤。
- en: The Random Forest pipeline with one-hot encoded categorical features resulted
    in the test set’s recall of `0.3542`. We will try to improve upon this score with
    alternative approaches to encoding categorical features.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用独热编码的随机森林管道在测试集上的召回率为`0.3542`。我们将尝试通过其他编码方法来提高这个分数。
- en: How to do it…
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Execute the following steps to fit the ML pipelines with various categorical
    encoders:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以使用不同的类别编码器拟合机器学习管道：
- en: 'Import the libraries:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE5]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Fit the pipeline using target encoding:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用目标编码拟合管道：
- en: '[PRE6]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下图表：
- en: '![](../Images/B18112_14_10.png)'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_10.png)'
- en: 'Figure 14.10: Performance evaluation of the pipeline with target encoding'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.10：使用目标编码进行管道性能评估
- en: The recall obtained using this pipeline is equal to `0.3677`. This improves
    the score by slightly over 1 p.p.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用此管道获得的召回率为`0.3677`。这使得分数提高了略超过1个百分点。
- en: 'Fit the pipeline using Leave One Out Encoding:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用“留一编码”拟合管道：
- en: '[PRE7]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下图表：
- en: '![](../Images/B18112_14_11.png)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_11.png)'
- en: 'Figure 14.11: Performance evaluation of the pipeline with Leave One Out Encoding'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.11：使用“留一编码”进行管道性能评估
- en: The recall obtained using this pipeline is equal to `0.1462`, which is significantly
    worse than the target encoding approach.
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用此管道获得的召回率为`0.1462`，明显低于目标编码方法。
- en: 'Fit the pipeline using Weight of Evidence encoding:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用证据权重编码拟合管道：
- en: '[PRE8]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下图表：
- en: '![](../Images/B18112_14_12.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_12.png)'
- en: 'Figure 14.12: Performance evaluation of the pipeline with Weight of Evidence
    encoding'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.12：使用证据权重编码进行管道性能评估
- en: The recall obtained using this pipeline is equal to `0.3708`, which is a small
    improvement over target encoding.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此管道获得的召回率为`0.3708`，相较于目标编码有小幅提升。
- en: How it works…
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: First, we executed the code from the *Getting ready* section, that is, instantiated
    the pipeline with one-hot encoding and Random Forest as the classifier.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们执行了*准备工作*部分的代码，即实例化了一个使用独热编码和随机森林作为分类器的管道。
- en: After importing the libraries, we cloned the entire pipeline using the `clone`
    function. Then, we used the `set_params` method to replace the `OneHotEncoder`
    with `TargetEncoder`. Just as when tuning the hyperparameters of a pipeline, we
    had to use the same double underscore notation to access the particular element
    of the pipeline. The encoder was located under `preprocessor__categorical__cat_encoding`.
    Then, we fitted the pipeline using the `fit` method and printed the evaluation
    scores using the `performance_evaluation_report` helper function.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入库后，我们使用`clone`函数克隆了整个管道。然后，我们使用`set_params`方法将`OneHotEncoder`替换为`TargetEncoder`。正如调优管道的超参数时，我们必须使用相同的双下划线表示法来访问管道中的特定元素。编码器位于`preprocessor__categorical__cat_encoding`下。接着，我们使用`fit`方法拟合管道，并通过`performance_evaluation_report`辅助函数打印评估结果。
- en: 'As we have mentioned in the introduction, target encoding is prone to overfitting.
    That is why instead of simply replacing the categories with the corresponding
    averages, the algorithm is capable of blending the posterior probabilities with
    the prior probability (global average). We can control the blending with two hyperparameters:
    `min_samples_leaf` and `smoothing`.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在介绍中提到的，目标编码容易导致过拟合。这就是为什么算法不仅仅用相应的平均值替换类别，而是能够将后验概率与先验概率（全局平均）结合起来的原因。我们可以通过两个超参数来控制这种混合：`min_samples_leaf`
    和 `smoothing`。
- en: In *Steps 3* and *4*, we followed the very same steps as with target encoding,
    but we replaced the encoder with `LeaveOneOutEncoder` and `WOEEncoder` respectively.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 3*和*4*中，我们按照与目标编码相同的步骤操作，但分别将编码器替换为`LeaveOneOutEncoder`和`WOEEncoder`。
- en: Just as with target encoding, the other encoders use the target to build the
    encoding and are thus prone to overfitting. Fortunately, they also offer certain
    measures to prevent that from happening.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 和目标编码一样，其他编码器也使用目标来构建编码，因此也容易出现过拟合。幸运的是，它们也提供了一些防止过拟合的措施。
- en: In the case of LOOE, we can add normally distributed noise to the encodings
    in order to reduce overfitting. We can control the standard deviation of the Normal
    distribution used for generating the noise with the `sigma` argument. It is worth
    mentioning that the random noise is added to the training data only, and the transformation
    of the test set is not impacted. Just by adding the random noise to our pipeline
    (`sigma = 0.05`), we can improve the measured recall score from `0.1462` to around
    `0.35` (depending on random number generation).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在LOOE的情况下，我们可以向编码中添加正态分布噪声以减少过拟合。我们可以通过`sigma`参数控制用于生成噪声的正态分布的标准差。值得一提的是，随机噪声仅添加到训练数据中，测试集的转换不受影响。仅通过向我们的管道中添加随机噪声（`sigma
    = 0.05`），我们可以将测量的召回率从`0.1462`提高到大约`0.35`（具体取决于随机数生成）。
- en: Similarly, we can add random noise for the WoE encoder. We control the noise
    with the `randomized` (Boolean flag) and `sigma` (standard deviation of the Normal
    distribution) arguments. Additionally, there is the `regularization` argument,
    which prevents errors caused by division by zero.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以为WoE编码器添加随机噪声。我们通过`randomized`（布尔标志）和`sigma`（正态分布的标准差）参数来控制噪声。此外，还有`regularization`参数，它可以防止由于除零错误而导致的错误。
- en: There’s more…
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: Encoding categorical variables is a very broad area of active research, and
    every now and then new approaches to it are being published. Before changing the
    topic, we would also like to discuss a couple of related concepts.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 编码分类变量是一个非常广泛的活跃研究领域，不时会发布新的方法。在切换主题之前，我们还想讨论一些相关概念。
- en: Handling data leakage with k-fold target encoding
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用k折目标编码处理数据泄露
- en: We have already mentioned a few approaches to reducing the overfitting problem
    of the target encoder. A very popular solution among Kaggle practitioners is to
    use *k*-fold target encoding. The idea is similar to k-fold cross-validation and
    it allows us to use all the training data we have. We start by dividing the data
    into *k* folds—they can be stratified or purely random, depending on the use case.
    Then, we replace the observations present in the *l*-th fold with the target’s
    mean calculated using all the folds except the *l*-th one. This way, we are not
    leaking the target from the observations within the same fold.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到了一些减少目标编码器过拟合问题的方法。Kaggle 从业者中非常流行的一个解决方案是使用 *k* 折目标编码。这个想法类似于 k 折交叉验证，它允许我们使用所有可用的训练数据。我们首先将数据划分为
    *k* 个折叠——这些折叠可以是分层的，也可以是完全随机的，具体取决于应用场景。然后，我们用除第 *l* 个折叠之外的所有折叠计算出的目标均值来替换第 *l*
    个折叠中的观察值。这样，我们就避免了同一折叠中目标泄漏的问题。
- en: An inquisitive reader might have noticed that the LOOE is a special case of
    *k*-fold target encoding, in which *k* is equal to the number of observations
    in the training dataset.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 一位好奇的读者可能已经注意到，LOOE 是 *k* 折目标编码的一种特殊情况，其中 *k* 等于训练数据集中的观察数量。
- en: Even more encoders
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多编码器
- en: 'The `category_encoders` library offers almost 20 different encoding transformers
    for categorical features. Aside from the ones we have already mentioned, you might
    want to explore the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`category_encoders` 库提供了近 20 种不同的分类特征编码转换器。除了我们已经提到的编码器外，你还可以探索以下内容：'
- en: '**Ordinal encoding**—very similar to label encoding; however, it ensures that
    the encoding retains the ordinal nature of the feature. For example, the hierarchy
    of bad < neutral < good is preserved.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有序编码**——与标签编码非常相似；然而，它确保编码保留特征的有序性质。例如，坏 < 中立 < 好的层级关系被保留下来。'
- en: '**Count encoder** (frequency encoder)—each category of a feature is mapped
    to the number of observations belonging to that category.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计数编码器**（频率编码器）——将特征的每个类别映射到属于该类别的观察数。'
- en: '**Sum encoder**—compares the mean of the target for a given category to the
    overall average of the target.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总和编码器**——将给定类别的目标均值与目标的总体均值进行比较。'
- en: '**Helmert encoder**—compares the mean of a certain category to the mean of
    the subsequent levels. If we had categories [A, B, C], the algorithm would first
    compare A to B and C and then B to C alone. This kind of encoding is useful in
    situations in which the levels of the categorical feature are ordered, for example,
    from lowest to highest.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Helmert 编码器**——将某个类别的均值与后续级别的均值进行比较。如果我们有类别 [A, B, C]，算法会先比较 A 与 B 和 C，然后再比较
    B 与 C。此种编码在类别特征的级别有顺序的情况下非常有用，例如从低到高的顺序。'
- en: '**Backward difference encoder**—similar to the Helmert encoder, with the difference
    that it compares the mean of the current category to the mean of the previous
    one.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向差异编码器**——类似于 Helmert 编码器，不同之处在于它将当前类别的均值与前一个类别的均值进行比较。'
- en: '**M-estimate encoder**—a simplified version of the target encoder, which has
    only one tunable parameter (responsible for the strength of regularization).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**M-估计编码器**——目标编码器的简化版本，只有一个可调参数（负责正则化强度）。'
- en: '**James-Stein encoder**—a variant of target encoding that aims to improve theestimation
    of the category’s mean by shrinking it toward the central/global mean. Its single
    hyperparameter is responsible for the strength of shrinkage (this means the same
    as regularization in this context)—the bigger the value of the hyperparameter,
    the bigger the weight of the global mean (which might lead to underfitting). On
    the other hand, reducing the hyperparameter’s value might lead to overfitting.
    The best value is usually determined by cross-validation. The approach’s biggest
    disadvantage is that the James-Stein estimator is defined only for Normal distribution,
    which is not the case for any binary classification problem.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**James-Stein 编码器**——一种目标编码的变体，旨在通过将类别的均值收缩到中心/全局均值来提高估计精度。它的单一超参数控制着收缩的强度（在这个上下文中，这与正则化相同）——超参数值越大，全球均值的权重越大（这可能导致欠拟合）。另一方面，减少超参数值可能会导致过拟合。通常，最佳值是通过交叉验证来确定的。该方法的最大缺点是，James-Stein
    估计器仅适用于正态分布，而这并不适用于任何二元分类问题。'
- en: '**Binary encoder**—converts a category into binary digits and each one is provided
    a separate column. Thanks to this encoding, we generate far fewer columns than
    with OHE. To illustrate, for a categorical feature with 100 unique categories,
    binary encoding just needs to create 7 features, instead of 100 in the case of
    OHE.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二进制编码器**—将类别转换为二进制数字，每个数字都有一个单独的列。得益于这种编码方法，我们生成的列数远少于 OHE。例如，对于一个具有 100
    个独特类别的类别特征，二进制编码只需创建 7 个特征，而 OHE 则需要 100 个。'
- en: '**Hashing encoder**—uses a hashing function (often used in data encryption)
    to transform the categorical features. The outcome is similar to OHE, but with
    fewer features (we can control that with the encoder’s hyperparameters). It has
    two significant disadvantages. First, the encoding results in information loss,
    as the algorithm transforms the full set of available categories into fewer features.
    The second issue is called collision and it occurs as we are transforming a potentially
    high number of categories into a smaller set of features. Then, different categories
    could be represented by the same hash values.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**哈希编码器**—使用哈希函数（通常用于数据加密）来转换类别特征。其结果与 OHE 相似，但特征更少（我们可以通过编码器的超参数来控制这一点）。它有两个显著的缺点。首先，编码会导致信息丢失，因为算法将所有可用类别转换为更少的特征。第二个问题称为碰撞，发生在我们将潜在的大量类别转换为较小的特征集时。此时，不同的类别可能会被相同的哈希值表示。'
- en: '**Catboost encoder**—an improved variant of Leave One Out Encoding, which aims
    to overcome the issues of target leakage.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Catboost 编码器**—一种改进的 Leave One Out 编码变种，旨在克服目标泄漏问题。'
- en: See also
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Micci-Barreca, D. 2001\. “A preprocessing scheme for high-cardinality categorical
    attributes in classification and prediction problems.” *ACM SIGKDD Explorations
    Newsletter* 3(1): 27–32.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Micci-Barreca, D. 2001. “分类与预测问题中高基数类别属性的预处理方案。” *ACM SIGKDD Explorations Newsletter*
    3(1): 27–32.'
- en: Investigating different approaches to handling imbalanced data
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查处理不平衡数据的不同方法
- en: A very common issue when working with classification tasks is that of **class
    imbalance**, that is, when one class is highly outnumbered in comparison to the
    second one (this can also be extended to multi-class cases). In general, we are
    dealing with imbalance when the ratio of the two classes is not 1:1\. In some
    cases, a delicate imbalance is not that big of a problem, but there are industries/problems
    in which we can encounter ratios of 100:1, 1000:1, or even more extreme.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理分类任务时，一个非常常见的问题是**类别不平衡**，即当一个类别的样本数量远少于另一个类别时（这也可以扩展到多类别问题）。通常，当两个类别的比例不是
    1:1 时，我们就面临不平衡问题。在某些情况下，轻微的不平衡并不是大问题，但在一些行业或问题中，我们可能会遇到 100:1、1000:1 或甚至更极端的比例。
- en: Dealing with highly imbalanced classes can result in the poor performance of
    ML models. That is because most of the algorithms implicitly assume balanced distribution
    of classes. They do so by aiming to minimize the overall prediction error, to
    which the minority class by definition contributes very little. As a result, classifiers
    trained on imbalanced data are biased toward the majority class.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 处理高度不平衡的类别可能导致机器学习模型的性能较差。这是因为大多数算法隐式地假设类别分布是平衡的。它们通过旨在最小化总体预测误差来实现这一点，而根据定义，少数类对总体误差的贡献非常小。因此，在不平衡数据上训练的分类器会偏向多数类。
- en: One of the potential solutions to dealing with class imbalance is to resample
    the data. On a high level, we can either undersample the majority class, oversample
    the minority class, or combine the two approaches. However, that is just the general
    idea. There are many ways to approach resampling and we describe a few selected
    methods below.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 解决类别不平衡的潜在解决方案之一是对数据进行重采样。总体而言，我们可以对多数类进行欠采样，对少数类进行过采样，或者将这两种方法结合起来。然而，这只是一个大致的思路。实际上，有很多处理重采样的方法，下面我们描述了几种常见的方法。
- en: When working with resampling techniques, we only resample the training data!
    The test data stays intact.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用重采样技术时，我们只对训练数据进行重采样！测试数据保持不变。
- en: '![](../Images/B18112_14_13.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_13.png)'
- en: 'Figure 14.13: Undersampling of the majority class and oversampling of the minority
    class'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.13：多数类的欠采样与少数类的过采样
- en: The simplest approach to undersampling is called **random undersampling**. In
    this approach, weundersample the majority class, that is, draw random samples
    (by default, without replacement) from the majority class until the classes are
    balanced (with a ratio of 1:1 or any other desired ratio). The biggest issue of
    this method is the information loss caused by discarding vast amounts of data,
    often the majority of the entire training dataset. As a result, a model trained
    on undersampled data can achieve lower performance. Another possible implication
    is a biased classifier with an increased number of false positives, as the distribution
    of the training and test sets is not the same after resampling.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的欠采样方法称为**随机欠采样**。在这种方法中，我们对多数类进行欠采样，也就是说，从多数类中随机抽取样本（默认情况下，无放回抽样），直到类别平衡（比例为1:1或其他所需的比例）。该方法最大的问题是由于丢弃大量数据（通常是整个训练数据集的大部分）而导致信息丢失。因此，在欠采样数据上训练的模型可能会表现得较差。另一个可能的影响是分类器偏向，导致更多的假阳性，因为重采样后训练集和测试集的分布不一致。
- en: Analogically, the simplest approach to oversampling is called **random oversampling**.In
    this approach, we sample multiple times with replacement from the minority class,
    until the desired ratio is achieved. This method often outperforms random undersampling,
    as there is no information loss caused by discarding training data. However, random
    oversampling comes with the danger of overfitting, caused by replicating observations
    from the minority class.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，最简单的过采样方法称为**随机过采样**。在这种方法中，我们从少数类中进行多次有放回的抽样，直到达到期望的比例。该方法通常比随机欠采样效果更好，因为没有因丢弃训练数据而导致信息丢失。然而，随机过采样存在过拟合的风险，因为它通过复制少数类的观察值来增加数据。
- en: '**Synthetic Minority Oversampling Technique** (**SMOTE**) is a more advanced
    oversampling algorithm that creates new, synthetic observations from the minority
    class. This way, it overcomes the previously mentioned problem of overfitting.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**合成少数类过采样技术**（**SMOTE**）是一种更先进的过采样算法，它通过少数类创建新的合成观察值。通过这种方式，它克服了前面提到的过拟合问题。'
- en: To create the synthetic samples, the algorithm picks an observation from the
    minority class, identifies its *k-*nearest neighbors (using the *k*-NN algorithm),
    and then creates new observations on the lines connecting (interpolating) the
    observation to the nearest neighbors. Then, the process is repeated for other
    minority observations until the classes are balanced.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建合成样本，算法从少数类中选取一个观察值，识别其*k*-最近邻（使用*k*-NN算法），然后在连接（插值）观察值和最近邻的线段上创建新的观察值。然后，该过程会重复进行，直到其他少数类观察值的样本也得到平衡。
- en: Aside from reducing the problem of overfitting, SMOTE causes no loss of information,
    as it does not discard observations belonging to the majority class. However,
    SMOTE can accidentally introduce more noise to the data and cause overlapping
    of classes. This is because while creating the synthetic observations, it does
    not take into account the observations from the majority class. Additionally,
    the algorithm is not very effective for high-dimensional data (due to the curse
    of dimensionality). Lastly, the basic variant of SMOTE is only suitable for numerical
    features. However, SMOTE’s extensions (mentioned in the *There’s more…* section)
    can handle categorical features as well.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 除了减少过拟合问题外，SMOTE不会丢失任何信息，因为它不会丢弃属于多数类的观察值。然而，SMOTE可能会无意中向数据中引入更多噪声，并导致类别重叠。这是因为在创建合成观察值时，它没有考虑到多数类的观察值。此外，该算法在高维数据上效果不佳（由于维度灾难）。最后，SMOTE的基本变种仅适用于数值特征。然而，SMOTE的扩展（在*更多内容...*部分提到）可以处理分类特征。
- en: The last of the considered oversampling techniques is called **Adaptive Synthetic
    Sampling** (**ADASYN**) and it is a modification of the SMOTE algorithm. In ADASYN,
    the number of observations to be created for a certain minority point is determined
    by a density distribution (instead of a uniform weight for all points, as in SMOTE).
    This is how ADASYN’s adaptive nature enables it to generate more synthetic samples
    for observations that come from hard-to-learn neighborhoods. For example, a minority
    observation is hard to learn if there are many majority class observations with
    very similar feature values. It is easier to imagine that scenario in the case
    of only two features. Then, in a scatterplot, such a minority class observation
    might simply be surrounded by many of the majority class observations.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑的最后一种过采样技术叫做**自适应合成采样**（**ADASYN**），它是SMOTE算法的一种改进。在ADASYN中，为某个少数类点创建的观测值数量是由密度分布决定的（而不是像SMOTE中那样为所有点提供统一的权重）。这种自适应特性使得ADASYN能够为来自难以学习的邻域的观测值生成更多的合成样本。例如，如果存在许多与少数类观察值特征值非常相似的多数类观察值，则该少数类观察值会变得难以学习。我们可以通过仅考虑两个特征的情况来更容易地理解这种情况。在散点图中，这样的少数类观察值可能会被许多多数类观察值包围。
- en: 'There are two additional elements worth mentioning:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两个额外的要点值得提及：
- en: In contrast to SMOTE, the synthetic points are not limited to linear interpolation
    between two points. They can also lie on a plane created by three or more observations.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与SMOTE不同，合成点并不局限于两点之间的线性插值。它们还可以位于由三个或更多观测值创建的平面上。
- en: After creating the synthetic observations, the algorithm adds a small random
    noise to increase the variance, thus making the samples more realistic.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在创建合成观测值后，算法会加入少量随机噪声以增加方差，从而使得样本更加真实。
- en: 'Potential drawbacks of ADASYN include:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ADASYN的潜在缺点包括：
- en: A possible decrease in precision (more false positives) of the algorithm caused
    by its adaptability. This means that the algorithm might generate more observations
    in the areas with high numbers of observations from the majority class. Such synthetic
    data might be very similar to those majority class observations, potentially resulting
    in more false positives.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其自适应性，算法的精度可能会下降（产生更多的假阳性）。这意味着该算法可能会在具有大量多数类观测值的区域生成更多观测值。这些合成数据可能与多数类观测值非常相似，从而可能导致更多的假阳性。
- en: Struggling with sparsely distributed minority observations. Then, a neighborhood
    can contain only one or very few points.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理稀疏分布的少数类观测值时，某些邻域可能仅包含一个或极少数的点。
- en: Resampling is not the only potential solution to the problem of imbalanced classes.
    Another one is based on adjusting the class weights, thus putting more weight
    on the minority class. In the background, the class weights are incorporated into
    calculating the loss function. In practice, this means that misclassifying observations
    from the minority class increases the value of the loss function significantly
    more than in the case of misclassifying the observations from the majority class.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 重采样并不是解决类别不平衡问题的唯一潜在方案。另一个方法是基于调整类别权重，从而增加少数类的权重。在后台，类别权重会被纳入到损失函数的计算中。实际上，这意味着将少数类观测值分类错误会显著增加损失函数的值，而多数类观测值分类错误的影响则较小。
- en: In this recipe, we show an example of a credit card fraud problem, where the
    fraudulent class is observed in only 0.17% of the entire sample. In such cases,
    gathering more data (especially of the fraudulent class) might simply not be feasible,
    and we need to resort to other techniques that can help us in improving the models’
    performance.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们展示了一个信用卡欺诈问题的例子，其中欺诈类在整个样本中的比例仅为0.17%。在这种情况下，收集更多的数据（特别是欺诈类数据）可能根本不可行，我们需要依赖其他技术来帮助我们提升模型的性能。
- en: Getting ready
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Before proceeding to the coding part, we provide a brief description of the
    dataset selected for this exercise. You can download the dataset from Kaggle (link
    in the *See also* section).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入编码部分之前，我们简要描述了本次练习中选用的数据集。你可以从Kaggle下载该数据集（链接在*另请参见*部分）。
- en: The dataset contains information about credit card transactions made over a
    period of two days in September 2013 by European cardholders. Due to confidentiality,
    almost all features (28 out of 30) were anonymized by using **Principal Components
    Analysis** (**PCA**). The only two features with clear interpretation are `Time`
    (seconds elapsed between each transaction and the first one in the dataset) and
    `Amount` (the transaction’s amount).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含了2013年9月欧洲持卡人在两天内进行的信用卡交易信息。由于保密原因，几乎所有特征（28个中的30个）都通过使用**主成分分析**（**PCA**）进行了匿名化。唯一两个有明确解释的特征是`Time`（每笔交易与数据集中第一笔交易之间的秒数）和`Amount`（交易金额）。
- en: Lastly, the dataset is highly imbalanced and the positive class is observed
    in 0.173% of all transactions. To be precise, out of 284,807 transactions, 492
    were identified as fraudulent.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据集严重失衡，正类在所有交易中只占0.173%。准确地说，在284,807笔交易中，有492笔被识别为欺诈交易。
- en: How to do it...
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Execute the following steps to investigate different approaches to handling
    class imbalance:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以研究不同处理类别失衡的方法：
- en: 'Import the libraries:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE9]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Load and prepare data:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载和准备数据：
- en: '[PRE10]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using `y.value_counts(normalize=True)` we can confirm that the positive class
    is observed in 0.173% of the observations.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`y.value_counts(normalize=True)`我们可以确认正类在0.173%的观察值中出现。
- en: 'Scale the features using `RobustScaler`:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`RobustScaler`对特征进行缩放：
- en: '[PRE11]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Train the baseline model:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练基准模型：
- en: '[PRE12]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Undersample the training data and train a Random Forest classifier:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练数据进行欠采样，并训练一个随机森林分类器：
- en: '[PRE13]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After random undersampling, the ratio of the classes is as follows: `{0: 394,
    1: 394}`.'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '随机欠采样后，类别的比例如下：`{0: 394, 1: 394}`。'
- en: 'Oversample the training data and train a Random Forest classifier:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练数据进行过采样，并训练一个随机森林分类器：
- en: '[PRE14]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After random oversampling, the ratio of the classes is as follows: `{0: 227451,
    1: 227451}`.'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '随机过采样后，类别的比例如下：`{0: 227451, 1: 227451}`。'
- en: 'Oversample the training data using SMOTE:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SMOTE对训练数据进行过采样：
- en: '[PRE15]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After oversampling with SMOTE, the ratio of the classes is as follows: `{0:
    227451, 1: 227451}`.'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '使用SMOTE过采样后，类别的比例如下：`{0: 227451, 1: 227451}`。'
- en: 'Oversample the training data using ADASYN:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用ADASYN对训练数据进行过采样：
- en: '[PRE16]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After oversampling with ADASYN, the ratio of the classes is as follows: `{0:
    227451, 1: 227449}`.'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '使用ADASYN过采样后，类别的比例如下：`{0: 227451, 1: 227449}`。'
- en: 'Use sample weights in the Random Forest classifier:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在随机森林分类器中使用样本权重：
- en: '[PRE17]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Train the `BalancedRandomForestClassifier`:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练`BalancedRandomForestClassifier`：
- en: '[PRE18]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Train the `BalancedRandomForestClassifier` with balanced classes:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用平衡类别训练`BalancedRandomForestClassifier`：
- en: '[PRE19]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Combine the results in a DataFrame:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果合并到一个DataFrame中：
- en: '[PRE20]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Executing the snippet prints the following table:'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码片段将打印以下表格：
- en: '![](../Images/B18112_14_14.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_14.png)'
- en: 'Figure 14.14: Performance evaluation metrics of the various approaches to dealing
    with imbalanced data'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.14：处理失衡数据的各种方法的性能评估指标
- en: In *Figure 14.14* we can see the performance evaluation of various approaches
    we have tried in this recipe. As we are dealing with a highly imbalanced problem
    (the positive class accounts for 0.17% of all the observations), we can clearly
    observe the case of the **accuracy paradox**. Many models have an accuracy of
    ≈99.9%, but they still fail to detect fraudulent cases, which are the most important
    ones.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图14.14*中，我们可以看到我们在本食谱中尝试的各种方法的性能评估。由于我们面临的是一个严重失衡的问题（正类占所有观察值的0.17%），我们可以清楚地观察到**准确率悖论**的情况。许多模型的准确率约为99.9%，但它们仍然未能检测出欺诈案件，而欺诈案件才是最重要的。
- en: The accuracy paradox refers to a case in which inspecting accuracy as the evaluation
    metric creates the impression of having a very good classifier (a score of 90%,
    or even 99.9%), while in reality it simply reflects the distribution of the classes.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率悖论指的是当将准确率作为评估指标时，会给人留下一个非常好的分类器印象（如90%的得分，甚至是99.9%），而实际上它只是反映了类别的分布情况。
- en: Taking that into consideration, we compare the performance of the models using
    metrics that account for that. While looking at precision, the best performing
    approach is Random Forest with class weights. When considering recall as the most
    important metric, the best performing approach is either undersampling followed
    by a Random Forest model or a Balanced Random Forest model. In terms of the F1
    score, the best approach seems to be the vanilla Random Forest model.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我们使用了考虑类别不平衡的评估指标来比较模型的表现。在查看精确度时，表现最佳的方案是使用类别权重的随机森林。当将召回率作为最重要的评估指标时，表现最好的方法是先进行欠采样然后使用随机森林模型，或者使用平衡随机森林模型。在
    F1 分数方面，最好的方法似乎是原始的随机森林模型。
- en: It is also important to mention that no hyperparameter tuning was performed,
    which could potentially improve the performance of all of the approaches.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要指出的是，本实验中没有进行超参数调优，这可能会提升所有方法的性能。
- en: How it works...
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: After importing the libraries, we loaded the credit card fraud dataset from
    a CSV file. In the same step, we additionally dropped the `Time` feature, separated
    the target from the features using the `pop` method, and created an 80–20 stratified
    train-test split. It is crucial to remember to use stratification when dealing
    with imbalanced classes.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入库后，我们从 CSV 文件加载了信用卡欺诈数据集。在同一步骤中，我们额外删除了`Time`特征，使用`pop`方法将目标与特征分开，并创建了一个
    80-20 的分层训练-测试集拆分。在处理类别不平衡时，记得使用分层抽样非常重要。
- en: In this recipe, we only focused on working with imbalanced data. That is why
    we did not cover any EDA, feature engineering, and so on. As all the features
    were numerical, we did not have to carry out any special encoding.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们仅专注于处理不平衡数据。因此，我们没有涵盖任何探索性数据分析（EDA）、特征工程等内容。由于所有特征都是数值型的，我们不需要进行特殊的编码。
- en: The only preprocessing step we did was to scale all the features using `RobustScaler`.
    While Random Forest does not require explicit feature scaling, some of the rebalancing
    approaches use *k*-NN under the hood. And for such distance-based algorithms,
    the scale does matter. We fitted the scaler using only the training data and then
    transformed both the training and test sets.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的唯一预处理步骤是使用`RobustScaler`对所有特征进行缩放。虽然随机森林不需要显式的特征缩放，但一些重采样方法在底层使用了*k*-最近邻算法。而对于这种基于距离的算法，特征缩放是很重要的。我们只使用训练数据来拟合缩放器，然后对训练集和测试集进行转换。
- en: In *Step 4*, we fitted a vanilla Random Forest model, which we used as a benchmark
    for the more complex approaches.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们拟合了一个原始的随机森林模型，并将其作为更复杂方法的基准。
- en: In *Step 5*, we used the `RandomUnderSampler` class from the `imblearn` library
    to randomly undersample the majority class in order to match the size of the minority
    sample. Conveniently, classes from `imblearn` follow `scikit-learn`'s API style.
    That is why we had to first define the class with the arguments (we only set the
    `random_state`). Then, we applied the `fit_resample` method to obtain the undersampled
    data. We reused the Random Forest object to train the model on the undersampled
    data and stored the results for later comparison.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们使用了`imblearn`库中的`RandomUnderSampler`类，随机欠采样多数类以匹配少数类样本的大小。方便的是，`imblearn`中的类遵循了`scikit-learn`的
    API 风格。因此，我们首先定义了类及其参数（我们只设置了`random_state`）。然后，我们应用了`fit_resample`方法来获得欠采样的数据。我们重新使用了随机森林对象，基于欠采样数据训练模型，并存储了结果以供后续比较。
- en: '*Step 6* is analogical to *Step 5*, with the only difference being the use
    of the `RandomOverSampler` to randomly oversample the minority class in order
    to match the size of the majority class.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 6*与*步骤 5*类似，唯一的区别是使用`RandomOverSampler`来随机过采样少数类，以匹配多数类的样本大小。'
- en: In *Step 7* and *Step 8*, we applied the SMOTE and ADASYN variants of oversampling.
    As the `imblearn` library makes it very easy to apply different sampling methods,
    we will not go deeper into the description of the process.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 7*和*步骤 8*中，我们应用了 SMOTE 和 ADASYN 变体的过采样方法。由于`imblearn`库使得应用不同的采样方法变得非常简单，我们不会深入描述该过程。
- en: In all the mentioned resampling methods, we can actually specify the desired
    ratio between classes by passing a float to the `sampling_strategy` argument.
    The number represents the desired ratio of the number of observations in the minority
    class over the number of observations in the majority class.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有提到的重采样方法中，我们实际上可以通过向`sampling_strategy`参数传递一个浮动值来指定类别之间的期望比例。该数字表示少数类样本与多数类样本的观察数之比。
- en: In *Step 9*, instead of resampling the training data, we used the `class_weight`
    hyperparameter of the `RandomForestClassifier` to account for the class imbalance.
    By passing “`balanced`" , the algorithm automatically assigns weights inversely
    proportional to class frequencies in the training data.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤9*中，我们没有重采样训练数据，而是使用了`RandomForestClassifier`的`class_weight`超参数来解决类别不平衡问题。通过传递`"balanced"`，算法会自动分配与训练数据中类别频率成反比的权重。
- en: There are different possible approaches to using the `class_weight` hyperparameter.
    Passing `"balanced_subsample"` results in a similar weights assignment as in `"balanced"`;
    however, the weights are computed based on the bootstrap sample for every tree.
    Alternatively, we can pass a dictionary containing the desired weights. One way
    of determining the weights can be by using the `compute_class_weight` function
    from `sklearn.utils.class_weight`.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`class_weight`超参数有不同的可能方法。传递`"balanced_subsample"`将得到与`"balanced"`类似的权重分配；然而，权重是基于每棵树的自助样本计算的。或者，我们可以传递一个包含期望权重的字典。一种确定权重的方法是使用`sklearn.utils.class_weight`中的`compute_class_weight`函数。
- en: The `imblearn` library also features some modified versions of popular classifiers.
    In *Steps 10* and *11*, we used a modified Random Forest classifier, that is,
    **Balanced Random Forest**. The difference is that in Balanced Random Forest the
    algorithm randomly undersamples each bootstrapped sample to balance the classes.
    In practical terms, its API is virtually the same as in the vanilla `scikit-learn`
    implementation (including the tunable hyperparameters).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`imblearn`库还提供了一些流行分类器的修改版本。在*步骤10*和*步骤11*中，我们使用了修改过的随机森林分类器，即**平衡随机森林**。不同之处在于，在平衡随机森林中，算法会随机欠采样每个自助样本，以平衡类别。实际上，其API与普通的`scikit-learn`实现几乎相同（包括可调节的超参数）。'
- en: In the last step, we combined all the results into a single DataFrame and displayed
    the results.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们将所有结果合并成一个单独的DataFrame并展示了结果。
- en: There’s more...
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In this recipe, we presented only some of the available resampling methods.
    Below, we list a few more possibilities.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们仅介绍了一些可用的重采样方法。以下是更多的一些可能性。
- en: 'Undersampling:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 欠采样：
- en: '**NearMiss**—the name refers to a collection of undersampling approaches that
    are essentially heuristic rules based on the Nearest Neighbors algorithm. They
    base the selection of the observations from the majority class to keep on the
    distance between the observations from the majority and minority classes. The
    rest is removed in order to balance the classes. For example, the NearMiss-1 method
    selects observations from the majority class that have the smallest average distance
    to the three closest observations from the minority class.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NearMiss**—这个名称指的是一组欠采样方法，基本上是基于最近邻算法的启发式规则。它们基于从多数类和少数类的观察之间的距离来选择要保留的多数类观察。其余的则被删除，以实现类别平衡。例如，NearMiss-1方法选择那些与三个位于少数类的观察点距离最小的多数类观察。'
- en: '**Edited Nearest Neighbors**—this approach removes any majority class observation
    whose class is different from the class of at least two of its three nearest neighbors.
    The underlying idea is to remove the instances from the majority class that are
    near the boundary of classes.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编辑最近邻**—这种方法删除任何多数类的观察，该观察的类别与其三个最近邻中的至少两个的类别不同。其基本思想是删除那些位于类别边界附近的多数类实例。'
- en: '**Tomek links**—in this undersampling heuristic we first identify all the pairs
    of observations that are nearest to each other (they are the nearest neighbors)
    but belong to different classes. Such pairs are called Tomek links. Then, from
    those pairs, we remove the observations that belong to the majority class. The
    underlying idea is that by removing those observations from the Tomek link we
    increase the class separation.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tomek链接**—在这个欠采样启发式方法中，我们首先识别出所有最接近的观察对（它们是最近邻）且属于不同类别的对。这些对称为Tomek链接。然后，从这些对中，我们删除属于多数类的观察。其基本思想是通过从Tomek链接中删除这些观察，我们可以增加类别之间的分离度。'
- en: 'Oversampling:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 过采样：
- en: '**SMOTE-NC** (**Synthetic Minority Oversampling Technique for Nominal and Continuous**)—a
    variant of SMOTE suitable for a dataset containing both numerical and categorical
    features. The vanilla SMOTE can create illogical values for one-hot-encoded features.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SMOTE-NC**（**用于名义和连续特征的合成少数类过采样技术**）—SMOTE的变体，适用于包含数值和类别特征的数据集。普通的SMOTE可能会为独热编码特征创建不合逻辑的值。'
- en: '**Borderline SMOTE**—this variant of the SMOTE algorithm will create new, synthetic
    observations along the decision boundary between the two classes, as those are
    more prone to being misclassified.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边界SMOTE**——这种SMOTE算法的变种会在两个类别之间的决策边界上创建新的合成观察点，因为这些点更容易被错误分类。'
- en: '**SVM SMOTE**—a variant of SMOTE in which an SVM algorithm is used to indicate
    which observations to use for generating new synthetic observations.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SVM SMOTE**——SMOTE的一种变体，使用SVM算法来指示哪些观察点应被用于生成新的合成观察点。'
- en: '**K-means SMOTE**—in this approach, we first apply *k*-means clustering to
    identify clusters with a high proportion of minority class observations. Then,
    the vanilla SMOTE is applied to the selected clusters and each of those clusters
    will have new synthetic observations.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K-means SMOTE**——在这种方法中，我们首先应用*k*-均值聚类来识别具有大量少数类观察点的聚类。然后，将原始SMOTE应用于选定的聚类，每个聚类都会生成新的合成观察点。'
- en: Alternatively, we could combine the undersampling and oversampling approaches.
    The underlying idea is to first use an oversampling method to create duplicate
    or artificial observations and then use an undersampling method to reduce the
    noise or remove unnecessary observations.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以结合欠采样和过采样方法。其基本思想是，首先使用过采样方法创建重复或人工观察点，然后使用欠采样方法减少噪声或删除不必要的观察点。
- en: For example, we could first oversample the data with SMOTE and then undersample
    it using random undersampling. `imbalanced-learn` offers two combined resamplers—SMOTE
    followed by Tomek links or Edited Nearest Neighbours.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以先使用SMOTE对数据进行过采样，然后使用随机下采样进行欠采样。`imbalanced-learn`提供了两种组合重采样方法——SMOTE后接Tomek链接或编辑最近邻。
- en: 'In this recipe, we have only covered a small selection of the available approaches.
    Before changing topics, we wanted to mention some general notes on tackling problems
    with imbalanced classes:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们只涵盖了可用方法的一小部分。在切换话题之前，我们想提一些关于解决不平衡类问题的通用注意事项：
- en: Do not apply under/oversampling on the test set.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要在测试集上应用欠采样/过采样。
- en: For evaluating problems with imbalanced data, use metrics that account for class
    imbalance, such as precision, recall, F1 score, Cohen’s kappa, or the PR-AUC.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估不平衡数据问题时，使用考虑类不平衡的度量标准，例如精确度、召回率、F1分数、Cohen's kappa或PR-AUC。
- en: Use stratification when creating folds for cross-validation.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在创建交叉验证的折叠时使用分层采样。
- en: Introduce under-/oversampling during cross-validation, not before. Doing so
    before leads to overestimating the model’s performance!
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在交叉验证过程中引入欠采样/过采样，而不是之前。这样做会导致高估模型的性能！
- en: When creating pipelines with resampling using the `imbalanced-learn` library,
    we also need to use the `imbalanced-learn` variants of the pipeline. This is because
    the resamplers use the `fit_resample` method instead of the `fit_transform` required
    by `scikit-learn`'s pipelines.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用`imbalanced-learn`库创建具有重采样的管道时，我们还需要使用`imbalanced-learn`的管道变种。这是因为重采样器使用`fit_resample`方法，而不是`scikit-learn`管道所需的`fit_transform`方法。
- en: Consider framing the problem differently. For example, instead of a classification
    task, we could treat it as an anomaly detection problem. Then, we could use different
    techniques, for example, **isolation forest**.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑从不同的角度框架问题。例如，我们可以将任务视为一个异常检测问题，而不是分类任务。然后，我们可以使用不同的技术，例如**孤立森林**。
- en: Experiment with selecting a different probability threshold than the default
    50% to potentially tune the performance. Instead of rebalancing the dataset, we
    can use the model trained using the imbalanced dataset to plot the false positive
    and false negative rates as a function of the decision threshold. Then, we can
    choose the threshold that results in the performance that best suits our needs.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试选择不同于默认50%的概率阈值，以可能调优模型性能。我们可以使用使用不平衡数据集训练的模型绘制假阳性率和假阴性率与决策阈值的关系图，而不是重新平衡数据集。然后，我们可以选择一个在性能上最适合我们需求的阈值。
- en: We use the decision threshold to determine over which probability or score (a
    classifier’s output) we consider that the given observation belongs to the positive
    class. By default, that is 0.5.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用决策阈值来确定在哪个概率或得分（分类器的输出）上，我们认为给定的观察属于正类。默认情况下，这个值是0.5。
- en: See also
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'The dataset we have used in this recipe is available on Kaggle:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本食谱中使用的数据集可以在Kaggle上找到：
- en: '[https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)'
- en: 'Additional resources are available here:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 额外资源可在此处获取：
- en: 'Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. 2002\. “SMOTE:
    synthetic minority oversampling technique.” *Journal of artificial intelligence
    research* 16: 321–357.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. 2002. “SMOTE:
    合成少数类过采样技术。” *人工智能研究期刊* 16: 321–357.'
- en: 'Chawla, N. V. 2009\. “Data mining for imbalanced datasets: An overview.” *Data
    mining and knowledge discovery handbook*: 875–886.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chawla, N. V. 2009. “面向不平衡数据集的数据挖掘：概述。” *数据挖掘与知识发现手册*：875–886.
- en: 'Chen, C., Liaw, A., & Breiman, L. 2004\. “Using random forest to learn imbalanced
    data.” *University of California, Berkeley 110*: 1–12.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen, C., Liaw, A., & Breiman, L. 2004. “使用随机森林学习不平衡数据。” *加利福尼亚大学伯克利分校* 110:
    1–12.'
- en: Elor, Y., & Averbuch-Elor, H. 2022\. “To SMOTE, or not to SMOTE?.” *arXiv preprint
    arXiv:2201.08528*.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elor, Y., & Averbuch-Elor, H. 2022. “是使用SMOTE，还是不使用SMOTE？” *arXiv 预印本 arXiv:2201.08528*.
- en: 'Han, H., Wang, W. Y., & Mao, B. H. 2005, August. Borderline-SMOTE: a new over-sampling
    method in imbalanced data sets learning. In *International conference on intelligent
    computing*, 878–887\. Springer, Berlin, Heidelberg.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han, H., Wang, W. Y., & Mao, B. H. 2005年8月。边界-SMOTE：一种在不平衡数据集学习中的新过采样方法。在 *智能计算国际会议*，878–887.
    Springer，柏林，海德堡。
- en: 'He, H., Bai, Y., Garcia, E. A., & Li, S. 2008, June. ADASYN: Adaptive synthetic
    sampling approach for imbalanced learning. In *2008 IEEE international joint conference
    on neural networks (IEEE world congress on computational intelligence)*,1322–1328\.
    IEEE.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He, H., Bai, Y., Garcia, E. A., & Li, S. 2008年6月。ADASYN：用于不平衡学习的自适应合成采样方法。在
    *2008年IEEE国际神经网络联合会议（IEEE世界计算智能大会）*，1322–1328. IEEE.
- en: Le Borgne, Y.-A., Siblini, W., Lebichot, B., & Bontempi, G. 2022\. Reproducible
    Machine Learning for Credit Card Fraud Detection – Practical Handbook.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le Borgne, Y.-A., Siblini, W., Lebichot, B., & Bontempi, G. 2022. 可重复的机器学习在信用卡欺诈检测中的应用——实践手册。
- en: Liu, F. T., Ting, K. M., & Zhou, Z. H. 2008, December. Isolation forest. In
    *2008 Eighth Ieee International Conference On Data Mining*, 413–422\. IEEE.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu, F. T., Ting, K. M., & Zhou, Z. H. 2008年12月。隔离森林。在 *2008年第八届IEEE国际数据挖掘会议*，413–422.
    IEEE.
- en: 'Mani, I., & Zhang, I. 2003, August. kNN approach to unbalanced data distributions:
    a case study involving information extraction. In *Proceedings of workshop on
    learning from imbalanced datasets*, 126: 1–7\. ICML.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mani, I., & Zhang, I. 2003年8月。kNN方法用于不平衡数据分布：一个涉及信息提取的案例研究。在 *从不平衡数据集学习研讨会论文集*，126:
    1–7. ICML.'
- en: 'Nguyen, H. M., Cooper, E. W., & Kamei, K. 2009, November. Borderline over-sampling
    for imbalanced data classification. In *Proceedings: Fifth International Workshop
    on Computational Intelligence & Applications*, 2009(1): 24–29\. IEEE SMC Hiroshima
    Chapter.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nguyen, H. M., Cooper, E. W., & Kamei, K. 2009年11月。边界过采样用于不平衡数据分类。在 *计算智能与应用国际研讨会论文集*，2009(1):
    24–29. IEEE SMC 广岛分会。'
- en: Pozzolo, A.D.et al. 2015\. Calibrating Probability with Undersampling for Unbalanced
    Classification, *2015 IEEE Symposium Series on Computational Intelligence*.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pozzolo, A.D. 等. 2015. 使用欠采样进行概率校准以应对不平衡分类，*2015年IEEE计算智能学会年会*。
- en: 'Tomek, I. (1976). Two modifications of CNN, IEEE Transactions on Systems *Man
    and Communications*, 6: 769-772.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tomek, I. (1976). CNN的两种修改，IEEE系统 *人类与通信学报*，6: 769-772.'
- en: 'Wilson, D. L. (1972). “Asymptotic properties of nearest neighbor rules using
    edited data.” *IEEE Transactions on Systems, Man, and Cybernetics* 3: 408–421.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wilson, D. L. (1972). “使用编辑数据的最近邻规则的渐近性质。” *IEEE系统、人类与控制论学报* 3: 408–421.'
- en: Leveraging the wisdom of the crowds with stacked ensembles
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用集体智慧与堆叠集成
- en: '**Stacking** (stacked generalization) refers to a technique of creating ensembles
    of potentially heterogeneous machine learning models. The architecture of a stacking
    ensemble comprises at least two base models (known as level 0 models) and a meta-model
    (the level 1 model) that combines the predictions of the base models. The following
    figure illustrates an example with two base models.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆叠**（堆叠泛化）是指创建潜在异质的机器学习模型集成的一种技术。堆叠集成的架构包括至少两个基础模型（称为第0层模型）和一个元模型（第1层模型），后者将基础模型的预测进行组合。下图展示了一个包含两个基础模型的示例。'
- en: '![Diagram  Description automatically generated](../Images/B18112_14_15.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图示 描述自动生成](../Images/B18112_14_15.png)'
- en: 'Figure 14.15: High-level schema of a stacking ensemble with two base learners'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.15：具有两个基础学习器的堆叠集成的高级结构图
- en: The goal of stacking is to combine the capabilities of a range of well-performing
    models and obtain predictions that result in a potentially better performance
    than any single model in the ensemble. That is possible as the stacked ensemble
    tries to leverage the different strengths of the base models. Because of that,
    the base models should often be complex and diverse. For example, we could use
    linear models, decision trees, various kinds of ensembles, k-nearest neighbors,
    support vector machines, neural networks, and so on.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠的目标是将一系列表现良好的模型的能力结合起来，获得的预测结果有可能比集成中任何单一模型的性能更好。这是可能的，因为堆叠集成试图利用基础模型的不同优势。因此，基础模型通常应该是复杂和多样的。例如，我们可以使用线性模型、决策树、各种集成方法、k近邻、支持向量机、神经网络等等。
- en: Stacking can be a bit more difficult to understand than the previously covered
    ensemble methods (bagging, boosting, and so on) as there are at least a few variants
    of stacking when it comes to splitting data, handling potential overfitting, and
    data leakage. In this recipe, we follow the approach used in the `scikit-learn`
    library.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠可能比之前介绍的集成方法（如自助法、提升法等）更难理解，因为在分割数据、处理潜在的过拟合和数据泄漏时，堆叠有至少几种变体。在本食谱中，我们遵循`scikit-learn`库中使用的方法。
- en: The procedure used for creating a stacked ensemble can be described in three
    steps. We assume that we already have representative training and test datasets.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 创建堆叠集成的方法可以通过三个步骤来描述。我们假设已经有了代表性的训练集和测试集。
- en: '*Step 1*: Train level 0 models'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤1*：训练层次0模型'
- en: The essence of this step is that each of the level 0 models is trained on the
    full training dataset and then those models are used to generate predictions.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程的本质是，每个层次0模型都在完整的训练数据集上进行训练，然后这些模型被用来生成预测。
- en: Then, we have a few things to consider for our ensemble. First, we have to pick
    what kind of predictions we want to use. For a regression problem, this is straightforward
    as we do not have any choice. However, when working with a classification problem
    we can use the predicted class or the predicted probability/score.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要考虑一些关于集成的事项。首先，我们必须选择想要使用的预测类型。对于回归问题，这很简单，因为我们没有其他选择。然而，在处理分类问题时，我们可以使用预测的类别或预测的概率/分数。
- en: Second, we can either use only the predictions (whichever variant we picked
    before) as the features for the level 1 model or combine the original feature
    set with the predictions from the level 0 models. In practice, combining the features
    tends to work a bit better. Naturally, this heavily depends on the use case and
    the considered dataset.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们可以仅使用预测结果（无论选择了哪个变体）作为层次1模型的特征，或者将原始特征集与层次0模型的预测结果结合。在实践中，结合特征通常会效果更好。当然，这在很大程度上取决于使用场景和考虑的数据集。
- en: '*Step 2*: Train the level 1 model'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤2*：训练层次1模型'
- en: The level 1 model (or the meta-model) is often quite simple and ideally can
    provide a smooth interpretation of the predictions made by the level 0 models.
    That is why linear models are often selected for this task.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 层次1模型（或元模型）通常相当简单，理想情况下可以提供对层次0模型所做预测的平滑解释。这就是为什么线性模型通常被选用于此任务的原因。
- en: The term **blending** often refers to using a simple linear model as the level
    1 model. This is because the predictions of the level 1 model are then a weighted
    average (or blending) of the predictions made by the level 0 models.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**融合**一词通常指的是使用简单的线性模型作为层次1模型。这是因为层次1模型的预测是层次0模型预测的加权平均值（或融合）。'
- en: In this *step*, the level 1 model is trained using the features from the previous
    step (either only the predictions or combined with the initial set of features)
    and some cross-validation scheme. The latter is used to select the meta-model’s
    hyperparameters and/or the set of base models to consider for the ensemble.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个*步骤*中，层次1模型使用前一步的特征（可能仅是预测结果，或者与最初的特征集结合）以及某种交叉验证方案进行训练。后者用于选择元模型的超参数和/或考虑用于集成的基础模型集。
- en: '![](../Images/B18112_14_16.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_16.png)'
- en: 'Figure 14.16: Low-level schema of a stacking ensemble with two base learners'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.16：具有两个基础学习器的堆叠集成的低级结构图
- en: In `scikit-learn`'s approach to stacking, we assume that any of the base models
    could have a tendency to overfit, either due to the algorithm itself or due to
    some combination of its hyperparameters. But if that is the case, it should be
    offset by the other base models not suffering from the same problem. That is why
    cross-validation is applied to tune the meta-model and not the base models as
    well.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在`scikit-learn`的堆叠方法中，我们假设任何基础模型可能会过拟合，这可能是由于算法本身或其超参数的某种组合导致的。但如果确实如此，应该通过其他没有同样问题的基础模型来进行补偿。这就是为什么交叉验证应用于调优元模型，而不是基础模型。
- en: After the best hyperparameters/base learners are selected, the final estimator
    is trained on the full training dataset.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择最佳的超参数/基础学习器后，最终估计器将在整个训练数据集上进行训练。
- en: '*Step 3*: Make predictions on unseen data'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤3*：对未见过的数据进行预测'
- en: This step is the easiest one, as we are essentially fitting all the base models
    to the new observations to obtain the predictions, which are then used by the
    meta-model to create the stacked ensemble’s final predictions.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤是最简单的，因为我们本质上是将所有基础模型拟合到新的观测数据上，以获得预测结果，这些预测结果随后由元模型用于生成堆叠集成的最终预测。
- en: In this recipe, we create a stacked ensemble of models applied to the credit
    card fraud dataset.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们创建了一个堆叠模型集成，应用于信用卡欺诈数据集。
- en: How to do it...
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Execute the following steps to create a stacked ensemble:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以创建堆叠集成：
- en: 'Import the libraries:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE21]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Load and preprocess data:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并预处理数据：
- en: '[PRE22]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define a list of base models:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义基础模型列表：
- en: '[PRE23]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the accompanying Jupyter notebook, we specified the random state of all the
    models to which it is applicable. Here, we omitted that part for brevity.
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在随附的Jupyter笔记本中，我们指定了所有适用模型的随机状态。这里为了简洁起见，省略了这一部分。
- en: 'Train the selected models and calculate the recall using the test set:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练选定的模型并使用测试集计算召回率：
- en: '[PRE24]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Executing the snippet generates the following output:'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下输出：
- en: '[PRE25]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Out of the considered models, the Naive Bayes classifier achieved the best recall
    on the test set.
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在考虑的模型中，朴素贝叶斯分类器在测试集上达到了最佳的召回率。
- en: 'Define, fit, and evaluate the stacked ensemble:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义、拟合并评估堆叠集成：
- en: '[PRE26]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Executing the snippet generates the following output:'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下输出：
- en: '[PRE27]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Our stacked ensemble resulted in a worse score than the best of the individual
    models. However, we can try to further improve the ensemble. For example, we can
    allow the ensemble to use the initial features for the meta-model and replace
    the logistic regression meta-model with a Random Forest classifier.
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的堆叠集成的得分比最好的单个模型还要差。然而，我们可以尝试进一步改善集成。例如，我们可以允许集成使用初始特征作为元模型，并将逻辑回归元模型替换为随机森林分类器。
- en: 'Improve the stacking ensemble with additional features and a more complex meta-model:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用额外的特征和更复杂的元模型来改进堆叠集成：
- en: '[PRE28]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The second stacked ensemble achieved a recall score of `0.8571`, which is better
    than the best of the individual models.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个堆叠集成的召回率为`0.8571`，优于最好的单个模型。
- en: How it works...
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作...
- en: In *Step 1*, we imported the required libraries. Then, we loaded the credit
    card fraud dataset, separated the target from the features, dropped the `Time`
    feature, split the data into training and test sets (using a stratified split),
    and finally, scaled the data with `RobustScaler`. The transformation is not necessary
    for tree-based models, however; we use various classifiers (each with its own
    set of assumptions about the input data) as base models. For simplicity, we did
    not investigate different properties of the features, such as normality. Please
    refer to the previous recipe for more details on those processing steps.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤1*中，我们导入了所需的库。然后，我们加载了信用卡欺诈数据集，将目标变量与特征分开，删除了`Time`特征，将数据拆分为训练集和测试集（使用分层拆分），最后，使用`RobustScaler`对数据进行了缩放。尽管树模型不需要这种转换，但我们使用了各种分类器（每个分类器对输入数据有不同的假设）作为基础模型。为了简单起见，我们没有调查特征的不同属性，比如正态性。有关这些处理步骤的更多细节，请参阅之前的食谱。
- en: In *Step 3*, we defined a list of base learners for the stacked ensemble. We
    decided to use a few simple classifiers, such as a decision tree, a Naive Bayes
    classifier, a support vector classifier, and logistic regression. For brevity,
    we will not describe the properties of the selected classifiers here.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤3*中，我们定义了一组用于堆叠集成的基础学习器。我们决定使用几个简单的分类器，如决策树、朴素贝叶斯分类器、支持向量分类器和逻辑回归。为了简洁起见，我们这里不描述所选分类器的属性。
- en: When preparing a list of base learners, we can also provide the entire pipelines
    instead of just the estimators. This can come in handy when only some of the ML
    models require dedicated preprocessing of the features, such as scaling or encoding
    categorical variables.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备基础学习器列表时，我们还可以提供整个管道，而不仅仅是估计器。当只有某些机器学习模型需要专门处理特征预处理（如特征缩放或编码分类变量）时，这一点非常有用。
- en: In *Step 4*, we iterated over the list of classifiers, fitted each model (with
    its default settings) to the training data, and calculated the recall score using
    the test set. Additionally, if the estimator had an `n_jobs` parameter, we set
    it to `-1` to use all the available cores for computations. This way, we could
    speed up the model’s training, provided our machine has multiple cores/threads
    available. The goal of this step was to investigate the performance of the individual
    base models so that we could compare them to the stacked ensemble.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们遍历了分类器列表，使用默认设置将每个模型拟合到训练数据，并使用测试集计算召回率得分。此外，如果估计器具有`n_jobs`参数，我们将其设置为`-1`，以便使用所有可用核心进行计算。通过这种方式，我们可以加速模型的训练，前提是我们的机器有多个核心/线程可用。本步骤的目标是研究各个基础模型的性能，以便将它们与堆叠集成进行比较。
- en: In *Step 5*, we first defined the meta-model (logistic regression) and the 5-fold
    stratified cross-validation scheme. Then, we instantiated the `StackingClassifier`
    by providing the list of the base classifiers, together with the cross-validation
    scheme and the meta-model. In the `scikit-learn` implementation of stacking, the
    base learners are fitted using the entire training set. Then, in order to avoid
    overfitting and improve the model’s generalization, the meta-estimator uses the
    selected cross-validation scheme to train the model on the out-samples. To be
    precise, it uses `cross_val_predict` for this task.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们首先定义了元模型（逻辑回归）和5折分层交叉验证方案。然后，我们通过提供基础分类器列表、交叉验证方案和元模型来实例化`StackingClassifier`。在`scikit-learn`的堆叠实现中，基础学习器使用整个训练集进行拟合。然后，为了避免过拟合并提高模型的泛化能力，元估计器使用选定的交叉验证方案对模型进行训练，使用的是外样本。准确来说，它使用`cross_val_predict`来完成这项任务。
- en: A possible shortcoming of this approach is that applying cross-validation only
    to the meta-learner can result in overfitting of the base learners. Different
    libraries (mentioned in the *There’s more*… section) employ different approaches
    to cross-validation with stacked ensembles.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个可能缺点是，仅对元学习器应用交叉验证可能导致基础学习器的过拟合。不同的库（在*更多内容*部分中提到）采用不同的堆叠集成交叉验证方法。
- en: In the last step, we tried to improve the performance of the stacked ensemble
    by modifying its two characteristics. First, we changed the level 1 model from
    logistic regression to a Random Forest classifier. Second, we allowed the level
    1 model to use the features used by the level 0 base models. To do so, we set
    the `passthrough` argument to `True` while instantiating the `StackingClassifier`.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们尝试通过修改堆叠集成的两个特征来提高其性能。首先，我们将一级模型从逻辑回归更改为随机森林分类器。其次，我们允许一级模型使用由零级基础模型使用的特征。为此，我们在实例化`StackingClassifier`时，将`passthrough`参数设置为`True`。
- en: There’s more...
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'In order to get a better understanding of stacking, we can take a peek at the
    output of *Step 1*, which is the data being used to train the level 1 model. To
    get that data, we can use the `transform` method of a fitted `StackedClassifier`.
    Alternatively, we can use the familiar `fit_transform` method when the classifier
    was not fitted. In our case, we look into the stacked ensemble using both the
    predictions and original data as features:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解堆叠集成，我们可以查看*步骤 1*的输出，即用于训练一级模型的数据。为了获得这些数据，我们可以使用拟合后的`StackedClassifier`的`transform`方法。或者，当分类器没有拟合时，我们可以使用熟悉的`fit_transform`方法。在我们的案例中，我们查看堆叠集成，使用预测和原始数据作为特征：
- en: '[PRE29]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Executing the snippet generates the following table (abbreviated):'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这段代码会生成如下表格（简写版）：
- en: '![](../Images/B18112_14_17.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_17.png)'
- en: 'Figure 14.17: Preview of the input for the level 1 model in the stacking ensemble'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.17：堆叠集成中一级模型输入的预览
- en: We can see that the first four columns correspond to the predictions made by
    the base learners. Next to those, we can see the rest of the features, that is,
    those used by the base learners to generate their predictions.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到前四列对应于基础学习器做出的预测。在这些预测旁边，我们可以看到其余的特征，也就是基础学习器用于生成预测的特征。
- en: 'It is also worth mentioning that when using the `StackingClassifier` we can
    use various outputs of the base models as inputs for the level 1 model. For example,
    we can either use the predicted probabilities/scores or the predicted labels.
    Using the default settings of the `stack_method` argument, the classifier will
    try to use the following types of outputs (in that specific order): `predict_proba`,
    `decision_function`, and `predict`.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得一提的是，当使用`StackingClassifier`时，我们可以将基础模型的不同输出作为一级模型的输入。例如，我们可以使用预测的概率/得分或预测的标签。使用`stack_method`参数的默认设置，分类器会尝试使用以下类型的输出（按此特定顺序）：`predict_proba`、`decision_function`和`predict`。
- en: If we had used `stack_method="predict"`, we would have seen four columns of
    zeros and ones corresponding to the models’ class predictions (using the default
    decision threshold of 0.5).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`stack_method="predict"`，我们会看到四列零和一，对应于模型的类别预测（使用默认的0.5决策阈值）。
- en: 'In this recipe, we presented a simple example of a stacked ensemble. There
    are multiple ways in which we could try to further improve it. Some of the possible
    extensions include:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们展示了一个堆叠集成的简单示例。我们可以尝试进一步改进它的多种方式。一些可能的扩展包括：
- en: Adding more layers to the stacked ensemble
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向堆叠集成中添加更多层
- en: Using more diverse models, such as k-NN, boosted trees, neural networks, and
    so on
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更多样化的模型，例如k-NN、增强树、神经网络等
- en: Tuning the hyperparameters of the base classifiers and/or the meta-model
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整基础分类器和/或元模型的超参数
- en: The `ensemble` module of `scikit-learn` also contains a `VotingClassifier`,
    which can aggregate the predictions of multiple classifiers. `VotingClassifier`
    uses one of the two available voting schemes. The first one is `hard`, and it
    is simply the majority vote. The `soft` voting scheme uses the `argmax` of the
    sums of the predicted probabilities to predict the class label.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`的`ensemble`模块还包含一个`VotingClassifier`，它可以聚合多个分类器的预测结果。`VotingClassifier`使用两种可用的投票方案之一。第一种是`hard`投票，即简单的多数投票。`soft`投票方案使用预测概率的和的`argmax`来预测类别标签。'
- en: 'There are also other libraries providing stacking functionalities:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他库提供堆叠功能：
- en: '`vecstack`'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vecstack`'
- en: '`mlxtend`'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlxtend`'
- en: '`h2o`'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h2o`'
- en: These libraries also differ in the way they approach stacking, for example,
    how they split the data or how they handle potential overfitting and data leakage.
    Please refer to the respective documentation for more details.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这些库在堆叠方法上也有所不同，例如它们如何划分数据或如何处理潜在的过拟合和数据泄漏问题。有关更多详细信息，请参阅相应的文档。
- en: See also
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Additional resources are available here:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 其他资源可以在此处获得：
- en: 'Raschka, S. 2018\. “MLxtend: Providing machine learning and data science utilities
    and extensions to Python’s scientific computing stack.” *The Journal of Open Source
    Software* 3(24): 638.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Raschka, S. 2018. “MLxtend: 为Python的科学计算堆栈提供机器学习和数据科学工具及扩展。” *The Journal of
    Open Source Software* 3(24): 638。'
- en: 'Wolpert, D. H. 1992\. “Stacked generalization”. *Neural networks* 5(2): 241–259.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolpert, D. H. 1992. “堆叠泛化”。*Neural networks* 5(2): 241–259。'
- en: Bayesian hyperparameter optimization
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯超参数优化
- en: In the *Tuning hyperparameters using* *grid search and cross-validation* recipe
    in the previous chapter, we described how to use various flavors of grid search
    to find the best possible set of hyperparameters for our model. In this recipe,
    we introduce an alternative approach to finding the optimal set of hyperparameters,
    this time based on the Bayesian methodology.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章的*使用网格搜索和交叉验证调优超参数*配方中，我们描述了如何使用不同形式的网格搜索来找到模型的最佳超参数。在本配方中，我们介绍了一种基于贝叶斯方法找到最优超参数集的替代方法。
- en: The main motivation for the Bayesian approach is that both grid search and randomized
    search make uninformed choices, either through an exhaustive search over all combinations
    or through a random sample. This way, they spend a lot of time evaluating combinations
    that result in far from optimal performance, thus basically wasting time. That
    is why the Bayesian approach makes informed choices of the next set of hyperparameters
    to evaluate, this way reducing the time spent on finding the optimal set. One
    could say that the Bayesian methods try to limit the time spent evaluating the
    objective function by spending more time on selecting the hyperparameters to investigate,
    which in the end is computationally cheaper.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法的主要动机在于，无论是网格搜索还是随机化搜索，都会做出无知的选择，要么是通过对所有组合的穷举搜索，要么是通过随机抽样。这样，它们会花费大量时间评估那些远未达到最佳性能的组合，从而基本上浪费了时间。这就是为什么贝叶斯方法会根据已知信息选择下一个需要评估的超参数集合，从而减少寻找最佳集合所花费的时间。可以说，贝叶斯方法通过在选择要研究的超参数时投入更多时间，从而限制了评估目标函数所花费的时间，最终在计算上更加高效。
- en: A formalization of the Bayesian approach is **Sequential Model-Based** **Optimization**
    (**SMBO**). On a very high level, SMBO uses a surrogate model together with an
    acquisition function to iteratively (hence “sequential”) select the most promising
    hyperparameters in the search space in order to approximate the actual objective
    function.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法的一个形式化是**基于序列模型的优化**（**SMBO**）。从非常高层次看，SMBO利用替代模型和获取函数，通过迭代（因此称为“序列”）选择搜索空间中最有前景的超参数，以逼近实际的目标函数。
- en: In the context of Bayesian HPO, the true objective function is often the cross-validation
    error of a trained machine learning model. It can be computationally very expensive
    and can take hours (or even days) to calculate. That is why in SMBO we create
    a **surrogate model**, which is a probability model of the objective function
    built using its past evaluations. It maps the input values (hyperparameters) to
    a probability of a score on the true objective function. Hence, we can think of
    it as an approximation of the true objective function. In the approach we follow
    (the one used by the `hyperopt` library), the surrogate model is created using
    the **Tree-Structured** **Parzen Estimator** (**TPE**). Other possibilities include
    Gaussian processes or Random Forest regression.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯超参数优化（HPO）的背景下，真实目标函数通常是已训练机器学习模型的交叉验证误差。计算这些目标函数可能非常昂贵，可能需要数小时（甚至数天）才能计算完成。这就是为什么在SMBO中我们创建了**替代模型**，它是一个基于历史评估构建的目标函数的概率模型。它将输入值（超参数）映射到真实目标函数的得分概率。因此，我们可以将其视为对真实目标函数的近似。在我们采用的方法中（即`hyperopt`库所使用的方法），替代模型是通过**树状****帕尔岑估计器**（**TPE**）构建的。其他可能的选择包括高斯过程或随机森林回归。
- en: In each iteration, we first fit the surrogate model to all observations of the
    target function we made so far. Then, we apply the acquisition function (such
    as **Expected Improvement**) to determine the next set of hyperparameters based
    on their expected utility. Intuitively, this approach uses the history of past
    evaluations to make the best possible selection for the next iteration. Values
    close to the ones that performed well in the past are more likely to improve the
    overall performance than those that historically performed poorly. The acquisition
    function also defines a balance between the exploration of new areas in the hyperparameter
    space and the exploitation of the areas that are already known to provide favorable
    results.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们首先将替代模型拟合到迄今为止对目标函数的所有观察数据。然后，我们应用获取函数（例如**期望改进**）来根据超参数的预期效用确定下一组超参数。从直观上讲，这种方法利用过去评估的历史数据，为下一次迭代做出最佳选择。与过去表现良好的值接近的超参数，较可能提升整体性能，而那些历史上表现不佳的值则不太可能带来改进。获取函数还在超参数空间的探索新领域和利用已知能提供良好结果的领域之间定义了一种平衡。
- en: 'The simplified steps of Bayesian optimization are:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化的简化步骤如下：
- en: Create the surrogate model of the true objective function.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建真实目标函数的替代模型。
- en: Find a set of hyperparameters that performs best on the surrogate.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到在替代模型中表现最好的超参数集合。
- en: Use that set to evaluate the true objective function.
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用该集合评估真实目标函数。
- en: Update the surrogate, using the results from evaluating the true objective.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用评估真实目标的结果更新替代模型。
- en: Repeat *Steps 2–4*, until reaching the stop criterion (the specified maximum
    number of iterations or amount of time).
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤 2-4*，直到达到停止准则（指定的最大迭代次数或时间量）。
- en: From these steps, we see that the longer the algorithm runs, the closer the
    surrogate function approximates the true objective function. That is because with
    each iteration it is updated based on the evaluation of the true objective function,
    and thus with each run it is a bit “less wrong.”
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些步骤中可以看出，算法运行的时间越长，代理函数就越接近真实目标函数。这是因为每次迭代都会根据真实目标函数的评估来更新，因此每次运行时都会“少一些错误”。
- en: 'As we have already mentioned, the biggest advantage of Bayesian HPO is that
    it decreases the time spent searching for the optimal set of parameters. That
    is especially significant when the number of parameters is high and evaluating
    the true objective is computationally expensive. However, it also comes with a
    few possible shortcomings:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经提到的，贝叶斯超参数优化的最大优势是它减少了寻找最优参数集的时间。这一点在参数数量较多且评估真实目标计算代价高的情况下尤其重要。然而，它也有一些可能的缺点：
- en: Some steps of the SMBO procedure cannot be executed in parallel, as the algorithm
    selects the set of hyperparameters sequentially based on past results.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMBO 程序的一些步骤无法并行执行，因为算法会根据过去的结果顺序选择一组超参数。
- en: Choosing a proper distribution/scale for the hyperparameters can be tricky.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为超参数选择合适的分布/尺度可能会很棘手。
- en: Exploration versus exploitation bias—when the algorithm finds a local optimum,
    it might concentrate on hyperparameter values around it, instead of exploring
    potential new values located far away in the search space. Randomized search is
    not troubled by this issue, as it does not concentrate on any values.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索与开发的偏差——当算法找到局部最优解时，它可能会集中在该解附近的超参数值上，而不是探索在搜索空间中远离它的潜在新值。随机搜索不会遇到这个问题，因为它不会集中于任何值。
- en: The values of hyperparameters are selected independently. For example, in Gradient
    Boosted Trees, it is recommended to jointly consider the learning rate and the
    number of estimators, in order to avoid overfitting and reduce computation time.
    TPE would not be able to discover this relationship. In cases where we know about
    such a relation, we can partially overcome this problem by using different choices
    to define the search space.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数的值是独立选择的。例如，在梯度提升树中，建议联合考虑学习率和估计器的数量，以避免过拟合并减少计算时间。TPE 无法发现这种关系。在我们知道有这种关系的情况下，可以通过使用不同的选择来定义搜索空间，从而部分解决这个问题。
- en: In this brief introduction, we presented a high-level overview of the methodology.
    However, there is much more ground to cover in terms of surrogate models, acquisition
    functions, and so on. That is why we refer to a list of papers in the *See also*
    section for a more in-depth explanation.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在这简短的介绍中，我们提供了该方法论的高层次概述。然而，关于代理模型、获取函数等方面还有很多内容需要涵盖。因此，我们在*另见*部分参考了更多论文，以便进行更深入的解释。
- en: In this recipe, we use the Bayesian hyperparameter optimization to tune a LightGBM
    model. We chose this model as it provides a very good balance between performance
    and training time. We will be using the already familiar credit card fraud dataset,
    which is a highly imbalanced dataset.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们使用贝叶斯超参数优化来调整 LightGBM 模型。我们选择这个模型，因为它在性能和训练时间之间提供了非常好的平衡。我们将使用已经熟悉的信用卡欺诈数据集，这是一个高度不平衡的数据集。
- en: How to do it...
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Execute the following steps to run Bayesian hyperparameter optimization of
    a LightGBM model:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以运行 LightGBM 模型的贝叶斯超参数优化：
- en: 'Load the libraries:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载库：
- en: '[PRE30]'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define parameters for later use:'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义后续使用的参数：
- en: '[PRE31]'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Load and prepare the data:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并准备数据：
- en: '[PRE32]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Train the benchmark LightGBM model with the default hyperparameters:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认超参数训练基准 LightGBM 模型：
- en: '[PRE33]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下图形：
- en: '![](../Images/B18112_14_18.png)'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_18.png)'
- en: 'Figure 14.18: Performance evaluation of the benchmark LightGBM model'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.18：基准 LightGBM 模型的性能评估
- en: Additionally, we learned that the benchmark’s recall score on the test set is
    equal to `0.4286`.
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，我们了解到基准模型在测试集上的召回率得分为 `0.4286`。
- en: 'Define the objective function:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义目标函数：
- en: '[PRE34]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Define the search space:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义搜索空间：
- en: '[PRE35]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can generate a single draw from the sample space using the `sample` function:'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以使用 `sample` 函数从样本空间中生成一个单一的抽样：
- en: '[PRE36]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Executing the snippet prints the following dictionary:'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段将打印以下字典：
- en: '[PRE37]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Find the best hyperparameters using Bayesian HPO:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用贝叶斯HPO寻找最佳超参数：
- en: '[PRE38]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Inspect the best set of hyperparameters:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查最佳超参数集：
- en: '[PRE39]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Executing the snippet prints the list of the best hyperparameters:'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段将打印出最佳超参数的列表：
- en: '[PRE40]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Fit a new model using the best hyperparameters:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最佳超参数拟合新模型：
- en: '[PRE41]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Evaluate the fitted model on the test set:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估拟合的模型：
- en: '[PRE42]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段将生成以下图表：
- en: '![](../Images/B18112_14_19.png)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_19.png)'
- en: 'Figure 14.19: Performance evaluation of the tuned LightGBM model'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.19：调优后的LightGBM模型的性能评估
- en: We can see that the tuned model achieved better performance on the test set.
    To make it more concrete, its recall score was `0.8980`, as compared to the benchmark
    value of `0.4286`.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，调优后的模型在测试集上表现更好。为了更具体地说明，它的召回率得分为`0.8980`，而基准值为`0.4286`。
- en: How it works...
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'After loading the required libraries, we defined a set of parameters that we
    used in this recipe: the number of folds for cross-validation, the maximum number
    of iterations in the optimization procedure, the random state, and the metric
    used for optimization.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 加载所需库后，我们定义了一组在本配方中使用的参数：交叉验证的折数、优化过程中的最大迭代次数、随机状态和用于优化的指标。
- en: In *Step 3*, we imported the dataset and created the training and test sets.
    We described a few preprocessing steps in previous recipes, so please refer to
    those for more information. Then, we trained a benchmark LightGBM model using
    the default hyperparameters.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3步*中，我们导入了数据集并创建了训练集和测试集。我们在之前的配方中描述了一些预处理步骤，更多信息请参考这些内容。接着，我们使用默认超参数训练了基准的LightGBM模型。
- en: While using LightGBM, we can actually define a few random seeds. There are separate
    ones used for bagging and selecting a subset of features for each tree. Also,
    there is a `deterministic` flag that we can specify. To make the results fully
    reproducible, we should also make sure those additional settings are correctly
    specified.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LightGBM时，我们实际上可以定义几个随机种子。每个树的装袋和特征子集选择都有各自的种子。此外，还有一个`deterministic`标志，我们可以指定它。为了使结果完全可重复，我们还应该确保这些额外的设置被正确指定。
- en: 'In *Step 5*, we defined the true objective function (the one for which the
    Bayesian optimization will create a surrogate). The function takes the set of
    hyperparameters as inputs and uses stratified 5-fold cross-validation to calculate
    the loss value to be minimized. In the case of fraud detection, we want to detect
    as much fraud as possible, even if it means creating more false positives. That
    is why we selected recall as the metric of interest. As the optimizer will minimize
    the function, we multiplied it by -1 to create a maximization problem. The function
    must return either a single value (the loss) or a dictionary with at least two
    key-value pairs:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5步*中，我们定义了真实目标函数（贝叶斯优化将为其创建代理函数）。该函数将超参数集作为输入，并使用分层的5折交叉验证计算要最小化的损失值。在欺诈检测的情况下，我们希望尽可能多地检测到欺诈，即使这意味着产生更多的假阳性。因此，我们选择召回率作为关注的指标。由于优化器将最小化该函数，我们将其乘以-1，以将问题转化为最大化问题。该函数必须返回一个单一值（损失）或一个包含至少两个键值对的字典：
- en: '`loss`—The value of the true objective function.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`—真实目标函数的值。'
- en: '`status`—An indicator that the loss value was calculated correctly. It can
    be either `STATUS_OK` or `STATUS_FAIL`.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`status`—指示损失值是否正确计算的指标。它可以是`STATUS_OK`或`STATUS_FAIL`。'
- en: Additionally, we returned the set of hyperparameters used for evaluating the
    objective function. We will get back to it in the *There’s more…* section.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还返回了用于评估目标函数的超参数集。在*更多内容…*部分我们将回到这一点。
- en: We used the `cross_val_score` function to calculate the validation score. However,
    there are cases in which we might want to manually iterate over the folds created
    with `StratifiedKFold`. One such case would be to access more functionalities
    of the native API of LightGBM, for example, early stopping.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`cross_val_score`函数计算验证得分。然而，在某些情况下，我们可能希望手动遍历由`StratifiedKFold`创建的折。例如，我们希望访问LightGBM原生API的更多功能，例如早期停止。
- en: 'In *Step 6*, we defined the hyperparameter grid. The search space is defined
    as a dictionary, but in comparison to the spaces defined for `GridSearchCV`, we
    used `hyperopt`''s built-in functions, such as the following:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第6步*中，我们定义了超参数网格。搜索空间被定义为一个字典，但与为`GridSearchCV`定义的空间相比，我们使用了`hyperopt`的内置函数，例如以下内容：
- en: '`hp.choice(label,` `list)`—returns one of the indicated options.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.choice(label,` `list)`—返回所指定选项中的一个。'
- en: '`hp.uniform(label,` `lower_value,` `upper_value)`—the uniform distribution
    between two values.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.uniform(label,` `lower_value,` `upper_value)`—在两个值之间的均匀分布。'
- en: '`hp.quniform(label,` `low,` `high,` `q)`—the quantized (or discrete) uniform
    distribution between two values. In practice, it means that we obtain uniformly
    distributed, evenly spaced (determined by `q`) integers.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.quniform(label,` `low,` `high,` `q)`—在两个值之间的量化（或离散）均匀分布。实际上，这意味着我们得到的是均匀分布、间隔均匀（由
    `q` 确定）的整数。'
- en: '`hp.loguniform(label,` `low,` `high)`—the logarithm of the returned value is
    uniformly distributed. In other words, the returned numbers are evenly distributed
    on a logarithmic scale. Such a distribution is useful for exploring values that
    vary over several orders of magnitude. For example, when tuning the learning rate
    we would like to test values such as 0.001, 0.01, 0.1, and 1, instead of a uniformly
    distributed set between 0 and 1.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.loguniform(label,` `low,` `high)`—返回值的对数是均匀分布的。换句话说，返回的数字在对数尺度上是均匀分布的。这种分布对于探索跨越多个数量级变化的值非常有用。例如，在调节学习率时，我们希望测试像
    0.001、0.01、0.1 和 1 这样的值，而不是在 0 和 1 之间均匀分布的值集。'
- en: '`hp.randint(label,` `upper_value)`—returns a random integer in the range `[0,
    upper_value)`.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.randint(label,` `upper_value)`—返回一个范围在 `[0, upper_value)` 之间的随机整数。'
- en: Bear in mind that in this setup we had to define the names (denoted as `label`
    in the snippets above) of the hyperparameters twice. Additionally, in some cases,
    we wanted to force the values to be integers using `scope.int`.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在这个设置中，我们必须将超参数的名称（上面代码片段中的 `label`）定义两次。此外，在某些情况下，我们希望强制值为整数，可以使用 `scope.int`。
- en: In *Step 7*, we ran the Bayesian optimization to find the best set of hyperparameters.
    First, we defined the `Trials` object, which was used for storing the history
    of the search. We could even use it to resume a search or expand an already finished
    one, that is, increase the number of iterations using the already stored history.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 7* 中，我们运行了贝叶斯优化，以寻找最佳的超参数集。首先，我们定义了 `Trials` 对象，用于存储搜索的历史记录。我们甚至可以使用它来恢复搜索或扩展已经完成的搜索，也就是说，通过使用已经存储的历史记录来增加迭代次数。
- en: Second, we ran the optimization by passing the objective function, the search
    space, the surrogate model, the maximum number of iterations, and the `trials`
    object for storing the history. For more details on tuning the TPE algorithm,
    please refer to `hyperopt`'s documentation. Additionally, we set the value of
    `rstate`, which is `hyperopt`'s equivalent of `random_state`. We can easily store
    the `trials` object in a pickle file for later use. To do so, we can use the `pickle.dump`
    and `pickle.load` functions.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们通过传递目标函数、搜索空间、代理模型、最大迭代次数和 `trials` 对象（用于存储历史记录）来运行优化。有关调节 TPE 算法的更多细节，请参阅
    `hyperopt` 的文档。此外，我们还设置了 `rstate` 的值，它是 `hyperopt` 中相当于 `random_state` 的设置。我们可以轻松地将
    `trials` 对象存储到 pickle 文件中，以便以后使用。为此，我们可以使用 `pickle.dump` 和 `pickle.load` 函数。
- en: After running the Bayesian HPO, the `trials` object contains a lot of interesting
    and useful information. We can find the best set of hyperparameters under `trials.best_trial`,
    while `trials.results` contains all the explored sets of hyperparameters. We will
    be using this information in the *There’s more…* section.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 运行贝叶斯优化（Bayesian HPO）后，`trials` 对象包含了许多有趣且有用的信息。我们可以通过 `trials.best_trial` 找到最佳的超参数集，而
    `trials.results` 则包含了所有探索过的超参数集。我们将在 *还有更多内容…* 部分使用这些信息。
- en: In *Step 8*, we inspected the best set of hyperparameters. Instead of just printing
    the dictionary, we had to use the `space_eval` function. This is because just
    by printing the dictionary we will see the indices of any categorical features
    instead of their names. As an example, by printing the `best_set` dictionary we
    could potentially see a `0` instead of `'gbdt'` for the `boosting_type` hyperparameter.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 8* 中，我们检查了最佳的超参数集。我们不仅仅是打印字典，而是必须使用 `space_eval` 函数。这是因为仅打印字典时，我们会看到任何分类特征的索引，而不是它们的名称。例如，打印
    `best_set` 字典时，我们可能会看到 `0`，而不是 `boosting_type` 超参数中的 `'gbdt'`。
- en: In the last two steps, we trained a LightGBM classifier using the identified
    hyperparameters and evaluated its performance on the test set.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后两步中，我们使用确定的超参数训练了一个 LightGBM 分类器，并在测试集上评估了它的性能。
- en: There’s more...
  id: totrans-483
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: There are still quite a lot of interesting and useful things to mention about
    Bayesian hyperparameter optimization. We try to present those in the following
    subsections. For brevity’s sake, we do not present all the code here. For the
    complete code walk-through, please refer to the Jupyter notebook available in
    the book’s GitHub repository.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有很多有趣且有用的内容需要提及关于贝叶斯超参数优化的内容。我们尝试在以下小节中进行介绍。为了简洁起见，我们不在此展示所有代码。如需完整的代码示例，请参考书籍
    GitHub 仓库中的 Jupyter notebook。
- en: Conditional hyperparameter spaces
  id: totrans-485
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 条件超参数空间
- en: Conditional hyperparameter spaces can be useful when we would like to experiment
    with different machine learning models, each of those coming with completely separate
    hyperparameters. Alternatively, some hyperparameters are simply not compatible
    with others, and this should be accounted for while tuning the model.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 条件超参数空间在我们想尝试不同的机器学习模型时非常有用，每个模型都有完全不同的超参数。或者，有些超参数彼此之间根本不兼容，在调优模型时需要考虑这一点。
- en: 'In the case of LightGBM, an example could be the following pair: `boosting_type`
    and `subsample`/`subsample_freq`. The boosting type `"goss"` is not compatible
    with subsampling, that is, selecting only a subsample of the training observations
    for each iteration. That is why we would like to set `subsample` to 1 when we
    are using GOSS, but tune it otherwise. `subsample_freq` is a complementary hyperparameter
    that determines how often (every *n-*th iteration) we should use subsampling.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LightGBM，一个例子可能是以下的超参数组合：`boosting_type` 和 `subsample`/`subsample_freq`。提升类型
    `"goss"` 与子采样不兼容，也就是说，在每次迭代中仅选择一部分训练样本进行训练。这就是为什么在使用 GOSS 时，我们希望将 `subsample`
    设置为 1，但在其他情况下进行调优。`subsample_freq` 是一个补充性超参数，决定我们在每 *n-* 次迭代中应使用多少频率的子采样。
- en: 'We define a conditional search space using `hp.choice` in the following snippet:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下代码片段中使用 `hp.choice` 定义了一个条件搜索空间：
- en: '[PRE43]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'And an example of a draw from this space looks as follows:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是从该空间中提取的一个示例：
- en: '[PRE44]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'There is one more step that we need to take before being able to use such a
    draw for our Bayesian HPO. As the search space is initially nested, we have to
    assign the drawn samples to the top-level key in the dictionary. We do so in the
    following snippet:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够使用这种抽取值进行贝叶斯超参数优化之前，还有一步需要完成。由于搜索空间最初是嵌套的，我们需要将抽取的样本分配到字典中的顶层键。我们可以通过以下代码片段来实现：
- en: '[PRE45]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The `get` method extracts the value of the requested key from the dictionary
    or returns the default value if the requested key does not exist.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '`get` 方法从字典中提取所请求键的值，如果请求的键不存在，则返回默认值。'
- en: 'Executing the snippet returns a properly formatted dictionary:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 执行该代码片段会返回一个格式正确的字典：
- en: '[PRE46]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Lastly, we should place the code cleaning up the dictionary in the objective
    function, which we then pass to the optimization routine.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应该将清理字典的代码放入目标函数中，然后将其传递给优化过程。
- en: In the Jupyter notebook, we have also tuned the LightGBM with the conditional
    search space. It achieved a recall score of `0.8980` on the test set, which is
    the same score as the model tuned without the conditional search space.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter notebook 中，我们还使用条件搜索空间对 LightGBM 进行了调优。它在测试集上达到了 `0.8980` 的召回率，和没有使用条件搜索空间的模型得分相同。
- en: '![](../Images/B18112_14_20.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_20.png)'
- en: 'Figure 14.20: Performance evaluation of the LightGBM model tuned with the conditional
    search space'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.20：使用条件搜索空间调优后的 LightGBM 模型的性能评估
- en: A deep dive into the explored hyperparameters
  id: totrans-501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深入探索已探索的超参数
- en: 'We have mentioned that `hyperopt` offers a wide range of distributions from
    which we could sample. It will be much easier to understand when we actually see
    what the distributions look like. First, we inspect the distribution of the learning
    rate. We have specified it as:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到过，`hyperopt` 提供了多种分布供我们进行采样。当我们实际看到这些分布的样子时，理解起来会更加容易。首先，我们检查学习率的分布。我们将其指定为：
- en: '[PRE47]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In the following figure, we can see a **kernel density estimate** (**KDE**)
    plot of 10,000 random draws from the log-uniform distribution of the learning
    rate.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到从学习率的对数均匀分布中抽取的 10,000 个随机值的 **核密度估计**（**KDE**）图。
- en: '![](../Images/B18112_14_21.png)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_21.png)'
- en: 'Figure 14.21: Distribution of the learning rate'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.21：学习率的分布
- en: As intended, we can see that the distribution puts more weight on observations
    from several orders of magnitude.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们可以看到分布在几个数量级的观测值上赋予了更多的权重。
- en: 'The next distribution worth inspecting is the quantized uniform distribution
    that we have used for the `min_child_samples` hyperparameter. We defined it as:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个值得检查的分布是我们为`min_child_samples`超参数使用的量化均匀分布。我们将其定义为：
- en: '[PRE48]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the following figure, we can see that the distribution reflects the assumptions
    we set for it, that is, the evenly spaced integers are uniformly distributed.
    In our case, we sampled every fifth integer. To keep the plot readable, we only
    displayed the first 20 bars. But the full distribution goes to 500, just as we
    have specified.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到分布反映了我们为其设置的假设，即均匀分布的整数是均匀分布的。在我们的例子中，我们每次采样一个间隔为5的整数。为了保持图表的可读性，我们只显示了前20个条形图。但完整的分布范围为500，正如我们所指定的那样。
- en: '![](../Images/B18112_14_22.png)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_22.png)'
- en: 'Figure 14.22: Distribution of the min_child_samples hyperparameter'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.22：`min_child_samples`超参数的分布
- en: So far, we have only looked at the information available in the search space.
    However, we can also derive much more information from the `Trials` object, which
    stores the entire history of the Bayesian HPO procedure, that is, which hyperparameters
    were explored and what the resulting score was.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只查看了搜索空间中可用的信息。然而，我们还可以从`Trials`对象中推导出更多信息，它存储了整个贝叶斯HPO过程的历史记录，即探索了哪些超参数，以及相应的得分是多少。
- en: 'For this part, we use the `Trials` object containing the search history, using
    the search space without the conditional `boosting_type` tuning. In order to easily
    explore that data, we prepare a DataFrame containing the required information
    per iteration: the hyperparameters and the value of the loss function. We can
    extract the information from `trials.results`. This is the reason why we additionally
    passed the `params` object to the final dictionary while defining the `objective`
    function.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们使用了包含搜索历史的`Trials`对象，使用不包含条件`boosting_type`调优的搜索空间。为了便于探索这些数据，我们准备了一个包含每次迭代所需信息的DataFrame：超参数和损失函数的值。我们可以从`trials.results`中提取这些信息。这也是我们在定义`objective`函数时额外传递`params`对象到最终字典中的原因。
- en: 'Initially, the hyperparameters are stored in one column as a dictionary. We
    can use the `json_normalize` function to break them up into separate columns:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，超参数作为字典存储在一列中。我们可以使用`json_normalize`函数将其拆分为单独的列：
- en: '[PRE49]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Executing the snippet prints the following table:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 执行该代码段后，会打印出以下表格：
- en: '![](../Images/B18112_14_23.png)'
  id: totrans-518
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_23.png)'
- en: 'Figure 14.23: A snippet of the DataFrame containing all the explored hyperparameter
    combinations and their corresponding losses'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.23：包含所有探索的超参数组合及其对应损失的DataFrame片段
- en: For brevity, we only printed a few of the available columns. Using this information,
    we can further explore the optimization that resulted in the best set of hyperparameters.
    For example, we can see that the best score was achieved in the 151st iteration
    (the first row of the DataFrame has an index of `150` and indices in Python start
    with `0`).
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们只打印了几个可用的列。利用这些信息，我们可以进一步探索优化过程，以便找到最佳的超参数组合。例如，我们可以看到最佳得分是在第151次迭代时取得的（DataFrame的第一行索引为`150`，而Python的索引从`0`开始）。
- en: 'In the next figure, we have plotted the two distributions of the `colsample_bytree`
    hyperparameter: the one we defined as the prior for sampling, and the one that
    was actually sampled during the Bayesian optimization. Additionally, we plotted
    the evolution of the hyperparameter over iterations and added a regression line
    to indicate the direction of change.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们绘制了`colsample_bytree`超参数的两种分布：一种是我们定义的用于采样的先验分布，另一种是在贝叶斯优化过程中实际采样的分布。此外，我们还绘制了超参数随迭代的变化，并添加了回归线以指示变化的方向。
- en: In the left plot, we can see that the posterior distribution of `colsample_bytree`
    was concentrated toward the right side, indicating the higher range of considered
    values. By inspecting the KDE plots it seems that there is a non-zero density
    for values above 1, which should not be allowed.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧图中，我们可以看到`colsample_bytree`的后验分布集中在右侧，表明考虑的值处于较高范围。通过检查KDE图，我们发现对于大于1的值似乎存在非零密度，而这些值是不允许的。
- en: This is just the artifact from using the plotting method; in the `Trials` object
    we can confirm that not a single value above 1.0 was sampled during the optimization.
    In the right plot, the values of `colsample_bytree` seem to be scattered all over
    the allowed range. By looking at the regression line, it seems that there is a
    somewhat increasing trend.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是使用绘图方法时的产物；在`Trials`对象中我们可以确认在优化过程中没有采样到任何超过1.0的值。在右侧的图中，`colsample_bytree`的值似乎散布在允许的范围内。通过观察回归线，似乎存在一定的上升趋势。
- en: '![](../Images/B18112_14_24.png)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_24.png)'
- en: 'Figure 14.24: Distribution of the colsample_bytree hyperparameter'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.24：`colsample_bytree`超参数的分布
- en: Lastly, we can look at the evolution of the loss over iterations. The loss represents
    the negative of the average recall score (from a 5-fold cross-validation on the
    training set). The lowest value (corresponding to maximum average recall) of `-0.90`
    occurred in the 151st iteration. With a few exceptions, the loss is quite stable
    in the `-0.75` to `-0.85` range.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以观察损失随迭代的演变。损失表示平均召回率的负值（来自训练集上的5折交叉验证）。最低值（对应最大平均召回率）为`-0.90`，发生在第151次迭代中。除了一些例外，损失在`-0.75`到`-0.85`之间相对稳定。
- en: '![](../Images/B18112_14_25.png)'
  id: totrans-527
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_25.png)'
- en: 'Figure 14.25: The evolution of the loss (average recall) over iterations. The
    best iteration is marked with a star'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.25：损失（平均召回率）随迭代的演变。最佳迭代用星号标记
- en: Other popular libraries for hyperparameter optimization
  id: totrans-529
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他流行的超参数优化库
- en: '`hyperopt` is one of the most popular Python libraries for hyperparameter optimization.
    However, it is definitely not the only one. Below you can find a list of popular
    alternatives:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '`hyperopt`是最流行的超参数优化Python库之一。然而，它绝对不是唯一的。下面是一些流行的替代方案：'
- en: '`optuna`—a library offering vast hyperparameter tuning capabilities, including
    exhaustive Grid Search, Random Search, Bayesian HPO, and evolutionary algorithms.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optuna`——一个提供广泛超参数调优功能的库，包括详尽的网格搜索、随机搜索、贝叶斯超参数优化和进化算法。'
- en: '`scikit-optimize`—a library offering the `BayesSearchCV` class, which is a
    Bayesian drop-in replacement for `scikit-learn`''s `GridSearchCV`.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-optimize`——一个提供`BayesSearchCV`类的库，`BayesSearchCV`是`scikit-learn`的`GridSearchCV`的贝叶斯替代品。'
- en: '`hyperopt-sklearn`—a spin-off library of `hyperopt` offering model selection
    among machine learning algorithms from `scikit-learn`. It allows you to search
    for the best option among preprocessing steps and ML models, thus covering the
    entire scope of ML pipelines. The library covers almost all classifiers/regressors/preprocessing
    transformers available in `scikit-learn`.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hyperopt-sklearn`——`hyperopt`的衍生库，提供`scikit-learn`中机器学习算法的模型选择。它允许在预处理步骤和机器学习模型之间搜索最佳选项，从而涵盖了整个机器学习管道的范围。该库涵盖了几乎所有在`scikit-learn`中可用的分类器/回归器/预处理变换器。'
- en: '`ray[tune]`—Ray is an open-source, general-purpose distributed computing framework.
    We can use its `tune` module to run distributed hyperparameter tuning. It is also
    possible to combine `tune`''s distributed computing capabilities with other well-established
    libraries such as `hyperopt` or `optuna`.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ray[tune]`——Ray是一个开源的通用分布式计算框架。我们可以使用它的`tune`模块进行分布式超参数调优。也可以将`tune`的分布式计算能力与其他成熟的库（如`hyperopt`或`optuna`）结合使用。'
- en: '`Tpot`—TPOT is an AutoML tool that optimizes ML pipelines using genetic programming.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tpot`——TPOT是一个使用遗传编程优化机器学习管道的AutoML工具。'
- en: '`bayesian-optimization`—a library offering general-purpose Bayesian global
    optimization with Gaussian processes.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bayesian-optimization`——一个提供通用贝叶斯全局优化的库，采用高斯过程。'
- en: '`smac`—SMAC is a general tool for optimizing the parameters of arbitrary algorithms,
    including hyperparameter optimization of ML models.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`smac`——SMAC是一个用于优化任意算法参数的通用工具，包括机器学习模型的超参数优化。'
- en: See also
  id: totrans-538
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Additional resources are available here:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 其他资源可以在这里找到：
- en: 'Bergstra, J. S., Bardenet, R., Bengio, Y., & Kégl, B. 2011\. Algorithms for
    hyper-parameter optimization. In *Advances in Neural Information Processing Systems*:
    2546–2554.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bergstra, J. S., Bardenet, R., Bengio, Y., & Kégl, B. 2011\. 超参数优化算法。载于《*神经信息处理系统进展*》：2546–2554。
- en: 'Bergstra, J., Yamins, D., & Cox, D. D. 2013, June. Hyperopt: A Python library
    for optimizing the hyperparameters of machine learning algorithms. In *Proceedings
    of the 12th Python in science conference:* 13–20.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bergstra, J., Yamins, D., & Cox, D. D. 2013年6月。Hyperopt：一个用于优化机器学习算法超参数的Python库。载于《*第12届Python科学会议论文集*》：13–20。
- en: 'Bergstra, J., Yamins, D., Cox, D. D. 2013\. Making a Science of Model Search:
    Hyperparameter Optimization in *Hundreds of Dimensions for Vision Architectures.
    Proc. of the 30th International Conference on Machine Learning* (ICML 2013).'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bergstra, J., Yamins, D., Cox, D. D. 2013\. 使模型搜索成为科学：视觉架构的*数百维度的超参数优化*。第30届国际机器学习大会（ICML
    2013）论文集。
- en: Claesen, M., & De Moor, B. 2015\. “Hyperparameter search in machine learning.”
    *arXiv preprint arXiv:1502.02127*.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Claesen, M., & De Moor, B. 2015\. “机器学习中的超参数搜索。” *arXiv预印本 arXiv:1502.02127*。
- en: 'Falkner, S., Klein, A., & Hutter, F. 2018, July. BOHB: Robust and efficient
    hyperparameter optimization at scale. In *International Conference on Machine
    Learning*: 1437–1446\. PMLR.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falkner, S., Klein, A., & Hutter, F. 2018年7月。BOHB：大规模的稳健高效的超参数优化。在 *国际机器学习大会*：1437–1446。PMLR。
- en: 'Hutter, F., Kotthoff, L., & Vanschoren, J. 2019\. *Automated machine learning:
    methods, systems, challenges*: 219\. Springer Nature.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hutter, F., Kotthoff, L., & Vanschoren, J. 2019\. *自动化机器学习：方法、系统、挑战*：219。Springer
    Nature。
- en: 'Klein, A., Falkner, S., Bartels, S., Hennig, P., & Hutter, F. 2017, April.
    Fast Bayesian optimization of machine learning hyperparameters on large datasets.
    In *Artificial intelligence and statistics*: 528–536\. PMLR.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Klein, A., Falkner, S., Bartels, S., Hennig, P., & Hutter, F. 2017年4月。大规模数据集上的机器学习超参数的快速贝叶斯优化。在
    *人工智能与统计学*：528–536。PMLR。
- en: 'Komer B., Bergstra J., & Eliasmith C. 2014\. “Hyperopt-Sklearn: automatic hyperparameter
    configuration for Scikit-learn” Proc. SciPy.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Komer B., Bergstra J., & Eliasmith C. 2014\. “Hyperopt-Sklearn：Scikit-learn的自动化超参数配置”
    Proc. SciPy。
- en: 'Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B., &
    Talwalkar, A. 2018\. Massively parallel hyperparameter tuning: [https://doi.org/10.48550/arXiv.1810.05934](https://doi.org/10.48550/arXiv.1810.05934)'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B., &
    Talwalkar, A. 2018\. 大规模并行超参数调优： [https://doi.org/10.48550/arXiv.1810.05934](https://doi.org/10.48550/arXiv.1810.05934)
- en: 'Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & De Freitas, N. 2015\.
    *Taking the human out of the loop: A review of Bayesian optimization*. Proceedings
    of the *IEEE*, 104(1): 148–175.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & De Freitas, N. 2015\.
    *从循环中去除人类：贝叶斯优化综述*。*IEEE*会议录，104(1)：148–175。
- en: 'Snoek, J., Larochelle, H., & Adams, R. P. 2012\. Practical Bayesian optimization
    of machine learning algorithms. *Advances in Neural Information Processing Systems*:
    *25*.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snoek, J., Larochelle, H., & Adams, R. P. 2012\. 机器学习算法的实用贝叶斯优化。*神经信息处理系统进展*：*25*。
- en: Investigating feature importance
  id: totrans-551
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查特征重要性
- en: We have already spent quite some time creating the entire pipeline and tuning
    the models to achieve better performance. However, what is equally—or in some
    cases even more—important is the model’s interpretability. That means not only
    giving an accurate prediction but also being able to explain the whybehind it.
    For example, we can look into the case of customer churn. Knowing what the actual
    predictors of the customers leaving are might be helpful in improving the overall
    service and potentially making them stay longer.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经花费了相当多的时间来创建整个管道并调优模型，以实现更好的性能。然而，同样重要——甚至在某些情况下更重要——的是模型的可解释性。这意味着不仅要给出准确的预测，还需要能够解释其背后的原因。例如，我们可以查看客户流失的案例。了解客户离开的实际预测因素可能有助于改善整体服务，并有可能让他们停留更长时间。
- en: In a financial setting, banks often use machine learning in order to predict
    a customer’s ability to repay credit or a loan. In many cases, they are obliged
    to justify their reasoning, that is, if they decline a credit application, they
    need to know exactly why this customer’s application was not approved. In the
    case of very complicated models, this might be hard, or even impossible.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融环境中，银行通常使用机器学习来预测客户偿还信用或贷款的能力。在许多情况下，他们必须为自己的推理提供正当理由，即如果他们拒绝了一份信用申请，他们需要确切知道为什么这位客户的申请没有被批准。对于非常复杂的模型来说，这可能是困难的，甚至是不可能的。
- en: 'We can benefit in multiple ways by knowing the importance of our features:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 通过了解特征的重要性，我们可以从多个方面受益：
- en: By understanding the model’s logic, we can theoretically verify its correctness
    (if a sensible feature is a good predictor), but also try to improve the model
    by focusing only on the important variables.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过理解模型的逻辑，我们可以理论上验证其正确性（如果某个合理的特征是一个好的预测因素），同时也能通过只关注重要变量来尝试改进模型。
- en: We can use the feature importances to only keep the *x* most important features
    (contributing to a specified percentage of total importance), which can not only
    lead to better performance by removing potential noise but also to a shorter training
    time.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用特征重要性来保留*前x*个最重要的特征（这些特征贡献了指定百分比的总重要性），这不仅能通过去除潜在的噪音提高性能，还能缩短训练时间。
- en: In some real-life cases, it makes sense to sacrifice some accuracy (or any other
    performance metric) for the sake of interpretability.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一些现实案例中，为了可解释性，牺牲一些准确性（或其他任何性能指标）是合理的。
- en: It is also important to be aware that the more accurate (in terms of a specified
    performance metric) the model is, the more reliable the feature importances are.
    That is why we investigate the importance of the features after tuning the models.
    Please note that we should also account for overfitting, as an overfitted model
    will not return reliable feature importances.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 同时需要注意的是，模型的准确性（就指定的性能指标而言）越高，特征重要性就越可靠。这就是为什么我们在调整模型后会调查特征的重要性。请注意，我们还应考虑过拟合，因为过拟合的模型不会返回可靠的特征重要性。
- en: In this recipe, we show how to calculate the feature importance on an example
    of a Random Forest classifier. However, most of the methods are model-agnostic.
    In other cases, there are often equivalent approaches (such as in the case of
    XGBoost and LightGBM). We mention some of those in the *There’s more…* section.
    We briefly present the three selected methods of calculating feature importance.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们展示了如何在随机森林分类器的示例中计算特征重要性。然而，大多数方法都是与模型无关的。在其他情况下，通常也有等效的方法（例如在XGBoost和LightGBM的情况下）。我们会在*更多内容...*部分提到其中的一些方法。我们简要介绍了计算特征重要性的三种选定方法。
- en: '**Mean Decrease in Impurity** (**MDI**): The default feature importance used
    by Random Forest (in `scikit-learn`), also known as the Gini importance. As we
    know, decision trees use a metric of impurity (Gini index/entropy/MSE) to create
    the best splits while growing. When training a decision tree, we can compute how
    much each feature contributes to decreasing the weighted impurity. To calculate
    the feature importance for the entire forest, the algorithm averages the decrease
    in impurity over all the trees.'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '**不纯度的平均减少**（**MDI**）：这是随机森林（在`scikit-learn`中使用的默认特征重要性），也称为基尼重要性。正如我们所知，决策树使用一种不纯度度量（基尼指数/熵/MSE）来创建最佳分裂。在训练决策树时，我们可以计算每个特征在减少加权不纯度方面的贡献。为了计算整个森林的特征重要性，算法会计算所有树的不纯度减少的平均值。'
- en: While working with impurity-based metrics, we should focus on the ranking of
    the variables (relative values) rather than the absolute values of the feature
    importances (which are also normalized to add up to 1).
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用基于不纯度的指标时，我们应该关注变量的排名（相对值），而不是特征重要性的绝对值（这些值也已归一化，使其总和为1）。
- en: 'Here are the advantages of this approach:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这种方法的优点：
- en: Fast calculation
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速计算
- en: Easy to retrieve
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于获取
- en: 'Here are the disadvantages of this approach:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这种方法的缺点：
- en: Biased—It tends to inflate the importance of continuous (numerical) features
    or high-cardinality categorical variables. This can sometimes lead to absurd cases,
    whereby an additional random variable (unrelated to the problem at hand) scores
    high in the feature importance ranking.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏倚—它倾向于高估连续（数值型）特征或高卡方类别变量的重要性。这有时会导致荒谬的情况，其中一个额外的随机变量（与当前问题无关）在特征重要性排名中得分很高。
- en: Impurity-based importances are calculated on the basis of the training set and
    do not reflect the model’s ability to generalize to unseen data.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于不纯度的特征重要性是基于训练集计算的，并不能反映模型对未见数据的泛化能力。
- en: '**Drop-column feature importance**:The idea behind this approach is very simple.
    We compare a model with all the features to a model with one of the features dropped
    for training and inference. We repeat this process for all the features.'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '**丢列特征重要性**：这种方法背后的理念非常简单。我们将一个包含所有特征的模型与一个去掉某个特征的模型进行比较，进行训练和推断。我们对所有特征都重复这个过程。'
- en: 'Here is the advantage of this approach:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这种方法的优点：
- en: Often considered the most accurate/reliable measure of feature importance
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常被认为是最准确/最可靠的特征重要性度量
- en: 'Here is the disadvantage of this approach:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这种方法的缺点：
- en: Potentially highest computation cost caused by retraining the model for each
    variant of the dataset
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于对每个数据集变体进行重新训练，可能会导致最高的计算成本
- en: '**Permutation feature importance**:This approach directly measures feature
    importance by observing how random reshuffling of each predictor influences the
    model’s performance. The permutation procedure breaks the relationship between
    the feature and the target. Hence, the drop in the model’s performance is indicative
    of how much the model is dependent on a particular feature. If the decrease in
    the performance after reshuffling a feature is small, then it was not a very important
    feature in the first place. Conversely, if the decrease in performance is significant,
    the feature can be considered an important one for the model.'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '**置换特征重要性**：这种方法通过观察每个预测变量的随机重排如何影响模型性能来直接测量特征重要性。置换过程破坏了特征与目标之间的关系。因此，模型性能的下降反映了模型在多大程度上依赖于某个特定特征。如果在重排特征后，性能的下降较小，则说明该特征本身并不是非常重要。相反，如果性能下降显著，则可以认为该特征对模型来说是重要的。'
- en: 'The steps of the algorithm are:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的步骤如下：
- en: Train the baseline model and record the score of interest.
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练基准模型并记录感兴趣的得分。
- en: Randomly permute (reshuffle) the values of one of the features, then use the
    entire dataset (with one reshuffled feature) to obtain predictions and record
    the score. The feature importance is the difference between the baseline score
    and the one from the permuted dataset.
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机打乱（重排）某个特征的值，然后使用整个数据集（包含已重排的特征）进行预测并记录得分。特征重要性是基准得分与重排数据集得分之间的差异。
- en: Repeat the second step for all features.
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有特征重复第二步。
- en: For evaluating the performance, we can either use the training data or the validation/test
    set. Using one of the latter two has the additional benefit of gaining insights
    into the model’s ability to generalize. For example, features that turn out to
    be important on the training set but not on the validation set might actually
    cause the model to overfit. For more discussion about the topic, please refer
    to the *Interpretable Machine Learning* book (referenced in the *See also* section).
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型性能时，我们可以使用训练数据或验证/测试集。使用后两者中的任意一个的额外好处是能够获得有关模型泛化能力的洞察。例如，某些在训练集上重要但在验证集上不重要的特征，可能实际上会导致模型过拟合。有关该话题的更多讨论，请参阅*可解释机器学习*书籍（在*另见*部分中有参考）。
- en: 'Here are the advantages of this approach:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这种方法的优点：
- en: Model-agnostic
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型无关
- en: Reasonably efficient—no need to retrain the model at every step
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合理高效——无需在每一步都重新训练模型
- en: Reshuffling preserves the distribution of the variables
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重排操作保持了变量的分布
- en: 'Here are the disadvantages of this approach:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这种方法的缺点：
- en: Computationally more expensive than the default feature importances
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算上比默认特征重要性更为昂贵
- en: Is likely to produce unreliable importances when features are highly correlated
    (see Strobl *et al.* for a detailed explanation)
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当特征高度相关时，可能会产生不可靠的重要性（请参阅Strobl *et al.* 以获取详细解释）
- en: In this recipe, we will explore the feature importance using the credit card
    default dataset we have already explored in the *Exploring ensemble classifiers*
    recipe.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 在本方案中，我们将使用在*探索集成分类器*方案中已经探索过的信用卡违约数据集来探讨特征重要性。
- en: Getting ready
  id: totrans-587
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we use the fitted Random Forest pipeline (called `rf_pipeline`)
    from the *Exploring ensemble classifiers* recipe. Please refer to this step in
    the Jupyter notebook to see all the initial steps not included here to avoid repetition.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个方案，我们使用了拟合的随机森林管道（称为`rf_pipeline`），该管道来自*探索集成分类器*方案。请参阅Jupyter笔记本中的这一步骤，以查看此处未包含的所有初始步骤，避免重复。
- en: How to do it...
  id: totrans-589
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到……
- en: 'Execute the following steps to evaluate the feature importance of a Random
    Forest model:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以评估随机森林模型的特征重要性：
- en: 'Import the libraries:'
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE50]'
  id: totrans-592
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Extract the classifier and preprocessor from the fitted pipeline:'
  id: totrans-593
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从拟合管道中提取分类器和预处理器：
- en: '[PRE51]'
  id: totrans-594
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Recover feature names from the preprocessing transformer and transform the
    training/test sets:'
  id: totrans-595
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从预处理变换器中恢复特征名称，并转换训练/测试集：
- en: '[PRE52]'
  id: totrans-596
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Extract the MDI feature importance and calculate the cumulative importance:'
  id: totrans-597
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取MDI特征重要性并计算累计重要性：
- en: '[PRE53]'
  id: totrans-598
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Define a function for plotting the top *x* features in terms of their importance:'
  id: totrans-599
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，用于绘制按重要性排序的前* x *个特征：
- en: '[PRE54]'
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We use the function as follows:'
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用以下函数：
- en: '[PRE55]'
  id: totrans-602
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段生成以下图表：
- en: '![](../Images/B18112_14_26.png)'
  id: totrans-604
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_26.png)'
- en: 'Figure 14.26: Top 10 most important features using the MDI metric'
  id: totrans-605
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.26：使用MDI指标计算的前10个最重要特征
- en: The most important features are categorical features indicating the payment
    status from July and September. After four of those, we can see continuous features
    such as `limit_balance`, `age`, various bill statements, and previous payments.
  id: totrans-606
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最重要的特征是分类特征，表示7月和9月的支付状态。在这四个特征之后，我们可以看到连续特征，如`limit_balance`、`age`、各种账单声明和之前的付款。
- en: 'Plot the cumulative importance of the features:'
  id: totrans-607
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制特征的重要性累积图：
- en: '[PRE56]'
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-609
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码片段会生成以下图形：
- en: '![](../Images/B18112_14_27.png)'
  id: totrans-610
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_27.png)'
- en: 'Figure 14.27: Cumulative MDI importance'
  id: totrans-611
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.27：累积MDI重要性
- en: The top 10 features account for 86.23% of the total importance, while the top
    17 features account for 95% of the total importance.
  id: totrans-612
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前10个特征占总重要性的86.23%，而前17个特征占总重要性的95%。
- en: 'Calculate and plot permutation importance using the training set:'
  id: totrans-613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练集计算并绘制置换重要性：
- en: '[PRE57]'
  id: totrans-614
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码片段会生成以下图形：
- en: '![](../Images/B18112_14_28.png)'
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_28.png)'
- en: 'Figure 14.28: Top 10 most important features according to permutation importance
    calculated on the training set'
  id: totrans-617
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.28：根据训练集计算的置换重要性排名前10的特征
- en: We can see that the set of the most important features was reshuffled in comparison
    to the MDI importance. The most important now is `payment_status_sep_Unknown`,
    which is an undefined label (not assigned a clear meaning in the original paper)
    in the `payment_status_sep` categorical feature. We can also see that `age` is
    not among the top 10 most important features determined using this approach.
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，最重要特征的集合与MDI重要性相比发生了重新排列。现在最重要的特征是`payment_status_sep_Unknown`，这是`payment_status_sep`分类特征中的一个未定义标签（在原文中没有明确赋予意义）。我们还可以看到，`age`不在使用这种方法确定的前10个最重要特征之中。
- en: 'Calculate and plot permutation importance using the test set:'
  id: totrans-619
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用测试集计算并绘制置换重要性：
- en: '[PRE58]'
  id: totrans-620
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码片段会生成以下图形：
- en: '![](../Images/B18112_14_29.png)'
  id: totrans-622
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_29.png)'
- en: 'Figure 14.29: Top 10 most important features according to permutation importance
    calculated on the test set'
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.29：根据测试集计算的置换重要性排名前10的特征
- en: Looking at the figures, we can state that the same four features were selected
    as the most important ones using the training and test sets. The other ones were
    slightly reshuffled.
  id: totrans-624
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过查看这些图形，我们可以得出结论，使用训练集和测试集选择的四个最重要特征是相同的。其他特征则略有调整。
- en: If we notice that the feature importances calculated using the training and
    test sets are significantly different, we should investigate whether the model
    is overfitted. To solve that, we might want to apply some form of regularization.
    In this case, we could try increasing the value of the `min_samples_leaf` hyperparameter.
  id: totrans-625
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们发现使用训练集和测试集计算的特征重要性有显著差异，应该调查模型是否存在过拟合的情况。为了解决这个问题，我们可能需要应用某种形式的正则化。在这种情况下，我们可以尝试增加`min_samples_leaf`超参数的值。
- en: 'Define a function for calculating the drop-column feature importance:'
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个计算移除列特征重要性的函数：
- en: '[PRE59]'
  id: totrans-627
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'There are two things worth mentioning here:'
  id: totrans-628
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有两点值得注意：
- en: We fixed the `random_state`, as we are specifically interested in performance
    changes caused by removing a feature. Hence, we are controlling the source of
    variability during the estimation procedure.
  id: totrans-629
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们固定了`random_state`，因为我们特别关注的是移除一个特征所导致的性能变化。因此，在估计过程中，我们控制了变异性的来源。
- en: In this implementation, we use the training data for evaluation. We leave it
    as an exercise for the reader to modify the function to accept additional objects
    for evaluation.
  id: totrans-630
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此实现中，我们使用训练数据进行评估。我们将修改该函数以接受额外的评估对象作为练习留给读者。
- en: 'Calculate and plot the drop-column feature importance:'
  id: totrans-631
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并绘制移除列特征重要性：
- en: '[PRE60]'
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'First, plot the top 10 most important features:'
  id: totrans-633
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，绘制最重要的前10个特征：
- en: '[PRE61]'
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-635
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码片段会生成以下图形：
- en: '![](../Images/B18112_14_30.png)'
  id: totrans-636
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_30.png)'
- en: 'Figure 14.30: Top 10 most important features according to drop-column feature
    importance'
  id: totrans-637
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.30：根据移除列特征重要性计算的前10个最重要特征
- en: Using the drop-column feature importance (evaluated on the training data), the
    most important feature was `payment_status_sep_Unknown`. The same feature was
    identified as the most important one using permutation feature importance.
  id: totrans-638
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用删除列特征重要性（在训练数据上评估），最重要的特征是`payment_status_sep_Unknown`。通过置换特征重要性计算得出的最重要特征也是这个。
- en: 'Then, plot the 10 least important features:'
  id: totrans-639
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，绘制 10 个最不重要的特征：
- en: '[PRE62]'
  id: totrans-640
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-641
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码段将生成以下图表：
- en: '![Chart  Description automatically generated](../Images/B18112_14_31.png)'
  id: totrans-642
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](../Images/B18112_14_31.png)'
- en: 'Figure 14.31: The 10 least important features according to drop-column feature
    importance'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.31：根据删除列特征重要性评估的 10 个最不重要的特征
- en: In the case of drop-column feature importance, negative importance indicates
    that removing a given feature from the model actually improves the performance.
    That is true as long as the considered metric treats higher values as better.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 在删除列特征重要性的情况下，负的特征重要性意味着从模型中删除某个特征实际上会提高模型的性能。这种情况在所考虑的度量标准将较高的值视为更好的时候是成立的。
- en: We can use these results to remove features that have negative importance and
    thus potentially improve the model’s performance and/or reduce the training time.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些结果来移除具有负面重要性的特征，从而可能提高模型的性能和/或减少训练时间。
- en: How it works...
  id: totrans-646
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 1*, we imported the required libraries. Then, we extracted the classifier
    and the `ColumnTransformer` preprocessor from the pipeline. In this recipe, we
    worked with a tuned Random Forest classifier (using the hyperparameters determined
    in the *Exploring ensemble classifiers* recipe).
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 1*中，我们导入了所需的库。接着，我们从管道中提取了分类器和`ColumnTransformer`预处理器。在这个示例中，我们使用了调优后的随机森林分类器（使用*探索集成分类器*示例中确定的超参数）。
- en: In *Step 3*, we first extracted the column names from the preprocessor using
    the `get_feature_names_out` method. Then, we prepared the training and test sets
    by applying the preprocessor’s transformations.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 3*中，我们首先使用`get_feature_names_out`方法从预处理器中提取了列名。然后，通过应用预处理器的转换，我们准备了训练集和测试集。
- en: In *Step 4*, we extracted the MDI feature importances using the `feature_importances_`
    attribute of the fitted Random Forest classifier. The values were automatically
    normalized so that they added up to `1`. Additionally, we calculated the cumulative
    feature importance.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们使用拟合后的随机森林分类器的`feature_importances_`属性提取了MDI特征重要性。值被自动归一化，使其加起来等于`1`。此外，我们计算了累积特征重要性。
- en: In *Step 5*, we defined a helper function to plot the most/least important features
    and plotted the top 10 most important features, calculated using the mean decrease
    in impurity.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们定义了一个辅助函数来绘制最重要/最不重要的特征，并绘制了通过计算平均减少不纯度得到的前 10 个最重要的特征。
- en: In *Step 6*, we plotted the cumulative importance of all the features. Using
    this plot, we could decide if we wanted to reduce the number of features in the
    model to account for a certain percentage of total importance. By doing so, we
    could potentially decrease the model’s training time.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 6*中，我们绘制了所有特征的累积重要性。通过此图表，我们可以决定是否希望减少模型中的特征数量，以考虑总重要性的某一百分比。通过这样做，我们可能会减少模型的训练时间。
- en: 'In *Step 7*, we calculated the permutation feature importance using the `permutation_importance`
    function available in `scikit-learn`. We decided to use recall as the scoring
    metric and set the `n_repeats` argument to `25`, so the algorithm reshuffled each
    feature `25` times. The output of the procedure is a dictionary containing three
    elements: the raw feature importances, the average value per feature, and the
    corresponding standard deviation. Additionally, while using `permutation_importance`
    we can evaluate multiple metrics at once by providing a list of selected metrics.'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 7*中，我们使用`scikit-learn`中的`permutation_importance`函数计算了置换特征重要性。我们决定使用召回率作为评分标准，并将`n_repeats`参数设置为`25`，这样算法就会将每个特征重新排列`25`次。该过程的输出是一个包含三个元素的字典：原始特征重要性、每个特征的平均值和相应的标准差。此外，在使用`permutation_importance`时，我们可以通过提供选定的度量标准列表来同时评估多个指标。
- en: We decided to use the `scikit-learn` implementation of permutation feature importance.
    However, there are alternative options available, for example, in the `rfpimp`
    or `eli5` libraries. The former also contains the drop-column feature importance.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定使用`scikit-learn`的置换特征重要性实现。然而，也有其他可用的选项，例如在`rfpimp`或`eli5`库中。前者还包含删除列特征重要性。
- en: In *Step 8*, we calculated and evaluated the permutation feature importance,
    this time using the test set.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第8步*中，我们计算并评估了置换特征重要性，这一次使用了测试集。
- en: 'We have mentioned in the introduction that permutation importance can return
    unreliable scores when our dataset has correlated features, that is, the importance
    score will be spread across the correlated features. We could try the following
    approaches to overcome this issue:'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在介绍中提到过，当数据集中存在相关特征时，置换重要性可能会返回不可靠的得分，也就是说，重要性得分会分散到相关特征上。我们可以尝试以下方法来克服这个问题：
- en: Permute groups of correlated features together. `rfpimp` offers such functionality
    in the `importances` function.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将相关特征组进行置换。`rfpimp`在`importances`函数中提供了此功能。
- en: We could use hierarchical clustering on the features’ Spearman’s rank correlations,
    pick a threshold, and then only keep a single feature from each of the identified
    clusters.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以对特征的Spearman等级相关性进行层次聚类，选择一个阈值，然后仅保留每个识别出的簇中的单个特征。
- en: In *Step 9*, we defined a function for calculating the drop-column feature importance.
    First, we trained and evaluated the baseline model using all features. As the
    scoring metric, we chose recall. Then, we used the `clone` function of `scikit-learn`
    to create a copy of the model with the exact same specification as the baseline
    one. We then iteratively trained the model on a dataset without one feature, calculated
    the selected evaluation metric, and stored the difference in scores.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第9步*中，我们定义了一个计算删除列特征重要性的函数。首先，我们使用所有特征训练并评估了基准模型。作为评分指标，我们选择了召回率。然后，我们使用`scikit-learn`的`clone`函数创建了一个与基准模型完全相同的模型副本。接着，我们迭代地在没有某个特征的数据集上训练模型，计算所选的评估指标，并存储得分差异。
- en: In *Step 10*, we applied the drop-column feature importance function and plotted
    the results, both the most and least important features.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第10步*中，我们应用了删除列特征重要性函数并绘制了结果，包括最重要和最不重要的特征。
- en: There’s more...
  id: totrans-660
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We have mentioned that the default feature importance of `scikit-learn`'s Random
    Forest is the MDI/Gini importance. It is also worth mentioning that the popular
    boosting algorithms (which we mentioned in the *Exploring ensemble classifiers*
    recipe) also adapted the `feature_importances_` attribute of the fitted model.
    However, they use different metrics of feature importance, depending on the algorithm.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到过，`scikit-learn`的随机森林默认的特征重要性是MDI/Gini重要性。值得一提的是，流行的提升算法（我们在*探索集成分类器*一节中提到过）也适配了已拟合模型的`feature_importances_`属性。然而，根据不同的算法，它们使用了不同的特征重要性度量标准。
- en: 'For XGBoost, we have the following possibilities:'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 对于XGBoost，我们有以下几种可能性：
- en: '`weight`—measures the number of times a feature is used to split the data across
    all trees. Similar to the Gini importance, however, it does not take into account
    the number of samples.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight`—衡量特征在所有树中用于分裂数据的次数。类似于Gini重要性，但它不考虑样本数量。'
- en: '`gain`—measures the average gain of the feature when it is used in trees. Intuitively
    we can think of it as the Gini importance measure, where Gini impurity is replaced
    by the objective of the gradient boosting model.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gain`—衡量使用特征时在树中获得的平均增益。直观地，我们可以将其视为Gini重要性度量，其中Gini不纯度被梯度提升模型的目标所取代。'
- en: '`cover`—measures the average coverage of the feature when it is used in trees.
    Coverage is defined as the number of samples affected by the split.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cover`—衡量在树中使用该特征时的平均覆盖率。覆盖率定义为受到分裂影响的样本数。'
- en: The `cover` method can overcome one of the potential issues of the `weight`
    approach—simply counting the number of splits may be misleading, as some splits
    might affect just a few observations, and are therefore not really relevant.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '`cover`方法可以克服`weight`方法的潜在问题之一——仅仅计算分裂次数可能具有误导性，因为有些分裂可能只影响少数几个观测值，因此并不是真正相关的。'
- en: 'For LightGBM, we have the following possibilities:'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LightGBM，我们有以下几种可能性：
- en: '`split`—measures the number of times the feature is used in a model'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split`—衡量特征在模型中使用的次数。'
- en: '`gain`—measures the total gains of splits that use the feature'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gain`—衡量使用特征时分裂的总增益。'
- en: See also
  id: totrans-670
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Additional resources are available here:'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 更多资源可以在此处找到：
- en: 'Altmann, A., Toloşi, L., Sander, O., & Lengauer, T. 2010\. “Permutation importance:
    a corrected feature importance measure.” *Bioinformatics*, 26(10): 1340–1347.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Altmann, A., Toloşi, L., Sander, O., & Lengauer, T. 2010。“置换重要性：一种修正的特征重要性度量。”*生物信息学*,
    26(10): 1340–1347。'
- en: 'Louppe, G. 2014\. “Understanding random forests: From theory to practice.”
    *arXiv preprint arXiv:1407.7502*.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Louppe, G. 2014\. “理解随机森林：从理论到实践。” *arXiv 预印本 arXiv:1407.7502*。
- en: Molnar, C. 2020\. *Interpretable Machine Learning:* [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molnar, C. 2020\. *可解释的机器学习：* [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)
- en: 'Hastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. 2009\. *The
    elements of statistical learning: data mining, inference, and prediction*, 2:
    1–758\. New York: Springer.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. 2009\. *统计学习的元素：数据挖掘、推理与预测*,
    2: 1–758\. 纽约：Springer。'
- en: 'Hooker, G., Mentch, L., & Zhou, S. 2021\. “Unrestricted permutation forces
    extrapolation: variable importance requires at least one more model, or there
    is no free variable importance.” *Statistics and Computing*, 31(6): 1–16.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hooker, G., Mentch, L., & Zhou, S. 2021\. “无限制排列强迫外推：变量重要性至少需要一个模型，否则没有自由的变量重要性。”
    *统计与计算*, 31(6): 1–16。'
- en: Parr, T., Turgutlu, K., Csiszar, C., & Howard, J. 2018\. Beware default random
    forest importances. March 26, 2018\. [https://explained.ai/rf-importance/](https://explained.ai/rf-importance/).
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parr, T., Turgutlu, K., Csiszar, C., & Howard, J. 2018\. 小心默认的随机森林重要性度量。2018年3月26日。[https://explained.ai/rf-importance/](https://explained.ai/rf-importance/)。
- en: 'Strobl, C., Boulesteix, A. L., Kneib, T., Augustin, T., & Zeileis, A. 2008\.
    “Conditional variable importance for random forests.” *BMC Bioinformatics*, 9(1):
    307.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Strobl, C., Boulesteix, A. L., Kneib, T., Augustin, T., & Zeileis, A. 2008\.
    “随机森林的条件变量重要性。” *BMC 生物信息学*, 9(1): 307。'
- en: 'Strobl, C., Boulesteix, A. L., Zeileis, A., & Hothorn, T. 2007\. “Bias in random
    forest variable importance measures: Illustrations, sources and a solution.” *BMC
    bioinformatics*, 8(1): 1–21.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Strobl, C., Boulesteix, A. L., Zeileis, A., & Hothorn, T. 2007\. “随机森林变量重要性度量中的偏差：插图、来源及解决方案。”
    *BMC 生物信息学*, 8(1): 1–21。'
- en: Exploring feature selection techniques
  id: totrans-680
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索特征选择技术
- en: In the previous recipe, we saw how to evaluate the importance of features used
    for training ML models. We can use that knowledge to carry out feature selection,
    that is, keeping only the most relevant features and discarding the rest.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个教程中，我们展示了如何评估用于训练机器学习模型的特征的重要性。我们可以利用这些知识进行特征选择，即仅保留最相关的特征，并舍弃其余的特征。
- en: Feature selection is a crucial part of any machine learning project. First,
    it allows us to remove features that are either completely irrelevant or are not
    contributing much to a model’s predictive capabilities. This can benefit us in
    multiple ways. Probably the most important benefit is that such unimportant features
    can actually negatively impact the performance of our model as they introduce
    noise and contribute to overfitting. As we have already established—*garbage in,
    garbage out*. Additionally, fewer features can often be translated into a shorter
    training time and help us avoid the curse of dimensionality.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择是任何机器学习项目中至关重要的一部分。首先，它允许我们剔除那些完全不相关或对模型的预测能力贡献不大的特征。这可以在多个方面为我们带来好处。可能最重要的好处是，这些不重要的特征实际上可能会对我们模型的性能产生负面影响，因为它们引入了噪声并导致过拟合。正如我们之前所确定的——*垃圾进，垃圾出*。此外，减少特征通常意味着更短的训练时间，并帮助我们避免维度灾难。
- en: Second, we should follow Occam’s razor and keep our models simple and explainable.
    When we have a moderate number of features, it is easier to explain what is actually
    happening in the model. This can be crucial for the ML project’s adoption by the
    stakeholders.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们应该遵循奥卡姆剃刀原则，保持我们的模型简单且可解释。当我们拥有适量的特征时，解释模型中实际发生的事情会更容易。这对机器学习项目获得利益相关者的支持至关重要。
- en: 'We have already established the *why* of feature selection. Now it is time
    to explore the *how*. On a high level, feature selection methods can be grouped
    into three categories:'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定了特征选择的*为什么*。现在是时候探讨*怎么做*了。从高层次来看，特征选择方法可以分为三类：
- en: '**Filter methods**—a generic set of univariate methods that specify a certain
    statistical measure and then filter the features based on it. This group does
    not incorporate any specific ML algorithm, hence it is characterized by (usually)
    lower computation time and is less prone to overfitting. A potential drawback
    of this group is that the methods evaluate the relationship between the target
    and each of the features individually. This can lead to them overlooking important
    relationships between the features. Examples include correlation, chi-squared
    test, **analysis of variance** (**ANOVA**), information gain, variance thresholding,
    and so on.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤方法**—一类通用的单变量方法，它们指定某个统计度量，然后基于该度量过滤特征。这个方法组不涉及任何特定的机器学习算法，因此其特点是（通常）较低的计算时间，并且不易过拟合。这个方法组的潜在缺点是它们会单独评估目标变量与每个特征之间的关系，这可能导致它们忽视特征之间的重要关系。示例包括相关性、卡方检验、**方差分析**（**ANOVA**）、信息增益、方差阈值等。'
- en: '**Wrapper methods**—this group of approaches considers feature selection a
    search problem, that is, it uses certain procedures to repeatedly evaluate a specific
    ML model with a different set of features to find the optimal set. It is characterized
    by the highest computational costs and the highest possibility of overfitting.
    Examples include forward selection, backward elimination, stepwise selection,
    recursive feature elimination, and so on.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包装方法**—这一类方法将特征选择视为一个搜索问题，即它使用某些程序反复评估特定的机器学习模型，并使用不同的特征集合来寻找最佳的特征集。其特点是计算成本最高，且过拟合的可能性也最高。示例包括前向选择、后向消除、逐步选择、递归特征消除等。'
- en: '**Embedded methods**—this set of methods uses ML algorithms that have built-in
    feature selection, for example, Lasso with its regularization or Random Forest.
    By using these implicit feature selection methods, the algorithms try to prevent
    overfitting. In terms of computational complexity, this method is usually somewhere
    between the filter and wrapper groups.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入方法**—这一类方法使用具有内置特征选择的机器学习算法，例如带正则化的 Lasso 或随机森林。通过使用这些隐式的特征选择方法，算法试图防止过拟合。在计算复杂度方面，这种方法通常介于过滤方法和包装方法之间。'
- en: In this recipe, we will apply a selection of feature selection methods to the
    credit card fraud dataset. We believe it provides a good example, especially given
    a lot of the features are anonymized and we do not know the exact meaning behind
    them. Hence, it is also likely that some of them do not really contribute much
    to the model’s performance.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将应用一系列特征选择方法来处理信用卡欺诈数据集。我们认为这个数据集是一个很好的示例，特别是因为许多特征已被匿名化，我们并不知道它们背后的确切含义。因此，也很可能其中一些特征对模型的性能贡献并不大。
- en: Getting ready
  id: totrans-689
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will be using the credit card fraud dataset that we introduced
    in the *Investigating different approaches to handling imbalanced data* recipe.
    For convenience, we have included all the necessary preparation steps in this
    section from the accompanying Jupyter notebook.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用在*研究不同处理不平衡数据的方法*中介绍的信用卡欺诈数据集。为了方便起见，我们在本节中已包含了来自随附 Jupyter 笔记本的所有必要准备步骤。
- en: Another interesting challenge to applying feature selection methods would be
    BNP Paribas Cardif Claims Management (the dataset is available at Kaggle—a link
    is provided in the *See also* section). Similar to the dataset used in this recipe,
    it contains 131 anonymized features.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 应用特征选择方法的另一个有趣挑战是 BNP Paribas Cardif Claims Management（数据集可以在 Kaggle 上找到——链接见*另见*部分）。与本节中使用的数据集类似，它包含131个匿名特征。
- en: How to do it…
  id: totrans-692
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Execute the following steps to experiment with various feature selection methods:'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来尝试不同的特征选择方法：
- en: 'Import the libraries:'
  id: totrans-694
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE63]'
  id: totrans-695
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Train the benchmark model:'
  id: totrans-696
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练基准模型：
- en: '[PRE64]'
  id: totrans-697
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Executing the snippet generates the following output:'
  id: totrans-698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段后会生成以下输出：
- en: '[PRE65]'
  id: totrans-699
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Looking at the recall scores, the model is clearly overfitted to the training
    data. Normally, we should try to address this. However, to keep the exercise simple
    we assume that the model is good enough to proceed.
  id: totrans-700
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从召回率得分来看，模型显然对训练数据过拟合。通常，我们应该尝试解决这个问题。然而，为了简化操作，我们假设该模型已经足够好，可以继续进行。
- en: 'Select the best features using Mutual Information:'
  id: totrans-701
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用互信息选择最佳特征：
- en: '[PRE66]'
  id: totrans-702
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Using the next snippet, we plot the results:'
  id: totrans-703
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用下一个代码片段，我们绘制结果：
- en: '[PRE67]'
  id: totrans-704
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-705
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段后会生成以下图表：
- en: '![](../Images/B18112_14_32.png)'
  id: totrans-706
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_32.png)'
- en: 'Figure 14.32: Performance of the model depending on the number of selected
    features. Features are selected using the Mutual Information criterion'
  id: totrans-707
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.32：模型的性能与所选特征数量的关系。特征是使用互信息准则选择的
- en: 'By inspecting the figure, we can see that we achieved the best recall score
    on the test set using `8`, `9`, `10`, and `12` features. As simplicity is desired,
    we decided to choose `8`. Using the following snippet, we extract the names of
    the 8 most important features:'
  id: totrans-708
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过检查图表，我们可以看到，使用 `8`、`9`、`10` 和 `12` 个特征时，在测试集上达到了最佳的召回得分。由于我们追求简洁，最终决定选择 `8`
    个特征。使用以下代码片段，我们提取出 8 个最重要特征的名称：
- en: '[PRE68]'
  id: totrans-709
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Executing the snippet returns the following output:'
  id: totrans-710
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会返回以下输出：
- en: '[PRE69]'
  id: totrans-711
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Select the best features using MDI feature importance, retrain the model, and
    evaluate its performance:'
  id: totrans-712
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 MDI 特征重要性选择最佳特征，重新训练模型，并评估其性能：
- en: '[PRE70]'
  id: totrans-713
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Executing the snippet generates the following output:'
  id: totrans-714
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下输出：
- en: '[PRE71]'
  id: totrans-715
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Using the following snippet, we extract the threshold used for feature selection
    and the most relevant features:'
  id: totrans-716
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用以下代码片段，我们提取用于特征选择的阈值和最相关的特征：
- en: '[PRE72]'
  id: totrans-717
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This generates the following output:'
  id: totrans-718
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这生成了以下输出：
- en: '[PRE73]'
  id: totrans-719
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The threshold value corresponds to the average feature importance of the RF
    model.
  id: totrans-720
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 阈值对应于 RF 模型的平均特征重要性。
- en: Using a loop similar to the one in *Step 3*, we can generate a bar chart showing
    the model’s performance depending on the number of features kept in the model.
    We iteratively select the top *k* features based on the MDI. To avoid repetition,
    we do not include the code here (it is available in the accompanying Jupyter notebook).
    By analyzing the figure, we can see that the model achieved the best score with
    `10` features, which is more than in the previous approach.
  id: totrans-721
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用类似于*步骤 3*中的循环，我们可以生成一个条形图，展示模型的性能与保留特征数量的关系。我们根据 MDI 迭代地选择前 *k* 个特征。为了避免重复，这里不包含代码（代码可以在随附的
    Jupyter notebook 中找到）。通过分析图表，我们可以看到，模型在使用`10`个特征时取得了最佳得分，这比前一种方法更多。
- en: '![](../Images/B18112_14_33.png)'
  id: totrans-722
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_33.png)'
- en: 'Figure 14.33: Performance of the model depending on the number of selected
    features. Features are selected using the Mean Decrease in Impurity feature importance'
  id: totrans-723
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.33：模型的性能与所选特征数量的关系。特征是使用均值减少不纯度特征重要性选择的
- en: 'Select the best 10 features using Recursive Feature Elimination:'
  id: totrans-724
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用递归特征消除法（Recursive Feature Elimination）选择最佳的 10 个特征：
- en: '[PRE74]'
  id: totrans-725
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'In order to avoid repetition, we present the most important features and the
    accompanying scores without the code, as it is almost identical to what we have
    covered in the previous steps:'
  id: totrans-726
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了避免重复，我们展示了最重要的特征及其附带的分数，而没有包括代码，因为它与我们在前面步骤中所涉及的内容几乎相同：
- en: '[PRE75]'
  id: totrans-727
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Select the best features using Recursive Feature Elimination with cross-validation:'
  id: totrans-728
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用带交叉验证的递归特征消除法选择最佳特征：
- en: '[PRE76]'
  id: totrans-729
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Below we present the outcome of the feature selection:'
  id: totrans-730
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面展示了特征选择的结果：
- en: '[PRE77]'
  id: totrans-731
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'This approach resulted in the selection of `16` features. Overall, `6` features
    appeared in each of the considered approaches: `V10`, `V11`, `V12`, `V14`, `V16`,
    and `V17`.'
  id: totrans-732
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该方法导致选择了`16`个特征。总体而言，在考虑的各个方法中，每个方法都有`6`个特征：`V10`、`V11`、`V12`、`V14`、`V16` 和
    `V17`。
- en: 'Additionally, using the following snippet we can visualize the cross-validation
    scores, that is, what the average recall of the `5` folds was for each of the
    considered numbers of retained features. We had to add `5` to the index of the
    DataFrame, as we chose to retain a minimum of `5` features in the `RFECV` procedure:'
  id: totrans-733
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，使用以下代码片段，我们可以可视化交叉验证得分，也就是每个考虑的特征保留数量下，`5`折交叉验证的平均召回率。由于我们选择在`RFECV`过程中至少保留`5`个特征，因此必须将`5`加到数据框的索引中：
- en: '[PRE78]'
  id: totrans-734
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-735
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成以下图表：
- en: '![](../Images/B18112_14_34.png)'
  id: totrans-736
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_34.png)'
- en: 'Figure 14.34: Average CV score for each step of the RFE procedure'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.34：RFE 过程每一步的平均交叉验证得分
- en: Inspecting the figure confirms that the highest average recall was obtained
    using 16 features.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 检查图表确认，使用 16 个特征时获得了最高的平均召回率。
- en: While evaluating the benefits of feature selection, we should consider two scenarios.
    In the more obvious one, the performance of the model improves when we remove
    some of the features. This does not need any further explanation. The second scenario
    is more interesting. After removing features, we can end up with a very similar
    performance to the initial one or slightly worse. However, this does not necessarily
    mean that we have failed. Consider a case in which we removed ~60% of the features
    while keeping the same performance. This could already be a major improvement
    that—depending on the dataset and model—can potentially reduce the training time
    by hours or days. Additionally, such a model would be easier to interpret.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估特征选择的好处时，我们应考虑两种情况。在更显而易见的一种情况下，当我们去除一些特征时，模型的表现得到了改善。这无需进一步解释。第二种情况则更为有趣。去除特征后，我们可能会得到与初始表现非常相似或者稍差的结果。然而，这并不一定意味着我们失败了。假设我们去除了约60%的特征，同时保持了相同的表现。这可能已经是一个重要的改进，具体改进的程度取决于数据集和模型，可能会将训练时间减少几个小时甚至几天。此外，这样的模型也会更容易解释。
- en: How it works…
  id: totrans-740
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: After importing the required libraries, we trained a benchmark Random Forest
    classifier and printed the recall score from the training and test sets.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的库后，我们训练了一个基准随机森林分类器，并打印了训练集和测试集的召回率分数。
- en: In *Step 3*, we applied the first of the considered feature selection approaches.
    It was an example of the univariate *filter* category of feature selection techniques.
    As the statistical criterion, we used the Mutual Information score. To calculate
    the metric, we used the `mutual_info_classif` function from `scikit-learn`, which
    is capable of working with a categorical target and numerical features only. Hence,
    any categorical features need to be appropriately encoded beforehand. Fortunately,
    we only have continuous numerical features in this dataset.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3步*中，我们应用了考虑的第一个特征选择方法。这是一个属于单变量*过滤器*类别的特征选择技术示例。作为统计标准，我们使用了互信息分数（Mutual
    Information score）。为了计算该指标，我们使用了`mutual_info_classif`函数，该函数来自`scikit-learn`，仅适用于分类目标和数值特征。因此，任何分类特征都需要提前进行适当的编码。幸运的是，在这个数据集中我们只有连续的数值特征。
- en: The **Mutual Information** (**MI**)score of two random variables is a measure
    of the mutual dependence between those variables. When the score is equal to zero,
    the two variables are independent. The higher the score, the higher the dependency
    between the variables. In general, calculating the MI requires knowledge of the
    probability distributions of each of the features, which we do not usually know.
    That is why the `scikit-learn` implementation uses a nonparametric approximation
    based on k-Nearest Neighbors distances. One of the advantages of using MI is that
    it can capture nonlinear relationships between the features.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: '**互信息**（**MI**）分数是两个随机变量之间互依赖性的度量。当分数为零时，表示这两个变量是独立的。分数越高，变量之间的依赖性越强。通常，计算MI需要了解每个特征的概率分布，而我们通常并不清楚这些分布。这就是为什么`scikit-learn`的实现使用基于k-最近邻距离的非参数近似法的原因。使用MI的一个优点是它能够捕捉特征之间的非线性关系。'
- en: Next, we combined the MI criterion with the `SelectKBest` class, which allows
    us to select the *k* best features determined by an arbitrary metric. Using this
    approach, we almost never know upfront how many features we would like to keep.
    Hence, we iterated over all the possible values (from `2` to `29`, where the latter
    is the total number of features in the dataset). The `SelectKBest` class employs
    the familiar `fit`/`transform` approach. Within each iteration, we fitted the
    class to the training data (both features and the target are required for this
    step) and then transformed the training and test sets. The transformation resulted
    in keeping only the *k* most important features according to the MI criterion.
    Then, we once again fitted the Random Forest classifier using only the selected
    features and recorded the relevant recall scores.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将MI标准与`SelectKBest`类结合使用，该类允许我们根据任意指标选择*k*个最佳特征。使用这种方法时，我们通常无法提前知道希望保留多少个特征。因此，我们遍历了所有可能的值（从`2`到`29`，后者是数据集中特征的总数）。`SelectKBest`类采用了熟悉的`fit`/`transform`方法。在每次迭代中，我们将该类拟合到训练数据（此步骤需要特征和目标变量）上，然后对训练集和测试集进行转换。转换的结果是根据MI标准仅保留最重要的*k*个特征。接着，我们再次使用仅包含选定特征的训练数据来拟合随机森林分类器，并记录相关的召回率分数。
- en: '`scikit-learn` allows us to easily use different metrics together with the
    `SelectKBest` class. For example, we could use the following scoring functions:'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`使我们可以轻松地将不同的度量与`SelectKBest`类一起使用。例如，我们可以使用以下评分函数：'
- en: '`f_classif`—the ANOVA F-value estimating the degree of linear dependency between
    two variables. The F statistic is calculated as the ratio of between-group variability
    to the within-group variability. In this case, the group is simply the class of
    the target. A potential drawback of this method is that it only accounts for linear
    relationships.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f_classif`—ANOVA F 值，用于估算两个变量之间的线性依赖程度。F 统计量是通过计算组间变异性与组内变异性的比值来得出的。在这种情况下，组即为目标的类别。该方法的一个潜在缺点是它仅考虑了线性关系。'
- en: '`chi2`—the chi-squared statistics. This metric is only suitable for non-negative
    features such as Booleans or frequencies, or more generally, for categorical features.
    Intuitively, it evaluates if a feature is independent of the target. If that is
    the case, it is also uninformative when it comes to classifying the observations.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chi2`—卡方统计量。该度量仅适用于非负特征，如布尔值或频率，或者更一般地，适用于分类特征。直观地说，它评估一个特征是否与目标变量独立。如果是这样，它在分类观察值时也不提供有用信息。'
- en: Aside from selecting the *k* best features, the `feature_selection` module of
    `scikit-learn` also offers classes that allow choosing features based on the percentile
    of the highest scores, a false positive rate test, an estimated false discovery
    rate, or a family-wise error rate.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择*最优*的 k 个特征外，`scikit-learn`的`feature_selection`模块还提供了其他类，允许根据最高得分的百分位、假阳性率测试、估计的假发现率或家族错误率来选择特征。
- en: In *Step 4*, we explored an example of the *embedded* feature selection techniques.
    In this group, feature selection is performed as part of the model building phase.
    We used the `SelectFromModel` class to select the best features based on the model’s
    built-in feature importance metric (in this case, the MDI feature importance).
    When instantiating the class, we can provide the `threshold` argument to determine
    the threshold used to select the most relevant features. Features with weights/coefficients
    above that threshold would be kept in the model. We can also use the `"mean"`
    (default one) and `"median"` keywords to use the mean/median values of all feature
    importances as the threshold. We can also combine those keywords with scaling
    factors, for example, `"1.5*mean"`. Using the `max_features` argument, we can
    determine the maximum number of features we allow to be selected.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们探索了*嵌入式*特征选择技术的一个示例。在这一组方法中，特征选择作为模型构建阶段的一部分进行。我们使用了`SelectFromModel`类，根据模型的内置特征重要性度量（在此情况下为
    MDI 特征重要性）来选择最佳特征。在实例化该类时，我们可以提供`threshold`参数来确定用于选择最相关特征的阈值。特征的权重/系数高于该阈值的将被保留在模型中。我们还可以使用“`mean`”（默认值）和“`median`”关键字，使用所有特征重要性的均值/中位数作为阈值。我们还可以将这些关键字与缩放因子结合使用，例如，`"1.5*mean"`。通过使用`max_features`参数，我们可以确定允许选择的最大特征数量。
- en: The `SelectFromModel` class works with any estimator that has either the `feature_importances_`
    (for example, Random Forest, XGBoost, LightGBM, and so on) or `coef_` (for example,
    Linear Regression, Logistic Regression, and Lasso) attribute.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: '`SelectFromModel`类适用于任何具有`feature_importances_`（例如，随机森林、XGBoost、LightGBM 等）或`coef_`（例如，线性回归、逻辑回归和
    Lasso）属性的估算器。'
- en: In this step, we demonstrated two approaches to recovering the selected features.
    The first one is the `get_support` method, which returns a list with Boolean flags
    indicating whether the given feature was selected. The second one is the `get_feature_names_out`
    method, which directly returns the names of the selected features. While fitting
    the Random Forest classifier, we manually selected the columns of the training
    dataset. However, we could have also used the `transform` method of the fitted
    `SelectFromModel` class to automatically extract only the relevant features as
    a `numpy` array.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，我们演示了两种恢复已选择特征的方法。第一种是`get_support`方法，它返回一个包含布尔标志的列表，指示给定的特征是否被选择。第二种是`get_feature_names_out`方法，它直接返回所选特征的名称。在拟合随机森林分类器时，我们手动选择了训练数据集的列。然而，我们也可以使用已拟合的`SelectFromModel`类的`transform`方法，自动提取相关特征并以`numpy`数组的形式返回。
- en: In *Step 5*, we used an example of the *wrapper* methods. **Recursive Feature
    Elimination** (**RFE**)is an algorithm that recursively trains an ML model, calculates
    the feature importances (via `coef_` or `feature_importances_`), and drops the
    least important feature or features.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5步*中，我们使用了*包装*方法的示例。**递归特征消除**（**RFE**）是一种递归训练机器学习模型、计算特征重要性（通过`coef_`或`feature_importances_`），并删除最不重要的特征的算法。
- en: The process starts by training the model using all the features. Then, the least
    important feature or features are pruned from the dataset. Next, the model is
    trained again with the reduced feature set, and the least important features are
    again eliminated. The process is repeated until it reaches the desired number
    of features. While instantiating the `RFE` class, we provided the Random Forest
    estimator together with the number of features to select. Additionally, we could
    provide the `step` argument, which determined how many features to eliminate during
    each iteration.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程首先使用所有特征训练模型。然后，从数据集中删除最不重要的特征。接下来，使用减少后的特征集重新训练模型，并再次删除最不重要的特征。这个过程会重复，直到达到所需的特征数量。在实例化`RFE`类时，我们提供了随机森林估计器以及要选择的特征数量。此外，我们还可以提供`step`参数，它决定了每次迭代中要删除多少个特征。
- en: RFE can be a computationally expensive algorithm to run, especially with a large
    feature set and cross-validation. Hence, it might be a good idea to apply some
    other feature selection technique before using RFE. For example, we could use
    the filtering approach and remove some of the correlated features.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: RFE可能是一个计算开销较大的算法，尤其是在特征集较大且进行交叉验证时。因此，使用RFE之前，最好先应用一些其他的特征选择技术。例如，我们可以使用筛选方法，去除一些相关性较高的特征。
- en: As we have mentioned before, we rarely know the optimal number of features upfront.
    That is why in *Step 6* we try to account for that drawback. By combining RFE
    with cross-validation, we can automatically determine the optimal number of features
    to keep using the RFE procedure. To do so, we used the `RFECV` class and provided
    some additional inputs. We had to specify the cross-validation scheme (5-fold
    stratified CV, as we are dealing with an imbalanced dataset), the scoring metric
    (recall), and the minimum number of features to retain. For the last argument,
    we arbitrarily chose 5.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所提到的，我们很少知道最佳的特征数量。这就是为什么在*第6步*中我们尝试解决这个缺点。通过将RFE与交叉验证结合使用，我们可以通过RFE过程自动确定保留的最佳特征数量。为此，我们使用了`RFECV`类并提供了一些额外的输入。我们必须指定交叉验证方案（5折分层交叉验证，因为我们处理的是不平衡数据集）、评分指标（召回率）以及保留的最小特征数量。对于最后一个参数，我们任意选择了5。
- en: Lastly, to explore the CV scores in more depth, we accessed the cross-validation
    scores for each fold using the `cv_results_` attribute of the fitted `RFECV` class.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了更深入地探讨交叉验证得分，我们通过已拟合的`RFECV`类的`cv_results_`属性访问了每个折的交叉验证得分。
- en: There’s more…
  id: totrans-757
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Some of the other available approaches
  id: totrans-758
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他一些可用的方法
- en: 'We have already mentioned quite a few univariate filter methods. Some other
    notable ones include:'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到了一些单变量筛选方法。其他一些值得注意的方法包括：
- en: '**Variance thresholding**—this method simply removes features with variance
    lower than a specified threshold. Thus, it can be used to remove constant and
    quasi-constant features. The latter ones are those that have very little variability
    as almost all the values are identical. By definition, this method does not look
    at the target value, only at the features.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差阈值**—这种方法仅删除方差低于指定阈值的特征。因此，它可以用来去除常量特征和准常量特征。后者指的是那些几乎所有值都相同，变化性非常小的特征。根据定义，这种方法仅查看特征，而不考虑目标值。'
- en: '**Correlation-based**—there are multiple ways to measure correlation, hence
    we will only focus on the general logic of this approach. First, we determine
    the correlation between the features and the target. We can choose a threshold
    above which we want to keep the features for modeling.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于相关性**—有多种方式可以衡量相关性，因此我们只关注这种方法的基本逻辑。首先，我们确定特征与目标之间的相关性。然后我们可以选择一个阈值，高于该阈值的特征将保留用于建模。'
- en: Then, we should also consider removing features that are highly correlated among
    themselves. We should identify such groups and then leave only one feature from
    each of the groups in our dataset. Alternatively, we could use the **Variance
    Inflation Factor** (**VIF**) to determine multicollinearity and drop features
    based on high VIF values. VIF is available in `statsmodels`.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们还应考虑去除那些高度相关的特征。我们应该识别这些特征组，并从每个组中只保留一个特征在我们的数据集中。另一种方法是使用**方差膨胀因子**（**VIF**）来判断多重共线性，并根据较高的VIF值去除特征。VIF可以在`statsmodels`中使用。
- en: We did not consider using correlation as a criterion in this recipe, as the
    features in the credit card fraud dataset are the outcomes of PCA. Hence, by definition
    they are orthogonal, that is, uncorrelated.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个方法中没有考虑使用相关性作为标准，因为信用卡欺诈数据集中的特征是PCA的结果。因此，按定义，它们是正交的，也就是不相关的。
- en: There are also multivariate filter methods available. For example, **Maximum
    Relevance Minimum Redundancy** (**MRMR**) is a family of algorithms that attempts
    to identify a subset of features that have high relevance with respect to the
    target variable, while having a small redundancy with each other.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 也有多变量筛选方法可用。例如，**最大相关最小冗余**（**MRMR**）是一类算法，旨在识别与目标变量高度相关且相互冗余较小的特征子集。
- en: 'We could also explore the following *wrapper* techniques:'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以探索以下*包装*技术：
- en: '**Forward feature selection**—we start with no features. We test each of the
    features separately and see which one most improves the model. We add that feature
    to our feature set. Then, we sequentially train models with a second feature added.
    Similarly, at this step, we again test all the remaining features individually.
    We select the best one and add it to the selected pool. We continue adding features
    one at a time until we reach a stopping criterion (max number of features or no
    further improvement). Traditionally, the feature to be added was based on the
    features’ p-values. However, modern libraries use the improvement on a cross-validated
    metric of choice as the selection criterion.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前向特征选择**—我们从没有特征开始。我们单独测试每个特征，看看哪个特征最能改善模型。然后，我们将该特征添加到我们的特征集。接着，我们依次训练模型，添加第二个特征。类似地，在此步骤中，我们再次单独测试所有剩余特征。我们选择最佳的特征并将其添加到已选择的特征池中。我们继续一次一个地添加特征，直到达到停止标准（最大特征数或没有进一步改进）。传统上，添加的特征是根据特征的p值来决定的。然而，现代库使用交叉验证度量的改进作为选择标准。'
- en: '**Backward feature selection**—similar to the previous approach, but we start
    with all the features in our set and sequentially remove one feature at a time
    until there is no further improvement (or all features are statistically significant).
    This method differs from RFE as it does not use the coefficients or feature importances
    to select the features to be removed. Instead, it optimizes for the performance
    improvement measured by the difference in the cross-validated score.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向后特征选择**—类似于前一种方法，但我们从所有特征开始，逐一去除每个特征，直到没有进一步的改进（或所有特征都是统计显著的）。该方法与RFE的不同之处在于，它不使用系数或特征重要性来选择要去除的特征。相反，它通过交叉验证得分的差异来优化性能提升。'
- en: '**Exhaustive feature selection**—simply speaking, in this brute-force approach
    we try all the possible combinations of the features. Naturally, this is the most
    computationally expensive of the wrapper techniques, as the number of feature
    combinations to be tested grows exponentially with the number of features. For
    example, if we had 3 features, we would have to test 7 combinations. Assume we
    have features `a`, `b`, and `c`. We would have to test the following combinations:
    `[a, b, c, ab, ac, bc, abc]`.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**穷举特征选择**—简单来说，在这种暴力方法中，我们尝试所有可能的特征组合。自然地，这是所有包装技术中计算开销最大的一种，因为特征组合的数量随着特征数量的增加而指数级增长。例如，如果我们有3个特征，我们需要测试7个组合。假设我们有特征`a`、`b`和`c`，我们需要测试以下组合：[a,
    b, c, ab, ac, bc, abc]。'
- en: '**Stepwise selection**—a hybrid approach combining forward and backward feature
    selection. The process starts with zero features and adds them one by one using
    the lowest significant p-value. At each addition step, the procedure also checks
    if any of the current features are statistically insignificant. If that is the
    case, they are dropped from the feature set and the algorithm continues to the
    next addition step. The procedure allows the final model to have only statistically
    significant features.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逐步选择**——一种结合了前向和后向特征选择的混合方法。该过程从零特征开始，并使用最低显著性p值逐一添加特征。在每一步添加时，过程还会检查当前特征中是否有任何在统计学上不显著的特征。如果有，这些特征将从特征集中过滤掉，算法继续进行下一步添加。该过程允许最终模型只包含统计学上显著的特征。'
- en: The first two approaches are implemented in `scikit-learn`. Alternatively, you
    can find all four of them in the `mlxtend` library.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种方法已在`scikit-learn`中实现。或者，你可以在`mlxtend`库中找到所有四种方法。
- en: 'We should also mention a few things to keep in mind about the *wrapper* techniques
    presented above:'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该提到一些关于上述*包装器*技术的注意事项：
- en: The optimal number of features depends on the ML algorithm.
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳特征数量取决于机器学习算法。
- en: Due to their iterative nature, they are able to detect certain interactions
    between the features.
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其迭代性质，它们能够检测特征之间的某些交互作用。
- en: These methods usually provide the best performing subset of features for a given
    ML algorithm.
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法通常会为给定的机器学习算法提供最佳表现的特征子集。
- en: They come at the highest computational cost, as they operate greedily and retrain
    the model multiple times.
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的计算成本最高，因为它们采用贪婪算法，并多次重训练模型。
- en: 'As the last wrapper method, we will mention the **Boruta** algorithm. Without
    going into too much detail, it creates a set of shadow features (permuted duplicates
    of the original features) and selects features using a simple heuristic: a feature
    is useful if it is doing better than the best of the randomized features. The
    entire process is repeated multiple times before the algorithm returns the best
    set of features. The algorithm is compatible with ML models from the `ensemble`
    module of `scikit-learn` and algorithms such as XGBoost and LightGBM. For more
    details on the algorithm, please refer to the paper mentioned in the *See also*
    section. The Boruta algorithm is implemented in the `boruta` library.'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一种包装器方法，我们将提到**Boruta**算法。不深入细节，它创建了一组影像特征（原始特征的置换副本），并使用一个简单的启发式规则选择特征：如果某个特征的表现优于所有随机化特征中的最佳者，那么该特征是有用的。整个过程会重复多次，直到算法返回最佳特征集。该算法兼容`scikit-learn`的`ensemble`模块中的机器学习模型，以及XGBoost和LightGBM等算法。有关该算法的更多细节，请参见*另见*部分中的论文。Boruta算法已在`boruta`库中实现。
- en: Lastly, it is worth mentioning that we can also combine multiple feature selection
    approaches to improve their reliability. For example, we could select features
    using a few approaches and then ultimately select the ones that appeared in all
    or most of them.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得一提的是，我们还可以结合多种特征选择方法，以提高其可靠性。例如，我们可以使用几种方法选择特征，然后最终选择所有方法中出现过的特征。
- en: Combining feature selection and hyperparameter tuning
  id: totrans-778
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合特征选择和超参数调优
- en: As we have already established, we do not know the optimal number of features
    to keep in advance. Hence, we might want to combine feature selection with hyperparameter
    tuning and treat the number of features to keep as another hyperparameter.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经确定的那样，我们无法事先知道应保留的最佳特征数量。因此，我们可能希望将特征选择与超参数调优相结合，并将保留的特征数量视为另一个超参数。
- en: 'We can easily do so using `pipelines` and `GridSearchCV` from `scikit-learn`
    :'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`scikit-learn`中的`pipelines`和`GridSearchCV`轻松实现：
- en: '[PRE79]'
  id: totrans-781
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Executing the snippet returns the best set of hyperparameters:'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码片段将返回最佳的超参数集：
- en: '[PRE80]'
  id: totrans-783
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: When combining filter feature selection methods with cross-validation, we should
    do the filtering within the cross-validation procedure. Otherwise, we are selecting
    the features using all the available observations and introducing bias.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 当将过滤特征选择方法与交叉验证结合使用时，我们应该在交叉验证过程中进行特征过滤。否则，我们就会使用所有可用的观测值来选择特征，从而引入偏差。
- en: One thing to keep in mind is that the features selected within various folds
    of the cross-validation can be different. Let’s consider an example of a `5`-fold
    cross-validation procedure that keeps `3` features. It can happen that in some
    of the `5` cross-validation rounds, the `3` selected features might not overlap.
    However, they should not be too different, as we assume that the overall patterns
    in the data and the distribution of the features are very similar across folds.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一点是，在交叉验证的不同折中选择的特征可能不同。假设我们有一个`5`折交叉验证过程，并选择了`3`个特征。可能在某些`5`折交叉验证轮次中，这`3`个选定的特征不会重叠。然而，它们不应差异太大，因为我们假设数据中的整体模式和特征的分布在各个折之间非常相似。
- en: See also
  id: totrans-786
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Additional references on the topic:'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 该主题的附加参考文献：
- en: 'Bommert, A., Sun, X., Bischl, B., Rahnenführer, J., & Lang, M. 2020\. “Benchmark
    for filter methods for feature selection in high-dimensional classification data.”
    *Computational Statistics & Data Analysis*, 143: 106839.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bommert, A., Sun, X., Bischl, B., Rahnenführer, J., & Lang, M. 2020\. “高维分类数据中特征选择过滤方法的基准。”
    *计算统计与数据分析*，143：106839。
- en: 'Ding, C., & Peng, H. 2005\. “Minimum redundancy feature selection from microarray
    gene expression data.” *Journal of bioinformatics and computational biology*,
    3(2): 185–205.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding, C., & Peng, H. 2005\. “来自微阵列基因表达数据的最小冗余特征选择。” *生物信息学与计算生物学杂志*，3(2)：185–205。
- en: 'Kira, K., & Rendell, L. A. 1992\. A practical approach to feature selection.
    In *Machine learning proceedings*, *1992*: 249–256\. Morgan Kaufmann.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kira, K., & Rendell, L. A. 1992\. 特征选择的实用方法。发表于*机器学习论文集*，*1992*：249–256\. Morgan
    Kaufmann。
- en: 'Kira, K., & Rendell, L. A. 1992, July. The feature selection problem: Traditional
    methods and a new algorithm. In Aaai, 2(1992a): 129-134.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kira, K., & Rendell, L. A. 1992年7月。特征选择问题：传统方法与新算法。发表于Aaai, 2(1992a)：129-134。
- en: 'Kuhn, M., & Johnson, K. 2019\. *Feature engineering and selection: A practical
    approach for predictive models*. CRC Press.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuhn, M., & Johnson, K. 2019\. *特征工程与选择：预测模型的实用方法*。CRC出版社。
- en: 'Kursa M., & Rudnicki W. Sep. 2010\. “Feature Selection with the Boruta Package”
    *Journal of Statistical Software*, 36(11): 1-13.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kursa M., & Rudnicki W. 2010年9月。“使用Boruta包进行特征选择” *统计软件杂志*，36(11)：1-13。
- en: 'Urbanowicz, RJ., et al. 2018\. “Relief-based feature selection: Introduction
    and review.” *Journal of biomedical informatics*, 85: 189–203.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Urbanowicz, RJ., 等. 2018\. “基于Relief的特征选择：介绍与回顾。” *生物医学信息学杂志*，85：189–203。
- en: 'Yu, L., & Liu, H. 2003\. Feature selection for high-dimensional data: A fast
    correlation-based filter solution. In *Proceedings of the 20th international conference
    on machine learning (ICML-03)*: 856–863.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu, L., & Liu, H. 2003\. 高维数据的特征选择：一种快速的基于相关性的过滤方法。发表于*第20届国际机器学习会议（ICML-03）*：856–863。
- en: 'Zhao, Z., Anand, R., & Wang, M. 2019, October. Maximum relevance and minimum
    redundancy feature selection methods for a marketing machine learning platform.
    In *2019 IEEE international conference on data science and advanced analytics
    (DSAA)*: 442–452\. IEEE.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao, Z., Anand, R., & Wang, M. 2019年10月。用于营销机器学习平台的最大相关性和最小冗余特征选择方法。发表于*2019
    IEEE国际数据科学与高级分析会议（DSAA）*：442–452\. IEEE。
- en: 'You can find the additional dataset mentioned in the *Getting ready* section
    here:'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*准备就绪*部分找到提到的附加数据集：
- en: '[https://www.kaggle.com/competitions/bnp-paribas-cardif-claims-management](https://www.kaggle.com/competitions/bnp-paribas-cardif-claims-management
    )'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/competitions/bnp-paribas-cardif-claims-management](https://www.kaggle.com/competitions/bnp-paribas-cardif-claims-management)'
- en: Exploring explainable AI techniques
  id: totrans-799
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索可解释的AI技术
- en: In one of the previous recipes, we looked into feature importance as one of
    the means of getting a better understanding of how the models work under the hood.
    While this might be quite a simple task in the case of linear regression, it gets
    increasingly difficult with the complexity of the models.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的一些案例中，我们探讨了特征重要性，作为更好理解模型内部工作原理的一种手段。尽管在线性回归的情况下，这可能是一个相对简单的任务，但随着模型复杂度的增加，这一任务变得愈加困难。
- en: One of the big trends in the ML/DL field is **explainable AI** (**XAI**). It
    refers to various techniques that allow us to better understand the predictions
    of black box models. While the current XAI approaches will not turn a black box
    model into a fully interpretable one (or a white box), they will definitely help
    us better understand why the model returns certain predictions for a given set
    of features.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习/深度学习领域的一个大趋势是**可解释AI**（**XAI**）。它指的是允许我们更好地理解黑箱模型预测的各种技术。尽管目前的XAI方法无法将黑箱模型转变为完全可解释的模型（或白箱模型），但它们无疑有助于我们更好地理解为什么模型在给定的特征集上返回特定的预测。
- en: 'Some of the benefits of having explainable AI models are as follows:'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有可解释AI模型的一些好处如下：
- en: Builds trust in the model—if the model’s reasoning (via its explanation) matches
    common sense or the beliefs of human experts, it can strengthen the trust in the
    model’s predictions
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立对模型的信任——如果模型的推理（通过其解释）符合常识或人类专家的信念，它可以增强对模型预测的信任
- en: Facilitates the model’s or project’s adoption by business stakeholders
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 促进模型或项目在业务利益相关者中的采纳
- en: Gives insights useful for human decision-making by providing reasoning for the
    model’s decision process
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过提供模型决策过程的推理，提供有助于人类决策的见解
- en: Makes debugging easier
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使调试变得更加容易
- en: Can steer the direction of future data gathering or feature engineering
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以引导未来数据收集或特征工程的方向
- en: Before mentioning the particular XAI techniques, it is worth clarifying the
    difference between interpretability and explainability. **Interpretability** can
    be considered a stronger version of explainability. It offers a causality-based
    explanation of a model’s predictions. On the other hand, **explainability** is
    used to make sense of the predictions made by black box models, which cannot be
    interpretable. In particular, XAI techniques can be used to explain what is going
    on in the model’s prediction process, but they are unable to causally prove why
    a certain prediction has been made.
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 在提到具体的XAI技术之前，值得澄清**可解释性**和**可解释性**之间的区别。**可解释性**可以看作是可解释性的一个更强版本。它提供基于因果关系的模型预测解释。另一方面，**可解释性**用于理解黑箱模型的预测，这些模型可能是不可解释的。具体来说，XAI技术可以用来解释模型预测过程中的发生情况，但它们无法因果性地证明为什么做出某个预测。
- en: In this recipe, we cover three XAI techniques. See the *There’s more…* section
    for a reference to more of the available approaches.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们介绍了三种XAI技术。请参见*更多内容...*部分以了解更多可用的方法。
- en: The first technique is called **Individual Conditional Expectation** (**ICE**)
    and it is a local and model-agnostic approach to explainability. The *local* part
    refers to the fact that this technique describes the impact of feature(s) at the
    observation level. ICE is most frequently presented in a plot and depicts how
    an observation’s prediction changes as a result of a change in a given feature’s
    value.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种技术被称为**个体条件期望**（**ICE**），它是一种局部且与模型无关的可解释性方法。*局部*部分指的是这项技术描述了在观察级别上特征（们）的影响。ICE通常以图表的形式呈现，展示了当给定特征的值发生变化时，观察值的预测如何变化。
- en: To obtain the ICE values for a single observation in our dataset and one of
    its features, we have to create multiple copies of that observation. In all of
    them, we keep the values of other features (except the considered one) constant,
    while replacing the value of the feature of interest with the values from a grid.
    Most commonly, the grid consists of all the distinct values of that feature in
    the entire dataset (for all observations). Then, we use the (black box) model
    to make predictions for each of the modified copies of the original observation.
    Those predictions are plotted as the ICE curve.
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得数据集中单个观察值及其特征的ICE值，我们需要创建多个该观察值的副本。在所有副本中，保持其他特征的值不变（除了被考虑的特征），同时将感兴趣特征的值替换为网格中的值。最常见的做法是，网格包含该特征在整个数据集中的所有不同值（对于所有观察值）。然后，我们使用（黑箱）模型对每个修改过的原始观察值副本进行预测。这些预测结果将绘制成ICE曲线。
- en: 'Advantages:'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: It is easy to calculate and intuitive to understand what the curves represent.
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算简单，直观理解曲线表示的内容
- en: ICE can uncover heterogeneous relationships, that is, when a feature has a different
    direction of impact on the target, depending on the intervals of the explored
    feature’s values.
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ICE可以揭示异质关系，即当特征对目标的影响方向因被探索特征值的区间不同而不同。
- en: 'Disadvantages:'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: We can meaningfully display only one feature at a time.
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们一次只能有意义地展示一个特征。
- en: Plotting many ICE curves (for multiple observations) can make the plot overcrowded
    and hard to interpret.
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制许多ICE曲线（对于多个观测值）可能会使图表过于拥挤，难以解读。
- en: ICE assumes independence of features—when features are correlated, some points
    in the curve might actually be invalid data points (either very unlikely or simply
    impossible) according to the joint feature distribution.
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ICE假设特征之间是独立的——当特征相关时，曲线中的某些点实际上可能是无效数据点（根据联合特征分布，它们可能是极不可能出现的，或者根本不可能出现）。
- en: The second approach is called the **Partial Dependence Plot** (**PDP**) and
    is heavily connected to ICE. It is also a model-agnostic method; however, it is
    a global one. It means that PDP describes the impact of feature(s) on the target
    in the context of the entire dataset.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法叫做**部分依赖图**（**PDP**），它与ICE密切相关。它也是一种模型无关的方法，但它是全局的。这意味着PDP描述的是特征对目标的影响，考虑的是整个数据集的上下文。
- en: PDP presents the marginal effect of a feature on the prediction. Intuitively,
    we can think of partial dependence as a mapping of the expected response of the
    target as a function of the feature of interest. It can also show whether the
    relationship between the feature and the target is linear or nonlinear. In terms
    of calculating the PDP, it is simply the average of all the ICE curves.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: PDP呈现特征对预测的边际效应。直观地讲，我们可以将部分依赖看作是目标的预期响应作为感兴趣特征的函数的映射。它还可以显示特征与目标之间的关系是线性还是非线性的。就PDP的计算而言，它实际上是所有ICE曲线的平均值。
- en: 'Advantages:'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: Similar to ICE, it is easy to calculate and intuitive to understand what the
    curves represent.
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与ICE类似，PDP计算起来很容易，且直观地可以理解曲线代表的含义。
- en: If the feature of interest is not correlated with other features, the PDP then
    perfectly represents how the selected feature impacts the prediction (on average).
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果感兴趣的特征与其他特征不相关，那么PDP就能完美地表示所选特征对预测的影响（平均而言）。
- en: The calculation for the PDPs has a causal interpretation (within the model)—by
    observing the changes in prediction caused by the changes to one of the features,
    we analyze the causal relationship between the two.
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PDP的计算具有因果解释（在模型内部）——通过观察特征变化所引起的预测变化，我们分析特征与预测之间的因果关系。
- en: 'Disadvantages:'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: PDPs also assume the independence of features.
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PDP还假设特征之间是独立的。
- en: PDPs can obscure heterogenous relationships created by interactions. For example,
    we could observe a linear relationship between the target and a certain feature.
    However, the ICE curves might show that there are exceptions to that pattern,
    for example, where the target remains constant in some ranges of the feature.
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PDP可能会掩盖由交互作用产生的异质关系。例如，我们可能会观察到目标与某个特征之间存在线性关系。然而，ICE曲线可能显示出该模式存在例外情况，例如，在某些特征的范围内，目标保持不变。
- en: PDPs can be used to analyze, at most, two features at a time.
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PDP可以用于分析至多两个特征。
- en: The last of the XAI techniques we cover in this recipe is called **SHapley Additive
    exPlanations** (**SHAP**). It is a model-agnostic framework for explaining predictions
    using a combination of game theory and local explanations.
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中讨论的最后一种XAI技术叫做**SHapley Additive exPlanations**（**SHAP**）。它是一个模型无关的框架，通过结合博弈论和局部解释来解释预测结果。
- en: The exact methodology and calculations involved in this method are outside of
    the scope of this book. We can briefly mention that **Shapley values** are a method
    used in game theory that involves a fair distribution of both gains and costs
    to players cooperating in a game. As each player contributes differently to the
    coalition, the Shapley value makes sure that each participant gets a fair share,
    depending on how much they contributed.
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 本方法涉及的具体方法论和计算超出了本书的范围。我们可以简要提到，**Shapley值**是博弈论中使用的一种方法，涉及对游戏中合作的参与者进行公平的收益和成本分配。由于每个玩家对联盟的贡献不同，Shapley值确保每个参与者根据他们的贡献获得公平的份额。
- en: 'We could compare it to the ML setting, in which features are the players, the
    cooperative game is creating the ML model’s prediction, and the payoff is the
    difference between the average prediction of the instance minus the average prediction
    of all instances. Hence, the interpretation of a Shapley value for a certain feature
    is as follows: the value of the feature contributed *x* to the prediction of this
    observation, compared to the average prediction for the dataset.'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其与机器学习（ML）设置进行比较，其中特征是玩家，合作游戏是创建机器学习模型的预测，而收益是实例的平均预测与所有实例的平均预测之间的差异。因此，对于某个特征的Shapley值的解释如下：该特征为该观察的预测贡献了*x*，与数据集的平均预测相比。
- en: Having covered the Shapley values, it is time to explain what SHAP is. It is
    an approach to explaining the outputs of any ML/DL model. SHAP combines optimal
    credit allocation with local explanations, using Shapley values (originating from
    game theory) and their extensions.
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 在讲解完Shapley值后，现在该解释SHAP是什么了。它是一种解释任何机器学习/深度学习模型输出的方法。SHAP结合了最优信用分配和局部解释，利用了Shapley值（源自博弈论）及其扩展。
- en: 'SHAP offers the following:'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP提供了以下内容：
- en: It is a computationally efficient and theoretically robust method of calculating
    Shapley values for ML models (ideally having trained the model only once).
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一种计算Shapley值的计算效率高且理论上稳健的方法，适用于机器学习模型（理想情况下只需要训练模型一次）。
- en: KernelSHAP—an alternative, kernel-based estimation method for estimating Shapley
    values. It was inspired by local surrogate models.
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KernelSHAP——一种替代的基于核的Shapley值估计方法，灵感来源于局部替代模型。
- en: TreeSHAP—an efficient estimation method for tree-based models.
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TreeSHAP——一种高效的基于树的模型估计方法。
- en: Various global interpretation methods based on aggregations of Shapley values.
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Shapley值的各种全局解释方法。
- en: To get a better understanding of SHAP, it is recommended to also get familiar
    with LIME. Please refer to the *There’s more…* section for a brief description.
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解SHAP，建议同时了解LIME。请参考*更多内容…*部分以获取简要描述。
- en: 'Advantages:'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 优势：
- en: Shapley values have a solid theoretical background (axioms of efficiency, symmetry,
    dummy, and additivity). Lundberg *et al.* (2017) explain minor discrepancies between
    those axioms in the context of Shapley values and their counterpart properties
    of the SHAP values, that is, local accuracy, missingness, and consistency.
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shapley值具有坚实的理论基础（效率、公平、虚拟和加法公理）。Lundberg *等*（2017）解释了这些公理与SHAP值的对应属性（即局部准确性、缺失性和一致性）之间的微小差异。
- en: Thanks to the efficiency property, SHAP might be the only framework in which
    the prediction is fairly distributed among the feature values.
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其高效性，SHAP可能是唯一一个能将预测公平分配到特征值中的框架。
- en: SHAP offers global interpretability—it shows feature importance, feature dependence,
    interactions, and an indication of whether a certain feature has a positive or
    negative impact on the model’s predictions.
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHAP提供了全局可解释性——它展示了特征的重要性、特征之间的依赖关系、相互作用，以及某个特征是否对模型预测产生正面或负面影响。
- en: SHAP offers local interpretability—while many techniques only focus on aggregate
    explainability, we can calculate SHAP values for each individual prediction to
    learn how features contribute to that particular prediction.
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHAP提供了局部可解释性——尽管许多技术仅关注整体可解释性，我们可以为每个单独的预测计算SHAP值，以了解特征是如何影响该特定预测的。
- en: SHAP can be used to explain a large variety of models, including linear models,
    tree-based models, and neural networks.
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHAP可以用于解释多种模型，包括线性模型、基于树的模型和神经网络。
- en: TreeSHAP (the fast implementation for tree-based models) makes it feasible to
    use the approach for real-life use cases.
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TreeSHAP（针对基于树的模型的快速实现）使得在实际应用中使用这种方法变得可行。
- en: 'Disadvantages:'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 劣势：
- en: Computation time—the number of possible combinations of the features increases
    exponentially with the number of considered features, which in turn increases
    the time of calculating SHAP values. That is why we have to revert to approximations.
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算时间——考虑的特征数量越多，特征的可能组合数量呈指数增长，这反过来增加了计算SHAP值的时间。因此，我们需要依赖近似方法。
- en: Similar to permutation feature importance, SHAP values are sensitive to high
    correlations among features. If that is the case, the impact of such features
    on the model score can be split among those features in an arbitrary way, leading
    us to believe that they are less important than if their impacts remained undivided.
    Also, correlated features might result in using unrealistic/impossible combinations
    of features.
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与置换特征重要性相似，SHAP 值对特征之间的高度相关性敏感。如果出现这种情况，这些特征对模型评分的影响可能会在这些特征之间被任意分配，从而使我们认为它们的重要性低于实际情况。另外，相关特征可能会导致使用不现实/不可能的特征组合。
- en: As Shapley values do not offer a prediction model (such as in the case of LIME),
    they cannot be used to make statements about how a change in the inputs corresponds
    to a change in the prediction. For example, we cannot state that “if the value
    of feature *Y* was higher by 50 units, then the predicted probability would increase
    by 1 percentage point.”
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 Shapley 值并未提供预测模型（如 LIME 的情况），因此不能用来陈述输入变化如何对应预测变化。例如，我们不能说“如果特征 *Y* 的值增加
    50 个单位，那么预测概率将增加 1 个百分点”。
- en: KernelSHAP is slow and, similarly to other permutation-based interpretation
    methods, ignores dependencies between features.
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KernelSHAP 运行较慢，且与其他基于置换的解释方法类似，忽略了特征之间的依赖关系。
- en: Getting ready
  id: totrans-851
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will be using the credit card fraud dataset that we introduced
    in the *Investigating different approaches to handling imbalanced data* recipe.
    For convenience, we have included all the necessary preparation steps in this
    section of the accompanying Jupyter notebook.
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用我们在*调查不同处理不平衡数据方法*教程中介绍的信用卡欺诈数据集。为了方便起见，我们已经将所有必要的准备步骤包含在了伴随的 Jupyter
    notebook 的这一部分中。
- en: How to do it…
  id: totrans-853
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Execute the following steps to investigate various approaches to explaining
    the predictions of an XGBoost model trained on the credit card fraud dataset:'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以调查对训练在信用卡欺诈数据集上的 XGBoost 模型预测进行解释的不同方法：
- en: 'Import the libraries:'
  id: totrans-855
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE81]'
  id: totrans-856
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Train the ML model:'
  id: totrans-857
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练机器学习模型：
- en: '[PRE82]'
  id: totrans-858
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Executing the snippet generates the following output:'
  id: totrans-859
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成如下输出：
- en: '[PRE83]'
  id: totrans-860
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: We can conclude that the model is overfitted to the training data and ideally
    we should try to fix that by, for example, using stronger regularization while
    training the XGBoost model. To keep the exercise concise, we assume that the model
    is good to go for further analysis.
  id: totrans-861
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以得出结论，模型对训练数据过拟合，理想情况下我们应该尝试通过例如在训练 XGBoost 模型时使用更强的正则化来解决这个问题。为了保持练习的简洁性，我们假设模型已准备好进行进一步分析。
- en: Similarly to investigating feature importance, we should first make sure that
    the model has satisfactory performance on the validation/test set before we start
    explaining its predictions.
  id: totrans-862
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与调查特征重要性类似，我们应首先确保模型在验证集/测试集上有令人满意的表现，然后再开始解释其预测。
- en: 'Plot the ICE curves:'
  id: totrans-863
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制ICE曲线：
- en: '[PRE84]'
  id: totrans-864
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-865
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段会生成如下图表：
- en: '![](../Images/B18112_14_35.png)'
  id: totrans-866
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_35.png)'
- en: 'Figure 14.35: The ICE plot of the V4 feature, created using 5,000 random samples
    from the training data'
  id: totrans-867
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.35：使用来自训练数据的 5,000 个随机样本创建的 V4 特征的 ICE 图
- en: '*Figure 14.35* presents the ICE curves for the `V4` feature, calculated using
    `5,000` random observations from the training data. In the plot, we can see that
    the vast majority of the observations are located around `0`, while a few of the
    curves show quite a significant change in predicted probability.'
  id: totrans-868
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*图 14.35* 展示了 `V4` 特征的 ICE 曲线，这些曲线是使用来自训练数据的 `5,000` 个随机观测值计算得出的。从图中可以看到，绝大多数观测值都位于
    `0` 附近，而少数曲线则显示出预测概率的显著变化。'
- en: The black marks at the bottom of the plot indicate the percentiles of the feature
    values. By default, the ICE plot and PDP are constrained to the 5th and 95th percentiles
    of the feature values; however, we can change this using the `percentiles` argument.
  id: totrans-869
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图表底部的黑色标记表示特征值的百分位数。默认情况下，ICE 图和PDP被限制在特征值的第5和第95百分位数；然而，我们可以通过 `percentiles`
    参数更改这一设置。
- en: A potential issue with the ICE curves is that it might be hard to see if the
    curves differ between observations, as they start at different predictions. A
    solution would be to center the curves at a certain point and display only the
    difference in the prediction compared to that point.
  id: totrans-870
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ICE 曲线的一个潜在问题是，很难看出曲线是否在不同观察值之间有所不同，因为它们的预测值起点不同。一个解决方案是将曲线集中在某一点，并仅显示与该点相比的预测差异。
- en: 'Plot the centered ICE curves:'
  id: totrans-871
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制集中ICE曲线：
- en: '[PRE85]'
  id: totrans-872
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-873
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段后会生成如下图表：
- en: '![](../Images/B18112_14_36.png)'
  id: totrans-874
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_36.png)'
- en: 'Figure 14.36: The centered ICE plot of the V4 feature, created using 5,000
    random samples from the training data'
  id: totrans-875
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.36：使用训练数据中5,000个随机样本创建的V4特征的集中ICE图
- en: The interpretation of the centered ICE curves is only slightly different. Instead
    of looking at the impact of changing the value of a feature on the prediction,
    we look at the relative change in the prediction, as compared to the average prediction.
    This way, it is easier to analyze the direction of the change in the predicted
    value.
  id: totrans-876
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 集中ICE曲线的解释仅略有不同。我们不再直接观察特征值变化对预测的影响，而是观察相对于平均预测的预测值变化。这种方式使得分析预测值变化的方向变得更加容易。
- en: 'Generate the Partial Dependence Plot:'
  id: totrans-877
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成部分依赖图：
- en: '[PRE86]'
  id: totrans-878
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-879
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段后会生成如下图表：
- en: '![](../Images/B18112_14_37.png)'
  id: totrans-880
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_37.png)'
- en: 'Figure 14.37: The Partial Dependence Plot of the V4 feature, prepared using
    the training data'
  id: totrans-881
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.37：使用训练数据准备的V4特征的部分依赖图
- en: By analyzing the plot, on average there seems to be a very small increase in
    the predicted probability with the increase of the `V4` feature.
  id: totrans-882
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过分析该图表，平均来看，随着`V4`特征的增加，预测概率似乎只有非常小的增加。
- en: Similar to the ICE curves, we can also center the PDP.
  id: totrans-883
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与ICE曲线类似，我们也可以对PDP进行集中处理。
- en: 'To get some further insights, we can generate the PDP together with the ICE
    curves. We can do so using the following snippet:'
  id: totrans-884
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了获得更多的见解，我们可以生成PDP并结合ICE曲线。我们可以使用以下代码片段来实现：
- en: '[PRE87]'
  id: totrans-885
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-886
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段后会生成如下图表：
- en: '![](../Images/B18112_14_38.png)'
  id: totrans-887
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_38.png)'
- en: 'Figure 14.38: The Partial Dependence Plot of the V4 feature (prepared using
    the training data), together with the ICE curves'
  id: totrans-888
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.38：V4特征的部分依赖图（使用训练数据准备），并结合ICE曲线展示
- en: As we can see, the partial dependence (PD) line is almost horizontal at 0\.
    Because of the differences in scale (please refer to *Figure 14.37*), the PD line
    is virtually meaningless in such a plot. To make the plot more readable or easier
    to interpret, we could try restricting the range of thy a-axis using the `plt.ylim`
    function. This way, we would focus on the area with the majority of the ICE curves,
    while neglecting the few ones that are far away from the bulk of the curves. However,
    we should keep in mind that those outlier curves are also important for the analysis.
  id: totrans-889
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们所见，部分依赖（PD）线几乎在0处水平。由于尺度的差异（请参阅*图14.37*），在这样的图中，PD线几乎没有任何意义。为了使图表更加易读或更容易解释，我们可以尝试使用`plt.ylim`函数限制y轴的范围。通过这种方式，我们可以集中关注ICE曲线大多数集中区域，同时忽略那些远离大部分曲线的少数曲线。然而，我们应该记住，这些异常值曲线对于分析也同样重要。
- en: 'Generate the individual PDPs of two features and a joint one:'
  id: totrans-890
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成两个特征的单独PDP以及一个联合PDP：
- en: '[PRE88]'
  id: totrans-891
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-892
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段后会生成如下图表：
- en: '![](../Images/B18112_14_39.png)'
  id: totrans-893
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_39.png)'
- en: 'Figure 14.39: The centered Partial Dependence Plot of the V4 and V8 features,
    individually and jointly'
  id: totrans-894
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.39：V4和V8特征的集中部分依赖图，分别和联合展示
- en: By jointly plotting the PDPs of two features, we are able to visualize the interactions
    among them. By looking at *Figure 14.39* we could draw a conclusion that the `V4`
    feature is more important, as most of the lines visible in the rightmost plot
    are perpendicular to the `V4` axis and parallel to the `V8` axis. However, there
    is some shift in the decision lines determined by the `V8` feature, for example,
    around the `0.25` value.
  id: totrans-895
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过联合绘制两个特征的PDP，我们能够可视化它们之间的交互关系。通过观察*图14.39*，我们可以得出结论：`V4`特征更为重要，因为在最右侧图中，绝大多数线条是垂直于`V4`轴并且与`V8`轴平行的。然而，由`V8`特征决定的决策线会有所偏移，例如，在`0.25`值附近。
- en: 'Instantiate an explainer and calculate the SHAP values:'
  id: totrans-896
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个解释器并计算SHAP值：
- en: '[PRE89]'
  id: totrans-897
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: The `shap_values` object is a `284807` by `29` `numpy` array containing the
    calculated SHAP values.
  id: totrans-898
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`shap_values`对象是一个`284807`行`29`列的`numpy`数组，包含计算得到的SHAP值。'
- en: 'Generate the SHAP summary plot:'
  id: totrans-899
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成SHAP汇总图：
- en: '[PRE90]'
  id: totrans-900
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-901
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行代码片段后会生成如下图表：
- en: '![](../Images/B18112_14_40.png)'
  id: totrans-902
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_40.png)'
- en: 'Figure 14.40: The summary plot calculated using SHAP values'
  id: totrans-903
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.40：使用SHAP值计算的汇总图
- en: 'When looking at the summary plot, we should be aware of the following:'
  id: totrans-904
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在查看汇总图时，我们需要注意以下几点：
- en: Features are sorted by the sum of the SHAP value magnitudes (absolute values)
    across all observations.
  id: totrans-905
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征按所有观察值的SHAP值绝对值的总和排序。
- en: The color of the points shows if that feature had a high or low value for that
    observation.
  id: totrans-906
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点的颜色显示该特征对于该观察值是否具有高或低的值。
- en: The horizontal location on the plot shows whether the effect of that feature’s
    value resulted in a higher or lower prediction.
  id: totrans-907
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图表中的横向位置显示该特征值的效应是导致更高还是更低的预测。
- en: By default, the plots display the `20` most important features. We can adjust
    that using the `max_display` argument.
  id: totrans-908
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，图表显示的是`20`个最重要的特征。我们可以使用`max_display`参数调整这个数值。
- en: Overlapping points are jittered in the *y* axis direction. Hence, we can get
    a sense of the distribution of the SHAP values per feature.
  id: totrans-909
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重叠的点在*y*轴方向上进行了抖动。因此，我们可以感知每个特征的SHAP值的分布情况。
- en: An advantage of this type of plot over other feature importance metrics (for
    example, permutation importance) is that it contains more information that can
    help with understanding the global feature importance. For example, let’s assume
    that a feature is of medium importance. Using this plot, we could see if that
    medium importance corresponds to the feature values having a large effect on the
    prediction for a few observations, but in general no effect. Or maybe it had a
    medium-sized effect on all predictions.
  id: totrans-910
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他特征重要性度量（例如置换重要性）相比，这种类型的图表的优势在于它包含更多的信息，可以帮助理解全局特征重要性。例如，假设某个特征的重要性适中。使用此图，我们可以查看该中等重要性的特征值是否对某些观察的预测有较大影响，但通常对其他预测没有影响。或者它可能对所有预测都有中等大小的影响。
- en: 'Having discussed the overall considerations, let’s mention a few observations
    from *Figure 14.40*:'
  id: totrans-911
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在讨论了总体考虑因素之后，让我们提及一些来自*图 14.40*的观察结果：
- en: Overall, high values of the `V4` feature (the most important one) contributed
    to higher predictions, while lower values resulted in lower predictions (observation
    being less likely to be a fraudulent one).
  id: totrans-912
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体而言，`V4`特征（最重要的特征）较高的值有助于提高预测值，而较低的值则导致较低的预测值（观察结果更不可能为欺诈行为）。
- en: The overall effect of the `V14` feature on the prediction was negative, but
    for quite a few observations with a low value of that feature, it resulted in
    a higher prediction.
  id: totrans-913
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V14`特征对预测的整体影响是负面的，但对于一些特征值较低的观察，其结果却导致更高的预测值。'
- en: 'Alternatively, we can present the same information using a bar chart. Then,
    we focus on the aggregate feature importance, while ignoring the insights into
    feature effects:'
  id: totrans-914
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另外，我们可以使用条形图展示相同的信息。这样，我们就可以关注特征的重要性汇总，而忽略对特征效应的深入理解：
- en: '[PRE91]'
  id: totrans-915
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-916
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码片段会生成以下图表：
- en: '![](../Images/B18112_14_41.png)'
  id: totrans-917
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_41.png)'
- en: 'Figure 14.41: The summary plot (bar chart) calculated using the SHAP values'
  id: totrans-918
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.41：使用SHAP值计算的汇总图（条形图）
- en: Naturally, the order of the features (their importance) is the same as in *Figure
    14.40*. We could use this plot as an alternative to the permutation feature importance.
    However, we should then keep in mind the underlying differences. Permutation feature
    importance is based on the decrease in model performance (measured using a metric
    of choice), while SHAP is based on the magnitude of feature attributions.
  id: totrans-919
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自然地，特征的顺序（它们的重要性）与*图 14.40*中的顺序相同。我们可以将这个图作为替代的置换特征重要性。然而，我们应该牢记其中的基本区别。置换特征重要性基于模型性能的下降（使用选择的度量标准来衡量），而SHAP则基于特征归因的大小。
- en: 'We can get an even more concise representation of the summary chart using the
    following command: `shap.plots.bar(explainer_x)`.'
  id: totrans-920
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令获取汇总图的更简洁表示：`shap.plots.bar(explainer_x)`。
- en: 'Locate an observation belonging to the positive and negative classes:'
  id: totrans-921
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定位属于正类和负类的观察结果：
- en: '[PRE92]'
  id: totrans-922
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Explain those observations:'
  id: totrans-923
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释这些观察结果：
- en: '[PRE93]'
  id: totrans-924
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-925
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码片段会生成以下图表：
- en: '![](../Images/B18112_14_42.png)'
  id: totrans-926
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_42.png)'
- en: 'Figure 14.42: An (abbreviated) force plot explaining an observation belonging
    to the negative class'
  id: totrans-927
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.42：解释属于负类的观察值的（简化版）力图
- en: In a nutshell, the force plot shows how features contribute to pushing the prediction
    from the base value (average prediction) to the actual prediction. As the plot
    contained much more information and it was too wide to fit the page, we only present
    the most relevant part. Please refer to the accompanying Jupyter notebook to inspect
    the full plot.
  id: totrans-928
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简而言之，力图展示了特征如何将预测从基准值（平均预测）推向实际预测。由于该图包含了更多的信息且过宽，无法适应页面，因此我们仅展示了最相关的部分。请参考随附的
    Jupyter 笔记本以查看完整图表。
- en: 'Below are some of the observations we can make based on *Figure 14.42*:'
  id: totrans-929
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是我们基于 *图 14.42* 可以做出的一些观察：
- en: The base value (*-8.589*) is the average prediction of the entire dataset.
  id: totrans-930
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准值 (*-8.589*) 是整个数据集的平均预测值。
- en: '*f(x) = -13.37* is the prediction of this observation.'
  id: totrans-931
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(x) = -13.37* 是此观察值的预测结果。'
- en: We can interpret the arrows as the impact of given features on the prediction.
    The red arrows indicate an increase in the prediction. The blue arrows indicate
    a decrease in the prediction. The size of the arrows corresponds to the magnitude
    of the feature’s effect. The values by the feature names show the feature values.
  id: totrans-932
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将箭头解释为给定特征对预测结果的影响。红色箭头表示预测结果的增加，蓝色箭头表示预测结果的减少。箭头的大小对应于特征影响的大小。特征名称旁边的值显示了特征的值。
- en: If we subtract the total length of the red arrows from the total length of the
    blue arrows, we will get the distance from the base value to the final prediction.
  id: totrans-933
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们将红色箭头的总长度从蓝色箭头的总长度中减去，就可以得到从基准值到最终预测的距离。
- en: As such, we can see that the biggest contributor to the decrease in the prediction
    (compared to the average prediction) was feature `V14`'s value of -*0.3112*.
  id: totrans-934
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，我们可以看到，相较于平均预测，导致预测减少的最大因素是特征 `V14` 的值 -*0.3112*。
- en: 'We then follow the same step for the positive observation:'
  id: totrans-935
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后我们对正类观察值执行相同的步骤：
- en: '[PRE94]'
  id: totrans-936
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-937
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码片段生成以下图表：
- en: '![](../Images/B18112_14_43.png)'
  id: totrans-938
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_43.png)'
- en: 'Figure 14.43: An (abbreviated) force plot explaining an observation belonging
    to the positive class'
  id: totrans-939
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.43：解释属于正类的观察值的（简化版）力图
- en: Compared to *Figure 14.42*, we can clearly see how outbalanced the blue features
    (negatively impacting the prediction, labeled *lower*) are compared to the red
    ones (labeled *higher*). We can also see that both figures have the same base
    value, as this is the dataset’s average predicted value.
  id: totrans-940
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与 *图 14.42* 相比，我们可以清楚地看到蓝色特征（负面影响预测，标记为 *lower*）与红色特征（标记为 *higher*）之间的失衡。我们还可以看到，两个图形具有相同的基准值，因为这是数据集的平均预测值。
- en: 'Create a waterfall plot for the positive observation:'
  id: totrans-941
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为正类观察值创建瀑布图：
- en: '[PRE95]'
  id: totrans-942
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-943
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码片段生成以下图表：
- en: '![](../Images/B18112_14_44.png)'
  id: totrans-944
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_44.png)'
- en: 'Figure 14.44: A waterfall plot explaining an observation from the positive
    class'
  id: totrans-945
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.44：解释来自正类的观察值的瀑布图
- en: 'Inspecting *Figure 14.44* reveals many similarities to *Figure 14.43*, as both
    plots are explaining the very same observation using a slightly different visualization.
    Hence, most of the insights on interpreting the waterfall plot are the same as
    for the force plot. Some nuances include:'
  id: totrans-946
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检查 *图 14.44* 可以发现它与 *图 14.43* 有许多相似之处，因为这两个图表使用稍有不同的可视化方式解释了同一个观察值。因此，解读瀑布图的大部分见解与力图相同。一些细微差别包括：
- en: The bottom of the plot starts at the baseline value (the model’s average prediction).
    Then, each row shows the positive or negative contribution of each feature that
    leads to the model’s final prediction for that particular observation.
  id: totrans-947
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图表的底部从基准值开始（模型的平均预测值）。然后，每一行显示了每个特征对该特定观察值模型最终预测的正面或负面贡献。
- en: SHAP explains XGBoost classifiers in terms of their margin output. This means
    that the units on the *x* axis are log-odds units. A negative value implies probabilities
    lower than `0.5` that the observation was a fraudulent one.
  id: totrans-948
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHAP 通过其边际输出解释 XGBoost 分类器。这意味着 *x* 轴上的单位是对数几率单位。负值表示该观察值为欺诈行为的概率低于 `0.5`。
- en: The least impactful features are collapsed into a joint term. We can control
    that using the `max_display` argument of the function.
  id: totrans-949
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最不重要的特征被合并为一个联合项。我们可以使用该函数的 `max_display` 参数来控制这一点。
- en: 'Create a dependence plot of the `V4` feature:'
  id: totrans-950
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `V4` 特征的依赖图：
- en: '[PRE96]'
  id: totrans-951
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Executing the snippet generates the following plot:'
  id: totrans-952
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行该代码片段生成以下图表：
- en: '![](../Images/B18112_14_45.png)'
  id: totrans-953
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B18112_14_45.png)'
- en: 'Figure 14.45: A dependence plot visualizing the dependence between the V4 and
    V12 features'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.45：展示V4和V12特征之间依赖关系的依赖图
- en: 'Some things to know about a dependence plot:'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 关于依赖图的一些要点：
- en: It is potentially the simplest global interpretation plot.
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可能是最简单的全局解释图。
- en: This type of plot is an alternative to Partial Dependence Plots. While PDPs
    show the average effects, the SHAP dependence plot additionally shows the variance
    on the *y* axis. Hence it contains information about the distribution of effects.
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种类型的图是部分依赖图的替代方案。虽然PDP显示了平均效应，SHAP依赖图还额外展示了*y*轴上的方差。因此，它包含了关于效应分布的信息。
- en: The plot presents the feature’s value (*x* axis) vs. the SHAP value of that
    feature (*y* axis) across all the observations in the dataset. Each dot represents
    a single observation.
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该图展示了特征值（*x*轴）与该特征的SHAP值（*y*轴）在数据集中所有观测值中的关系。每个点代表一个单独的观测值。
- en: Given we are explaining an XGBoost classification model, the unit of the *y*
    axis is the log odds of being a fraudulent case.
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们正在解释一个XGBoost分类模型，*y*轴的单位是属于欺诈案件的对数几率。
- en: The color corresponds to a second feature that may have an interaction effect
    with the feature we specified. It is automatically selected by the `shap` library.
    The documentation states that if an interaction effect is present between the
    two features, it will show up as a distinct vertical pattern of coloring. In other
    words, we should look out for clear vertical spreads between colors for the same
    values on the *x* axis.
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 颜色对应于可能与我们指定的特征存在交互效应的第二个特征。它由`shap`库自动选择。文档中指出，如果两个特征之间存在交互效应，它将以一种独特的垂直颜色模式显示。换句话说，我们应该注意观察在同一*x*轴值上，不同颜色之间是否有明显的垂直扩展。
- en: To complete the analysis, we can mention a potential conclusion from *Figure
    14.45*. Unfortunately, it will not be quite intuitive, as the features were anonymized.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成分析，我们可以提到从*图14.45*中得到的潜在结论。不幸的是，这将不会非常直观，因为特征已经被匿名化。
- en: For example, let’s look at observations with the value of feature `V4` around
    5\. For those samples, observations with lower values of feature `V12` are more
    likely to be fraudulent than the observations with higher values of the `V12`
    feature.
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们查看特征`V4`值大约为5的观测值。对于这些样本，特征`V12`值较低的观测值比特征`V12`值较高的观测值更有可能是欺诈的。
- en: How it works…
  id: totrans-963
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: After importing the libraries, we trained an XGBoost model to detect credit
    card fraud.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入库之后，我们训练了一个XGBoost模型来检测信用卡欺诈。
- en: In *Step 3*, we plotted the ICE curves using `PartialDependenceDisplay` class.
    We had to provide the fitted model, the dataset (we used the training set), and
    the feature(s) of interest. Additionally, we provided the `subsample` argument,
    which specified the number of random observations from the dataset for which the
    ICE curves were plotted. As the dataset has over *200,000* observations, we arbitrarily
    chose *5,000* as a manageable number of curves to be plotted.
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3步*中，我们使用`PartialDependenceDisplay`类绘制了ICE曲线。我们必须提供拟合的模型、数据集（我们使用了训练集）和感兴趣的特征。此外，我们还提供了`subsample`参数，指定了用于绘制ICE曲线的数据集中的随机观测值数量。由于数据集有超过*200,000*个观测值，我们任意选择了*5,000*个曲线作为可管理的绘制数量。
- en: We have mentioned that the grid used for calculating the ICE curves most frequently
    consists of all the unique values available in the dataset. `scikit-learn` by
    default creates an equally spaced grid, covering the range between the extreme
    values of the feature. We can customize the grid’s density using the `grid_resolution`
    argument.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到过，计算ICE曲线时使用的网格通常由数据集中的所有唯一值组成。`scikit-learn`默认创建一个等距网格，覆盖特征的极值范围。我们可以使用`grid_resolution`参数自定义网格的密度。
- en: 'The `from_estimator` method of `PartialDependenceDisplay` also accepts the
    `kind` argument, which can take the following values:'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: '`PartialDependenceDisplay`的`from_estimator`方法也接受`kind`参数，可以取以下值：'
- en: '`kind="individual"`—the method will plot the ICE curves.'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kind="individual"`—该方法将绘制ICE曲线。'
- en: '`kind="average"`—the method will display the Partial Dependence Plot.'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kind="average"`—该方法将显示部分依赖图（PDP）。'
- en: '`kind="both"`—the method will display both the PDP and ICE curves.'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kind="both"`—该方法将显示PDP和ICE曲线。'
- en: In *Step 4*, we plotted the same ICE curves; however, we centered them at the
    origin. We did so by setting the `centered` argument to `True`. This effectively
    subtracts the average target value from the target vector and centers the target
    value at `0`.
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4步*中，我们绘制了相同的ICE曲线；然而，我们将它们居中于原点。我们通过将`centered`参数设置为`True`来实现这一点。这实际上是从目标向量中减去平均目标值，并将目标值居中于`0`。
- en: In *Step 5*, we plotted the Partial Dependence Plot, also using the `PartialDependenceDisplay.from_estimator`.
    As the PDP is the default value, we did not have to specify the `kind` argument.
    We also showed the outcome of plotting both the PDP and ICE curves in the same
    figure. As plotting the two-way PDP takes quite a bit of time, we sampled (without
    replacement) *20,000* observations from the training set.
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5步*中，我们绘制了部分依赖图，同样使用了`PartialDependenceDisplay.from_estimator`。由于PDP是默认值，我们无需指定`kind`参数。我们还展示了在同一图中绘制PDP和ICE曲线的结果。由于绘制双向PDP需要相当长的时间，我们从训练集中随机抽取了（不放回）*20,000*个样本。
- en: One thing to keep in mind about `PartialDependenceDisplay` is that it treats
    categorical features as numeric.
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，`PartialDependenceDisplay`将分类特征视为数值特征。
- en: Partial Dependence Plots are also available in the `pdpbox` library.
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: 部分依赖图（PDP）也可以在`pdpbox`库中找到。
- en: In *Step 6*, we created a more complex figure using the same functionality of
    `PartialDependenceDisplay`. In one figure, we plotted the individual PD plots
    of two features (`V4` and `V8`), and their joint (also called two-way) PD plot.
    To obtain the last one, we had to provide the two features of interest as a tuple.
    By specifying `features=["V4", "V8", ("V4", "V8")]`, we indicated that we wanted
    to plot two individual PD plots and then a joint one for the two features. Naturally,
    there is no need to plot all `3` plots in the same figure. We could have used
    `features=[("V4", "V8")]` to create just the joint PDP.
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第6步*中，我们使用相同的`PartialDependenceDisplay`功能创建了一个更复杂的图形。在一个图中，我们绘制了两个特征（`V4`和`V8`）的单独PD图，以及它们的联合（也叫双向）PD图。为了获得最后一个图，我们需要将这两个感兴趣的特征作为元组提供。通过指定`features=["V4",
    "V8", ("V4", "V8")]`，我们表明希望绘制两个单独的PD图，然后绘制这两个特征的联合图。当然，没有必要将所有`3`个图都绘制在同一图中。我们可以使用`features=[("V4",
    "V8")]`只绘制联合PDP。
- en: Another interesting angle to explore would be to overlay two Partial Dependence
    Plots, calculated for the same feature but using different ML models. Then we
    could compare if the expected impact on the prediction is similar across different
    models.
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的角度是叠加两个部分依赖图，它们是针对相同特征但使用不同的机器学习模型计算的。然后，我们可以比较不同模型之间对预测的预期影响是否相似。
- en: 'We have focused on plotting the ICE curves and the Partial Dependence line.
    However, we can also calculate those values without automatically plotting them.
    To do so, we can use the `partial_dependence` function. It returns a dictionary
    containing `3` elements: the values that create the evaluated grid, the predictions
    for all the points in the grid for all samples in the dataset (used for ICE curves),
    and the averaged values of the predictions for each point in the grid (used for
    the PDP).'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: 我们集中于绘制ICE曲线和部分依赖线。然而，我们也可以在不自动绘制它们的情况下计算这些值。为此，我们可以使用`partial_dependence`函数。它返回一个包含`3`个元素的字典：用于创建评估网格的值、数据集中所有样本的所有网格点的预测值（用于ICE曲线）以及每个网格点的预测值的平均值（用于PDP）。
- en: In *Step 7*, we instantiated the `explainer` object, which is the primary class
    used to explain any ML/DL model using the `shap` library. To be more precise,
    we used the `TreeExplainer` class, as we were trying to explain an XGBoost model,
    that is, a tree-based model. Then, we calculated the SHAP values using the `shap_values`
    method of the instantiated `explainer`. To explain the model’s predictions, we
    used the entire dataset. At this point, we could have also decided to use the
    training or validation/test sets.
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第7步*中，我们实例化了`explainer`对象，这是用于通过`shap`库解释任何机器学习/深度学习模型的主要类。更准确地说，我们使用了`TreeExplainer`类，因为我们尝试解释的是XGBoost模型，即基于树的模型。然后，我们使用实例化的`explainer`的`shap_values`方法计算了SHAP值。为了说明模型的预测，我们使用了整个数据集。在这一点上，我们也可以选择使用训练集或验证/测试集。
- en: By definition, SHAP values are very complicated to compute (an NP-hard class
    problem). However, thanks to the simplicity of linear models, we can read the
    SHAP values from a partial dependence plot. Please refer to `shap`'s documentation
    for more information on this topic.
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，SHAP值的计算非常复杂（属于NP难问题）。然而，得益于线性模型的简单性，我们可以从部分依赖图中读取SHAP值。有关此主题的更多信息，请参阅`shap`文档。
- en: In *Step 8*, we started with global explanation approaches. We generated two
    variants of a summary plot using the `shap.summary_plot` function. The first one
    was a density scatterplot of SHAP values for each of the features. It combines
    the overall feature importance with feature effects. We can use that information
    to evaluate the impact each feature has on the model’s predictions (also on the
    observation level).
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第8步*中，我们首先使用全局解释方法。我们使用`shap.summary_plot`函数生成了两种版本的总结图。第一个是每个特征的SHAP值的密度散点图。它结合了整体特征重要性和特征效应。我们可以利用这些信息评估每个特征对模型预测的影响（也包括观察级别的影响）。
- en: The second one was a bar chart, showing the average of the absolute SHAP values
    across the entire dataset. In both cases, we can use the plot to infer the feature
    importance calculated using SHAP values; however, the first plot provides additional
    information. To generate this plot, we had to additionally pass `plot_type="bar"`
    while calling the `shap.summary_plot` function.
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个图表是一个条形图，显示了整个数据集中绝对SHAP值的平均值。在这两种情况下，我们都可以使用图表来推断通过SHAP值计算的特征重要性；然而，第一个图表提供了更多的信息。为了生成这个图表，我们在调用`shap.summary_plot`函数时必须额外传入`plot_type="bar"`参数。
- en: After looking at the global explanations, we wanted to look into local ones.
    To make the analysis more interesting, we wanted to present the explanations for
    observations belonging to both the negative and positive classes. That is why
    in *Step 9* we identified the indices of such observations.
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看了全局解释后，我们希望查看局部解释。为了使分析更有趣，我们想要展示属于正类和负类观察的解释。这就是为什么在*第9步*中我们识别了这些观察的索引。
- en: 'In *Step 10*, we used `shap.force_plot` to explain observation-level predictions
    of both observations. While calling the function, we had to provide three inputs:'
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第10步*中，我们使用`shap.force_plot`来解释两个观察的观察级预测。在调用该函数时，我们必须提供三个输入：
- en: The baseline value (the average prediction for the entire dataset), which is
    available in the explainer object (`explainer.expected_value`)
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准值（整个数据集的平均预测值），该值在解释器对象中可用（`explainer.expected_value`）
- en: The SHAP values for the particular observation
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定观察的SHAP值
- en: The feature values of the particular observation
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定观察的特征值
- en: In *Step 11*, we also created an observation-level plot explaining the predictions;
    however, we used a slightly different representation. We created a waterfall plot
    (using the `shap.plots.waterfall` function) to explain the positive observation.
    The only thing worth mentioning is that the function expects a single row of an
    `Explanation` object as input.
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第11步*中，我们还创建了一个观察级别的图表来解释预测结果；然而，我们使用了略微不同的表示方法。我们创建了一个瀑布图（使用`shap.plots.waterfall`函数）来解释正向观察。值得一提的是，该函数需要一个`Explanation`对象的单行作为输入。
- en: In the last step, we created a SHAP dependence plot (a global-level explanation)
    using the `shap.dependence_plot` function. We had to provide the feature of interest,
    the SHAP values, and the feature values. As the considered feature, we selected
    the `V4` one as it was identified as the most important one by the summary plot.
    The second feature (`V12`) was determined automatically by the library.
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们使用`shap.dependence_plot`函数创建了一个SHAP依赖图（全局级别的解释）。我们需要提供感兴趣的特征、SHAP值和特征值。作为被考虑的特征，我们选择了`V4`，因为它在总结图中被确定为最重要的特征。第二个特征（`V12`）由库自动确定。
- en: There’s more…
  id: totrans-989
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: In this recipe, we have only provided a glimpse of the field of XAI. The field
    is constantly growing, as explainable methods are becoming more and more important
    for practitioners and businesses.
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们只提供了XAI领域的一个初步了解。随着可解释方法在实践者和企业中的重要性日益增加，这一领域也在不断发展壮大。
- en: Another popular XAI technique is called LIME, which stands for **Local Interpretable
    Model-Agnostic Explanations**. It is an observation-level approach used for explaining
    the predictions of any model in an interpretable and faithful manner. To obtain
    the explanations, LIME locally approximates the selected hard-to-explain model
    with an interpretable one (such as linear models with regularization). The interpretable
    models are trained on small perturbations (with additional noise) of the original
    observations, thus providing a good local approximation.
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的 XAI 技术被称为 LIME，代表**局部可解释模型无关解释**。它是一种观察级别的方法，用于以可解释且忠实的方式解释任何模型的预测结果。为了获得解释，LIME
    通过可解释模型（如带正则化的线性模型）在局部近似选择的难以解释的模型。可解释模型是在原始观察的微小扰动（带有附加噪声）上进行训练，从而提供良好的局部近似。
- en: '**Treeinterpreter** is another observation-level XAI method useful for explaining
    Random Forest models. The idea is to use the underlying trees to explain how each
    feature contributes to the end result. The prediction is defined as the sum of
    each feature’s contributions and the average given by the initial node that is
    based on the entire training set. Using this approach, we can observe how the
    value of the prediction changes along the prediction path within the decision
    tree (after every split), combined with the information on which features caused
    the split, that is, a change in prediction.'
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: '**Treeinterpreter** 是另一种观察级别的 XAI 方法，适用于解释随机森林模型。其思路是利用底层的树结构来解释每个特征对最终结果的贡献。预测被定义为每个特征贡献的总和，以及由基于整个训练集的初始节点给出的平均值。通过这种方法，我们可以观察预测值如何沿着决策树中的预测路径（每次分裂后）变化，并结合导致分裂的特征信息，也就是预测变化的原因。'
- en: 'Naturally, there are many more available approaches, for example:'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，还有许多其他可用的方法，例如：
- en: Ceteris-paribus profiles
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设不变轮廓
- en: Break-down plots
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分解图
- en: Accumulated Local Effects (ALE)
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累积局部效应（ALE）
- en: Global surrogate models
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局代理模型
- en: Counterfactual explanations
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反事实解释
- en: Anchors
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 锚点
- en: 'We recommend investigating the following Python libraries focusing on AI explainability:'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议调查以下专注于人工智能可解释性的 Python 库：
- en: '`shapash`—compiles various visualizations from SHAP/LIME as an interactive
    dashboard in the form of a web app.'
  id: totrans-1001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shapash`—将 SHAP/LIME 的各种可视化结果编译为交互式仪表板 Web 应用。'
- en: '`explainerdashboard`—prepares a dashboard web app that explains `scikit-learn`-compatible
    ML models. The dashboard covers model performance, feature importance, feature
    contributions to individual predictions, a “what if” analysis, PDPs, SHAP values,
    visualization of individual decision trees, and more.'
  id: totrans-1002
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`explainerdashboard`—准备一个仪表板 Web 应用程序，用于解释与 `scikit-learn` 兼容的机器学习模型。该仪表板涵盖模型性能、特征重要性、特征对单个预测的贡献、“如果”分析、PDP、SHAP
    值、单个决策树的可视化等。'
- en: '`dalex`—the library covers various XAI methods, including variable importance,
    PDPs and ALE plots, breakdown and SHAP waterfall plots, and more.'
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dalex`—该库涵盖了各种 XAI 方法，包括变量重要性、PDP 和 ALE 图、分解图和 SHAP 瀑布图等。'
- en: '`interpret`—the InterpretML library was created by Microsoft. It covers popular
    explanation methods of black-box models (such as PDPs, SHAP, LIME, and so on)
    and allows you to train so-called glass-box models, which are interpretable. For
    example, `ExplainableBoostingClassifier` is designed to be fully interpretable,
    but at the same time provides similar accuracy to the state-of-the-art algorithms.'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`interpret`—InterpretML 库由微软创建。它涵盖了流行的黑箱模型解释方法（如 PDP、SHAP、LIME 等），并允许训练所谓的玻璃箱模型，这些模型是可解释的。例如，`ExplainableBoostingClassifier`
    被设计为完全可解释，但同时提供与最先进算法相似的准确性。'
- en: '`eli5`—an explainability library that provides various global and local explanations.
    It also covers text explanation (powered by LIME) and permutation feature importance.'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eli5`—一个可解释性库，提供各种全局和局部解释。它还涵盖文本解释（由 LIME 提供支持）和置换特征重要性。'
- en: '`alibi`—a library focusing on model inspection and interpretation. It covers
    approaches such as anchors explanations, integrated gradients, counterfactual
    examples, the Contrastive Explanation Method, and accumulated local effects.'
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alibi`—一个专注于模型检查和解释的库。它涵盖了诸如锚点解释、集成梯度、反事实示例、对比解释方法以及累积局部效应等方法。'
- en: See also
  id: totrans-1007
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Additional resources are available here:'
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: 额外资源可以在此获取：
- en: 'Biecek, P., & Burzykowski, T. 2021\. *Explanatory model analysis: Explore,
    explain and examine predictive models*. Chapman and Hall/CRC.'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biecek, P., & Burzykowski, T. 2021\. *解释性模型分析：探索、解释和检查预测模型*。Chapman and Hall/CRC。
- en: 'Friedman, J. H. 2001\. “Greedy function approximation: a gradient boosting
    machine.” *Annals of Statistics*: 1189–1232.'
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Friedman, J. H. 2001\. “贪婪函数逼近：梯度提升机。” *统计年鉴*：1189–1232。
- en: 'Goldstein, A., Kapelner, A., Bleich, J., & Pitkin, E. 2015\. “Peeking inside
    the black box: Visualizing statistical learning with plots of individual conditional
    expectation.” *Journal of Computational and Graphical Statistics*, 24(1): 44–65.'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goldstein, A., Kapelner, A., Bleich, J., & Pitkin, E. 2015\. “窥视黑盒：通过个体条件期望的可视化图形展示统计学习。”
    *计算与图形统计学杂志*，24(1)：44–65。
- en: 'Hastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. 2009\. *The
    Elements of Statistical Learning: Data Mining, Inference, and Prediction*, 2:
    1–758). New York: Springer.'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. 2009\. *统计学习要素：数据挖掘、推理与预测*，2：1–758）。纽约：Springer。
- en: 'Lundberg, S. M., Erion, G., Chen, H., DeGrave, A., Prutkin, J. M., Nair, B.,
    ... & Lee, S. I. 2020\. “From local explanations to global understanding with
    explainable AI for trees.” *Nature Machine Intelligence*, 2(1): 56–67.'
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lundberg, S. M., Erion, G., Chen, H., DeGrave, A., Prutkin, J. M., Nair, B.,
    ... & Lee, S. I. 2020\. “从局部解释到全局理解：树的可解释AI。” *自然机器智能*，2(1)：56–67。
- en: Lundberg, S. M., Erion, G. G., & Lee, S. I. 2018\. “Consistent individualized
    feature attribution for tree ensembles.” *arXiv preprint arXiv:1802.03888*.
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lundberg, S. M., Erion, G. G., & Lee, S. I. 2018\. “树集成方法的一致性个性化特征归因。” *arXiv预印本
    arXiv:1802.03888*。
- en: Lundberg, S. M., & Lee, S. I. 2017\. A unified approach to interpreting model
    predictions. *Advances in Neural Information Processing Systems*, *30*.
  id: totrans-1015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lundberg, S. M., & Lee, S. I. 2017\. 一种统一的模型预测解释方法。*神经信息处理系统进展*，*30*。
- en: Molnar, C. 2020\. *Interpretable machine learning.* [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/).
  id: totrans-1016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molnar, C. 2020\. *可解释的机器学习*。 [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)。
- en: 'Ribeiro, M.T., Singh, S., & Guestrin, C. 2016\. “Why should I trust you?: Explaining
    the predictions of any classifier.” Proceedings of the *22nd ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining*. ACM.'
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro, M.T., Singh, S., & Guestrin, C. 2016\. “我为什么要相信你？：解释任何分类器的预测。” *第22届ACM
    SIGKDD国际知识发现与数据挖掘会议*论文集。ACM。
- en: Saabas, A. *Interpreting random forests*. [http://blog.datadive.net/interpreting-random-forests/](http://blog.datadive.net/interpreting-random-forests/).
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saabas, A. *解释随机森林*。 [http://blog.datadive.net/interpreting-random-forests/](http://blog.datadive.net/interpreting-random-forests/)。
- en: Summary
  id: totrans-1019
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have covered a wide variety of useful concepts that can
    help with improving almost any ML or DL project. We started by exploring more
    complex classifiers (which also have their corresponding variants for regression
    problems), considering alternative approaches to encoding categorical features,
    creating stacked ensembles, and looking into possible solutions to class imbalance.
    We also showed how to use the Bayesian approach to hyperparameter tuning, in order
    to find an optimal set of hyperparameters faster than using the more popular yet
    uninformed grid search approaches.
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了各种有助于改进几乎所有机器学习（ML）或深度学习（DL）项目的有用概念。我们从探讨更复杂的分类器开始（这些分类器也有相应的回归问题变种），考虑了对分类特征编码的替代方法，创建了堆叠集成，并研究了可能解决类别不平衡的方案。我们还展示了如何使用贝叶斯方法进行超参数调优，以便比使用更流行但信息不足的网格搜索方法更快地找到最优的超参数组合。
- en: We have also dived into the topic of feature importance and AI explainability.
    This way, we can better understand what is happening in the so-called black box
    models. This is crucial not only for the people working on the ML/DL project but
    also for any business stakeholders. Additionally, we can combine those insights
    with feature selection techniques to potentially further improve a model’s performance
    or reduce its training time.
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还深入探讨了特征重要性和AI可解释性的话题。通过这种方式，我们可以更好地理解所谓的黑盒模型中发生的事情。这不仅对从事ML/DL项目的人至关重要，对任何业务相关方也同样如此。此外，我们可以将这些见解与特征选择技术结合起来，可能进一步提升模型的性能或减少其训练时间。
- en: 'Naturally, the data science field is constantly growing and more and more useful
    tools are becoming available every day. We cannot cover all of them, but below
    you can find a short list of libraries/tools that you might find useful in your
    projects:'
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，数据科学领域不断发展，每天都有越来越多有用的工具可供使用。我们无法涵盖所有工具，但在下面您可以找到一份简短的库/工具列表，您可能会在项目中找到有用的资源：
- en: '`DagsHub`—a platform similar to GitHub, but tailor-made for data scientists
    and machine learning practitioners. By integrating powerful open-source tools
    such as Git, DVC, MLFlow, and Label Studio and doing the DevOps heavy lifting
    for its users, you can easily build, manage and scale your ML project - all in
    one place.'
  id: totrans-1023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DagsHub`—一个类似于 GitHub 的平台，但专门为数据科学家和机器学习从业者量身定制。通过集成 Git、DVC、MLFlow 和 Label
    Studio 等强大的开源工具，并为用户完成 DevOps 的繁重工作，您可以轻松地构建、管理和扩展您的机器学习项目——一切都在一个地方。'
- en: '`deepchecks`—an open-source Python library for testing ML/DL models and data.
    We can use the library for various testing and validation needs throughout our
    projects; for example, we can verify our data’s integrity, inspect the features’
    and target’s distributions, confirm valid data splits, and evaluate the performance
    of our models.'
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deepchecks`—一个开源的 Python 库，用于测试机器学习/深度学习模型和数据。我们可以在整个项目中使用该库进行各种测试和验证需求；例如，我们可以验证数据的完整性，检查特征和目标的分布，确认数据分割是否有效，并评估模型的性能。'
- en: '`DVC`—an open-source version control system for ML projects. Using **DVC**
    (**data version control**), we can store the information about different versions
    of our data (be it tabular, images, or something else) and models in Git, while
    storing the actual data elsewhere (cloud storage like AWS, GCS, Google Drive,
    and so on). Using DVC, we can also create reproducible data pipelines, while storing
    the intermediate versions of the datasets along the way. And to make using it
    easier, DVC uses the same syntax as Git.'
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DVC`—一个开源的机器学习项目版本控制系统。通过**DVC**（**数据版本控制**），我们可以将不同版本的数据（无论是表格数据、图片还是其他类型）和模型信息存储在
    Git 中，同时将实际数据存储在其他地方（如 AWS、GCS、Google Drive 等云存储）。使用 DVC，我们还可以创建可复现的数据管道，同时存储数据集的中间版本。为了简化使用，DVC
    采用与 Git 相同的语法。'
- en: '`MLFlow`—an open-source platform for managing the ML life cycle. It covers
    aspects such as experimentation, reproducibility, deployment, and model registry.'
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MLFlow`—一个开源平台，用于管理机器学习生命周期。它涵盖了实验、可复现性、部署和模型注册等方面。'
- en: '`nannyML`—an open-source Python library for post-deployment data science. We
    can use it to identify data drift (a change in the distribution of the features
    between the data used for training a model and inference in production) or to
    estimate the model’s performance in the absence of ground truth. The latter one
    can be especially interesting for projects in which the ground truth becomes available
    after a long period of time, for example, a loan default within multiple months
    from the moment of making the prediction.'
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nannyML`—一个开源的 Python 库，用于部署后数据科学。我们可以使用它来识别数据漂移（训练模型时使用的数据与生产环境推断时特征分布的变化）或估计在没有真实标签的情况下模型的性能。后者对于那些真实标签在长时间后才可获得的项目尤其有趣，例如，预测贷款违约后几个月的情况。'
- en: '`pycaret`—an open-source, low-code Python library that automates a lot of the
    components of ML workflows. For example, we can train and tune dozens of machine
    learning models for a classification or regression task using as little as a few
    lines of code. It also contains separate modules for anomaly detection or time
    series forecasting.'
  id: totrans-1028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pycaret`—一个开源的低代码 Python 库，自动化机器学习工作流中的许多组件。例如，我们可以通过极少的代码训练和调优多个分类或回归任务的机器学习模型。它还包含用于异常检测或时间序列预测的独立模块。'
