["```py\nfrom statsmodels.api import \nX_ols = add_constant(X)\nmodel = OLS(y, X_ols).fit()\nmodel.summary()\n```", "```py\nbeta = np.linalg.inv(X_ols.T.dot(X_ols)).dot(X_ols.T.dot(y))\npd.Series(beta, index=X_ols.columns)\n\nconst   50.94\nX_1      1.08\nX_2      2.93\n\n```", "```py\nscaler = StandardScaler()\nX_ = scaler.fit_transform(X)\n```", "```py\nsgd = SGDRegressor(loss='squared_loss', fit_intercept=True, \n                   shuffle=True, random_state=42,  # shuffle training data for better gradient estimates\n                   learning_rate='invscaling',     # reduce learning rate over time\n                   eta0=0.01, power_t=0.25)        # parameters for learning rate path\n```", "```py\nsgd.fit(X=X_, y=y)\nresids = pd.DataFrame({'sgd': y - sgd.predict(X_),\n                      'ols': y - model.predict(sm.add_constant(X))})\nresids.pow(2).sum().div(len(y)).pow(.5)\n\nols   50.06\nsgd   50.06\n\n```", "```py\nimport pandas_datareader.data as web\nff_factor = 'F-F_Research_Data_5_Factors_2x3'\nff_factor_data = web.DataReader(ff_factor, 'famafrench', start='2010', end='2017-12')[0]\nff_factor_data.info()\n\nPeriodIndex: 96 entries, 2010-01 to 2017-12\nFreq: M\nData columns (total 6 columns):\nMkt-RF 96 non-null float64\nSMB 96 non-null float64\nHML 96 non-null float64\nRMW 96 non-null float64\nCMA 96 non-null float64\nRF 96 non-null float64\n```", "```py\nff_portfolio = '17_Industry_Portfolios'\nff_portfolio_data = web.DataReader(ff_portfolio, 'famafrench', start='2010', end='2017-12')[0]\nff_portfolio_data = ff_portfolio_data.sub(ff_factor_data.RF, axis=0)\nff_factor_data = ff_factor_data.drop('RF', axis=1)\nff_portfolio_data.info()\n\nPeriodIndex: 96 entries, 2010-01 to 2017-12\nFreq: M\nData columns (total 17 columns):\nFood     96 non-null float64\nMines    96 non-null float64\nOil      96 non-null float64\n...\nRtail    96 non-null float64\nFinan    96 non-null float64\nOther    96 non-null float64\n```", "```py\nbetas = []\nfor industry in ff_portfolio_data:\n    step1 = OLS(endog=ff_portfolio_data[industry],\n                exog=add_constant(ff_factor_data)).fit()\n    betas.append(step1.params.drop('const'))\n\nbetas = pd.DataFrame(betas,\n                     columns=ff_factor_data.columns,\n                     index=ff_portfolio_data.columns)\nbetas.info()\nIndex: 17 entries, Food  to Other\nData columns (total 5 columns):\nMkt-RF    17 non-null float64\nSMB       17 non-null float64\nHML       17 non-null float64\nRMW       17 non-null float64\nCMA       17 non-null float64\n```", "```py\nlambdas = []\nfor period in ff_portfolio_data.index:\n    step2 = OLS(endog=ff_portfolio_data.loc[period, betas.index],\n                exog=betas).fit()\n    lambdas.append(step2.params)\n\nlambdas = pd.DataFrame(lambdas,\n                       index=ff_portfolio_data.index,\n                       columns=betas.columns.tolist())\nlambdas.info()\nPeriodIndex: 96 entries, 2010-01 to 2017-12\nFreq: M\nData columns (total 5 columns):\nMkt-RF    96 non-null float64\nSMB       96 non-null float64\nHML       96 non-null float64\nRMW       96 non-null float64\nCMA       96 non-null float64\n```", "```py\nlambdas.mean()\nMkt-RF    1.201304\nSMB       0.190127\nHML      -1.306792\nRMW      -0.570817\nCMA      -0.522821\n```", "```py\nmodel = LinearFactorModel(portfolios=ff_portfolio_data, \n                          factors=ff_factor_data)\nres = model.fit()\n```", "```py\ndef Q100US():\n    return filters.make_us_equity_universe(\n        target_size=100,\n        rankby=factors.AverageDollarVolume(window_length=200),\n        mask=filters.default_us_equity_universe_mask(),\n        groupby=classifiers.fundamentals.Sector(),\n        max_group_weight=0.3,\n        smoothing_func=lambda f: f.downsample('month_start'),\n    )\n```", "```py\nlookahead = [1, 5, 10, 20]\nreturns = run_pipeline(Pipeline({'Returns{}D'.format(i): Returns(inputs=[USEquityPricing.close],\n                                          window_length=i+1, mask=UNIVERSE) for i in lookahead},\n                                screen=UNIVERSE),\n                       start_date=START,\n                       end_date=END)\nreturn_cols = ['Returns{}D'.format(i) for i in lookahead]\nreturns.info()\n\nMultiIndex: 50362 entries, (2014-01-02 00:00:00+00:00, Equity(24 [AAPL])) to (2015-12-31 00:00:00+00:00, Equity(47208 [GPRO]))\nData columns (total 4 columns):\nReturns10D    50362 non-null float64\nReturns1D     50362 non-null float64\nReturns20D    50360 non-null float64\nReturns5D     50362 non-null float64\n```", "```py\ndata = pd.concat([returns, value_factors, momentum_factors,\n                  quality_factors, payout_factors, growth_factors,\n                  efficiency_factors, risk_factors], axis=1).sortlevel()\ndata.index.names = ['date', 'asset']\ndata['stock'] = data.index.get_level_values('asset').map(lambda x: x.asset_name)\n```", "```py\nrows_before, cols_before = data.shape\ndata = (data\n        .dropna(axis=1, thresh=int(len(data) * .8))\n        .dropna(thresh=int(len(data.columns) * .8)))\ndata = data.fillna(data.median())\nrows_after, cols_after = data.shape\nprint('{:,d} rows and {:,d} columns dropped'.format(rows_before - rows_after, cols_before - cols_after))\n2,985 rows and 3 columns dropped\n```", "```py\ndata.sort_index(1).info()\n\nMultiIndex: 47377 entries, (2014-01-02, Equity(24 [AAPL])) to (2015-12-\n                            31, Equity(47208 [GPRO]))\nData columns (total 52 columns):\nAssetToEquityRatio             47377 non-null float64\nAssetTurnover                  47377 non-null float64\nCFO To Assets                  47377 non-null float64\n...\nWorkingCapitalToAssets         47377 non-null float64\nWorkingCapitalToSales          47377 non-null float64\nstock                          47377 non-null object\ndtypes: float64(51), object(1)\n```", "```py\ndf = pd.DataFrame({'categories': ['A','B', 'C']})\n\n  categories\n0          A\n1          B\n2          C\n\npd.get_dummies(df)\n\n   categories_A  categories_B  categories_C\n0             1             0             0\n1             0             1             0\n2             0             0             1\n```", "```py\npd.get_dummies(df, drop_first=True)\n\n   categories_B  categories_C\n0             0             0\n1             1             0\n2             0             1\n```", "```py\nX = pd.get_dummies(data.drop(return_cols, axis=1), drop_first=True)\nX.info()\n\nMultiIndex: 47377 entries, (2014-01-02 00:00:00+00:00, Equity(24 [AAPL])) to (2015-12-31 00:00:00+00:00, Equity(47208 [GPRO]))\nColumns: 181 entries, DividendYield to stock_YELP INC\ndtypes: float64(182)\nmemory usage: 66.1+ MB\n```", "```py\ny = data.loc[:, return_cols]\nshifted_y = []\nfor col in y.columns:\n    t = int(re.search(r'\\d+', col).group(0))\n    shifted_y.append(y.groupby(level='asset')['Returns{}D'.format(t)].shift(-t).to_frame(col))\ny = pd.concat(shifted_y, axis=1)\ny.info()\n\nMultiIndex: 47377 entries, (2014-01-02, Equity(24 [AAPL])) to (2015-12-31, Equity(47208 [GPRO]))\nData columns (total 4 columns):\nReturns1D     47242 non-null float64\nReturns5D     46706 non-null float64\nReturns10D    46036 non-null float64\nReturns20D    44696 non-null float64\ndtypes: float64(4)\n```", "```py\ntarget = 'Returns10D'\nmodel_data = pd.concat([y[[target]], X], axis=1).dropna()\nmodel_data = model_data[model_data[target].between(model_data[target].quantile(.025), \n                                                   model_data[target].quantile(.975))]\n\nmodel = OLS(endog=model_data[target], exog=model_data.drop(target, axis=1))\ntrained_model = model.fit()\ntrained_model.summary()\n```", "```py\ndef time_series_split(d=model_data, nfolds=5, min_train=21):\n    \"\"\"Generate train/test dates for nfolds \n    with at least min_train train obs\n    \"\"\"\n    train_dates = d[:min_train].tolist()\n    n = int(len(dates)/(nfolds + 1)) + 1\n    test_folds = [d[i:i + n] for i in range(min_train, len(d), n)]\n    for test_dates in test_folds:\n        if len(train_dates) > min_train:\n            yield train_dates, test_dates\n        train_dates.extend(test_dates)\n```", "```py\ntarget = 'Returns10D'\noutliers = .01\nmodel_data = pd.concat([y[[target]], X], axis=1).dropna().reset_index('asset', drop=True)\nmodel_data = model_data[model_data[target].between(*model_data[target].quantile([outliers, 1-outliers]).values)]\n\nmodel_data[target] = np.log1p(model_data[target])\nfeatures = model_data.drop(target, axis=1).columns\ndates = model_data.index.unique()\n\nDatetimeIndex: 45114 entries, 2014-01-02 to 2015-12-16\nColumns: 183 entries, Returns10D to stock_YELP INC\ndtypes: float64(183)\n```", "```py\nnfolds = 250\nlr = LinearRegression()\n\ntest_results, result_idx, preds = [], [], pd.DataFrame()\nfor train_dates, test_dates in time_series_split(dates, nfolds=nfolds):\n    X_train = model_data.loc[idx[train_dates], features]\n    y_train = model_data.loc[idx[train_dates], target]\n    lr.fit(X=X_train, y=y_train)\n\n    X_test = model_data.loc[idx[test_dates], features]\n    y_test = model_data.loc[idx[test_dates], target]\n    y_pred = lr.predict(X_test)\n\n    rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_test))\n    ic, pval = spearmanr(y_pred, y_test)\n\n    test_results.append([rmse, ic, pval])\n    preds = preds.append(y_test.to_frame('actuals').assign(predicted=y_pred))\n    result_idx.append(train_dates[-1])\n```", "```py\nfig, axes = plt.subplots(nrows=2)\nrolling_result = test_result.rolling(21).mean()\nrolling_result[['ic', 'pval']].plot(ax=axes[0], title='Information Coefficient')\naxes[0].axhline(test_result.ic.mean(), lw=1, ls='--', color='k')\nrolling_result[['rmse']].plot(ax=axes[1], title='Root Mean Squared Error')\naxes[1].axhline(test_result.rmse.mean(), lw=1, ls='--', color='k')\n```", "```py\nnfolds = 250\nalphas = np.logspace(-5, 5, 21)\nscaler = StandardScaler()\n\nridge_result, ridge_coeffs = pd.DataFrame(), pd.DataFrame()\nfor i, alpha in enumerate(alphas):\n    coeffs, test_results = [], []\n    lr_ridge = Ridge(alpha=alpha)\n    for train_dates, test_dates in time_series_split(dates, nfolds=nfolds):\n        X_train = model_data.loc[idx[train_dates], features]\n        y_train = model_data.loc[idx[train_dates], target]\n        lr_ridge.fit(X=scaler.fit_transform(X_train), y=y_train)\n        coeffs.append(lr_ridge.coef_)\n\n        X_test = model_data.loc[idx[test_dates], features]\n        y_test = model_data.loc[idx[test_dates], target]\n        y_pred = lr_ridge.predict(scaler.transform(X_test))\n\n        rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_test))\n        ic, pval = spearmanr(y_pred, y_test)\n\n        test_results.append([train_dates[-1], rmse, ic, pval, alpha])\n    test_results = pd.DataFrame(test_results, columns=['date', 'rmse', 'ic', 'pval', 'alpha'])\n    ridge_result = ridge_result.append(test_results)\n    ridge_coeffs[alpha] = np.mean(coeffs, axis=0)\n```", "```py\nnfolds = 250\nalphas = np.logspace(-8, -2, 13)\nscaler = StandardScaler()\n\nlasso_results, lasso_coeffs = pd.DataFrame(), pd.DataFrame()\nfor i, alpha in enumerate(alphas):\n    coeffs, test_results = [], []\n    lr_lasso = Lasso(alpha=alpha)\n    for i, (train_dates, test_dates) in enumerate(time_series_split(dates, nfolds=nfolds)):\n        X_train = model_data.loc[idx[train_dates], features]\n        y_train = model_data.loc[idx[train_dates], target]\n        lr_lasso.fit(X=scaler.fit_transform(X_train), y=y_train)\n\n        X_test = model_data.loc[idx[test_dates], features]\n        y_test = model_data.loc[idx[test_dates], target]\n        y_pred = lr_lasso.predict(scaler.transform(X_test))\n\n        rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_test))\n        ic, pval = spearmanr(y_pred, y_test)\n\n        coeffs.append(lr_lasso.coef_)\n        test_results.append([train_dates[-1], rmse, ic, pval, alpha])\n    test_results = pd.DataFrame(test_results, columns=['date', 'rmse', 'ic', 'pval', 'alpha'])\n    lasso_results = lasso_results.append(test_results)\n    lasso_coeffs[alpha] = np.mean(coeffs, axis=0)\n```", "```py\nimport statsmodels.api as sm\n\ndata = pd.get_dummies(data.drop(drop_cols, axis=1), columns=['quarter'], drop_first=True).dropna()\nmodel = sm.Logit(data.target, sm.add_constant(data.drop('target', axis=1)))\nresult = model.fit()\nresult.summary()\n```", "```py\ntarget = 'Returns10D'\nlabel = (y[target] > 0).astype(int).to_frame(target)\n```", "```py\nnfolds = 250\nCs = np.logspace(-5, 5, 11)\nscaler = StandardScaler()\n\nlogistic_results, logistic_coeffs = pd.DataFrame(), pd.DataFrame()\nfor C in Cs:\n    coeffs = []\n    log_reg = LogisticRegression(C=C)\n    for i, (train_dates, test_dates) in enumerate(time_series_split(dates, nfolds=nfolds)):\n        X_train = model_data.loc[idx[train_dates], features]\n        y_train = model_data.loc[idx[train_dates], target]\n        log_reg.fit(X=scaler.fit_transform(X_train), y=y_train)\n\n        X_test = model_data.loc[idx[test_dates], features]\n        y_test = model_data.loc[idx[test_dates], target]\n        y_pred = log_reg.predict_proba(scaler.transform(X_test))[:, 1]\n\n        coeffs.append(log_reg.coef_.squeeze())\n        logistic_results = (logistic_results\n                            .append(y_test\n                                    .to_frame('actuals')\n                                    .assign(predicted=y_pred, C=C)))\n    logistic_coeffs[C] = np.mean(coeffs, axis=0)\n```", "```py\nauc_by_C = logistic_results.groupby('C').apply(lambda x: roc_auc_score(y_true=x.actuals.astype(int), \n                                                         y_score=x.predicted))\n```"]