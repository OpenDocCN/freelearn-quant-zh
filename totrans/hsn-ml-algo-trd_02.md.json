["```py\n8=FIX.5.0|9=127|35=A|59=theBroker.123456|56=CSERVER|34=1|32=20180117- 08:03:04|57=TRADE|50=any_string|98=2|108=34|141=Y|553=12345|554=passw0rd!|10=131|\n```", "```py\nformats = {\n    ('integer', 2): 'H',  # int of length 2 => format string 'H'\n    ('integer', 4): 'I',\n    ('integer', 6): '6s', # int of length 6 => parse as string, \n      convert later\n    ('integer', 8): 'Q',\n    ('alpha', 1)  : 's',\n    ('alpha', 2)  : '2s',\n    ('alpha', 4)  : '4s',\n    ('alpha', 8)  : '8s',\n    ('price_4', 4): 'I',\n    ('price_8', 8): 'Q',\n}\n```", "```py\n# Get ITCH specs and create formatting (type, length) tuples\nspecs = pd.read_csv('message_types.csv')\nspecs['formats'] = specs[['value', 'length']].apply(tuple, \n                           axis=1).map(formats)\n\n# Formatting for alpha fields\nalpha_fields = specs[specs.value == 'alpha'].set_index('name')\nalpha_msgs = alpha_fields.groupby('message_type')\nalpha_formats = {k: v.to_dict() for k, v in alpha_msgs.formats}\nalpha_length = {k: v.add(5).to_dict() for k, v in alpha_msgs.length}\n\n# Generate message classes as named tuples and format strings\nmessage_fields, fstring = {}, {}\nfor t, message in specs.groupby('message_type'):\n    message_fields[t] = namedtuple(typename=t, field_names=message.name.tolist())\n    fstring[t] = '>' + ''.join(message.formats.tolist())\n```", "```py\ndef format_alpha(mtype, data):\n    for col in alpha_formats.get(mtype).keys():\n        if mtype != 'R' and col == 'stock': # stock name only in \n                                              summary message 'R'\n            data = data.drop(col, axis=1)\n            continue\n        data.loc[:, col] = data.loc[:, col].str.decode(\"utf-\n                                    8\").str.strip()\n        if encoding.get(col):\n            data.loc[:, col] = data.loc[:, \n                     col].map(encoding.get(col)) # int encoding\n    return data\n```", "```py\nwith (data_path / file_name).open('rb') as data:\n    while True:\n        message_size = int.from_bytes(data.read(2), byteorder='big', \n                       signed=False)\n        message_type = data.read(1).decode('ascii')\n        message_type_counter.update([message_type])\n        record = data.read(message_size - 1)\n        message = message_fields[message_type]._make(unpack(fstring[message_type],  \n                                   record))\n        messages[message_type].append(message)\n\n        # deal with system events like market open/close\n        if message_type == 'S':\n            timestamp = int.from_bytes(message.timestamp, \n                                       byteorder='big')\n            if message.event_code.decode('ascii') == 'C': # close\n                store_messages(messages)\n                break\n```", "```py\nwith pd.HDFStore(hdf_store) as store:\n    stocks = store['R'].loc[:, ['stock_locate', 'stock']]\n    trades = store['P'].append(store['Q'].rename(columns=\n                        {'cross_price': 'price'}).merge(stocks)\ntrades['value'] = trades.shares.mul(trades.price)\ntrades['value_share'] = trades.value.div(trades.value.sum())\ntrade_summary = \n    trades.groupby('stock').value_share.sum().sort_values\n                            (ascending=False)\ntrade_summary.iloc[:50].plot.bar(figsize=(14, 6), color='darkblue', \n                                 title='% of Traded Value')\nplt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: \n                                    '{:.0%}'.format(y)))\n```", "```py\ndef get_messages(date, stock=stock):\n    \"\"\"Collect trading messages for given stock\"\"\"\n    with pd.HDFStore(itch_store) as store:\n        stock_locate = store.select('R', where='stock = \n                                     stock').stock_locate.iloc[0]\n        target = 'stock_locate = stock_locate'\n\n        data = {}\n        # relevant message types\n        messages = ['A', 'F', 'E', 'C', 'X', 'D', 'U', 'P', 'Q']\n        for m in messages:\n            data[m] = store.select(m,  \n              where=target).drop('stock_locate', axis=1).assign(type=m)\n\n    order_cols = ['order_reference_number', 'buy_sell_indicator', \n                  'shares', 'price']\n    orders = pd.concat([data['A'], data['F']], sort=False,  \n                        ignore_index=True).loc[:, order_cols]\n\n    for m in messages[2: -3]:\n        data[m] = data[m].merge(orders, how='left')\n\n    data['U'] = data['U'].merge(orders, how='left',\n                                right_on='order_reference_number',\n                                left_on='original_order_reference_number',\n                                suffixes=['', '_replaced'])\n\n    data['Q'].rename(columns={'cross_price': 'price'}, inplace=True)\n    data['X']['shares'] = data['X']['cancelled_shares']\n    data['X'] = data['X'].dropna(subset=['price'])\n\n    data = pd.concat([data[m] for m in messages], ignore_index=True, \n                      sort=False)\n```", "```py\ndef get_trades(m):\n    \"\"\"Combine C, E, P and Q messages into trading records\"\"\"\n    trade_dict = {'executed_shares': 'shares', 'execution_price': \n                  'price'}\n    cols = ['timestamp', 'executed_shares']\n    trades = pd.concat([m.loc[m.type == 'E', cols + \n             ['price']].rename(columns=trade_dict),\n             m.loc[m.type == 'C', cols + \n             ['execution_price']].rename(columns=trade_dict),\n             m.loc[m.type == 'P', ['timestamp', 'price', 'shares']],\n             m.loc[m.type == 'Q', ['timestamp', 'price', \n             'shares']].assign(cross=1),\n             ], sort=False).dropna(subset=['price']).fillna(0)\n    return trades.set_index('timestamp').sort_index().astype(int)\n```", "```py\ndef add_orders(orders, buysell, nlevels):\n    new_order = []\n    items = sorted(orders.copy().items())\n    if buysell == -1:\n        items = reversed(items)  \n    for i, (p, s) in enumerate(items, 1):\n        new_order.append((p, s))\n        if i == nlevels:\n            break\n    return orders, new_order\n```", "```py\nfor message in messages.itertuples():\n    i = message[0]\n    if np.isnan(message.buy_sell_indicator):\n        continue\n    message_counter.update(message.type)\n\n    buysell = message.buy_sell_indicator\n    price, shares = None, None\n\n    if message.type in ['A', 'F', 'U']:\n        price, shares = int(message.price), int(message.shares)\n\n        current_orders[buysell].update({price: shares})\n        current_orders[buysell], new_order = \n          add_orders(current_orders[buysell], buysell, nlevels)\n        order_book[buysell][message.timestamp] = new_order\n\n    if message.type in ['E', 'C', 'X', 'D', 'U']:\n        if message.type == 'U':\n            if not np.isnan(message.shares_replaced):\n                price = int(message.price_replaced)\n                shares = -int(message.shares_replaced)\n        else:\n            if not np.isnan(message.price):\n                price = int(message.price)\n                shares = -int(message.shares)\n\n        if price is not None:\n            current_orders[buysell].update({price: shares})\n            if current_orders[buysell][price] <= 0:\n                current_orders[buysell].pop(price)\n            current_orders[buysell], new_order = \n              add_orders(current_orders[buysell], buysell, nlevels)\n            order_book[buysell][message.timestamp] = new_order\n\n```", "```py\nstock, date = 'AAPL', '20180329'\ntitle = '{} | {}'.format(stock, pd.to_datetime(date).date()\n\nwith pd.HDFStore(itch_store) as store:\n    s = store['S'].set_index('event_code') # system events\n    s.timestamp = s.timestamp.add(pd.to_datetime(date)).dt.time\n    market_open = s.loc['Q', 'timestamp'] \n    market_close = s.loc['M', 'timestamp']\n\nwith pd.HDFStore(stock_store) as store:\n    trades = store['{}/trades'.format(stock)].reset_index()\ntrades = trades[trades.cross == 0] # excluding data from open/close crossings\ntrades.price = trades.price.mul(1e-4)\n\ntrades.price = trades.price.mul(1e-4) # format price\ntrades = trades[trades.cross == 0]    # exclude crossing trades\ntrades = trades.between_time(market_open, market_close) # market hours only\n\ntick_bars = trades.set_index('timestamp')\ntick_bars.index = tick_bars.index.time\ntick_bars.price.plot(figsize=(10, 5), title=title), lw=1)\n```", "```py\nfrom scipy.stats import normaltest\nnormaltest(tick_bars.price.pct_change().dropna())\n\nNormaltestResult(statistic=62408.76562431228, pvalue=0.0)\n```", "```py\ndef get_bar_stats(agg_trades):\n    vwap = agg_trades.apply(lambda x: np.average(x.price, \n           weights=x.shares)).to_frame('vwap')\n    ohlc = agg_trades.price.ohlc()\n    vol = agg_trades.shares.sum().to_frame('vol')\n    txn = agg_trades.shares.size().to_frame('txn')\n    return pd.concat([ohlc, vwap, vol, txn], axis=1)\n\nresampled = trades.resample('1Min')\ntime_bars = get_bar_stats(resampled)\n```", "```py\ndef price_volume(df, price='vwap', vol='vol', suptitle=title):\n    fig, axes = plt.subplots(nrows=2, sharex=True, figsize=(15, 8))\n    axes[0].plot(df.index, df[price])\n    axes[1].bar(df.index, df[vol], width=1 / (len(df.index)), \n                color='r')\n\n    xfmt = mpl.dates.DateFormatter('%H:%M')\n    axes[1].xaxis.set_major_locator(mpl.dates.HourLocator(interval=3))\n    axes[1].xaxis.set_major_formatter(xfmt)\n    axes[1].get_xaxis().set_tick_params(which='major', pad=25)\n    axes[0].set_title('Price', fontsize=14)\n    axes[1].set_title('Volume', fontsize=14)\n    fig.autofmt_xdate()\n    fig.suptitle(suptitle)\n    fig.tight_layout()\n    plt.subplots_adjust(top=0.9)\n\nprice_volume(time_bars)\n```", "```py\nresampled = trades.resample('5Min') # 5 Min bars for better print\ndf = get_bar_stats(resampled)\n\nincrease = df.close > df.open\ndecrease = df.open > df.close\nw = 2.5 * 60 * 1000 # 2.5 min in ms\n\nWIDGETS = \"pan, wheel_zoom, box_zoom, reset, save\"\n\np = figure(x_axis_type='datetime', tools=WIDGETS, plot_width=1500, title = \"AAPL Candlestick\")\np.xaxis.major_label_orientation = pi/4\np.grid.grid_line_alpha=0.4\n\np.segment(df.index, df.high, df.index, df.low, color=\"black\")\np.vbar(df.index[increase], w, df.open[increase], df.close[increase], fill_color=\"#D5E1DD\", line_color=\"black\")\np.vbar(df.index[decrease], w, df.open[decrease], df.close[decrease], fill_color=\"#F2583E\", line_color=\"black\")\nshow(p)\n\n```", "```py\ntrades_per_min = trades.shares.sum()/(60*7.5) # min per trading day\ntrades['cumul_vol'] = trades.shares.cumsum()\ndf = trades.reset_index()\nby_vol = \n   df.groupby(df.cumul_vol.div(trades_per_min).round().astype(int))\nvol_bars = pd.concat([by_vol.timestamp.last().to_frame('timestamp'), \n                      get_bar_stats(by_vol)], axis=1)\nprice_volume(vol_bars.set_index('timestamp'))\n```", "```py\nsp_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\nsp = pd.read_html(sp_url, header=0)[0] # returns a list for each table\nsp.info()\n\nRangeIndex: 505 entries, 0 to 504\nData columns (total 9 columns):\nTicker symbol             505 non-null object\nSecurity                  505 non-null object\nSEC filings               505 non-null object\nGICS Sector               505 non-null object\nGICS Sub Industry         505 non-null object\nLocation                  505 non-null object\nDate first added[3][4]    398 non-null object\nCIK                       505 non-null int64\nFounded                   139 non-null object\n```", "```py\nimport pandas_datareader.data as web\nfrom datetime import datetime\n\nstart = '2014'              # accepts strings\nend = datetime(2017, 5, 24) # or datetime objects\n\nyahoo= web.DataReader('FB', 'yahoo', start=start, end=end)\nyahoo.info()\n\nDatetimeIndex: 856 entries, 2014-01-02 to 2017-05-25\nData columns (total 6 columns):\nHigh         856 non-null float64\nLow          856 non-null float64\nOpen         856 non-null float64\nClose        856 non-null float64\nVolume       856 non-null int64\nAdj Close    856 non-null float64\n\ndtypes: float64(5), int64(1)\n```", "```py\nbook = web.get_iex_book('AAPL')\norders = pd.concat([pd.DataFrame(book[side]).assign(side=side) for side in ['bids', 'asks']])\norders.sort_values('timestamp').head()\n\n  price  size timestamp      side\n4 140.00  100  1528983003604 bids\n3 175.30  100  1528983900163 bids\n3 205.80  100  1528983900163 asks\n1 187.00  200  1528996876005 bids\n2 186.29  100  1528997296755 bids\n```", "```py\n%load_ext zipline\n%%zipline --start 2010-1-1 --end 2018-1-1 --data-frequency daily\nfrom zipline.api import order_target, record, symbol\n\ndef initialize(context):\n context.i = 0\n context.assets = [symbol('FB'), symbol('GOOG'), symbol('AMZN')]\n\ndef handle_data(context, data):\n df = data.history(context.assets, fields=['price', 'volume'], \n                   bar_count=1, frequency=\"1d\")\n df = df.to_frame().reset_index()\n\n if context.i == 0:\n df.columns = ['date', 'asset', 'price', 'volumne']\n df.to_csv('stock_data.csv', index=False)\n else:\n     df.to_csv('stock_data.csv', index=False, mode='a', header=None)\n                context.i += 1\n\ndf = pd.read_csv('stock_data.csv')\ndf.date = pd.to_datetime(df.date)\ndf.set_index('date').groupby('asset').price.plot(lw=2, legend=True, \n       figsize=(14, 6));\n```", "```py\nimport quandl\noil = quandl.get('EIA/PET_RWTC_D').squeeze()\noil.plot(lw=2, title='WTI Crude Oil Price')\n```", "```py\nSEC_URL = 'https://www.sec.gov/files/dera/data/financial-statement-and-notes-data-sets/'\n\nfirst_year, this_year, this_quarter = 2014, 2018, 3\npast_years = range(2014, this_year)\nfiling_periods = [(y, q) for y in past_years for q in range(1, 5)]\nfiling_periods.extend([(this_year, q) for q in range(1, this_quarter + \n                                                     1)])\nfor i, (yr, qtr) in enumerate(filing_periods, 1):\n    filing = f'{yr}q{qtr}_notes.zip'\n    path = data_path / f'{yr}_{qtr}' / 'source'\n    response = requests.get(SEC_URL + filing).content\n    with ZipFile(BytesIO(response)) as zip_file:\n        for file in zip_file.namelist():\n            local_file = path / file\n            with local_file.open('wb') as output:\n                for line in zip_file.open(file).readlines():\n                    output.write(line)\n```", "```py\nfor f in data_path.glob('**/*.tsv'):\n    file_name = f.stem  + '.parquet'\n    path = Path(f.parents[1]) / 'parquet'\n    df = pd.read_csv(f, sep='\\t', encoding='latin1', low_memory=False)\n    df.to_parquet(path / file_name)\n```", "```py\napple = sub[sub.name == 'APPLE INC'].T.dropna().squeeze()\nkey_cols = ['name', 'adsh', 'cik', 'name', 'sic', 'countryba',  \n            'stprba', 'cityba', 'zipba', 'bas1', 'form', 'period', \n            'fy', 'fp', 'filed']\napple.loc[key_cols]\n\nname                    APPLE INC\nadsh                    0000320193-18-000070\ncik                     320193\nname                    APPLE INC\nsic                     3571\ncountryba               US\nstprba                  CA\ncityba                  CUPERTINO\nzipba                   95014\nbas1                    ONE APPLE PARK WAY\nform                    10-Q\nperiod                  20180331\nfy                      2018\nfp                      Q2\nfiled                   20180502\n```", "```py\naapl_subs = pd.DataFrame()\nfor sub in data_path.glob('**/sub.parquet'):\n    sub = pd.read_parquet(sub)\n    aapl_sub = sub[(sub.cik.astype(int) == apple.cik) & (sub.form.isin(['10-Q', '10-K']))]\n    aapl_subs = pd.concat([aapl_subs, aapl_sub])\n\naapl_subs.form.value_counts()\n10-Q    15\n10-K     4\n```", "```py\naapl_nums = pd.DataFrame()\nfor num in data_path.glob('**/num.parquet'):\n    num = pd.read_parquet(num)\n    aapl_num = num[num.adsh.isin(aapl_subs.adsh)]\n    aapl_nums = pd.concat([aapl_nums, aapl_num])\n\naapl_nums.ddate = pd.to_datetime(aapl_nums.ddate, format='%Y%m%d')    \naapl_nums.shape\n(28281, 16)\n```", "```py\nfield = 'EarningsPerShareDiluted'\nstock_split = 7\nsplit_date = pd.to_datetime('20140604')\n\n# Filter by tag; keep only values measuring 1 quarter\neps = aapl_nums[(aapl_nums.tag == 'EarningsPerShareDiluted')\n                & (aapl_nums.qtrs == 1)].drop('tag', axis=1)\n\n# Keep only most recent data point from each filing\neps = eps.groupby('adsh').apply(lambda x: x.nlargest(n=1, columns=['ddate']))\n\n# Adjust earnings prior to stock split downward\neps.loc[eps.ddate < split_date,'value'] = eps.loc[eps.ddate < \n        split_date, 'value'].div(7)\neps = eps[['ddate', 'value']].set_index('ddate').squeeze()\neps = eps.rolling(4, min_periods=4).sum().dropna() # create trailing \n                  12-months eps from quarterly data\n```", "```py\nimport pandas_datareader.data as web\nsymbol = 'AAPL.US'\naapl_stock = web.DataReader(symbol, 'quandl', start=eps.index.min())\naapl_stock = aapl_stock.resample('D').last() # ensure dates align with \n                                               eps data\n```", "```py\npe = aapl_stock.AdjClose.to_frame('price').join(eps.to_frame('eps'))\npe = pe.fillna(method='ffill').dropna()\npe['P/E Ratio'] = pe.price.div(pe.eps)\naxes = pe.plot(subplots=True, figsize=(16,8), legend=False, lw=2);\n```"]