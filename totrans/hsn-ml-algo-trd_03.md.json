["```py\nfrom bs4 import BeautifulSoup\nimport requests\n\n# set and request url; extract source code\nurl = \"https://www.opentable.com/new-york-restaurant-listings\"\nhtml = requests.get(url)\nhtml.text[:500]\n\n' <!DOCTYPE html><html lang=\"en\"><head><meta charset=\"utf-8\"/><meta http-equiv=\"X-UA-Compatible\" content=\"IE=9; IE=8; IE=7; IE=EDGE\"/> <title>Restaurant Reservation Availability</title> <meta name=\"robots\" content=\"noindex\" > </meta> <link rel=\"shortcut icon\" href=\"//components.otstatic.com/components/favicon/1.0.4/favicon/favicon.ico\" type=\"image/x-icon\"/><link rel=\"icon\" href=\"//components.otstatic.com/components/favicon/1.0.4/favicon/favicon-16.png\" sizes=\"16x16\"/><link rel='\n```", "```py\n# parse raw html => soup object\nsoup = BeautifulSoup(html.text, 'html.parser')\n\n# for each span tag, print out text => restaurant name\nfor entry in soup.find_all(name='span', attrs={'class':'rest-row-name-\ntext'}):\n    print(entry.text)\n\nWade Coves\nAlley\nDolorem Maggio\nIslands\n...\n```", "```py\n# get the number of dollars signs for each restaurant\nfor entry in soup.find_all('div', {'class':'rest-row-pricing'}):\n    price = entry.find('i').text\n```", "```py\nsoup.find_all('div', {'class':'booking'})\n[]\n```", "```py\nfrom selenium import webdriver\n\n# create a driver called Firefox\ndriver = webdriver.Firefox()\n```", "```py\n# close it\ndriver.close()\n```", "```py\nimport time, re\n\n# visit the opentable listing page\ndriver = webdriver.Firefox()\ndriver.get(url)\n\ntime.sleep(1) # wait 1 second\n\n# retrieve the html source\nhtml = driver.page_source\nhtml = BeautifulSoup(html, \"lxml\")\n\nfor booking in html.find_all('div', {'class': 'booking'}):\n    match = re.search(r'\\d+', booking.text)\n    if match:\n        print(match.group())\n```", "```py\ndef parse_html(html):\n    data, item = pd.DataFrame(), {}\n    soup = BeautifulSoup(html, 'lxml')\n    for i, resto in enumerate(soup.find_all('div', class_='rest-row-\n           info')):\n        item['name'] = resto.find('span', class_='rest-row-name-\n                                   text').text\n\n        booking = resto.find('div', class_='booking')\n        item['bookings'] = re.search('\\d+', booking.text).group() if \n                                       booking else 'NA'\n\n        rating = resto.select('div.all-stars.filled')\n        item['rating'] = int(re.search('\\d+', \n                rating[0].get('style')).group()) if rating else 'NA'\n\n        reviews = resto.find('span', class_='star-rating-text--review-\n                              text')\n        item['reviews'] = int(re.search('\\d+', reviews.text).group()) if reviews else 'NA'\n\n        item['price'] = int(resto.find('div', class_='rest-row-\n                            pricing').find('i').text.count('$'))\n        item['cuisine'] = resto.find('span', class_='rest-row-meta--\n                                      cuisine').text\n        item['location'] = resto.find('span', class_='rest-row-meta--\n                                       location').text\n        data[i] = pd.Series(item)\n    return data.T\n```", "```py\nrestaurants = pd.DataFrame()\ndriver = webdriver.Firefox()\nurl = \"https://www.opentable.com/new-york-restaurant-listings\"\ndriver.get(url)\nwhile True:\n    sleep(1)\n    new_data = parse_html(driver.page_source)\n    if new_data.empty:\n        break\n    restaurants = pd.concat([restaurants, new_data], ignore_index=True)\n    print(len(restaurants))\n    driver.find_element_by_link_text('Next').click()\ndriver.close()\n```", "```py\nfrom opentable.items import OpentableItem\nfrom scrapy import Spider\nfrom scrapy_splash import SplashRequest\n\nclass OpenTableSpider(Spider):\n    name = 'opentable'\n    start_urls = ['https://www.opentable.com/new-york-restaurant-\n                   listings']\n\n    def start_requests(self):\n        for url in self.start_urls:\n            yield SplashRequest(url=url,\n                                callback=self.parse,\n                                endpoint='render.html',\n                                args={'wait': 1},\n                                )\n\n    def parse(self, response):\n        item = OpentableItem()\n        for resto in response.css('div.rest-row-info'):\n            item['name'] = resto.css('span.rest-row-name-\n                                      text::text').extract()\n            item['bookings'] = \n                  resto.css('div.booking::text').re(r'\\d+')\n            item['rating'] = resto.css('div.all-\n                  stars::attr(style)').re_first('\\d+')\n            item['reviews'] = resto.css('span.star-rating-text--review-\n                                         text::text').re_first(r'\\d+')\n            item['price'] = len(resto.css('div.rest-row-pricing > \n                                i::text').re('\\$'))\n            item['cuisine'] = resto.css('span.rest-row-meta--\n                                         cuisine::text').extract()\n            item['location'] = resto.css('span.rest-row-meta--\n                               location::text').extract()\n            yield item\n```", "```py\nimport re\nfrom pathlib import Path\nfrom time import sleep\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nfrom furl import furl\nfrom selenium import webdriver\n\ntranscript_path = Path('transcripts')\n\nSA_URL = 'https://seekingalpha.com/'\nTRANSCRIPT = re.compile('Earnings Call Transcript')\n\nnext_page = True\npage = 1\ndriver = webdriver.Firefox()\nwhile next_page:\n    url = f'{SA_URL}/earnings/earnings-call-transcripts/{page}'\n    driver.get(urljoin(SA_URL, url))\n    response = driver.page_source\n    page += 1\n    soup = BeautifulSoup(response, 'lxml')\n    links = soup.find_all(name='a', string=TRANSCRIPT)\n    if len(links) == 0:\n        next_page = False\n    else:\n        for link in links:\n            transcript_url = link.attrs.get('href')\n            article_url = furl(urljoin(SA_URL, \n                           transcript_url)).add({'part': 'single'})\n            driver.get(article_url.url)\n            html = driver.page_source\n            meta, participants, content = parse_html(html)\n            meta['link'] = link\n\ndriver.close()\n```", "```py\ndef parse_html(html):\n    date_pattern = re.compile(r'(\\d{2})-(\\d{2})-(\\d{2})')\n    quarter_pattern = re.compile(r'(\\bQ\\d\\b)')\n    soup = BeautifulSoup(html, 'lxml')\n\n    meta, participants, content = {}, [], []\n    h1 = soup.find('h1', itemprop='headline').text\n    meta['company'] = h1[:h1.find('(')].strip()\n    meta['symbol'] = h1[h1.find('(') + 1:h1.find(')')]\n\n    title = soup.find('div', class_='title').text\n    match = date_pattern.search(title)\n    if match:\n        m, d, y = match.groups()\n        meta['month'] = int(m)\n        meta['day'] = int(d)\n        meta['year'] = int(y)\n\n    match = quarter_pattern.search(title)\n    if match:\n        meta['quarter'] = match.group(0)\n\n    qa = 0\n    speaker_types = ['Executives', 'Analysts']\n    for header in [p.parent for p in soup.find_all('strong')]:\n        text = header.text.strip()\n        if text.lower().startswith('copyright'):\n            continue\n        elif text.lower().startswith('question-and'):\n            qa = 1\n            continue\n        elif any([type in text for type in speaker_types]):\n            for participant in header.find_next_siblings('p'):\n                if participant.find('strong'):\n                    break\n                else:\n                    participants.append([text, participant.text])\n        else:\n            p = []\n            for participant in header.find_next_siblings('p'):\n                if participant.find('strong'):\n                    break\n                else:\n                    p.append(participant.text)\n            content.append([header.text, qa, '\\n'.join(p)])\n    return meta, participants, content\n```", "```py\ndef store_result(meta, participants, content):\n    path = transcript_path / 'parsed' / meta['symbol']\n    if not path.exists():\n        path.mkdir(parents=True, exist_ok=True)\n    pd.DataFrame(content, columns=['speaker', 'q&a', \n              'content']).to_csv(path / 'content.csv', index=False)\n    pd.DataFrame(participants, columns=['type', 'name']).to_csv(path / \n                 'participants.csv', index=False)\n    pd.Series(meta).to_csv(path / 'earnings.csv'\n```"]