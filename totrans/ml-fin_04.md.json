["```py\ntrain = pd.read_csv('../input/train_1.csv').fillna(0)\ntrain.head()\n```", "```py\ndef parse_page(page):\n    x = page.split('_')\n    return ' '.join(x[:-3]), x[-3], x[-2], x[-1]\n```", "```py\nparse_page(train.Page[0])\n```", "```py\nOut:\n('2NE1', 'zh.wikipedia.org', 'all-access', 'spider')\n\n```", "```py\nl = list(train.Page.apply(parse_page))\ndf = pd.DataFrame(l)\ndf.columns = ['Subject','Sub_Page','Access','Agent']\n```", "```py\ntrain = pd.concat([train,df],axis=1)\ndel train['Page']\n```", "```py\ntrain.Sub_Page.value_counts().plot(kind='bar')\n```", "```py\ntrain.Access.value_counts().plot(kind='bar')\n```", "```py\ntrain.Agent.value_counts().plot(kind='bar')\n```", "```py\nidx = 39457\n\nwindow = 10\n\ndata = train.iloc[idx,0:-4]\nname = train.iloc[idx,-4]\ndays = [r for r in range(data.shape[0] )]\n\nfig, ax = plt.subplots(figsize=(10, 7))\n\nplt.ylabel('Views per Page')\nplt.xlabel('Day')\nplt.title(name)\n\nax.plot(days,data.values,color='grey')\nax.plot(np.convolve(data, \n                    np.ones((window,))/window, \n                    mode='valid'),color='black')\n\nax.set_yscale('log')\n```", "```py\nfig, ax = plt.subplots(figsize=(10, 7))\nplt.ylabel('Views per Page')\nplt.xlabel('Day')\nplt.title('Twenty One Pilots Popularity')\nax.set_yscale('log')\n\nfor country in ['de','en','es','fr','ru']:\n    idx= np.where((train['Subject'] == 'Twenty One Pilots') \n                  & (train['Sub_Page'] == '{}.wikipedia.org'.format(country)) & (train['Access'] == 'all-access') & (train['Agent'] == 'all-agents'))\n\n    idx=idx[0][0]\n\n    data = train.iloc[idx,0:-4]\n    handle = ax.plot(days,data.values,label=country)\n\nax.legend()\n```", "```py\ntime = np.linspace(0,10,1000)\nseries = time\nseries = series + np.random.randn(1000) *0.2\n\nmdl = sm.OLS(time, series).fit()\ntrend = mdl.predict(time)\n```", "```py\ndata = train.iloc[:,0:-4]\nfft_complex = fft(data)\nfft_mag = [np.sqrt(np.real(x)*np.real(x)+np.imag(x)*np.imag(x)) for x in fft_complex]\n```", "```py\narr = np.array(fft_mag)\nfft_mean = np.mean(arr,axis=0)\n```", "```py\nfft_xvals = [day / fft_mean.shape[0] for day in range(fft_mean.shape[0])]\n```", "```py\nnpts = len(fft_xvals) // 2 + 1\nfft_mean = fft_mean[:npts]\nfft_xvals = fft_xvals[:npts]\n```", "```py\nfig, ax = plt.subplots(figsize=(10, 7))\nax.plot(fft_xvals[1:],fft_mean[1:])\nplt.axvline(x=1./7,color='red',alpha=0.3)\nplt.axvline(x=2./7,color='red',alpha=0.3)\nplt.axvline(x=3./7,color='red',alpha=0.3)\n```", "```py\nfrom pandas.plotting import autocorrelation_plot\n\nautocorrelation_plot(data.iloc[110])\nplt.title(' '.join(train.loc[110,['Subject', 'Sub_Page']]))\n```", "```py\na = np.random.choice(data.shape[0],1000)\n\nfor i in a:\n    autocorrelation_plot(data.iloc[i])\n\nplt.title('1K Autocorrelations')\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX = data.iloc[:,:500]\ny = data.iloc[:,500:]\n\nX_train, X_val, y_train, y_val = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n```", "```py\ndef mape(y_true,y_pred):\n    eps = 1\n    err = np.mean(np.abs((y_true - y_pred) / (y_true + eps))) * 100\n    return err\n```", "```py\nlookback = 50\n\nlb_data = X_train[:,-lookback:]\n\nmed = np.median(lb_data,axis=1,keepdims=True)\n\nerr = mape(y_train,med)\n```", "```py\nidx = 15000\n\nfig, ax = plt.subplots(figsize=(10, 7))\n\nax.plot(np.arange(500),X_train[idx], label='X')\nax.plot(np.arange(500,550),y_train[idx],label='True')\n\nax.plot(np.arange(500,550),np.repeat(med[idx],50),label='Forecast')\n\nplt.title(' '.join(train.loc[idx,['Subject', 'Sub_Page']]))\nax.legend()\nax.set_yscale('log')\n```", "```py\nfrom statsmodels.tsa.arima_model import ARIMA\n```", "```py\nmodel = ARIMA(X_train[0], order=(5,1,5))\n```", "```py\nmodel = model.fit()\n```", "```py\nresiduals = pd.DataFrame(model.resid)\nax.plot(residuals)\n\nplt.title('ARIMA residuals for 2NE1 pageviews')\n```", "```py\nresiduals.plot(kind='kde',figsize=(10,7),title='ARIMA residual distribution 2NE1 ARIMA',legend = False)\n```", "```py\npredictions, stderr, conf_int = model.forecast(50)\n```", "```py\nfig, ax = plt.subplots(figsize=(10, 7))\n\nax.plot(np.arange(480,500),basis[480:], label='X')\nax.plot(np.arange(500,550),y_train[0], label='True')\nax.plot(np.arange(500,550),predictions, label='Forecast')\n\nplt.title('2NE1 ARIMA forecasts')\nax.legend()\nax.set_yscale('log')\n```", "```py\nn_seasons = 7\n\nstate_transition = np.zeros((n_seasons+1, n_seasons+1))\n\nstate_transition[0,0] = 1\n\nstate_transition[1,1:-1] = [-1.0] * (n_seasons-1)\nstate_transition[2:,1:-1] = np.eye(n_seasons-1)\n```", "```py\narray([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0., -1., -1., -1., -1., -1., -1.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.]])\n```", "```py\nobservation_model = [[1,1] + [0]*(n_seasons-1)]\n```", "```py\n[[1, 1, 0, 0, 0, 0, 0, 0]]\n\n```", "```py\nsmoothing_factor = 5.0\n\nlevel_noise = 0.2 / smoothing_factor\nobservation_noise = 0.2\nseason_noise = 1e-3\n\nprocess_noise_cov = np.diag([level_noise, season_noise] + [0]*(n_seasons-1))**2\nobservation_noise_cov = observation_noise**2\n```", "```py\npip install simdkalman\n\n```", "```py\nimport simdkalman\n```", "```py\nkf = simdkalman.KalmanFilter(state_transition = state_transition,process_noise = process_noise_cov,observation_model = observation_model,observation_noise = observation_noise_cov)\n```", "```py\nresult = kf.compute(X_train[0], 50)\n```", "```py\nfig, ax = plt.subplots(figsize=(10, 7))\nax.plot(np.arange(480,500),X_train[0,480:], label='X')\nax.plot(np.arange(500,550),y_train[0],label='True')\n\nax.plot(np.arange(500,550),\n        result.predicted.observations.mean,\n        label='Predicted observations')\n\nax.plot(np.arange(500,550),\n        result.predicted.states.mean[:,0],\n        label='predicted states')\n\nax.plot(np.arange(480,500),\n        result.smoothed.observations.mean[480:],\n        label='Expected Observations')\n\nax.plot(np.arange(480,500),\n        result.smoothed.states.mean[480:,0],\n        label='States')\n\nax.legend()\nax.set_yscale('log')\n```", "```py\nimport datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nweekdays = [datetime.datetime.strptime(date, '%Y-%m-%d').strftime('%a')\n            for date in train.columns.values[:-4]]\n```", "```py\nday_one_hot = LabelEncoder().fit_transform(weekdays)\nday_one_hot = day_one_hot.reshape(-1, 1)\n```", "```py\nday_one_hot = OneHotEncoder(sparse=False).fit_transform(day_one_hot)\nday_one_hot = np.expand_dims(day_one_hot,0)\n```", "```py\nagent_int = LabelEncoder().fit(train['Agent'])\nagent_enc = agent_int.transform(train['Agent'])\nagent_enc = agent_enc.reshape(-1, 1)\nagent_one_hot = OneHotEncoder(sparse=False).fit(agent_enc)\n\ndel agent_enc\n```", "```py\npage_int = LabelEncoder().fit(train['Sub_Page'])\npage_enc = page_int.transform(train['Sub_Page'])\npage_enc = page_enc.reshape(-1, 1)\npage_one_hot = OneHotEncoder(sparse=False).fit(page_enc)\n\ndel page_enc\n\nacc_int = LabelEncoder().fit(train['Access'])\nacc_enc = acc_int.transform(train['Access'])\nacc_enc = acc_enc.reshape(-1, 1)\nacc_one_hot = OneHotEncoder(sparse=False).fit(acc_enc)\n\ndel acc_enc\n```", "```py\ndef lag_arr(arr, lag, fill):\n    filler = np.full((arr.shape[0],lag,1),-1)\n    comb = np.concatenate((filler,arr),axis=1)\n    result = comb[:,:arr.shape[1]]\n    return result\n```", "```py\ndef single_autocorr(series, lag):\n    s1 = series[lag:]\n    s2 = series[:-lag]\n    ms1 = np.mean(s1)\n    ms2 = np.mean(s2)\n    ds1 = s1 - ms1\n    ds2 = s2 - ms2\n    divider = np.sqrt(np.sum(ds1 * ds1)) * np.sqrt(np.sum(ds2 * ds2))\n    return np.sum(ds1 * ds2) / divider if divider != 0 else 0\n```", "```py\ndef batc_autocorr(data,lag,series_length):\n    corrs = []\n    for i in range(data.shape[0]):\n        c = single_autocorr(data, lag)\n        corrs.append(c)\n    corr = np.array(corrs)\n    corr = np.expand_dims(corr,-1)\n    corr = np.expand_dims(corr,-1)\n    corr = np.repeat(corr,series_length,axis=1)\n    return corr\n```", "```py\ndef get_batch(train,start=0,lookback = 100):                  #1\n    assert((start + lookback) <= (train.shape[1] - 5))        #2\n    data = train.iloc[:,start:start + lookback].values        #3\n    target = train.iloc[:,start + lookback].values\n    target = np.log1p(target)                                 #4\n    log_view = np.log1p(data)\n    log_view = np.expand_dims(log_view,axis=-1)               #5\n    days = day_one_hot[:,start:start + lookback]\n    days = np.repeat(days,repeats=train.shape[0],axis=0)      #6\n    year_lag = lag_arr(log_view,365,-1)\n    halfyear_lag = lag_arr(log_view,182,-1)\n    quarter_lag = lag_arr(log_view,91,-1)                     #7\n    agent_enc = agent_int.transform(train['Agent'])\n    agent_enc = agent_enc.reshape(-1, 1)\n    agent_enc = agent_one_hot.transform(agent_enc)\n    agent_enc = np.expand_dims(agent_enc,1)\n    agent_enc = np.repeat(agent_enc,lookback,axis=1)          #8\n    page_enc = page_int.transform(train['Sub_Page'])\n    page_enc = page_enc.reshape(-1, 1)\n    page_enc = page_one_hot.transform(page_enc)\n    page_enc = np.expand_dims(page_enc, 1)\n    page_enc = np.repeat(page_enc,lookback,axis=1)            #9\n    acc_enc = acc_int.transform(train['Access'])\n    acc_enc = acc_enc.reshape(-1, 1)\n    acc_enc = acc_one_hot.transform(acc_enc)\n    acc_enc = np.expand_dims(acc_enc,1)\n    acc_enc = np.repeat(acc_enc,lookback,axis=1)              #10\n    year_autocorr = batc_autocorr(data,lag=365,series_length=lookback)\n    halfyr_autocorr = batc_autocorr(data,lag=182,series_length=lookback)\n    quarter_autocorr = batc_autocorr(data,lag=91,series_length=lookback)                                       #11\n    medians = np.median(data,axis=1)\n    medians = np.expand_dims(medians,-1)\n    medians = np.expand_dims(medians,-1)\n    medians = np.repeat(medians,lookback,axis=1)              #12\n    batch = np.concatenate((log_view,\n                            days, \n                            year_lag, \n                            halfyear_lag, \n                            quarter_lag,\n                            page_enc,\n                            agent_enc,\n                            acc_enc, \n                            year_autocorr, \n                            halfyr_autocorr,\n                            quarter_autocorr, \n                            medians),axis=2)\n\n    return batch, target\n```", "```py\ndef generate_batches(train,batch_size = 32, lookback = 100):\n    num_samples = train.shape[0]\n    num_steps = train.shape[1] - 5\n    while True:\n        for i in range(num_samples // batch_size):\n            batch_start = i * batch_size\n            batch_end = batch_start + batch_size\n\n            seq_start = np.random.randint(num_steps - lookback)\n            X,y = get_batch(train.iloc[batch_start:batch_end],start=seq_start)\n            yield X,y\n```", "```py\nn_features = 29\nmax_len = 100\n\nmodel = Sequential()\n\nmodel.add(Conv1D(16,5, input_shape=(100,29)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool1D(5))\n\nmodel.add(Conv1D(16,5))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool1D(5))\nmodel.add(Flatten())\nmodel.add(Dense(1))\n```", "```py\nmodel.compile(optimizer='adam',loss='mean_absolute_percentage_error')\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nbatch_size = 128\ntrain_df, val_df = train_test_split(train, test_size=0.1)\ntrain_gen = generate_batches(train_df,batch_size=batch_size)\nval_gen = generate_batches(val_df, batch_size=batch_size)\n\nn_train_samples = train_df.shape[0]\nn_val_samples = val_df.shape[0]\n```", "```py\nmodel.fit_generator(train_gen, epochs=20,steps_per_epoch=n_train_samples // batch_size, validation_data= val_gen, validation_steps=n_val_samples // batch_size)\n```", "```py\nmodel.add(Conv1D(16,5, padding='causal'))\n```", "```py\nmodel.add(Conv1D(16,5, padding='causal', dilation_rate=4))\n```", "```py\nfrom keras.layers import SimpleRNN\n\nmodel = Sequential()\nmodel.add(SimpleRNN(16,input_shape=(max_len,n_features)))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam',loss='mean_absolute_percentage_error')\n```", "```py\nfrom keras.layers import SimpleRNN\n\nmodel = Sequential()\nmodel.add(SimpleRNN(16,return_sequences=True,input_shape=(max_len,n_features)))\nmodel.add(SimpleRNN(32, return_sequences = True))\nmodel.add(SimpleRNN(64))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam',loss='mean_absolute_percentage_error')\n\nYou can then fit the model on the generator as before:\n\nmodel.fit_generator(train_gen,epochs=20,steps_per_epoch=n_train_samples // batch_size, validation_data= val_gen, validation_steps=n_val_samples // batch_size)\n```", "```py\nfrom keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(LSTM(16,input_shape=(max_len,n_features)))\nmodel.add(Dense(1))\n```", "```py\nmodel = Sequential()\nmodel.add(LSTM(32,return_sequences=True,input_shape=(max_len,n_features)))\nmodel.add(SimpleRNN(16, return_sequences = True))\nmodel.add(LSTM(16))\nmodel.add(Dense(1))\n```", "```py\nmodel.compile(optimizer='adam',loss='mean_absolute_percentage_error')\n\nmodel.fit_generator(train_gen, epochs=20,steps_per_epoch=n_train_samples // batch_size, validation_data= val_gen, validation_steps=n_val_samples // batch_size)\n```", "```py\nmodel = Sequential()\nmodel.add(LSTM(16, recurrent_dropout=0.1,return_sequences=True,input_shape=(max_len,n_features)))\n\nmodel.add(LSTM(16,recurrent_dropout=0.1))\n\nmodel.add(Dense(1))\n```", "```py\nX = np.random.rand(20,1) * 10-5\ny = np.sin(X)\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\n\nmodel = Sequential()\n\nmodel.add(Dense(1,input_dim = 1))\nmodel.add(Dropout(0.05))\n\nmodel.add(Dense(20))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.05))\n\nmodel.add(Dense(20))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.05))\n\nmodel.add(Dense(20))\nmodel.add(Activation('sigmoid'))\n\nmodel.add(Dense(1))\n```", "```py\nfrom keras.optimizers import SGD\nmodel.compile(loss='mse',optimizer=SGD(lr=0.01))\nmodel.fit(X,y,epochs=10000,batch_size=10,verbose=0)\n```", "```py\nX_test = np.arange(-10,10,0.1)\nX_test = np.expand_dims(X_test,-1)\n```", "```py\nimport keras.backend as K\nK.clear_session()\nK.set_learning_phase(1)\n```", "```py\nprobs = []\nfor i in range(100):\n    out = model.predict(X_test)\n    probs.append(out)\n```", "```py\np = np.array(probs)\n\nmean = p.mean(axis=0)\nstd = p.std(axis=0)\n```", "```py\nplt.figure(figsize=(10,7))\nplt.plot(X_test,mean,c='blue')\n\nlower_bound = mean - std * 0.5\nupper_bound =  mean + std * 0.5\nplt.fill_between(X_test.flatten(),upper_bound.flatten(),lower_bound.flatten(),alpha=0.25, facecolor='blue')\n\nlower_bound = mean - std\nupper_bound =  mean + std\nplt.fill_between(X_test.flatten(),upper_bound.flatten(),lower_bound.flatten(),alpha=0.25, facecolor='blue')\n\nlower_bound = mean - std * 2\nupper_bound =  mean + std * 2\nplt.fill_between(X_test.flatten(),upper_bound.flatten(),lower_bound.flatten(),alpha=0.25, facecolor='blue')\n\nplt.scatter(X,y,c='black')\n```"]