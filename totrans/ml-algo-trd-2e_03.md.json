["```py\nfrom bs4 import BeautifulSoup\nimport requests\n# set and request url; extract source code\nurl = https://www.opentable.com/new-york-restaurant-listings\nhtml = requests.get(url)\nhtml.text[:500]\n' <!DOCTYPE html><html lang=\"en\"><head><meta charset=\"utf-8\"/><meta http-equiv=\"X-UA-Compatible\" content=\"IE=9; IE=8; IE=7; IE=EDGE\"/> <title>Restaurant Reservation Availability</title> <meta name=\"robots\" content=\"noindex\" > </meta> <link rel=\"shortcut icon\" href=\"//components.otstatic.com/components/favicon/1.0.4/favicon/favicon.ico\" type=\"image/x-icon\"/><link rel=\"icon\" href=\"//components.otstatic.com/components/favicon/1.0.4/favicon/favicon-16.png\" sizes=\"16x16\"/><link rel=' \n```", "```py\n# parse raw html => soup object\nsoup = BeautifulSoup(html.text, 'html.parser')\n# for each span tag, print out text => restaurant name\nfor entry in soup.find_all(name='span', attrs={'class':'rest-row-name-text'}):\n    print(entry.text)\nWade Coves\nAlley\nDolorem Maggio\nIslands\n... \n```", "```py\n# get the number of dollars signs for each restaurant\nfor entry in soup.find_all('div', {'class':'rest-row-pricing'}):\n    price = entry.find('i').text \n```", "```py\nsoup.find_all('div', {'class':'booking'})\n[] \n```", "```py\nfrom selenium import webdriver\n# create a driver called Firefox\ndriver = webdriver.Firefox() \n```", "```py\n# close it\ndriver.close() \n```", "```py\nimport time, re\n# visit the opentable listing page\ndriver = webdriver.Firefox()\ndriver.get(url)\ntime.sleep(1) # wait 1 second\n# retrieve the html source\nhtml = driver.page_source\nhtml = BeautifulSoup(html, \"lxml\")\nfor booking in html.find_all('div', {'class': 'booking'}):\n    match = re.search(r'\\d+', booking.text)\n    if match:\n        print(match.group()) \n```", "```py\ndef parse_html(html):\n    data, item = pd.DataFrame(), {}\n    soup = BeautifulSoup(html, 'lxml')\n    for i, resto in enumerate(soup.find_all('div',\n                                           class_='rest-row-info')):\n        item['name'] = resto.find('span',\n                                 class_='rest-row-name-text').text\n        booking = resto.find('div', class_='booking')\n        item['bookings'] = re.search('\\d+', booking.text).group() \\\n            if booking else 'NA'\n        rating = resto.find('div', class_='star-rating-score')\n        item['rating'] = float(rating['aria-label'].split()[0]) \\\n            if rating else 'NA'\n        reviews = resto.find('span', class_='underline-hover')\n        item['reviews'] = int(re.search('\\d+', reviews.text).group()) \\\n            if reviews else 'NA'\n        item['price'] = int(resto.find('div', class_='rest-row-pricing')\n                            .find('i').text.count('$'))\n        cuisine_class = 'rest-row-meta--cuisine rest-row-meta-text sfx1388addContent'\n        item['cuisine'] = resto.find('span', class_=cuisine_class).text\n        location_class = 'rest-row-meta--location rest-row-meta-text sfx1388addContent'\n        item['location'] = resto.find('span', class_=location_class).text\n        data[i] = pd.Series(item)\n    return data.T \n```", "```py\nrestaurants = pd.DataFrame()\ndriver = webdriver.Firefox()\nurl = https://www.opentable.com/new-york-restaurant-listings\ndriver.get(url)\nwhile True:\n    sleep(1)\n    new_data = parse_html(driver.page_source)\n    if new_data.empty:\n        break\n    restaurants = pd.concat([restaurants, new_data], ignore_index=True)\n    print(len(restaurants))\n    driver.find_element_by_link_text('Next').click()\ndriver.close() \n```", "```py\nfrom opentable.items import OpentableItem\nfrom scrapy import Spider\nfrom scrapy_splash import SplashRequest\nclass OpenTableSpider(Spider):\n    name = 'opentable'\n    start_urls = ['https://www.opentable.com/new-york-restaurant-\n                   listings']\n    def start_requests(self):\n        for url in self.start_urls:\n            yield SplashRequest(url=url,\n                                callback=self.parse,\n                                endpoint='render.html',\n                                args={'wait': 1},\n                                )\n    def parse(self, response):\n        item = OpentableItem()\n        for resto in response.css('div.rest-row-info'):\n            item['name'] = resto.css('span.rest-row-name-\n                                      text::text').extract()\n            item['bookings'] = \n                  resto.css('div.booking::text').re(r'\\d+')\n            item['rating'] = resto.css('div.all-\n                  stars::attr(style)').re_first('\\d+')\n            item['reviews'] = resto.css('span.star-rating-text--review-\n                                         text::text').re_first(r'\\d+')\n            item['price'] = len(resto.css('div.rest-row-pricing > \n                                i::text').re('\\$'))\n            item['cuisine'] = resto.css('span.rest-row-meta—\n                                         cuisine::text').extract()\n            item['location'] = resto.css('span.rest-row-meta—\n                               location::text').extract()\n            yield item \n```", "```py\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nfrom furl import furl\nfrom selenium import webdriver\ntranscript_path = Path('transcripts')\nSA_URL = 'https://seekingalpha.com/'\nTRANSCRIPT = re.compile('Earnings Call Transcript')\nnext_page = True\npage = 1\ndriver = webdriver.Firefox() \n```", "```py\nwhile next_page:\n    url = f'{SA_URL}/earnings/earnings-call-transcripts/{page}'\n    driver.get(urljoin(SA_URL, url))\n    response = driver.page_source\n    page += 1\n    soup = BeautifulSoup(response, 'lxml')\n    links = soup.find_all(name='a', string=TRANSCRIPT)\n    if len(links) == 0:\n        next_page = False\n    else:\n        for link in links:\n            transcript_url = link.attrs.get('href')\n            article_url = furl(urljoin(SA_URL, \n                           transcript_url)).add({'part': 'single'})\n            driver.get(article_url.url)\n            html = driver.page_source\n            meta, participants, content = parse_html(html)\n            meta['link'] = link\ndriver.close() \n```", "```py\ndef parse_html(html):\n    date_pattern = re.compile(r'(\\d{2})-(\\d{2})-(\\d{2})')\n    quarter_pattern = re.compile(r'(\\bQ\\d\\b)')\n    soup = BeautifulSoup(html, 'lxml')\n    meta, participants, content = {}, [], []\n    h1 = soup.find('h1', itemprop='headline').text\n    meta['company'] = h1[:h1.find('(')].strip()\n    meta['symbol'] = h1[h1.find('(') + 1:h1.find(')')]\n    title = soup.find('div', class_='title').text\n    match = date_pattern.search(title)\n    if match:\n        m, d, y = match.groups()\n        meta['month'] = int(m)\n        meta['day'] = int(d)\n        meta['year'] = int(y)\n    match = quarter_pattern.search(title)\n    if match:\n        meta['quarter'] = match.group(0)\n    qa = 0\n    speaker_types = ['Executives', 'Analysts']\n    for header in [p.parent for p in soup.find_all('strong')]:\n        text = header.text.strip()\n        if text.lower().startswith('copyright'):\n            continue\n        elif text.lower().startswith('question-and'):\n            qa = 1\n            continue\n        elif any([type in text for type in speaker_types]):\n            for participant in header.find_next_siblings('p'):\n                if participant.find('strong'):\n                    break\n                else:\n                    participants.append([text, participant.text])\n        else:\n            p = []\n            for participant in header.find_next_siblings('p'):\n                if participant.find('strong'):\n                    break\n                else:\n                    p.append(participant.text)\n            content.append([header.text, qa, '\\n'.join(p)])\n    return meta, participants, content \n```", "```py\ndef store_result(meta, participants, content):\n    path = transcript_path / 'parsed' / meta['symbol']\n    pd.DataFrame(content, columns=['speaker', 'q&a', \n              'content']).to_csv(path / 'content.csv', index=False)\n    pd.DataFrame(participants, columns=['type', 'name']).to_csv(path / \n                 'participants.csv', index=False)\n    pd.Series(meta).to_csv(path / 'earnings.csv') \n```"]