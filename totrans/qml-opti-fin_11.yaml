- en: '12'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Power of Parameterised Quantum Circuits
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen in the previous chapters, there is a wide range of QML models
    based on parameterised quantum circuits. One reason for this is their tolerance
    to noise  [[222](Biblography.xhtml#XNguyen2020)], which is important when we work
    with the NISQ hardware. However, this does not fully explain the popularity of
    PQCs or why they are considered strong competitors to classical ML models. There
    must be some fundamental properties of PQCs that make them superior to their classical
    counterparts. In this chapter, we discuss two such properties: resistance to overfitting
    and larger expressive power.'
  prefs: []
  type: TYPE_NORMAL
- en: Resistance to overfitting is a direct consequence of the fact that a typical
    PQC – one without mid-circuit measurement – can be represented by a linear unitary
    operator. Linear models impose strong regularisation, thus preventing overfitting.
    At the same time, the model remains powerful due to the mapping of the input into
    the higher-dimensional Hilbert space where it may be easier to perform classification
    if the PQC is trained as a discriminative model (QNN).
  prefs: []
  type: TYPE_NORMAL
- en: Expressive power is related to the model’s ability to express different relationships
    between variables, i.e., its ability to learn complex data structures. It appears
    that PQCs trained as generative models (QCBM) have strictly larger expressive
    power than their equivalent classical versions (such as RBM).
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 Strong Regularisation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Parameterised quantum circuits trained as classifiers face the same challenge
    as classical models: the need to generalise well to unseen data points. Classically,
    we have a wide range of supervised learning models and regularisation techniques
    to choose from. These regularisation techniques that fight overfitting are model
    specific. For example, we can try to restrict the depth of the decision trees
    or to impose a penalty term in the cost function when training neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a conventional feedforward neural network as, arguably, the most direct
    classical counterpart of a quantum classifier. In both classical and quantum cases,
    the signal travels through the network in one direction and the layers of quantum
    gates can be compared to the layers of classical activation units. Regardless
    of whether we apply *L*[1] (Lasso) or *L*[2] (Ridge) penalty terms, or use dropout
    techniques, we would like to have a measure of regularisation present in the network.
    This is an interesting theoretical problem as well as an important practical task
    that allows us to develop an optimal strategy for fighting overfitting. Ideally,
    such a measure should be applicable to both classical and quantum neural networks
    to provide a meaningful comparison of their respective regularisation properties.
  prefs: []
  type: TYPE_NORMAL
- en: Very often, relatively small network weights are associated with a high degree
    of regularisation and relatively high network weights are symptoms of overfitting.
    However, it would be highly desirable to have a formal mathematical tool for quantifying
    the network capacity to overfit. One such possible well-defined measure that captures
    the degree of regularisation is the Lipschitz constant.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.1 Lipschitz constant
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Following Gouk  [[115](Biblography.xhtml#XGouk2018)], given two metric spaces
    (𝒳*,d*[𝒳]) and (*,d*[) a function *f* : 𝒳 →, is said to be Lipschitz continuous
    if there exists a constant *k* ≥ 0 such that]'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![d (f (x1),f(x2)) ≤ kd𝒳(x1,x2), for all x1,x2 ∈ 𝒳 . ](img/file1184.jpg)
    |  |'
  prefs: []
  type: TYPE_TB
- en: The value of *k* is known as the Lipschitz constant, and the function is referred
    to as *k*-Lipschitz. We are interested in the smallest possible Lipschitz constant
    or, at least, its upper bound. To obtain the upper bound estimate, we should note
    some useful properties of feedforward neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a *j*-th layer of a feedforward neural network, x[1] and x[2]
    are the *n*-dimensional sample outputs of the previous layer, *j* − 1, and *f*(x[1])
    and *f*(x[2]) are the *m*-dimensional outputs of layer *j*. The metrics *d*[𝒳]
    and *d* [can be, for example, *L*[1] or *L*[2] norms.]
  prefs: []
  type: TYPE_NORMAL
- en: 'A feedforward neural network consisting of *l* fully connected layers can be
    expressed as a series of function compositions:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![f(x) = (ϕl ∘ ϕl− 1 ∘...∘ ϕ1)(x), ](img/file1185.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: 'where each *ϕ*[j] implements the *j*-th layer affine transformation of the
    *n*-dimensional input x, parameterised by an *m* × *n* weight matrix W[j] and
    an *m*-dimensional bias vector b[j]:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ϕj(x) = Wjx + bj. ](img/file1186.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: The composition of a *k*[1]-Lipschitz function with a *k*[2]-Lipschitz function
    is a *k*[1]*k*[2]-Lipschitz function  [[115](Biblography.xhtml#XGouk2018)]. Therefore,
    we can compute the Lipschitz constants for each layer separately and combine them
    together to obtain an upper bound on the Lipschitz constant for the entire network.
  prefs: []
  type: TYPE_NORMAL
- en: Choose *d*[𝒳] and *d* [to be the *L*[2] norms ∥⋅∥[2]. In this case, we obtain
    the following relationship from the definition of Lipschitz continuity for the
    fully connected network layer *j*:]
  prefs: []
  type: TYPE_NORMAL
- en: '| ![∥(Wjx1 + bj)− (Wjx2 + bj)∥2 ≤ k∥x1 − x2∥2\. ](img/file1187.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: Introducing a = x[1] − x[2] and assuming that x[1]*≠*x[2] we arrive at the estimate
  prefs: []
  type: TYPE_NORMAL
- en: '| ![∥Wja-∥2-≤ k. ∥a∥2 ](img/file1188.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: 'The smallest Lipschitz constant of the fully connected network layer, *L*(*ϕ*[j]),
    is equal to the supremum of the left-hand side of inequality ([12.1.1](#x1-2270001)):'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ ∥W a∥ L (ϕj) := sup ---j--2-. a⁄=0 ∥a∥2 ](img/file1189.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: The operator norm ([12.1.1](#x1-2270001)) is given by the largest singular value
    of the weight matrix W[j], which corresponds to the spectral norm – the maximum
    scale by which the matrix can stretch a vector. It is straightforward to calculate
    using any of the suitable open-source packages, for example sklearn.decomposition.TruncatedSVD
    from the `scikit-learn` package.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of quantum neural networks, any parameterised quantum circuit operating
    on *n* qubits, regardless how complex and deep, can be represented by a 2^n ×
    2^n unitary matrix. Since all singular values of the unitary matrix are equal
    to one, this gives us a natural benchmark for comparison of regularisation capabilities
    of various networks.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.2 Regularisation example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Australian Credit Approval (ACA) dataset  [[241](Biblography.xhtml#XUCI_ACA), [242](Biblography.xhtml#XQuinlan1987)]
    we analysed in Chapter [8](Chapter_8.xhtml#x1-1620008) can serve as a good illustrative
    example. We can compare the performance of classical and quantum neural networks
    while monitoring regularisation as measured by the Lipschitz constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'The classical neural network is an MLP Classifier with two hidden layers. Each
    hidden layer holds the same number of activation units as the number of features
    in the ACA dataset (14), so that we have to calculate the largest singular values
    for two 14 × 14 square matrices. The features are standardised with `sklearn.preprocessing.StandardScaler`.
    We also use `sklearn.neural_network.MLPClassifier` to construct the classifier
    with the set of hyperparameters shown in Table [12.1](#x1-228002r1):'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameter | Value |'
  prefs: []
  type: TYPE_TB
- en: '| Number of hidden layers: | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of activation units in each layer: | 14 |'
  prefs: []
  type: TYPE_TB
- en: '| Activation function: | tanh |'
  prefs: []
  type: TYPE_TB
- en: '| Solver: | adam |'
  prefs: []
  type: TYPE_TB
- en: '| Intial learning rate: | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of iterations: | 5000 |'
  prefs: []
  type: TYPE_TB
- en: '| Random state: | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Regularisation parameter, *α*: | variable |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12.1: MLP Classifier hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The MLP Classifier regularisation parameter *α* is our control variable. It
    controls the *L*[2] regularisation term in the network cost function: the larger
    this parameter, the more large network weights are penalised. All other parameters
    were set at their default values.'
  prefs: []
  type: TYPE_NORMAL
- en: The quantum neural network is shown in Figure [8.5](Chapter_8.xhtml#8.5). The
    parameterised quantum circuit consists of just 7 fixed two-qubit gates (CZ) and
    15 adjustable one-qubit gates (R[X] and R[Y]). Table [12.2](#x1-228005r2) compares
    the MLP and the QNN classifiers on the in-sample and out-of-sample datasets (the
    ACA dataset was split 50:50 into training and test datasets using sklearn.preprocessing.StandardScaler).
  prefs: []
  type: TYPE_NORMAL
- en: We observe that QNN provides strong regularisation with similar performance
    on the in-sample and out-of-sample datasets as expected from the network represented
    by the unitary matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '| Classifier | Average *F*[1] score | Average *F*[1] score | Lipschitz Constant
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | (in-sample) | (out-of-sample) | (upper bound) |'
  prefs: []
  type: TYPE_TB
- en: '| MLP, *α* = 0*.*001 | 1.00 | 0.78 | 36.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MLP, *α* = 0*.*01 | 1.00 | 0.79 | 33.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MLP, *α* = 0*.*1 | 1.00 | 0.80 | 18.6 |'
  prefs: []
  type: TYPE_TB
- en: '| MLP, *α* = 1 | 0.99 | 0.83 | 7.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MLP, *α* = 10 | 0.90 | 0.86 | 1.3 |'
  prefs: []
  type: TYPE_TB
- en: '| MLP, *α* = 40 | 0.85 | 0.86 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MLP, *α* = 50 | 0.35 | 0.37 | 1e-05 |'
  prefs: []
  type: TYPE_TB
- en: '| QNN | 0.86 | 0.85 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12.2: F[1] scores and Lipschitz constants for MLP and QNN classifiers
    trained on the ACA dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Further, we observe that the equivalent degree of regularisation can be achieved
    by MLP only with exceptionally large values of the regularisation parameter *α*.
    Making *α* any larger completely destroys the learning abilities of the network.
    For the chosen MLP configuration, the critical value of *α* is between 40 and 50.
  prefs: []
  type: TYPE_NORMAL
- en: Parameterised quantum circuits can be represented as (high-dimensional) norm-preserving
    unitary matrices. This ensures strong regularisation properties of the quantum
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can move to the next feature of the parameterised quantum circuits:
    their expressive power. We can define the expressivity of a PQC as the circuit’s
    ability to generate pure quantum states that are well representative of the Hilbert
    space  [[266](Biblography.xhtml#XSim2019)]. In other words, from the QML point
    of view, the expressive power of a PQC is its ability to learn ("express") complex
    data structures. In the following section, we will try to quantify the degree
    of expressivity inherent in different PQC types.'
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Expressive Power
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We saw in previous chapters how PQCs can be applied to solving optimisation
    problems (QAOA and VQE) as well as to various machine learning tasks covering
    both discriminative (QNN classifier) and generative (QCBM market generator) use
    cases. In general, the PQCs we used for quantum machine learning tasks can be
    divided into two types  [[88](Biblography.xhtml#XDu2018)]: tensor network PQC
    (similar to the QNN circuit in Figure [8.4](Chapter_8.xhtml#8.4)) and multilayer
    PQC (similar to the QCBM circuit in Figure [9.1](Chapter_9.xhtml#9.1)). What is
    their expressive power and how can we rank them? Before trying to answer this
    question, let us have a look at a simple illustrative example: quantum circuits
    specified on a single quantum register.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1: PQCs with different expressive powers. ](img/file1190.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: PQCs with different expressive powers.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [12.1](#12.1) displays four one-qubit circuits with dramatically different
    expressive powers, where *U*[−*π,π*] denotes the Uniform distribution over the
    closed interval [−*π,π*]. Let us go through them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: PQC A starts with the qubit state initialised as ![|0⟩](img/file1191.jpg) –
    North Pole on the Bloch sphere (Figure [7.2](Chapter_7.xhtml#x1-1520002)). The
    only gate is the Hadamard gate H that moves ![|0⟩](img/file1192.jpg) to (![|0⟩](img/file1193.jpg)
    + ![|1⟩](img/file1194.jpg))*∕*![√ -- 2](img/file1195.jpg). Thus, state ![|ψA⟩](img/file1196.jpg)
    can only be a single point on the Bloch sphere.
  prefs: []
  type: TYPE_NORMAL
- en: PQC B also starts with the qubit state initialised as ![|0⟩](img/file1197.jpg)
    and applies the Hadamard gate transforming the initial state into (![|0⟩](img/file1198.jpg)
    + ![|1⟩](img/file1199.jpg))*∕*![√ -- 2](img/file1200.jpg) before applying the
    rotation R[Z] around the *z*-axis by an angle *𝜃*[z] drawn from the Uniform distribution
    on [−*π,π*]. The final state ![|ψB ⟩](img/file1201.jpg) can be any point on the
    equator, all reached with equal probability.
  prefs: []
  type: TYPE_NORMAL
- en: PQC C adds a rotation R[X] to PQC B, by an angle *𝜃*[x] drawn from the Uniform
    distribution on [−*π,π*]. With two rotations around two orthogonal axes we can
    reach any point on the Bloch sphere. However, with angles *𝜃*[z] and *𝜃*[x] drawn
    from the Uniform distribution on [−*π,π*] we do not have a Uniform distribution
    of points on the Bloch sphere for state ![|ψ ⟩ C](img/file1202.jpg). We observe
    the highest density around points (![|0⟩](img/file1203.jpg) + ![|1⟩](img/file1204.jpg))*∕*![√--
    2](img/file1205.jpg) and (![|0⟩](img/file1206.jpg)−![|1⟩](img/file1207.jpg))*∕*![√
    -- 2](img/file1208.jpg), which are the points where the equator crosses the 0^∘
    and 180^∘ meridians, and the lowest density along the 90^∘ and 270^∘ meridians.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, PQC D adds one more rotation R[Y] around the *y*-axis by an angle *𝜃*[y]
    drawn from the Uniform distribution on [−*π,π*]. This rotation results in spreading
    the previously clustered points more evenly around the Bloch sphere, thus making
    all points on the Bloch sphere equally accessible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, in terms of our ability to explore the Hilbert space, we have the
    following hierarchy of the expressive power of the PQCs introduced above:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PQC D > PQC C > PQC B > PQC A. ](img/file1209.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can now return to the PQCs developed in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Multilayer PQC
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A multilayer PQC (MPQC) consists of multiple blocks of quantum circuits in which
    the arrangement of quantum gates in each block is identical  [[28](Biblography.xhtml#XBenedetti2018), [189](Biblography.xhtml#XLiuWang2018)].
    Figure [12.2](#12.2) shows a schematic representation of the MPQC.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2: Schematic representation of a multilayer PQC. ](img/file1210.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: Schematic representation of a multilayer PQC.'
  prefs: []
  type: TYPE_NORMAL
- en: The following mathematical formalism can be used to describe MPQC. The input
    *n*-qubit quantum state with all qubits initialised as ![|0⟩](img/file1211.jpg)
    in the computational basis is ![|0⟩](img/file1212.jpg)^(⊗n), the total number
    of blocks is denoted *l*, and the *i*-th block is denoted U(𝜃^i), where the number
    of parameters is proportional to the number of qubits, and *n* is logarithmically
    proportional to the dimension of the generated data (this reflects our assumption
    about the data encoding scheme). The generated output state of the circuit thus
    reads
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ ∏ l i ⊗n &#124;ψ ⟩ = U (𝜃 )&#124;0⟩ . i=1 ](img/file1213.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: 12.2.2 Tensor network PQC
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A tensor network PQC (TPQC) treats each block as a local tensor. The arrangement
    of the blocks follows a particular network structure, such as matrix product states
    or tree tensor networks  [[144](Biblography.xhtml#XHuggins2018)]. A schematic
    representation of TPQC is shown in Figure [12.3](#12.3).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3: Schematic representation of a tensor network PQC. ](img/file1214.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: Schematic representation of a tensor network PQC.'
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, the *i*-th block U(𝜃^i) is composed of *M*[i] local tensor blocks,
    with *M*[i] ∝ *n∕*2^i, denoted as U(𝜃^i) = ⊗ [j=1]^(M[i])U(𝜃[j]^i). Note that
    many of these tensor blocks may be identity operators. The generated state is
    thus of the form
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ M ∏l ⊗ i i ⊗n &#124;ψ⟩ = U(𝜃j)&#124;0⟩ . i=1 j=1 ](img/file1215.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: 12.2.3 Measures of expressive power
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main question to answer is whether MPQC and TPQC have larger expressive
    power in comparison with their classical counterparts, such as classical neural
    networks. The expressive power of a model can be defined in many different ways,
    for example, as a model capacity to express different relationships between variables  [[22](Biblography.xhtml#XBaldi2019)].
    Deep neural networks serve as a good example of powerful models capable of learning
    complex data structures  [[94](Biblography.xhtml#XDziugaite2017)]. Therefore,
    the power of a model can be quantified by its complexity, with the *Vapnik-Chervonenkis*
    *dimension* being a complexity measure of choice  [[293](Biblography.xhtml#XVapnik1971)].
    The objective is to provide an estimate of how well a model generalises to the
    unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular approach is the *Fisher information*, which describes the geometry
    of a model parameter space  [[247](Biblography.xhtml#XRissanen1996)]. Arguably,
    the *effective dimension* based on Fisher information, rather than Vapnik-Chervonenkis
    dimension, is a better measure to study the power of quantum and classical neural
    networks  [[1](Biblography.xhtml#XAbbas2020)].
  prefs: []
  type: TYPE_NORMAL
- en: However, one of the most natural metrics of expressive power is *entanglement*
    *entropy*, which allows us to establish a well-defined ranking of quantum and
    classical machine learning models. In this chapter, we will present the expressive
    power estimates obtained in  [[88](Biblography.xhtml#XDu2018)] for TPQC and MPQC
    based on entanglement entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us recall the definitions of entropy in statistical mechanics (the Gibbs
    entropy *S*) and in information theory (the Shannon entropy H) introduced in Chapter [6](Chapter_6.xhtml#x1-1190006):'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ ∑ ∑ S := − kB pilog(pi) and H := − pilog2(pi). i i ](img/file1216.jpg)
    |  |'
  prefs: []
  type: TYPE_TB
- en: Here, *p*[i] is the probability that the microstate *i* is taken from an equilibrium
    ensemble in the case of the Gibbs entropy, and that the message *i* is picked
    from the message space in the case of the Shannon entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'These definitions of entropy can be extended to the quantum case. In Chapter [1](Chapter_1.xhtml#x1-220001),
    we introduced the density matrix as a universal tool for describing pure and mixed
    quantum states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ N N ∑ ∑ ρ := ρij |i⟩⟨j|, i=1 j=1 ](img/file1217.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where (![|i⟩](img/file1218.jpg))[i=1,…,N] are the basis vectors of a given quantum
    system. The *von* *Neumann entropy* 𝒮 is then defined as
  prefs: []
  type: TYPE_NORMAL
- en: '| ![𝒮(ρ) := − Tr(ρlog(ρ)). ](img/file1219.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: Since the density matrix is Hermitian, it is *diagonalisable*, so that there
    exists a basis (![|k⟩](img/file1220.jpg))[k=1,…,N] such that
  prefs: []
  type: TYPE_NORMAL
- en: '![ N N N ρ = ∑ ρ |k⟩ ⟨k| =:∑ p |k⟩⟨k|, where ∑ p = 1\. kk k k k=1 k=1 k=1 ](img/file1221.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The eigenvalues of the operator *ρ*log(*ρ*) are thus (*p*[k] log(*p*[k]))[k=1,…,N],
    and we obtain the following expression for the von Neumann entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ ∑ 𝒮(ρ) = − Tr (ρ log(ρ)) = − pklog(pk). k ](img/file1222.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: From ([12.2.3](#x1-2320003)) and ([12.2.3](#x1-2320003)) we see that for the
    orthogonal mixture of quantum states, the quantum and classical entropies coincide.
  prefs: []
  type: TYPE_NORMAL
- en: If the system has two component parts, *A* and *B*, we can define the *reduced*
    *density matrix* as the *partial trace* of the density matrix over the subspace
    of the Hilbert space we are not interested in. Let (![|a⟩ i](img/file1223.jpg))[i=1,...,N]
    be the standard orthonormal basis of the Hilbert space ℍ[A] of system *A*, and
    (![|b ⟩ j](img/file1224.jpg))[j=1,...,M] be the standard orthonormal basis of
    the Hilbert space ℍ[B] of system *B*. The density matrix *ρ*[AB] of the bipartite
    system *AB* on the tensor product Hilbert space ℍ[A] ⊗ℍ[B] can then be represented
    as
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ ∑N ∑M N∑ ∑M ρAB = cijkl &#124;ai⟩⟨ak&#124;⊗ &#124;bj⟩⟨bl&#124;, i=1j=1
    k=1l=1 ](img/file1225.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: for some coefficients *c*[ijkl]. The partial traces then read
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ ∑N M∑ ∑N ∑M TrB(ρAB ) = cijkl &#124;ai⟩⟨ak&#124;⟨bl&#124;bj⟩, i=1 j=1
    k=1 l=1 ](img/file1226.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: which is a reduced density matrix *ρ*[A] on ℍ[A], and
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ N M N M Tr (ρ ) = ∑ ∑ ∑ ∑ c &#124;b ⟩⟨b &#124;⟨a &#124;a ⟩, A AB i=1 j=1
    ijkl j l k i k=1 l=1 ](img/file1227.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: which is a reduced density matrix *ρ*[B] on ℍ[B]. Note that Tr(![|ai⟩](img/file1228.jpg)⟨*a*[k]|)
    = ![⟨ak|ai⟩](img/file1229.jpg) and Tr(![|b ⟩ j](img/file1230.jpg)⟨*b*[l]|) = ![⟨b|b
    ⟩ l j](img/file1231.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '**Example:** Consider the two-qubit system in the state'
  prefs: []
  type: TYPE_NORMAL
- en: '![|ψ ⟩ = 1√--(|01⟩ + |10 ⟩), 2 ](img/file1232.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'which is one of the four maximally entangled Bell states (Section [6.5.2](Chapter_6.xhtml#x1-1360002)).
    We assume that the first qubit is system *A* and the second qubit is system *B*.
    This state corresponds to the following density matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) ρ := |ψ ⟩⟨ψ| = 1- |01⟩⟨01|+ |01⟩⟨10|+ |10⟩⟨01|+ |10⟩⟨10| . AB 2 ](img/file1233.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us now act on this state with the partial trace Tr[B](⋅):'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![ 1 ( ) ρA := TrB(ρAB ) =-- &#124;0⟩⟨0&#124;⟨1&#124;1 ⟩+ &#124;0⟩⟨1&#124;⟨0&#124;1⟩+
    &#124;1⟩ ⟨0&#124;⟨1&#124;0⟩+ &#124;1⟩⟨1&#124;⟨0&#124;0⟩ 2 ⌊ ⌋ 1 ( ) 1 1 0 = 2-
    &#124;0⟩⟨0&#124;+ &#124;1⟩⟨1&#124; = 2-⌈ ⌉ . 0 1 ](img/file1234.jpg) |  |'
  prefs: []
  type: TYPE_TB
- en: The reduced density matrix *ρ*[A] in ([12.2.3](#x1-2320003)) is the same as
    the density matrix *ρ* in ([1.3.3](Chapter_1.xhtml#x1-44003r3)), which describes
    a statistical ensemble of states ![|0⟩](img/file1235.jpg) and ![|1⟩](img/file1236.jpg)
    (mixed state), i.e., a physical system prepared to be either in state ![|0⟩](img/file1237.jpg)
    or state ![|1⟩](img/file1238.jpg) with equal probability.
  prefs: []
  type: TYPE_NORMAL
- en: The *entanglement entropy* of a bipartite system *AB* is then defined as
  prefs: []
  type: TYPE_NORMAL
- en: '| ![𝒮(ρA) := − Tr(ρA log(ρA )) = − Tr(ρB log(ρB)) =: 𝒮 (ρB), ](img/file1239.jpg)
    |  |'
  prefs: []
  type: TYPE_TB
- en: and can be used as a measure of expressive power of a model in the following
    way. First, note that TPQC, MPQC and classical neural networks have a close connection
    with *tensor networks*, such as matrix product states (MPS)  [[88](Biblography.xhtml#XDu2018)].
    The key question is then whether the given quantum system can be efficiently represented
    by the MPS.
  prefs: []
  type: TYPE_NORMAL
- en: A quantum system that satisfies the *area law* (its entanglement entropy grows
    proportionally with the boundary area) has an efficient MPS representation. At
    the same time, a quantum system that satisfies the *volume law* (its entanglement
    entropy grows proportionally with the volume) cannot be efficiently represented
    by MPS  [[88](Biblography.xhtml#XDu2018)].
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.4 Expressive power of PQC
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Chapter [5](Chapter_5.xhtml#x1-960005), we introduced the Restricted Boltzmann
    Machine (RBM) – a neural network operating on stochastic binary activation units,
    which is a natural classical counterpart of parameterised quantum circuits. We
    considered two types of RBM:'
  prefs: []
  type: TYPE_NORMAL
- en: a shallow two-layer network where the activation units in the visible layer
    are connected to the activation units in the hidden layer with no connections
    between the activation units within the same layer;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a deeper multi-layer network of stacked RBMs where the hidden layer of the *k*-th
    RBM serves as the visible layer of the (*k* + 1)-th RBM. Such stacked RBMs (trained
    sequentially) are called Deep Boltzmann Machines (DBMs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also possible to impose further restrictions on the connections between
    the RBM layers. In *short-range* RBMs, we restrict the connectivity of the hidden
    layer activation units such that they are allowed to connect to the limited number
    of activation units in the visible layer that are in close proximity to each other
    (local connectivity)  [[84](Biblography.xhtml#XDeng2017)]. In *long-range* RBMs,
    we allow connections between the hidden layer activation units and the visible
    layer activation units that are not necessarily local.
  prefs: []
  type: TYPE_NORMAL
- en: It has been established by Deng, Li, and Sarma  [[85](Biblography.xhtml#XDeng2017X)]
    that the entanglement entropy of all short-range RBM states satisfies an area
    law for arbitrary dimensions and bipartition geometry. For long-range RBM states,
    such states could exhibit volume law entanglement. Therefore long-range RBMs are
    capable of representing quantum states with large entanglement.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is probably not surprising that a DBM would have even larger expressive
    power than a single RBM. However, using entanglement entropy as a measure of expressive
    power, Du, Hsieh, Liu, and Tao have proven in  [[88](Biblography.xhtml#XDu2018)]
    that MPQC have strictly larger expressive power than DBM. The main result can
    be formulated as the following theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Theorem 10** (Expressive Power Theorem)**.** *The expressive power of MPQC
    and* *TPQC with* 𝒪(*poly*(*n*)) *single qubit gates and* CNOT *gates, and classical
    neural* *networks with* 𝒪(*poly*(*n*)) *trainable parameters, where* *n* *refers
    to the number of* *qubits or visible units, can be ordered as*'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![MPQC > DBM > long-range RBM > TPQC > short-range RBM. ](img/file1240.jpg)
    |  |'
  prefs: []
  type: TYPE_TB
- en: Theorem [10](#x1-233002r10) provides a solid theoretical foundation for experimental
    works aimed at establishing quantum advantage of PQC-based QML models. The larger
    expressive power of PQCs in comparison with their classical counterparts prompted
    the development of many such models in recent years. For example, a hybrid quantum-classical
    approach, suitable for NISQ devices and harnessing the greater expressive power
    of quantum entanglement, was proposed in  [[59](Biblography.xhtml#XChen2020S)].
    It was shown through numerical simulations that the Quantum Long Short Term Memory
    (QLSTM) model learns faster than the equivalent classical LSTM with a similar
    number of network parameters. In addition, the convergence of QLSTM was shown
    to be more stable than that of its classical counterpart. A Quantum Convolutional
    Neural Network (QCNN) was proposed in  [[58](Biblography.xhtml#XChen2020D)] which,
    due to its larger expressive power, achieved greater test accuracy compared to
    classical CNNs. The source of expressive power was the replacement of the classical
    convolutional filters with quantum convolutional kernels based on variational
    quantum circuits.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer parameterised quantum circuits such as QCBM have strictly more expressive
    power than classical models such as RBM when only a polynomial number of parameters
    is allowed. For systems that exhibit quantum supremacy, a classical model cannot
    learn to reproduce the statistics unless it uses exponentially scaling resources  [[29](Biblography.xhtml#XBenedetti2019)].
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we learned where the power of parameterised quantum circuits
    comes from. We started with the observation that quantum neural networks enjoy
    strong regularisation inherent in their architecture. This is due to the fact
    that any PQC, however wide and deep, is a unitary linear operator.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we considered the expressive power of parameterised quantum circuits and
    established the concept of the expressive power hierarchy. The main result (Theorem [10](#x1-233002r10))
    supports the experimental findings indicating the presence of the elements of
    quantum advantage in various QML models compatible with the main characteristics
    of NISQ devices.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go deeper into the less explored territory of new
    quantum algorithms, an area of very active research.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/quantum](https://packt.link/quantum)'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
