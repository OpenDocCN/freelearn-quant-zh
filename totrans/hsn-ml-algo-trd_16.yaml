- en: Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this book was to enable you to apply **machine learning** (**ML**)
    to a variety of data sources and the extract signals useful for the design and
    execution of an investment strategy. To this end, we introduced ML as an important
    element in the trading strategy process. We saw that ML can add value at multiple
    steps in the process of designing, testing, executing, and evaluating a strategy.
  prefs: []
  type: TYPE_NORMAL
- en: It became clear that the core value proposition of ML consists of the ability
    to extract actionable information from much larger amounts of data more systematically
    than human experts would ever be able to. On the one hand, this value proposition
    has really gained currency with the explosion of digital data that made it both
    more promising and necessary to leverage computing power for data processing.
    On the other hand, the application of ML still requires significant human intervention
    and expertise to define objectives, select and curate data, design and optimize
    a model and make appropriate use of the results.
  prefs: []
  type: TYPE_NORMAL
- en: In this concluding chapter, we will briefly summarize the key tools, applications,
    and lessons learned throughout the book to avoid losing sight of the big picture
    after so much detail. We will then identify areas that we did not cover but would
    be worthwhile to focus on as you aim to expand on the many ML techniques we introduced
    and become productive in their daily use. We will highlight skill sets that are
    valuable for individual productivity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, in this chapter, we will go through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Review key takeaways and lessons learned,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Point out the next steps to build on the techniques in this book,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suggest ways to incorporate ML into your investment process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key takeaways and lessons learned
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Important insights to keep in mind as you proceed to the practice of ML for
    trading include:'
  prefs: []
  type: TYPE_NORMAL
- en: Data is the single most important ingredient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain expertise helps realize the potential value in the data, especially in
    finance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML offers tools for many use cases that should be further developed and combined
    to create solutions for new problems using data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of model objectives and performance diagnostics are key to productive
    iterations towards an optimal system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backtest overfitting is a huge challenge that requires significant attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transparency around black-box models can help build confidence and facilitate
    adoption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will elaborate a bit more on each of these ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Data is the single most important ingredient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The rise of ML in trading and everywhere else largely complements the data explosion
    that we covered in great detail. We illustrated in [Chapter 2](e7bd6fc7-7ef7-4c4e-acec-ac1d083f8902.xhtml), *Market
    and Fundamental Data* how to access and work with these data sources, historically
    the mainstay of quantitative investment. In [Chapter 3](a7cec22f-095e-49c0-a2bb-e179f6e824a8.xhtml), *Alternative
    Data for Finance*, we laid out a framework with the criteria to assess the potential
    value of alternative datasets.
  prefs: []
  type: TYPE_NORMAL
- en: A key insight is that the state-of-the-art ML techniques like deep neural networks
    are successful because their predictive performance continues to improve with
    more data. On the flip side, model and data complexity need to match to balance
    the bias-variance trade-off. Managing data quality and integrating datasets are
    key steps in realizing the potential value.
  prefs: []
  type: TYPE_NORMAL
- en: Quality control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like oil, a popular comparison these days, data passes through a pipeline
    with several stages from its raw form to a refined product that can fuel a trading
    strategy. It is critical to pay careful attention to the quality of the final
    product to get the desired mileage out of it.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, you get data in raw form and control the numerous transformations
    required for your purposes. More often, you deal with an intermediate product
    and should get clarity about what exactly the data measures at this point.
  prefs: []
  type: TYPE_NORMAL
- en: Different from oil, there is often no objective quality standard as data sources
    continue to proliferate. Instead, the quality depends on its signal content, which
    in turn depends on your investment objectives. The cost-effective evaluation of
    new datasets requires a productive workflow, including appropriate infrastructure
    that we will address in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Data integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The value of data for an investment strategy often depends on combining complementary
    sources of market, fundamental and alternative data. We saw that the predictive
    power of ML algorithms like tree-based ensembles or neural networks is in part
    due to their ability to detect non-linear relationships, in particular, interaction
    effects among variables.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to modulate the impact of a variable as a function of other model
    features thrives on data inputs that capture different aspects of a target outcome.
    The combination of asset prices with macro fundamentals, social sentiment, credit
    card payment, and satellite data will likely yield significantly more reliable
    predictions throughout different economic and market regimes than each source
    on its own (provided there the data is large enough to learn the hidden relationships).
  prefs: []
  type: TYPE_NORMAL
- en: Working with data from multiple sources increases the challenges of proper labeling.
    It is vital to assign accurate timestamps to avoid a lookahead bias by testing
    an algorithm with data before it actually became available. Data, for example,
    may have timestamps assigned by a provider that require adjustments to reflect
    the point in time when they would have been available for a live algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Domain expertise helps unlock value in data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We emphasized that data is a necessary driver of successful ML applications,
    but that domain expertise is also crucial to inform strategic direction, feature
    engineering and data selection, and model design.
  prefs: []
  type: TYPE_NORMAL
- en: In any domain, practitioners have theories about the drivers of key outcomes
    and relationships among them. Finance stands out by the amount of relevant quantitative
    research, both theoretical and empirical. Marcos López de Prado and others (see
    GitHub for references [https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Trading](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Trading))
    criticize most empirical results given pervasive data mining that may invalidate
    the findings. Nonetheless, a robust understanding of how financial markets work
    exists and should inform the selection and use of data as well as the justification
    of strategies that rely on machine learning. We outlined key ideas in [Chapter
    4](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml), *Alpha Factor Research* and [Chapter
    5](1de6a332-69f8-4530-8d18-1007d0a3eb7e.xhtml), *Strategy Evaluation*.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, novel ML techniques will likely uncover new hypotheses about
    drivers of financial outcomes that will inform ML theory and should then be independently
    tested.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering and alpha factor research
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More than the raw data, feature engineering is often the key to making signal
    useful for an algorithm. Leveraging decades of research into risk factors that
    drive returns on theoretical and empirical grounds is a good starting point to
    prioritize data transformations that are more likely to reflect relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: However, only creative feature engineering will lead to innovative strategies
    that can compete in the market over time. Even for new alpha factors, a compelling
    narrative that explains how they work, given established ideas on market dynamics
    and investor behavior, will provide more confidence to allocate capital.
  prefs: []
  type: TYPE_NORMAL
- en: The risks of false discoveries and overfitting to historical data make it even
    more necessary to prioritize strategies prior to testing rather than *let the
    data speak*. We covered how to deflate the Sharpe ratio in light of the number
    of experiments.
  prefs: []
  type: TYPE_NORMAL
- en: ML is a toolkit for solving problems with data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ML offers algorithmic solutions and techniques that can be applied to many
    use cases. Parts 2, 3, and 4 of the book (as mentioned in [Chapter 1](d68f12f8-66fd-4857-b60e-399e5bbd9ea2.xhtml),
    *Machine Learning for Trading*) have presented ML as a diverse set of tools that
    can add value to various steps of the strategy process, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Idea generation and alpha factor research,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Signal aggregation and portfolio optimization,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategy testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trade execution, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategy evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even more so, ML algorithms are designed to be further developed, adapted and
    combined to solve new problems in different contexts. For these reasons, it is
    important to understand key concepts and ideas underlying these algorithms, in
    addition to being able to apply them to data for productive experimentation and
    research as outlined in [Chapter 6](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml), *The
    Machine Learning Process*.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the best results are often achieved by combining human experts with
    ML tools. In [Chapter 1](d68f12f8-66fd-4857-b60e-399e5bbd9ea2.xhtml), *Machine
    Learning fo**r Trading*, we covered the quantamental investment style where discretionary
    and algorithmic trading converge. This approach will likely further grow in importance
    and depends on the flexible and creative application of the fundamental tools
    that we covered and their extensions to a variety of data sets.
  prefs: []
  type: TYPE_NORMAL
- en: Model diagnostics help speed up optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 6](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml), *The Machine Learning
    Process*, we outlined some of the most important concepts. ML algorithms learn
    relationships between input data and a target by making assumptions about the
    functional form. If the learning is based on noise rather than signal, predictive
    performance will suffer.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we do not know today how to separate signal and noise from the perspective
    of tomorrow's outcomes. Model diagnostics, for example, using learning curves
    and the optimization verification test can help alleviate this fundamental challenge
    and calibrate the choice or configuration of an algorithm to the data or task
    at hand. This task can be made easier by defining focused model objectives and,
    for complex models, distinguishing between performance shortcomings due to issues
    with the optimization algorithm or the objective itself.
  prefs: []
  type: TYPE_NORMAL
- en: Making do without a free lunch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No system—computer program or human—has a basis to reliably predict outcomes
    for new examples beyond those it observed during training. The only way out is
    to have some additional prior knowledge or make assumptions that go beyond the
    training examples. We covered a broad range of algorithms from [Chapter 7](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml), *Linear
    Models* and [Chapter 8](4ea03bd9-a996-462b-803f-2a27365ff636.xhtml), *Time Series
    Models*, to non-linear ensembles in [Chapter 10](7d7aa662-362a-4c3a-acda-a18fb1bad6e7.xhtml), *Decision
    Trees and Random Forest* and [Chapter 11](2fbfa6b5-87f3-49c3-b13a-5ead63471370.xhtml), *Gradient
    Boosting Machines* as well as neural networks in various chapters of part 4 of
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: We saw that a linear model makes a strong assumption that the relationship between
    inputs and outputs has a very simple form, whereas the models discussed later
    aim to learn more complex functions. While it's probably obvious that a simple
    model will fail in most circumstances, a complex model is not always better. If
    the true relationship is linear but the data is noisy, the complex model will
    learn the noise as part of the complex relationship that it assumes to exist.
    This is the basic idea behind the *No Free Lunch Theorem* that states no algorithm
    is universally superior for all tasks. Good fit in some instances comes at the
    cost of poor performance elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: The key tools to tailor the choice of the algorithm to the data are data exploration
    and experiments based on an understanding of the assumptions the model makes.
  prefs: []
  type: TYPE_NORMAL
- en: Managing the bias-variance trade-off
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A different perspective on the challenge of adapting an algorithm to data is
    the trade-off between bias and variance that cause prediction errors beyond the
    natural noisiness of the data. A simple model that does not adequately capture
    the relationships in the data will underfit and exhibit bias, that is, make systematically
    wrong predictions. A model that is too complex will overfit and learn the noise
    in addition to any signal so that the result will show a lot of variance for different
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key tool to diagnose this trade-off at any given iteration of the model
    selection and optimization process is the learning curve. It shows how training
    and validation errors depend on the sample size. This allows us to decide between
    different options to improve performance: adjust the complexity of the model or
    get more data points.'
  prefs: []
  type: TYPE_NORMAL
- en: The closer the training error is to human or other benchmarks, the more likely
    the model will overfit. The low validation error tells us that we are lucky and
    found a good model. If the validation error is high, we are not. If it continues
    to decline with the training size, however, more data may help. If the training
    error is high, more data is unlikely to help and we should instead add features
    or use a more flexible algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Define targeted model objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the first steps in the machine learning process is the definition of
    an objective for the algorithm to optimize. Sometimes, the choice is simple, for
    example, in a regression problem. A classification task can be more difficult,
    for example, when we care about precision and recall. Consolidating conflicting
    objectives into a single metric like the F1 score helps to focus optimization
    efforts. We can also include conditions that need to be met rather than optimized
    for. We also saw that reinforcement learning is all about defining the right reward
    function to guide the agent's learning process.
  prefs: []
  type: TYPE_NORMAL
- en: The optimization verification test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Andrew Ng (see references on GitHub: [https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading))
    emphasizes the distinction between performance shortcomings due to a problem with
    the learning algorithm or the optimization algorithm. Complex models like neural
    networks assume non-linear relationships and the search process of the optimization
    algorithm may end up in a local rather than a global optimum.
  prefs: []
  type: TYPE_NORMAL
- en: If, for example, a model fails to correctly translate a phrase, the test compares
    the scores for the correct prediction and the solution discovered by the search
    algorithm. If the learning algorithm scores the correct solution higher, the search
    algorithm requires improvements. Otherwise, the learning algorithm is optimizing
    for the wrong objective.
  prefs: []
  type: TYPE_NORMAL
- en: Beware of backtest overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered the risks of false discoveries due to overfitting to historical data
    repeatedly throughout the book. [Chapter 5](1de6a332-69f8-4530-8d18-1007d0a3eb7e.xhtml), *Strategy
    Evaluation* lays out the main drivers and potential remedies. The low noise-to-signal
    ratio and relatively small datasets (compared to a web-scale image or text data)
    make this challenge particularly serious in the trading domain. Awareness is critical
    because the ease of access to data and tools to apply ML increase the risks exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: There is no escape because there is no method of prevention. However, we presented
    methods to adjust backtest metrics to account for repeated trials such as the
    deflated Sharpe ratio. When working towards a live trading strategy, stage paper-trading
    and closely monitored performance during execution in the market need to be part
    of the implementation process.
  prefs: []
  type: TYPE_NORMAL
- en: How to gain insights from black-box models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep neural networks and complex ensembles can raise suspicion when they are
    considered impenetrable black-box models, in particular in light of the risks
    of backtest overfitting. We introduced several methods to gain insights into how
    these models make predictions in [Chapter 11](2fbfa6b5-87f3-49c3-b13a-5ead63471370.xhtml), *Gradient
    Boosting Machines*.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to conventional measures of feature importance, the recent game-theoretic
    innovation of **SHapley Additive exPlanations** (**SHAP**) is a significant step
    towards understanding the mechanics of complex models. SHAP values allow for exact
    attribution of features and their values to predictions so that it becomes easier
    to validate the logic of a model in the light of specific theories about market
    behavior for a given investment target. Besides justification, exact feature importance
    scores and attribution of predictions allow for deeper insights into the drivers
    of the investment outcome of interest.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, there is some controversy to which extend transparency around
    model predictions should be a goal in itself. Geoffrey Hinton, one of the inventors
    of deep learning, argues that human decisions are also often obscure and machines
    should similarly be evaluated by their results, as we expect from investment managers.
  prefs: []
  type: TYPE_NORMAL
- en: ML for trading in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you proceed to integrate the numerous tools and techniques into your investment
    and trading process, there are numerous things you could focus your efforts on.
    If your goal is to make better decisions, you should select projects that are
    realistic yet ambitious given your current skill set. This will help you to develop an
    efficient workflow underpinned by productive tools and gain practical experience.
  prefs: []
  type: TYPE_NORMAL
- en: We will briefly list some of the tools that are useful to expand on the Python
    ecosystem covered in this book and refer to the links listed on GitHub to dive
    deeper. These include big data technologies that will eventually be necessary
    to implement ML for trading strategies at scale. We will also list some of the
    platforms that allow you to implement trading strategies using Python, often with
    access to data sources and ML algorithms and libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Data management technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The central role of data in the ML process requires familiarity with a range
    of technologies to store, transform, and analyze data at scale, including the
    use of cloud-based services such as Amazon Web Services, Azure, and Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Database systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data storage implies the use of databases, historically dominated by **relational
    database management systems** (**RDBMS**) that use SQL to store and retrieve data
    in a well-defined table format with commercial providers like Oracle and Microsoft
    and open-source implementations like PostgreSQL and MySQL. More recently, alternatives
    have emerged that are often collectively labeled NoSQL but are quite diverse,
    namely:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Key-value storage**: Fast read/write access to objects. We covered the HDF5
    format in [Chapter 2](e7bd6fc7-7ef7-4c4e-acec-ac1d083f8902.xhtml), *Market and
    Fundamental Data* that facilitates fast access to a pandas DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Columnar storage**: Capitalizes on the homogeneity of data in a column to
    facilitates compression and faster column-based operations such as aggregation.
    Used in the popular Amazon Redshift data warehouse solution, Apache Parquet, Cassandra,
    or Google''s Big Table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document store**: Designed to store data that defies the rigid schema definition
    required by an RDBMS. Popularized by web applications that use JSON or XML format
    that we encountered in [Chapter 4](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml),
    *Alpha Factor Research*. Used, for example, in MongoDB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph database**: Designed to store networks that have nodes and edges and
    specializes in queries about network metrics and relationships. Used in Neo4J
    and Apache Giraph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There has been some conversion towards the conventions established by the relational
    database systems. The Python ecosystem facilitates the interaction with many standard
    data sources and provides fast HDF5 and Parquet formats as demonstrated throughout
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: Big Data technologies – Hadoop and Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data management at scale, that is, hundreds of GB and beyond, require the use
    of multiple machines that form a cluster to conduct read, write and compute operations
    in parallel, that is, it is distributed over various machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hadoop ecosystem has emerged as an open-source software framework for distributed
    storage and processing of big data using the MapReduce programming model developed
    by Google. The ecosystem has diversified under the roof of the Apache Foundation
    and today includes numerous projects that cover different aspects of data management
    at scale. Key tools within Hadoop include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Pig**: Data processing language to implement large-scale **extract-transform-load**
    (**ETL**) pipelines using MapReduce, developed at Yahoo'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Hive**: The defacto standard for interactive SQL queries over petabytes
    of data developed at Facebook'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache HBASE**: NoSQL database for real-time read/write access that scales
    linearly to billions of rows and millions of columns, and can combines data sources
    using a variety of different schemas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark has become the most popular platform for interactive analytics
    on a cluster. The MapReduce framework allowed for parallel computation but required
    repeated read/write operations from disk to ensure data redundancy. Spark has
    dramatically accelerated computation at scale due to the **Resilient Distributed
    Data** (**RDD**) structure that allows for highly optimized in-memory computation.
    This includes iterative computation as required for optimization, for example
    gradient descent for numerous ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: ML tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered many libraries of the Python ecosystem in this book. Python has evolved
    to become the language of choice for data science and ML and the set of open-source
    libraries continues to both diversify and mature, built on the robust core of
    scientific computing libraries NumPy and SciPy. The popular pandas library that
    has contributed significantly to popularizing the use of Python for data science
    is planning its 1.0 release. The scikit-learn interface has become the standard
    for modern ML libraries like `xgboost` or `lightgbm` that often interface with
    the various workflow automation tools like `GridSearchCV` and `Pipeline` that
    we used repeatedly throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several providers that aim to facilitate the ML workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: H2O.ai ([https://www.h2o.ai/](https://www.h2o.ai/)) offers the H2O platform
    that integrates cloud computing with ML automation. It allows users to fit thousands
    of potential models to their data to explore patterns in the data. It has interfaces
    in Python as well as R and Java.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataRobot aims to automate the model development process by providing a platform
    to rapidly build and deploy predictive models in the cloud or on-premise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataiku is a collaborative data science platform designed to help the analysts
    and engineers explore, prototype, build, and deliver their own data products
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are also several open-source initiatives led by companies that build
    on and expand the Python ecosystem:'
  prefs: []
  type: TYPE_NORMAL
- en: The quantitative hedge fund Two Sigma contributes quantitative analysis tools
    to the Jupyter Notebook environment under the `beakerx` project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bloomberg has integrated the Jupyter Notebook into its terminal to facilitate
    the interactive analysis of their financial data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Online trading platforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main options to develop trading strategies that use machine learning are
    online platforms that often look for and allocate capital to successful trading
    strategies. Popular solutions include Quantopian, Quantconnect, QuantRocket, and
    the more recent Alpha Trading Labs that focuses on high-frequency trading.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, **Interactive Brokers** (**IB**) offers a Python API that you can
    use to develop your own trading solution.
  prefs: []
  type: TYPE_NORMAL
- en: Quantopian
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We introduced the Quantopian platform and demonstrated the use of its research
    and trading environment to analyze and test trading strategies against historical
    data. Quantopian uses Python and offers lots of educational material.
  prefs: []
  type: TYPE_NORMAL
- en: Quantopian hosts ongoing daily competitions to recruit algorithms for its crowd-sourced
    hedge fund portfolio. Quantopian provides capital to the winning algorithm. Live-trading
    was discontinued in September 2017, but the platform still provides a large range
    of historical data and attracts an active community of developers and traders
    that is a good starting point to discuss ideas and learn from others.
  prefs: []
  type: TYPE_NORMAL
- en: QuantConnect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: QuantConnect is another open source, community-driven algorithmic trading platform
    that competes with Quantopian. It also provides an IDE to backtest and live-trade
    algorithmic strategies using Python and other languages.
  prefs: []
  type: TYPE_NORMAL
- en: QuantConnect also has a dynamic, global community from all over the world, and
    provides access to numerous asset classes, including equities, futures, forex,
    and cryptocurrency. It offers live-trading integration with various brokers such
    as IB, OANDA, and GDAX.
  prefs: []
  type: TYPE_NORMAL
- en: QuantRocket
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: QuantRocket is a Python-based platform for researching, backtesting, and running
    automated, quantitative trading strategies. It provides data collection tools,
    multiple data vendors, a research environment, multiple backtesters, and live
    and paper trading through IB. It prides itself on support for international equity
    trading and sets itself apart with its flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: QuantRocket supports multiple engines — its own Moonshot, as well as third-party
    engines chosen by the user. While QuantRocket doesn't have a traditional IDE,
    it is integrated well with Jupyter to produce something similar. QuantRocket is
    not free, however, and pricing starts at 19 USD/month at the time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started by highlighting the explosion of digital data and the emergence of
    ML as a strategic capability for investment and trading strategies. This dynamic
    reflects global business and technology trends beyond finance and is much more
    likely to continue than to stall or reverse. Many investment firms are just getting
    started to leverage the range of artificial intelligence tools, just as individuals
    are acquiring the relevant skills and business processes are adapting to these
    new opportunities for value creation, as outlined in the introductory chapter.
  prefs: []
  type: TYPE_NORMAL
- en: There are also numerous exciting developments for the application of ML to trading on
    the horizon that are likely to further propel the current momentum. They are likely
    to become relevant in the coming years and include the automation of the ML process,
    the generation of synthetic training data, and the emergence of quantum computing.
    The extraordinary vibrancy of the field implies that this alone could fill a book
    and the journey will continue to remain exciting.
  prefs: []
  type: TYPE_NORMAL
