["```py\nimport statsmodels.api as sm\nX_ols = sm.add_constant(X)\nmodel = sm.OLS(y, X_ols).fit()\nmodel.summary() \n```", "```py\nbeta = np.linalg.inv(X_ols.T.dot(X_ols)).dot(X_ols.T.dot(y))\npd.Series(beta, index=X_ols.columns)\nconst   53.29\nX_1      0.99\nX_2      2.96 \n```", "```py\nthree_dee = plt.figure(figsize=(15, 5)).gca(projection='3d')\nthree_dee.scatter(data.X_1, data.X_2, data.Y, c='g')\ndata['y-hat'] = model.predict()\nto_plot = data.set_index(['X_1', 'X_2']).unstack().loc[:, 'y-hat']\nthree_dee.plot_surface(X_1, X_2, to_plot.values, color='black', alpha=0.2, linewidth=1, antialiased=True)\nfor _, row in data.iterrows():\n    plt.plot((row.X_1, row.X_1), (row.X_2, row.X_2), (row.Y, row['y-hat']),              'k-');\nthree_dee.set_xlabel('$X_1$');three_dee.set_ylabel('$X_2$');three_dee.set_zlabel('$Y, \\hat{Y}$') \n```", "```py\nscaler = StandardScaler()\nX_ = scaler.fit_transform(X) \n```", "```py\nsgd = SGDRegressor(loss='squared_loss', \n                   fit_intercept=True,\n                   shuffle=True, # shuffle data for better estimates\n                   random_state=42,\n                   learning_rate='invscaling', # reduce rate over time\n                   eta0=0.01, # parameters for learning rate path\n                   power_t=0.25) \n```", "```py\nsgd.fit(X=X_, y=y)\nresids = pd.DataFrame({'sgd': y - sgd.predict(X_),\n                      'ols': y - model.predict(sm.add_constant(X))})\nresids.pow(2).sum().div(len(y)).pow(.5)\nols   48.22\nsgd   48.22 \n```", "```py\nimport pandas_datareader.data as web\nff_factor = 'F-F_Research_Data_5_Factors_2x3'\nff_factor_data = web.DataReader(ff_factor, 'famafrench', start='2010', \n                               end='2017-12')[0]\nff_factor_data.info()\nPeriodIndex: 96 entries, 2010-01 to 2017-12\nFreq: M\nData columns (total 6 columns):\nMkt-RF 96 non-null float64\nSMB    96 non-null float64\nHML    96 non-null float64\nRMW    96 non-null float64\nCMA    96 non-null float64\nRF     96 non-null float64 \n```", "```py\nff_portfolio = '17_Industry_Portfolios'\nff_portfolio_data = web.DataReader(ff_portfolio, 'famafrench', start='2010', \n                                  end='2017-12')[0]\nff_portfolio_data = ff_portfolio_data.sub(ff_factor_data.RF, axis=0)\nff_factor_data = ff_factor_data.drop('RF', axis=1)\nff_portfolio_data.info()\nPeriodIndex: 96 entries, 2010-01 to 2017-12\nFreq: M\nData columns (total 17 columns):\nFood     96 non-null float64\nMines    96 non-null float64\nOil      96 non-null float64\n...\nRtail    96 non-null float64\nFinan    96 non-null float64\nOther    96 non-null float64 \n```", "```py\nbetas = []\nfor industry in ff_portfolio_data:\n    step1 = OLS(endog=ff_portfolio_data.loc[ff_factor_data.index, industry],\n                exog=add_constant(ff_factor_data)).fit()\n    betas.append(step1.params.drop('const'))\nbetas = pd.DataFrame(betas,\n                     columns=ff_factor_data.columns,\n                     index=ff_portfolio_data.columns)\nbetas.info()\nIndex: 17 entries, Food  to Other\nData columns (total 5 columns):\nMkt-RF    17 non-null float64\nSMB       17 non-null float64\nHML       17 non-null float64\nRMW       17 non-null float64\nCMA       17 non-null float64 \n```", "```py\nlambdas = []\nfor period in ff_portfolio_data.index:\n    step2 = OLS(endog=ff_portfolio_data.loc[period, betas.index],\n                exog=betas).fit()\n    lambdas.append(step2.params)\nlambdas = pd.DataFrame(lambdas,\n                       index=ff_portfolio_data.index,\n                       columns=betas.columns.tolist())\nlambdas.info()\nPeriodIndex: 96 entries, 2010-01 to 2017-12\nFreq: M\nData columns (total 5 columns):\nMkt-RF    96 non-null float64\nSMB       96 non-null float64\nHML       96 non-null float64\nRMW       96 non-null float64\nCMA       96 non-null float64 \n```", "```py\nlambdas.mean()\nMkt-RF    1.243632\nSMB      -0.004863\nHML      -0.688167\nRMW      -0.237317\nCMA      -0.318075\nRF       -0.013280 \n```", "```py\nmodel = LinearFactorModel(portfolios=ff_portfolio_data, \n                          factors=ff_factor_data)\nres = model.fit() \n```", "```py\nSTART = '2013-01-01'\nEND = '2017-12-31'\nidx = pd.IndexSlice # to select from pd.MultiIndex\nDATA_STORE = '../data/assets.h5'\nwith pd.HDFStore(DATA_STORE) as store:\n    prices = (store['quandl/wiki/prices']\n              .loc[idx[START:END, :],\n                   ['adj_open', 'adj_close', 'adj_low', \n                    'adj_high', 'adj_volume']]\n              .rename(columns=lambda x: x.replace('adj_', ''))\n              .swaplevel()\n              .sort_index())\n    stocks = (store['us_equities/stocks']\n              .loc[:, ['marketcap', 'ipoyear', 'sector']]) \n```", "```py\nMONTH = 21\nYEAR = 12 * MONTH\nmin_obs = 2 * YEAR\nnobs = prices.groupby(level='ticker').size()\nkeep = nobs[nobs > min_obs].index\nprices = prices.loc[idx[keep, :], :] \n```", "```py\nstocks = stocks[~stocks.index.duplicated() & stocks.sector.notnull()]\n# clean up sector names\nstocks.sector = stocks.sector.str.lower().str.replace(' ', '_')\nstocks.index.name = 'ticker'\nshared = (prices.index.get_level_values('ticker').unique()\n          .intersection(stocks.index))\nstocks = stocks.loc[shared, :]\nprices = prices.loc[idx[shared, :], :] \n```", "```py\nprices.info(null_counts=True)\nMultiIndex: 2748774 entries, (A, 2013-01-02) to (ZUMZ, 2017-12-29)\nData columns (total 5 columns):\nopen      2748774 non-null float64\nclose     2748774 non-null float64\nlow       2748774 non-null float64\nhigh      2748774 non-null float64\nvolume    2748774 non-null float64\nmemory usage: 115.5+ MB \n```", "```py\nstocks.info()\nIndex: 2224 entries, A to ZUMZ\nData columns (total 3 columns):\nmarketcap    2222 non-null float64\nipoyear      962 non-null float64\nsector       2224 non-null object\nmemory usage: 69.5+ KB \n```", "```py\nprices['dollar_vol'] = prices.loc[:, 'close'].mul(prices.loc[:, 'volume'], axis=0)\nprices['dollar_vol'] = (prices\n                        .groupby('ticker',\n                                 group_keys=False,\n                                 as_index=False)\n                        .dollar_vol\n                        .rolling(window=21)\n                        .mean()\n                        .reset_index(level=0, drop=True)) \n```", "```py\nprices['dollar_vol_rank'] = (prices\n                             .groupby('date')\n                             .dollar_vol\n                             .rank(ascending=False)) \n```", "```py\nprices['rsi'] = prices.groupby(level='ticker').close.apply(RSI) \n```", "```py\n(prices[prices.dollar_vol_rank<100]\n .groupby('rsi_signal')['target_5d'].describe()) \n```", "```py\ndef compute_bb(close):\n    high, mid, low = BBANDS(close)\n    return pd.DataFrame({'bb_high': high, 'bb_low': low}, index=close.index)\nprices = (prices.join(prices\n                      .groupby(level='ticker')\n                      .close\n                      .apply(compute_bb))) \n```", "```py\nprices['bb_high'] = prices.bb_high.sub(prices.close).div(prices.bb_high).apply(np.log1p)\nprices['bb_low'] = prices.close.sub(prices.bb_low).div(prices.close).apply(np.log1p) \n```", "```py\ndef compute_atr(stock_data):\n    df = ATR(stock_data.high, stock_data.low, \n             stock_data.close, timeperiod=14)\n    return df.sub(df.mean()).div(df.std())\nprices['atr'] = (prices.groupby('ticker', group_keys=False)\n                 .apply(compute_atr)) \n```", "```py\ndef compute_macd close:\n   macd = MACD(close)[0]\n    return (macd - np.mean(macd))/np.std(macd)\nprices['macd'] = (prices\n                  .groupby('ticker', group_keys=False)\n                  .close\n                  .apply(lambda x: MACD(x)[0])) \n```", "```py\nq = 0.0001\nlags = [1, 5, 10, 21, 42, 63]\nfor lag in lags:\n    prices[f'return_{lag}d'] = (prices.groupby(level='ticker').close\n                                .pct_change(lag)\n                                .pipe(lambda x: x.clip(lower=x.quantile(q),\n                                                       upper=x.quantile(1 - q)\n                                                       ))\n                                .add(1)\n                                .pow(1 / lag)\n                                .sub(1)\n                                ) \n```", "```py\nfor t in [1, 2, 3, 4, 5]:\n    for lag in [1, 5, 10, 21]:\n        prices[f'return_{lag}d_lag{t}'] = (prices.groupby(level='ticker')\n                                           [f'return_{lag}d'].shift(t * lag)) \n```", "```py\nfor t in [1, 5, 10, 21]:\n    prices[f'target_{t}d'] = prices.groupby(level='ticker')[f'return_{t}d'].shift(-t) \n```", "```py\ndf = pd.DataFrame({'categories': ['A','B', 'C']})\n  categories\n0          A\n1          B\n2          C\npd.get_dummies(df)\n   categories_A  categories_B  categories_C\n0             1             0             0\n1             0             1             0\n2             0             0             1 \n```", "```py\npd.get_dummies(df, drop_first=True)\n   categories_B  categories_C\n0             0             0\n1             1             0\n2             0             1 \n```", "```py\nprices['year'] = prices.index.get_level_values('date').year\nprices['month'] = prices.index.get_level_values('date').month \n```", "```py\nprices = prices.join(stocks[['sector']])\nprices = pd.get_dummies(prices,\n                        columns=['year', 'month', 'sector'],\n                        prefix=['year', 'month', ''],\n                        prefix_sep=['_', '_', ''],\n                        drop_first=True) \n```", "```py\ndata = data[data.dollar_vol_rank<100] \n```", "```py\ny = data.filter(like='target')\nX = data.drop(y.columns, axis=1) \n```", "```py\ntarget = 'target_5d'\nmodel = OLS(endog=y[target], exog=add_constant(X))\ntrained_model = model.fit()\ntrained_model.summary() \n```", "```py\n=====================================================================\nOmnibus:               33104.830   Durbin-Watson:               0.436\nProb(Omnibus):             0.000   Jarque-Bera (JB):      1211101.670\nSkew:                     -0.780   Prob(JB):                     0.00\nKurtosis:                 19.205   Cond. No.                     79.8\n===================================================================== \n```", "```py\ntrain_period_length = 63\ntest_period_length = 10\nn_splits = int(3 * YEAR/test_period_length)\nlookahead =1 \ncv = MultipleTimeSeriesCV(n_splits=n_splits,\n                          test_period_length=test_period_length,\n                          lookahead=lookahead,\n                          train_period_length=train_period_length) \n```", "```py\ntarget = f'target_{lookahead}d'\nlr_predictions, lr_scores = [], []\nlr = LinearRegression()\nfor i, (train_idx, test_idx) in enumerate(cv.split(X), 1):\n    X_train, y_train, = X.iloc[train_idx], y[target].iloc[train_idx]\n    X_test, y_test = X.iloc[test_idx], y[target].iloc[test_idx]\n    lr.fit(X=X_train, y=y_train)\n    y_pred = lr.predict(X_test)\n    preds_by_day = (y_test.to_frame('actuals').assign(predicted=y_pred)\n                    .groupby(level='date'))\n    ic = preds_by_day.apply(lambda x: spearmanr(x.predicted,\n                                                x.actuals)[0] * 100)\n    rmese = preds_by_day.apply(lambda x: np.sqrt(\n                               mean_squared_error(x.predicted, x.actuals)))\n    scores = pd.concat([ic.to_frame('ic'), rmse.to_frame('rmse')], axis=1)\n\n    lr_scores.append(scores)\n    lr_predictions.append(preds) \n```", "```py\nridge_alphas = np.logspace(-4, 4, 9)\nridge_alphas = sorted(list(ridge_alphas) + list(ridge_alphas * 5)) \n```", "```py\nfor alpha in ridge_alphas:\n    model = Ridge(alpha=alpha,\n                  fit_intercept=False,\n                  random_state=42)\n    pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('model', model)])\n    for i, (train_idx, test_idx) in enumerate(cv.split(X), 1):\n        X_train, y_train = X.iloc[train_idx], y[target].iloc[train_idx]\n        X_test, y_test = X.iloc[test_idx], y[target].iloc[test_idx]\n        pipe.fit(X=X_train, y=y_train)\n        y_pred = pipe.predict(X_test)\n        preds = y_test.to_frame('actuals').assign(predicted=y_pred)\n        preds_by_day = preds.groupby(level='date')\n        scores = pd.concat([preds_by_day.apply(lambda x: \n                                               spearmanr(x.predicted, \n                                                   x.actuals)[0] * 100)\n                            .to_frame('ic'),\n                            preds_by_day.apply(lambda x: np.sqrt(\n                                                    mean_squared_error(\n                                                    y_pred=x.predicted, \n                                                    y_true=x.actuals)))\n                            .to_frame('rmse')], axis=1)\n        ridge_scores.append(scores.assign(alpha=alpha))\n        ridge_predictions.append(preds.assign(alpha=alpha))\n        coeffs.append(pipe.named_steps['model'].coef_) \n```", "```py\nlasso_alphas = np.logspace(-10, -3, 8)\nfor alpha in lasso_alphas:\n    model = Lasso(alpha=alpha,\n                  fit_intercept=False,\n                  random_state=42,\n                  tol=1e-4,\n                  max_iter=1000,\n                  warm_start=True,\n                  selection='random')\n    pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('model', model)]) \n```", "```py\nimport statsmodels.api as sm\ndata = pd.get_dummies(data.drop(drop_cols, axis=1), columns=['quarter'], drop_first=True).dropna()\nmodel = sm.Logit(data.target, sm.add_constant(data.drop('target', axis=1)))\nresult = model.fit()\nresult.summary() \n```", "```py\ntarget = 'target_1d'\ny['label'] = (y[target] > 0).astype(int) \n```", "```py\ny.label.value_counts()\n1    56443\n0    53220 \n```", "```py\nn_splits = 4*252\ncv = TimeSeriesCV(n_splits=n_splits,\n                  test_period_length=1,\n                  train_period_length=252)\nCs = np.logspace(-5, 5, 11) \n```", "```py\nfor C in Cs:\n    model = LogisticRegression(C=C, fit_intercept=True)\n    pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('model', model)])\n    for i, (train_idx, test_idx) in enumerate(cv.split(X), 1):\n        X_train, y_train, = X.iloc[train_idx], y.label.iloc[train_idx]\n        pipe.fit(X=X_train, y=y_train)\n        X_test, y_test = X.iloc[test_idx], y.label.iloc[test_idx]\n        y_score = pipe.predict_proba(X_test)[:, 1]\n        auc = roc_auc_score(y_score=y_score, y_true=y_test) \n```", "```py\n actuals = y[target].iloc[test_idx]\n        ic, pval = spearmanr(y_score, actuals) \n```"]