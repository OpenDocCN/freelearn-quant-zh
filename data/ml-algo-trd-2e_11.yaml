- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Random Forests – A Long-Short Strategy for Japanese Stocks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林 - 用于日本股票的长短策略
- en: 'In this chapter, we will learn how to use two new classes of machine learning
    models for trading: **decision trees** and **random forests**. We will see how
    decision trees learn rules from data that encode nonlinear relationships between
    the input and the output variables. We will illustrate how to train a decision
    tree and use it for prediction with regression and classification problems, visualize
    and interpret the rules learned by the model, and tune the model''s hyperparameters
    to optimize the bias-variance trade-off and prevent overfitting.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用两种新的机器学习模型进行交易：**决策树**和**随机森林**。我们将看到决策树如何从数据中学习规则，这些规则编码了输入和输出变量之间的非线性关系。我们将说明如何训练决策树，并使用它进行回归和分类问题的预测，可视化并解释模型学到的规则，并调整模型的超参数以优化偏差-方差权衡，并防止过拟合。
- en: Decision trees are not only important standalone models but are also frequently
    used as components in other models. In the second part of this chapter, we will
    introduce ensemble models that combine multiple individual models to produce a
    single aggregate prediction with lower prediction-error variance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树不仅是重要的独立模型，而且经常被用作其他模型的组成部分。在本章的第二部分中，我们将介绍集成模型，这些模型将多个单独模型组合起来，产生一个具有更低预测误差方差的单一聚合预测。
- en: We will illustrate **bootstrap aggregation**, often called *bagging*, as one
    of several methods to randomize the construction of individual models and reduce
    the correlation of the prediction errors made by an ensemble's components. We
    will illustrate how bagging effectively reduces the variance and learn how to
    configure, train, and tune random forests. We will see how random forests, as
    an ensemble of a (potentially large) number of decision trees, can dramatically
    reduce prediction errors, at the expense of some loss in interpretation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将阐述**自助聚合**，通常称为*bagging*，作为几种方法之一，用于随机化个体模型的构建，并减少由集合成员的预测误差造成的相关性。我们将说明如何通过
    bagging 有效地减少方差，并学习如何配置、训练和调整随机森林。我们将看到随机森林作为一个（可能较大的）决策树集合，可以显著减少预测误差，但可能会牺牲一些解释性。
- en: Then, we will proceed and build a long-short trading strategy that uses a random
    forest to generate profitable signals for large-cap Japanese equities over the
    last 3 years. We will source and prepare the stock price data, tune the hyperparameters
    of a random forest model, and backtest trading rules based on the model's signals.
    The resulting long-short strategy uses machine learning rather than the cointegration
    relationship we saw in *Chapter 9*, *Time-Series Models for Volatility Forecasts
    and Statistical Arbitrage*, to identify and trade baskets of securities whose
    prices will likely move in opposite directions over a given investment horizon.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将继续构建一个使用随机森林为过去 3 年大型日本股票生成盈利信号的长短交易策略。我们将获取并准备股价数据，调整随机森林模型的超参数，并根据模型的信号进行交易规则的回测。由此产生的长短策略使用机器学习而不是我们在第
    9 章中看到的共整关系，以识别和交易在给定投资期限内价格可能朝相反方向移动的证券篮子。
- en: 'In short, after reading this chapter, you will be able to:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，阅读完本章后，你将能够：
- en: Use decision trees for regression and classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树进行回归和分类
- en: Gain insights from decision trees and visualize the rules learned from the data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从决策树中获得见解，并可视化从数据中学到的规则
- en: Understand why ensemble models tend to deliver superior results
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解为什么集成模型往往能够提供优越的结果
- en: Use bootstrap aggregation to address the overfitting challenges of decision
    trees
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自助聚合来解决决策树过拟合的挑战
- en: Train, tune, and interpret random forests
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练、调整和解释随机森林
- en: Employ a random forest to design and evaluate a profitable trading strategy
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林设计和评估盈利交易策略
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 仓库的相应目录中找到本章的代码示例和其他资源的链接。笔记本包括图像的彩色版本。
- en: Decision trees – learning rules from data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树 - 从数据中学习规则
- en: A decision tree is a machine learning algorithm that predicts the value of a
    target variable based on **decision rules learned from data**. The algorithm can
    be applied to both regression and classification problems by changing the objective
    function that governs how the tree learns the rules.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种机器学习算法，它根据从数据中学习到的**决策规则**来预测目标变量的值。该算法可以通过改变管理树如何学习规则的目标函数应用于回归和分类问题。
- en: We will discuss how decision trees use rules to make predictions, how to train
    them to predict (continuous) returns as well as (categorical) directions of price
    movements, and how to interpret, visualize, and tune them effectively. See Rokach
    and Maimon (2008) and Hastie, Tibshirani, and Friedman (2009) for additional details
    and further background information.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论决策树如何使用规则进行预测，如何训练它们来预测（连续的）收益以及（分类的）价格走势方向，以及如何有效地解释、可视化和调整它们。有关更多详细信息和背景信息，请参阅Rokach和Maimon（2008）以及Hastie、Tibshirani和Friedman（2009）。
- en: How trees learn and apply decision rules
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树是如何学习和应用决策规则的
- en: The **linear models** we studied in *Chapter 7*, *Linear Models – From Risk
    Factors to Return Forecasts*, and *Chapter 9*, *Time-Series Models for Volatility
    Forecasts and Statistical Arbitrage*, learn a set of parameters to predict the
    outcome using a linear combination of the input variables, possibly after being
    transformed by an S-shaped link function, in the case of logistic regression.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第7章*，*线性模型 - 从风险因素到收益预测*和*第9章*，*时间序列模型用于波动率预测和统计套利*中学习的**线性模型**通过学习一组参数来预测输出，可能在逻辑回归的情况下通过S形链接函数进行转换。
- en: 'Decision trees take a different approach: they learn and sequentially apply
    a set of rules that split data points into subsets and then make one prediction
    for each subset. The predictions are based on the outcome values for the subset
    of training samples that result from the application of a given sequence of rules.
    **Classification trees** predict a probability estimated from the relative class
    frequencies or the value of the majority class directly, whereas **regression
    trees** compute prediction from the mean of the outcome values for the available
    data points.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树采用不同的方法：它们学习并依次应用一组规则，将数据点分成子集，然后为每个子集做出一个预测。这些预测基于应用给定规则序列所导致的训练样本子集的结果值。**分类树**预测从相对类频率或最多类的值直接估计的概率，而**回归树**计算可用数据点的结果值均值的预测。
- en: 'Each of these rules relies on one particular feature and uses a threshold to
    split the samples into two groups, with values either below or above the threshold
    for this feature. A **binary tree** naturally represents the logic of the model:
    the root is the starting point for all samples, nodes represent the application
    of the decision rules, and the data moves along the edges as it is split into
    smaller subsets until it arrives at a leaf node, where the model makes a prediction.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每一条规则都依赖于一个特定的特征，并使用一个阈值将样本分成两组，其中值要么低于要么高于该特征的阈值。**二叉树**自然地表示了模型的逻辑：根是所有样本的起点，节点代表决策规则的应用，数据沿着边移动，当它被分成更小的子集时，直到到达叶节点，模型进行预测。
- en: For a linear model, the parameter values allow an interpretation of the impact
    of the input variables on the output and the model's prediction. In contrast,
    for a decision tree, the various possible paths from the root to the leaves determine
    how the features and their values lead to specific decisions by the model. As
    a consequence, decision trees are **capable of capturing interdependence** among
    features that linear models cannot capture "out of the box."
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性模型，参数值允许解释输入变量对输出和模型预测的影响。相反，对于决策树，从根到叶的各种可能路径确定了特征及其值如何导致模型做出具体决策。因此，决策树能够捕捉线性模型无法“开箱即用”捕捉的特征之间的**相互依赖**。
- en: 'The following diagram highlights how the model learns a rule. During training,
    the algorithm scans the features and, for each feature, seeks to find a cutoff
    that splits the data to minimize the loss that results from predictions made.
    It does so using the subsets that would result from the split, weighted by the
    number of samples in each subset:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表突出显示了模型如何学习一条规则。在训练过程中，算法扫描特征，并且对于每个特征，它寻求找到一个分割数据以最小化由预测造成的损失的切分点。它使用将结果来自于分割的子集，按每个子集中的样本数量加权：
- en: '![](img/B15439_11_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_01.png)'
- en: 'Figure 11.1: How a decision tree learns rules from data'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：决策树如何从数据中学习规则
- en: To build an entire tree during training, the learning algorithm repeats this
    process of dividing the feature space, that is, the set of possible values for
    the *p* input variables, *X*[1], *X*[2], ..., *X*[p], into mutually-exclusive
    and collectively exhaustive regions, each represented by a leaf node. Unfortunately,
    the algorithm will not be able to evaluate every possible partition of the feature
    space, given the explosive number of possible combinations of sequences of features
    and thresholds. Tree-based learning takes a **top-down**, **greedy approach**,
    known as **recursive binary splitting**, to overcome this computational limitation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间构建整个树，学习算法重复此过程，即将特征空间，即*p*个输入变量*X*[1]、*X*[2]、...、*X*[p]的可能值集合，划分为互斥且集体穷尽的区域，每个区域由一个叶节点表示。不幸的是，由于特征空间的可能组合和阈值序列的爆炸性数量，算法将无法评估特征空间的每种可能分区。基于树的学习采用了一种**自顶向下**、**贪婪的方法**，称为**递归二元分割**，以克服这种计算限制。
- en: This process is recursive because it uses subsets of data resulting from prior
    splits. It is top-down because it begins at the root node of the tree, where all
    observations still belong to a single region, and then successively creates two
    new branches of the tree by adding one more split to the predictor space. It is
    greedy because the algorithm picks the best rule in the form of a feature-threshold
    combination based on the immediate impact on the objective function, rather than
    looking ahead and evaluating the loss several steps ahead. We will return to the
    splitting logic in the more specific context of regression and classification
    trees because this represents the major difference between them.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程是递归的，因为它使用来自先前分割的数据子集。它是自顶向下的，因为它从树的根节点开始，所有观察仍然属于单个区域，然后通过向预测器空间添加一个以上的分割来连续创建树的两个新分支。它是贪婪的，因为算法根据对目标函数的直接影响选择最佳规则，而不是向前看并评估数步之后的损失。我们将在更具体的回归和分类树的上下文中返回分割逻辑，因为这代表了它们之间的主要差异。
- en: The number of training samples continues to shrink as recursive splits add new nodes
    to the tree. If rules split the samples evenly, resulting in a perfectly balanced
    tree with an equal number of children for every node, then there would be 2^n nodes
    at level *n*, each containing a corresponding fraction of the total number of
    observations. In practice, this is unlikely, so the number of samples along some
    branches may diminish rapidly, and trees tend to grow to different levels of depth
    along different paths.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 训练样本数量随着递归分割向树中添加新节点而不断减少。如果规则将样本均匀分割，导致树完美平衡，每个节点都有相同数量的子节点，那么在第*n*级就会有2^n个节点，每个节点包含总观测数的相应部分。实际上，这是不太可能的，因此沿某些分支的样本数量可能会迅速减少，并且树倾向于沿不同路径生长到不同的深度。
- en: Recursive splitting would continue until each leaf node contains only a single
    sample and the training error has been reduced to zero. We will introduce several
    methods to limit splits and prevent this natural tendency of decision trees to
    produce extreme overfitting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 递归分割将继续，直到每个叶节点仅包含单个样本，并且训练误差已经降低到零。我们将介绍几种方法来限制分割并防止决策树产生极端过拟合的自然倾向。
- en: To arrive at a **prediction** for a new observation, the model uses the rules
    that it inferred during training to decide which leaf node the data point should
    be assigned to, and then uses the mean (for regression) or the mode (for classification)
    of the training observations in the corresponding region of the feature space.
    A smaller number of training samples in a given region of the feature space, that
    is, in a given leaf node, reduces the confidence in the prediction and may reflect
    overfitting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对新的观测值进行**预测**，模型使用在训练期间推断出的规则来决定数据点应分配到哪个叶节点，然后使用特征空间相应区域中训练观测的平均值（用于回归）或模式（用于分类）。特征空间中给定区域（即给定叶节点）中训练样本数量较少，会降低预测的置信度，并可能反映出过拟合。
- en: Decision trees in practice
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践中的决策树
- en: In this section, we will illustrate how to use tree-based models to gain insight
    and make predictions. To demonstrate regression trees, we predict returns, and
    for the classification case, we return to the example of positive and negative
    asset price moves. The code examples for this section are in the notebook `decision_trees`,
    unless stated otherwise.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将说明如何使用基于树的模型来获得洞察并进行预测。为了演示回归树，我们预测收益，对于分类案例，我们回到了正向和负向资产价格变动的示例。本节的代码示例位于笔记本`decision_trees`中，除非另有说明。
- en: The data – monthly stock returns and features
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据 - 月度股票收益和特征
- en: 'We will select a subset of the Quandl US equity dataset covering the period
    2006-2017 and follow a process similar to our first feature engineering example
    in *Chapter 4*, *Financial Feature Engineering – How to Research Alpha Factors*.
    We will compute monthly returns and 25 (hopefully) predictive features for the
    500 most-traded stocks based on the 5-year moving average of their dollar volume,
    yielding 56,756 observations. The features include:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择涵盖2006-2017年期间的Quandl美国股票数据集的子集，并按照我们第一个特征工程示例中的过程进行操作，*第4章*，*金融特征工程 -
    如何研究阿尔法因子*。我们将计算月度收益和基于它们的5年移动平均值的500种最常交易的股票的25个（希望是）预测性特征，产生56,756个观察值。这些特征包括：
- en: '**Historical returns** for the past 1, 3, 6, and 12 months.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过去1、3、6和12个月的**历史收益**。
- en: '**Momentum indicators** that relate the most recent 1- or 3-month returns to
    those for longer horizons.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动量指标**将最近1个或3个月的收益与较长时间跨度的收益相关联。'
- en: '**Technical indicators** designed to capture volatility like the (normalized)
    average true range (NATR and ATR) and momentum like the **relative strength index**
    (**RSI**).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计用于捕捉波动性的**技术指标**，如（归一化的）平均真实范围（NATR和ATR）和像**相对强弱指数**（**RSI**）这样的动量指标。
- en: '**Factor loadings** for the five Fama-French factors based on rolling OLS regressions.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据滚动OLS回归的**五个Fama-French因子的因子加载**。
- en: '**Categorical variables** for year and month, as well as sector.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**年份和月份**以及部门的**分类变量**。'
- en: '*Figure 11.2* displays the mutual information between these features and the
    monthly returns we use for regression (left panel) and their binarized classification
    counterpart, which represents positive or negative price moves for the same period.
    It shows that, on a univariate basis, there appear to be substantial differences
    in the signal content regarding both outcomes across the features.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.2*显示了这些特征与我们用于回归的月度收益之间的互信息（左侧面板），以及它们的二值化分类对应物，代表了相同期间的正向或负向价格变动。它显示，从单变量的角度来看，无论是对于这些特征的哪一种结果，都存在着信号内容的显著差异。'
- en: 'More details can be found in the `data_prep` notebook in the GitHub repository
    for this chapter. The decision tree models in this chapter are not equipped to
    handle missing or categorical variables, so we will drop the former and apply
    dummy encoding (see *Chapter 4*, *Financial Feature Engineering – How to Research
    Alpha Factors* and *Chapter 6*, *The Machine Learning Process*) to the categorical
    sector variable:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节可以在这一章的GitHub存储库中的`data_prep`笔记本中找到。本章的决策树模型不具备处理缺失或分类变量的能力，因此我们将放弃前者并对分类部门变量应用虚拟编码（参见*第4章*，*金融特征工程
    - 如何研究阿尔法因子*和*第6章*，*机器学习过程*）：
- en: '![](img/B15439_11_02.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_02.png)'
- en: 'Figure 11.2: Mutual information for features and returns or price move direction'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：特征与收益或价格变动方向的互信息
- en: Building a regression tree with time-series data
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用时间序列数据构建回归树
- en: Regression trees make predictions based on the mean outcome value for the training
    samples assigned to a given node, and typically rely on the mean-squared error
    to select optimal rules during recursive binary splitting.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 回归树根据分配给给定节点的训练样本的平均结果值进行预测，并且通常依靠平均平方误差在递归二进制分割过程中选择最佳规则。
- en: Given a training set, the algorithm iterates over the *p* predictors, *X*[1],
    *X*[2], ..., *X*[p], and *n* possible cutpoints, *s*[1], *s*[2], ..., *s*[n],
    to find an optimal combination. The optimal rule splits the feature space into
    two regions, {*X*|*X*[i] < *s*[j]} and {*X*|*X*[i] > *s*[j]}, with values for
    the *X*[i] feature either below or above the *s*[j] threshold, so that predictions
    based on the training subsets maximize the reduction of the squared residuals
    relative to the current node.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个训练集，算法在*p*个预测变量 *X*[1]、*X*[2]、...、*X*[p] 和*n*个可能的切分点 *s*[1]、*s*[2]、...、*s*[n]
    上进行迭代，以找到最佳组合。最佳规则将特征空间分割成两个区域，{*X*|*X*[i] < *s*[j]} 和 {*X*|*X*[i] > *s*[j]}，其中
    *X*[i] 特征的值要么在 *s*[j] 阈值以下，要么在 *s*[j] 阈值以上，以便基于训练子集的预测最大程度地减少相对于当前节点的平方残差。
- en: 'Let''s start with a simplified example to facilitate visualization and also
    demonstrate how we can use time-series data with a decision tree. We will only
    use 2 months of lagged returns to predict the following month, in the vein of
    an AR(2) model from the previous chapter:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简化的示例开始，以便进行可视化，并演示如何使用时间序列数据与决策树。我们将只使用 2 个月的滞后回报来预测以下月份，与前一章中的 AR(2)
    模型类似：
- en: '![](img/B15439_11_001.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_001.png)'
- en: 'Using scikit-learn, configuring and training a regression tree is very straightforward:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn，配置和训练回归树非常简单：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The OLS summary and a visualization of the first two levels of the decision
    tree reveal the striking differences between the models (see *Figure 11.3*). The
    OLS model provides three parameters for the intercepts and the two features in
    line with the linear assumption this model makes about the function.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: OLS 摘要和决策树前两个层级的可视化展示了模型之间的显著差异（见 *图 11.3*）。OLS 模型提供了三个参数，分别为截距和两个特征，符合该模型对函数的线性假设。
- en: 'In contrast, the regression tree chart displays, for each node of the first
    two levels, the feature and threshold used to split the data (note that features
    can be used repeatedly), as well as the current value of the **mean-squared error**
    (**MSE**), the number of samples, and the predicted value based on these training
    samples. Also, note that training the decision tree takes 58 milliseconds compared
    to 66 microseconds for the linear regression. While both models run fast with
    only two features, the difference is a factor of 1,000:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，回归树图表显示了前两个层级的每个节点使用的特征和阈值来拆分数据（请注意特征可以重复使用），以及**均方误差（MSE）**、样本数量和基于这些训练样本的预测值的当前值。此外，请注意，与线性回归的
    66 微秒相比，训练决策树需要 58 毫秒。虽然两种模型在只有两个特征时运行速度很快，但差异是 1,000 倍：
- en: '![](img/B15439_11_03.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_03.png)'
- en: 'Figure 11.3: OLS results and regression tree'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3：OLS 结果和回归树
- en: The tree chart also highlights the uneven distribution of samples across the
    nodes as the numbers vary between 545 and 55,000 samples after the first splits.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 树状图还突出显示了节点间样本分布的不均匀性，因为在第一个分割之后，样本数量在 545 到 55,000 之间变化。
- en: 'To further illustrate the different assumptions about the functional form of
    the relationships between the input variables and the output, we can visualize
    the current return predictions as a function of the feature space, that is, as
    a function of the range of values for the lagged returns. The following image
    shows the current monthly return as a function of returns one and two periods
    ago for linear regression (left panel) and the regression tree:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明输入变量和输出之间的不同假设关系的功能形式，我们可以将当前回报预测可视化为特征空间的函数，即基于滞后回报值的值范围的函数。下图显示了线性回归（左侧面板）和回归树的当前月回报与一段时间前回报之间的关系：
- en: '![](img/B15439_11_04.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_04.png)'
- en: 'Figure 11.4: Decision surfaces for linear regression and the regression tree'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4：线性回归和回归树的决策表面
- en: The linear regression model result on the left-hand side underlines the linearity
    of the relationship between lagged and current returns, whereas the regression
    tree chart on the right illustrates the nonlinear relationship encoded in the
    recursive partitioning of the feature space.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的线性回归模型结果强调了滞后和当前回报之间关系的线性性，而右侧的回归树图表则说明了特征空间的递归分区中编码的非线性关系。
- en: Building a classification tree
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建分类树
- en: A classification tree works just like the regression version, except that the
    categorical nature of the outcome requires a different approach to making predictions
    and measuring the loss. While a regression tree predicts the response for an observation
    assigned to a leaf node using the mean outcome of the associated training samples,
    a classification tree uses the mode, that is, the most common class among the
    training samples in the relevant region. A classification tree can also generate
    probabilistic predictions based on relative class frequencies.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树的工作方式与回归版本相同，只是结果的分类性质需要不同的方法来进行预测和衡量损失。虽然回归树使用相关训练样本的平均结果来预测分配给叶节点的观测值的响应，但是分类树使用模式，即相关区域内训练样本中最常见的类别。分类树还可以基于相对类频率生成概率预测。
- en: How to optimize for node purity
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何优化节点纯度
- en: When growing a classification tree, we also use recursive binary splitting,
    but instead of evaluating the quality of a decision rule using the reduction of
    the mean-squared error, we can use the **classification error rate**, which is
    simply the fraction of the training samples in a given (leaf) node that do not
    belong to the most common class.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建分类树时，我们也使用递归二元拆分，但是我们不是使用减少均方误差来评估决策规则的质量，而是可以使用**分类错误率**，它简单地是给定（叶）节点中不属于最常见类别的训练样本的比例。
- en: However, the alternative measures, either **Gini impurity** or **cross-entropy**,
    are preferred because they are more sensitive to node purity than the classification
    error rate, as you can see in *Figure 11.5*. **Node purity** refers to the extent
    of the preponderance of a single class in a node. A node that only contains samples
    with outcomes belonging to a single class is pure and implies successful classification
    for this particular region of the feature space.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，更倾向于使用**基尼不纯度**或**交叉熵**等替代度量方法，因为它们对节点纯度的敏感性比分类错误率更高，正如您在*图11.5*中所见。**节点纯度**指的是节点中单个类别占优势的程度。一个只包含属于单个类别结果的样本的节点是纯净的，并且意味着在特征空间的这个特定区域的成功分类。
- en: 'Let''s see how to compute these measures for a classification outcome with
    *K* categories 0,1,…, *K*-1 (with *K*=2, in the binary case). For a given node
    *m*, let *p*[mk] be the proportion of samples from the *k*^(th) class:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 看看如何计算具有*K*类别0,1,...,*K*-1（在二进制情况下为*K*=2）的分类结果的这些度量值。对于给定的节点*m*，让*p*[mk]为来自*k*^(th)类的样本比例：
- en: '![](img/B15439_11_002.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_002.png)'
- en: 'The following plot shows that both the Gini impurity and cross-entropy measures
    are maximized over the [0, 1] interval when the class proportions are even, or
    0.5 in the binary case. Both measures decline when the class proportions approach
    zero or one and the child nodes tend toward purity as a result of a split. At
    the same time, they imply a higher penalty for node impurity than the classification
    error rate:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了当类别比例均匀时（在二进制情况下为0.5），基尼不纯度和交叉熵度量在[0,1]区间内达到最大值。当类别比例接近零或一时，这些度量值会下降，并且由于拆分而导致的子节点趋向纯净。与此同时，它们对节点不纯度的惩罚比分类错误率更高：
- en: '![](img/B15439_11_05.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_05.png)'
- en: 'Figure 11.5: Classification loss functions'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：分类损失函数
- en: Note that cross-entropy takes almost 20 times as long to compute as the Gini
    measure (see the notebook for details).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与基尼测度相比，交叉熵的计算时间几乎要长20倍（详见笔记本中的详情）。
- en: How to train a classification tree
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何训练分类树
- en: We will now train, visualize, and evaluate a classification tree with up to
    five consecutive splits using 80 percent of the samples for training to predict
    the remaining 20 percent. We will take a shortcut here to simplify the illustration
    and use the built-in `train_test_split`, which does not protect against lookahead
    bias, as the custom `MultipleTimeSeriesCV` iterator we introduced in *Chapter
    6*, *The Machine Learning Process* and will use later in this chapter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用80%的样本进行训练，预测剩余的20%来训练、可视化和评估一个最多进行五次连续拆分的分类树。为了简化说明，我们将采用一种捷径并使用内置的`train_test_split`，它不会防止前瞻偏差，就像我们在*第6章*——*机器学习过程*中介绍的自定义`MultipleTimeSeriesCV`迭代器一样，稍后我们将在本章中使用。
- en: 'The tree configuration implies up to 2⁵=32 leaf nodes that, on average, in
    the balanced case, would contain over 1,400 of the training samples. Take a look
    at the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 树的配置意味着最多有2⁵=32个叶节点，平衡情况下平均会包含超过1,400个训练样本。看一下下面的代码：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output after training the model displays all the `DecisionTreeClassifier`
    parameters. We will address these in more detail in the *Hyperparameter tuning*
    section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后的输出显示了所有`DecisionTreeClassifier`的参数。我们将在*超参数调整*部分详细讨论这些。
- en: Visualizing a decision tree
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化决策树
- en: 'You can visualize the tree using the Graphviz library (see GitHub for installation
    instructions) because scikit-learn can output a description of the tree using
    the DOT language used by that library. You can configure the output to include
    feature and class labels and limit the number of levels to keep the chart readable,
    as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Graphviz库（请参阅GitHub安装说明）来可视化树，因为scikit-learn可以输出使用该库使用的DOT语言描述的树的描述。您可以配置输出以包括特征和类标签，并限制级别的数量以使图表可读，如下所示：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following diagram shows how the model uses different features and indicates
    the split rules for both continuous and categorical (dummy) variables. Under the
    label value for each node, the chart shows the number of samples from each class
    and, under the label class, the most common class (there were more up months during
    the sample period):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了模型如何使用不同的特征，并指示了连续和分类（虚拟）变量的分裂规则。在每个节点的标签值下，图表显示了来自每个类的样本数量，并在类标签下显示了最常见的类（在样本期间上涨的月份更多）：
- en: '![](img/B15439_11_06.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_06.png)'
- en: 'Figure 11.6: Visualization of a classification tree'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：分类树的可视化
- en: Evaluating decision tree predictions
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估决策树预测
- en: 'To evaluate the predictive accuracy of our first classification tree, we will
    use our test set to generate predicted class probabilities, as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们第一个分类树的预测准确度，我们将使用测试集生成预测的类概率，如下所示：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `.predict_proba()` method produces one probability for each class. In the
    binary class, these probabilities are complementary and sum to 1, so we only need
    the value for the positive class. To evaluate the generalization error, we will
    use the area under the curve based on the receiver-operating characteristic, which
    we introduced in *Chapter 6*, *The Machine Learning Process*. The result indicates
    a significant improvement above and beyond the baseline value of 0.5 for a random
    prediction (but keep in mind that the cross-validation method here does not respect
    the time-series nature of the data):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`.predict_proba()`方法为每个类别生成一个概率。在二元类别中，这些概率是互补的并总和为1，因此我们只需要正类的值。为了评估泛化误差，我们将使用基于接收器操作特征的曲线下面积，我们在*第6章*，*机器学习过程*中介绍过。结果表明，相对于随机预测的基准值0.5，有了显著的改进（但请记住，这里的交叉验证方法不考虑数据的时间序列性）：'
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Overfitting and regularization
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合和正则化
- en: Decision trees have a strong tendency to overfit, especially when a dataset
    has a large number of features relative to the number of samples. As discussed
    in previous chapters, overfitting increases the prediction error because the model
    does not only learn the signal contained in the training data, but also the noise.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在数据集相对于样本数量具有大量特征时很容易过拟合。正如前几章讨论的那样，过拟合会增加预测误差，因为模型不仅学习了训练数据中包含的信号，还学习了噪音。
- en: 'There are multiple ways to **address the risk of overfitting**, including:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以**解决过拟合的风险**，包括：
- en: '**Dimensionality reduction** (see *Chapter 13*, *Data-Driven Risk Factors and
    Asset Allocation with Unsupervised Learning*) improves the feature-to-sample ratio
    by representing the existing features with fewer, more informative, and less noisy
    features.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**（请参阅*第13章*，*使用无监督学习的数据驱动风险因子和资产配置*）通过用更少、更具信息性和更少噪声的特征表示现有特征来改善特征与样本的比率。'
- en: '**Ensemble models**, such as random forests, combine multiple trees while randomizing
    the tree construction, as we will see in the second part of this chapter.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成模型**，例如随机森林，结合了多个树，同时随机化树的构建，我们将在本章的第二部分中看到。'
- en: Decision trees provide several regularization hyperparameters to limit the growth
    of a tree and the associated complexity. While every split increases the number
    of nodes, it also reduces the number of samples available per node to support
    a prediction. For each additional level, twice the number of samples is needed
    to populate the new nodes with the same sample density.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树提供了几个正则化超参数来限制树的增长和相关复杂度。虽然每次分裂都会增加节点数，但也会减少每个节点可用于支持预测的样本数量。对于每个额外的层级，需要两倍数量的样本才能使新节点以相同的样本密度填充。
- en: '**Tree pruning** is an additional tool to reduce the complexity of a tree.
    It does so by eliminating nodes or entire parts of a tree that add little value
    but increase the model''s variance. Cost-complexity-pruning, for instance, starts
    with a large tree and recursively reduces its size by replacing nodes with leaves,
    essentially running the tree construction in reverse. The various steps produce
    a sequence of trees that can then be compared using cross-validation to select
    the ideal size.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**树修剪**是减少树复杂性的另一种工具。它通过消除添加了很少价值但增加了模型方差的节点或整个树的部分来实现。例如，成本复杂度修剪从一个大树开始，并通过将节点替换为叶子来递归地减小其大小，本质上是反向运行树构建。各种步骤产生一系列树，然后可以使用交叉验证来选择理想的大小。'
- en: How to regularize a decision tree
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何规范化决策树
- en: 'The following table lists the key parameters available for this purpose in
    the scikit-learn decision tree implementation. After introducing the most important
    parameters, we will illustrate how to use cross-validation to optimize the hyperparameter
    settings with respect to the bias-variance trade-off and lower prediction errors:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了 scikit-learn 决策树实现中用于此目的的关键参数。在介绍了最重要的参数后，我们将说明如何使用交叉验证来优化超参数设置，以便在偏差-方差权衡和降低预测误差方面：
- en: '| Parameter | Description | Default | Options |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 描述 | 默认 | 选项 |'
- en: '| `max_depth` | The maximum number of levels: split the nodes until `max_depth`
    has been reached. All leaves are pure or contain fewer samples than `min_samples_split`.
    | None | int |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `max_depth` | 最大级别数：分割节点直到达到`max_depth`。所有叶子都是纯的，或者包含的样本少于`min_samples_split`。
    | None | int |'
- en: '| `max_features` | Number of features to consider for a split. | None | None:
    all features `int`: # features`float`: fraction`auto`, `sqrt`: sqrt(`n_features`)`log2`:
    log2(`n_features`) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `max_features` | 考虑用于分割的特征数量。 | None | None：所有特征 `int`：＃ 特征`float`：分数`auto`，`sqrt`：sqrt（`n_features`）`log2`：log2（`n_features`）
    |'
- en: '| `max_leaf_nodes` | Split nodes until creating this many leaves. | None |
    None: unlimited `int` |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `max_leaf_nodes` | 分割节点直到创建这么多叶子。 | None | None：无限 `int` |'
- en: '| `min_impurity_decrease` | Split node if impurity decreases by at least this
    value. | 0 | `float` |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `min_impurity_decrease` | 如果不纯度减少至少这个值，则分割节点。 | 0 | `float` |'
- en: '| `min_samples_leaf` | A split will only be considered if there are at least
    `min_samples_leaf` training samples in each of the left and right branches. |
    1 | `int`;`float` (as a percent of *N*) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `min_samples_leaf` | 只有在左右分支的每个中至少有`min_samples_leaf`训练样本时，才会考虑分割。 | 1 |
    `int`;`float`（作为百分比*N*） |'
- en: '| `min_samples_split` | The minimum number of samples required to split an
    internal node. | 2 | `int`; `float` (percent of *N*) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `min_samples_split` | 分割内部节点所需的最小样本数。 | 2 | `int`；`float`（百分之*N*） |'
- en: '| `min_weight_fraction_leaf` | The minimum weighted fraction of the sum total
    of all sample weights needed at a leaf node. Samples have equal weight unless
    `sample_weight` is provided in the fit method. | 0 |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `min_weight_fraction_leaf` | 在叶子节点上需要的所有样本权重的最小加权分数。除非在拟合方法中提供了`sample_weight`，否则样本具有相同的权重。
    | 0 |  |'
- en: The `max_depth` parameter imposes a hard limit on the number of consecutive
    splits and represents the most straightforward way to cap the growth of a tree.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth` 参数对连续分割的次数施加了硬限制，并代表了限制树生长的最直接方式。'
- en: The `min_samples_split` and `min_samples_leaf` parameters are alternative, data-driven
    ways to limit the growth of a tree. Rather than imposing a hard limit on the number
    of consecutive splits, these parameters control the minimum number of samples
    required to further split the data. The latter guarantees a certain number of
    samples per leaf, while the former can create very small leaves if a split results
    in a very uneven distribution. Small parameter values facilitate overfitting,
    while a high number may prevent the tree from learning the signal in the data.
    The default values are often quite low, and you should use cross-validation to
    explore a range of potential values. You can also use a float to indicate a percentage,
    as opposed to an absolute number.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_samples_split` 和 `min_samples_leaf` 参数是另一种基于数据的方法来限制树的生长。与对连续分割的次数施加硬限制不同，这些参数控制进一步分割数据所需的最小样本数。后者保证了每个叶子的一定样本数量，而前者在分割导致非常不均匀的分布时可能会创建非常小的叶子。较小的参数值有助于过拟合，而较高的数字可能会阻止树学习数据中的信号。默认值通常相当低，您应该使用交叉验证来探索一系列潜在值。您还可以使用浮点数来表示百分比，而不是绝对数字。'
- en: The scikit-learn documentation contains additional details about how to use
    the various parameters for different use cases; see the resources linked on GitHub
    for more information.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn文档中包含有关如何在不同用例中使用各种参数的其他详细信息；有关更多信息，请参阅GitHub上链接的资源。
- en: Decision tree pruning
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树修剪
- en: Recursive binary-splitting will likely produce good predictions on the training
    set but tends to overfit the data and produce poor generalization performance.
    This is because it leads to overly complex trees, which are reflected in a large
    number of leaf nodes, or partitioning of the feature space. Fewer splits and leaf
    nodes imply an overall smaller tree and often lead to better predictive performance,
    as well as interpretability.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 递归二元分裂很可能会在训练集上产生良好的预测结果，但往往会导致数据过度拟合，产生较差的泛化性能。这是因为它导致了过于复杂的树，这在大量叶节点或特征空间的划分中反映出来。较少的分裂和叶节点意味着总体较小的树，并且通常会导致更好的预测性能，以及可解释性。
- en: One approach to limit the number of leaf nodes is to avoid further splits unless
    they yield significant improvements in the objective metric. The downside of this
    strategy, however, is that sometimes, splits that result in small improvements
    enable more valuable splits later as the composition of the samples keeps changing.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 限制叶节点数量的一种方法是除非它们产生目标度量的显着改善，否则避免进一步分裂。然而，这种策略的缺点是，有时候，导致小幅改善的分裂在样本组成不断变化时会使后续更有价值的分裂变得更加困难。
- en: Tree pruning, in contrast, starts by growing a very large tree before removing
    or pruning nodes to reduce the large tree to a less complex and overfit subtree.
    Cost-complexity-pruning generates a sequence of subtrees by adding a penalty for
    adding leaf nodes to the tree model and a regularization parameter, similar to
    the lasso and ridge linear-regression models, that modulates the impact of the
    penalty. Applied to the large tree, an increasing penalty will automatically produce
    a sequence of subtrees. Cross-validation of the regularization parameter can be
    used to identify the optimal, pruned subtree.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，树修剪首先通过生长一个非常大的树，然后移除或修剪节点，以将大树减小为一个较少复杂且过度拟合的子树。成本复杂度修剪通过对向树模型添加叶节点增加惩罚并引入正则化参数来生成一系列子树，类似于套索和岭线性回归模型，调节惩罚的影响。应用于大树，增加的惩罚将自动产生一系列子树。可以使用正则化参数的交叉验证来识别最佳的修剪子树。
- en: This method was introduced in scikit-learn version 0.22; see Esposito et al.
    (1997) for a survey of how various methods work and perform.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是在scikit-learn版本0.22中引入的；有关各种方法的工作原理和性能，请参见Esposito等人(1997)的调查。
- en: Hyperparameter tuning
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整
- en: Decision trees offer an array of hyperparameters to control and tune the training
    result. Cross-validation is the most important tool to obtain an unbiased estimate
    of the generalization error, which, in turn, permits an informed choice among
    the various configuration options. scikit-learn offers several tools to facilitate
    the process of cross-validating numerous parameter settings, namely the `GridSearchCV`
    convenience class, which we will illustrate in the next section. Learning curves
    also allow diagnostics that evaluate potential benefits of collecting additional
    data to reduce the generalization error.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树提供了一系列超参数来控制和调整训练结果。交叉验证是获得对泛化误差的无偏估计的最重要工具，反过来又允许在各种配置选项之间做出明智选择。scikit-learn提供了几个工具来简化交叉验证大量参数设置的过程，即我们将在下一节中介绍的`GridSearchCV`便利类。学习曲线还允许进行诊断，评估收集额外数据以减少泛化误差的潜在好处。
- en: Using GridsearchCV with a custom metric
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自定义度量标准的GridsearchCV
- en: As highlighted in *Chapter 6*, *The Machine Learning Process*, scikit-learn
    provides a method to define ranges of values for multiple hyperparameters. It
    automates the process of cross-validating the various combinations of these parameter
    values to identify the optimal configuration. Let's walk through the process of
    automatically tuning your model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如*第6章*，*机器学习流程*中所强调的，scikit-learn提供了一种定义多个超参数值范围的方法。它自动化了交叉验证各种参数值组合的过程，以确定最佳配置。让我们逐步了解自动调整模型的过程。
- en: 'The first step is to instantiate a model object and define a dictionary where
    the keywords name the hyperparameters, and the values list the parameter settings
    to be tested:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是实例化一个模型对象，并定义一个字典，其中关键词命名超参数，值列出要测试的参数设置：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then, instantiate the `GridSearchCV` object, providing the estimator object
    and parameter grid, as well as a scoring method and cross-validation choice, to
    the initialization method.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，实例化`GridSearchCV`对象，提供估算器对象和参数网格，以及评分方法和交叉验证选择，传递给初始化方法。
- en: 'We set our custom `MultipleTimeSeriesSplit` class to train the model for 60
    months, or 5 years, of data and to validate performance using the subsequent 6
    months, repeating the process over 10 folds to cover an out-of-sample period of
    5 years:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将自定义的`MultipleTimeSeriesSplit`类设置为对模型进行60个月或5年的数据训练，并使用随后的6个月验证性能，重复此过程10次以覆盖5年的样本外期间：
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We use the `roc_auc` metric to score the classifier, and define a custom information
    coefficient (IC) metric using scikit-learn''s `make_scorer` function for the regression
    model:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`roc_auc`指标对分类器进行评分，并使用scikit-learn的`make_scorer`函数为回归模型定义自定义信息系数（IC）指标：
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can parallelize the search using the `n_jobs` parameter and automatically
    obtain a trained model that uses the optimal hyperparameters by setting `refit=True`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`n_jobs`参数并通过设置`refit=True`自动获取使用最佳超参数的训练模型。
- en: 'With all the settings in place, we can fit `GridSearchCV` just like any other
    model:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所有设置都已就绪后，我们可以像任何其他模型一样拟合`GridSearchCV`：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The training process produces some new attributes for our `GridSearchCV` object,
    most importantly the information about the optimal settings and the best cross-validation
    score (now using the proper setup, which avoids lookahead bias).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程为我们的`GridSearchCV`对象生成了一些新的属性，最重要的是有关最佳设置和最佳交叉验证分数的信息（现在使用适当的设置，避免了前瞻偏差）。
- en: 'The following table lists the parameters and scores for the best regression
    and classification model, respectively. With a shallower tree and more regularized
    leaf nodes, the regression tree achieves an IC of 0.083, while the classifier''s
    AUC score is 0.525:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格分别列出了最佳回归模型和分类模型的参数和分数。具有更浅的树和更加正则化的叶节点，回归树的IC为0.083，而分类器的AUC得分为0.525：
- en: '| Parameter | Regression | Classification |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 回归 | 分类 |'
- en: '| **max_depth** | 6 | 12 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| **max_depth** | 6 | 12 |'
- en: '| **max_features** | sqrt | sqrt |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| **max_features** | sqrt | sqrt |'
- en: '| **min_samples_leaf** | 50 | 5 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| **min_samples_leaf** | 50 | 5 |'
- en: '| **Score** | 0.0829 | 0.5250 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **分数** | 0.0829 | 0.5250 |'
- en: The automation is quite convenient, but we also would like to inspect how the
    performance evolves for different parameter values. Upon completion of this process,
    the `GridSearchCV` object makes detailed cross-validation results available so
    that we can gain more insights.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化非常方便，但我们也希望检查性能如何随不同参数值的变化而变化。完成此过程后，`GridSearchCV`对象提供了详细的交叉验证结果，以便我们可以获得更多见解。
- en: How to inspect the tree structure
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何检查树结构
- en: 'The notebook also illustrates how to run cross-validation more manually to
    obtain custom tree attributes, such as the total number of nodes or leaf nodes
    associated with certain hyperparameter settings. The following function accesses
    the internal `.tree_ attribute` to retrieve information about the total node count,
    as well as how many of these nodes are leaf nodes:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本还说明了如何手动运行交叉验证以获取自定义树属性，例如与某些超参数设置相关联的总节点数或叶节点数。以下函数访问内部的`.tree_属性`以检索有关总节点计数以及其中多少个节点是叶节点的信息：
- en: '[PRE9]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can combine this information with the train and test scores to gain detailed
    knowledge about the model behavior throughout the cross-validation process, as
    follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此信息与训练和测试分数结合起来，以获取有关模型在整个交叉验证过程中行为的详细知识，如下所示：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following plot displays how the number of leaf nodes increases with the
    depth of the tree. Due to the sample size of each cross-validation fold containing
    60 months with around 500 data points each, the number of leaf nodes is limited
    to around 3,000 when limiting the number of `min_samples_leaf` to 10 samples:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了叶节点数随树深度增加而增加的情况。由于每个交叉验证折叠的样本大小为60个月，每个样本约包含500个数据点，因此将`min_samples_leaf`限制为10个样本时，叶节点数限制在约3,000个：
- en: '![](img/B15439_11_07.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_07.png)'
- en: 'Figure 11.7: Visualization of a classification tree'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：分类树的可视化
- en: Comparing regression and classification performance
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较回归和分类的性能
- en: To take a closer look at the performance of the models, we will show the cross-validation
    performance for various levels of depth, while maintaining the other parameter
    settings that produced the best grid search results. *Figure 11.8* displays the
    train and the validation scores and highlights the degree of overfitting for deeper
    trees. This is because the training scores steadily increase, whereas validation
    performance remains flat or decreases.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更仔细地观察模型的性能，我们将展示各种深度的交叉验证性能，同时保持产生最佳网格搜索结果的其他参数设置。*图11.8*显示了训练和验证分数，并突出了更深的树的过拟合程度。这是因为训练分数稳步增加，而验证性能保持不变或下降。
- en: 'Note that, for the classification tree, the grid search suggested 12 levels
    for the best predictive accuracy. However, the plot shows similar AUC scores for
    less complex trees, with three or seven levels. We would prefer a shallower tree
    that promises comparable generalization performance while reducing the risk of
    overfitting:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于分类树，网格搜索建议使用12个级别以获得最佳预测准确性。然而，图表显示较简单的树（具有三个或七个级别）的AUC得分相似。我们更倾向于一个更浅的树，它承诺具有可比较的泛化性能，同时减少了过拟合的风险：
- en: '![](img/B15439_11_08.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_08.png)'
- en: 'Figure 11.8: Train and validation scores for both models'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8：两个模型的训练和验证分数
- en: Diagnosing training set size with learning curves
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用学习曲线诊断训练集规模
- en: A **learning curve** is a useful tool that displays how the validation and training
    scores evolve as the number of training samples increases.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习曲线**是一种有用的工具，显示了随着训练样本数量增加，验证和训练分数如何演变。'
- en: The purpose of the learning curve is to find out whether and how much the model
    would benefit from using more data during training. It also helps to diagnose
    whether the model's generalization error is more likely driven by bias or variance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线的目的是找出模型在训练过程中是否以及在多大程度上会受益于使用更多数据。它还有助于诊断模型的泛化误差更可能是由偏差还是方差驱动。
- en: If the training score meets performance expectations and the validation score
    exhibits significant improvement as the training sample grows, training for a
    longer lookback period or obtaining more data might add value. If, on the other
    hand, both the validation and the training score converge to a similarly poor
    value, despite an increasing training set size, the error is more likely due to
    bias, and additional training data is unlikely to help.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练分数符合预期的性能，并且随着训练样本数量的增加，验证分数表现出显著改善，那么训练更长的回溯期或获取更多数据可能会增加价值。另一方面，如果尽管训练集大小增加，但验证分数和训练分数都收敛到类似较差的值，则错误更可能是由于偏差，并且额外的训练数据不太可能有所帮助。
- en: 'The following image depicts the learning curves for the best regression and
    classification models:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片显示了最佳回归和分类模型的学习曲线：
- en: '![](img/B15439_11_09.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_09.png)'
- en: 'Figure 11.9: Learning curves for the best version of each model'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9：每个模型最佳版本的学习曲线
- en: Especially for the regression model, the validation performance improves with
    a larger training set. This suggests that a longer training period may yield better
    results. Try it yourself to see if it works!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是对于回归模型，随着训练集规模的扩大，验证性能有所提高。这表明延长训练周期可能会产生更好的结果。你可以自己试一试，看看是否奏效！
- en: Gaining insight from feature importance
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从特征重要性中获得洞察力
- en: Decision trees can not only be visualized to inspect the decision path for a
    given feature, but can also summarize the contribution of each feature to the
    rules learned by the model to fit the training data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树不仅可以可视化以检查给定特征的决策路径，还可以总结每个特征对模型学习以拟合训练数据的规则的贡献。
- en: Feature importance captures how much the splits produced by each feature help
    optimize the model's metric used to evaluate the split quality, which in our case
    is the Gini impurity. A feature's importance is computed as the (normalized) total
    reduction of this metric and takes into account the number of samples affected
    by a split. Hence, features used earlier in the tree where the nodes tend to contain
    more samples are typically considered of higher importance.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性捕捉到每个特征产生的分裂如何帮助优化模型用于评估分裂质量的度量，我们的情况下是基尼不纯度。特征的重要性是计算为这个度量的（标准化的）总减少，并考虑到受分裂影响的样本数量。因此，在树的较早节点使用的特征，其中节点往往包含更多的样本，通常被认为是更重要的。
- en: '*Figure 11.10* shows the plots for feature importance for the top 15 features
    of each model. Note how the order of features differs from the univariate evaluation
    based on the mutual information scores given at the beginning of this section.
    Clearly, the ability of decision trees to capture interdependencies, such as between
    time periods and other features, can alter the value of each feature:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11.10* 显示了每个模型前 15 个特征的特征重要性的图表。注意特征的顺序如何与本节开头给出的互信息分数的单变量评估不同。显然，决策树捕获时间段与其他特征之间的相互依赖关系的能力可能改变每个特征的价值：'
- en: '![](img/B15439_11_10.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_10.png)'
- en: 'Figure 11.10: Feature importance for the best regression and classification
    models'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10：最佳回归和分类模型的特征重要性
- en: Strengths and weaknesses of decision trees
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树的优势和劣势
- en: 'Regression and classification trees approach making predictions very differently
    from the linear models we have explored in the previous chapters. How do you **decide
    which model is more suitable** for the problem at hand? Consider the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 回归树和分类树在预测方面与我们在前几章中探讨过的线性模型有着非常不同的方法。如何**决定哪种模型更适合**当前的问题？考虑以下内容：
- en: If the relationship between the outcome and the features is approximately linear
    (or can be transformed accordingly), then linear regression will likely outperform
    a more complex method, such as a decision tree that does not exploit this linear
    structure.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果结果与特征之间的关系大致是线性的（或可以相应地转换），那么线性回归可能会胜过更复杂的方法，例如不利用这种线性结构的决策树。
- en: If the relationship appears highly nonlinear and more complex, decision trees
    will likely outperform the classical models. Keep in mind that the complexity
    of the relationship needs to be systematic or "real," rather than driven by noise,
    which leads more complex models to overfit.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果关系呈现高度非线性和更复杂，决策树可能会胜过传统模型。请记住，关系的复杂性需要是系统的或“真实的”，而不是由噪声驱动，这会导致更复杂的模型过拟合。
- en: 'Several **advantages** have made decision trees very popular:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树具有几个**优点**：
- en: They are fairly straightforward to understand and to interpret, not least because
    they can be easily visualized and are thus more accessible to a non-technical
    audience. Decision trees are also referred to as white-box models, given the high
    degree of transparency about how they arrive at a prediction. Black-box models,
    such as ensembles and neural networks, may deliver better prediction accuracy,
    but the decision logic is often much more challenging to understand and interpret.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们相当容易理解和解释，尤其是因为它们可以很容易地可视化，因此更容易让非技术人员理解。决策树也被称为白盒模型，因为它们在如何得出预测方面具有高度透明性。黑盒模型，例如集成和神经网络，可能会提供更好的预测精度，但是决策逻辑往往更难理解和解释。
- en: Decision trees require less data preparation than models that make stronger
    assumptions about the data or are more sensitive to outliers and require data
    standardization (such as regularized regression).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树比那些对数据做出更强假设或对异常值更敏感且需要数据标准化的模型需要更少的数据准备（例如正则化回归）。
- en: Some decision tree implementations handle categorical input, do not require
    the creation of dummy variables (improving memory efficiency), and can work with
    missing values, as we will see in *Chapter 12*, *Boosting Your Trading Strategy*,
    but this is not the case for scikit-learn.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些决策树实现可以处理分类输入，不需要创建虚拟变量（提高内存效率），并且可以处理缺失值，正如我们将在 *第 12 章*，*提升您的交易策略* 中看到的，但这不适用于
    scikit-learn。
- en: Prediction is fast because it is logarithmic in the number of leaf nodes (unless
    the tree becomes extremely unbalanced).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测速度很快，因为它与叶节点的数量呈对数关系（除非树变得极不平衡）。
- en: It is possible to validate the model using statistical tests and account for
    its reliability (see the references for more details).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用统计测试验证模型并考虑其可靠性（有关更多细节，请参阅参考文献）。
- en: 'Decision trees also have several key **disadvantages**:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树还具有几个关键**劣势**：
- en: Decision trees have a built-in tendency to overfit to the training set and produce
    a high generalization error. The key steps to address this weakness are pruning
    and regularization using the early-stopping criteria that limits tree growth,
    as outlined in this section.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树内置了对训练集的过度拟合的倾向，并产生了高的泛化误差。解决这一弱点的关键步骤是修剪和正则化，使用限制树生长的早停准则，如本节所述。
- en: Decision trees are also sensitive to unbalanced class weights and may produce
    biased trees. One option is to oversample the underrepresented classes or undersample
    the more frequent class. It is typically better, though, to use class weights
    and directly adjust the objective function.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树还对不平衡类权重敏感，可能会产生偏向某一类的树。一种选择是对少数类进行过采样或对更频繁出现的类进行欠采样。通常更好的选择是使用类权重并直接调整目标函数。
- en: The high variance of decision trees is tied to their ability to closely adapt
    to a training set. As a result, minor variations in the data can produce wide
    swings in the tree's structure and, consequently, the model's predictions. A key
    prevention mechanism is the use of an ensemble of randomized decision trees that
    have low bias and produce uncorrelated prediction errors.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的高方差与它们密切适应训练集的能力有关。因此，数据中的细微变化可能会导致树结构和模型预测的广泛波动。一个关键的预防机制是使用一组具有低偏差且产生不相关预测误差的随机决策树的集成。
- en: The greedy approach to decision-tree learning optimizes local criteria that
    reduce the prediction error at the current node and do not guarantee a globally
    optimal outcome. Again, ensembles consisting of randomized trees help to mitigate
    this problem.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树学习的贪婪方法优化了减少当前节点预测误差的局部标准，并不保证全局最优结果。同样，由随机树组成的集成有助于缓解这个问题。
- en: We will now turn to the ensemble method of mitigating the risk of overfitting
    that's inherent when using decision trees.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将转向缓解决策树使用时固有的过拟合风险的集成方法。
- en: Random forests – making trees more reliable
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林——使树更可靠
- en: Decision trees are not only useful for their transparency and interpretability.
    They are also fundamental building blocks for more powerful ensemble models that
    combine many individual trees, while randomly varying their design to address
    the overfitting problems we just discussed.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树不仅因其透明度和解释性而有用。它们也是更强大的集成模型的基本构建模块，这些模型组合了许多个体树，同时随机变化其设计，以解决我们刚刚讨论的过拟合问题。
- en: Why ensemble models perform better
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么集成模型表现更好
- en: Ensemble learning involves combining several machine learning models into a
    single new model that aims to make better predictions than any individual model.
    More specifically, an ensemble integrates the predictions of several base estimators,
    trained using one or more learning algorithms, to reduce the generalization error
    that these models produce on their own.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习涉及将几个机器学习模型组合成一个新的模型，旨在比任何单个模型做出更好的预测。更具体地说，集成整合了几个基本估计器的预测，这些估计器使用一个或多个学习算法进行训练，以减少它们自己产生的泛化误差。
- en: 'For ensemble learning to achieve this goal, **the individual models must be**:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使集成学习达到这个目标，**个体模型必须是**：
- en: '**Accurate**: Outperform a naive baseline (such as the sample mean or class
    proportions)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确**: 胜过一个简单的基线（例如样本平均值或类比例）'
- en: '**Independent**: Predictions are generated differently to produce different
    errors'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立**: 预测是通过不同的方式产生的，以产生不同的误差'
- en: Ensemble methods are among the most successful machine learning algorithms,
    in particular for standard numerical data. Large ensembles are very successful
    in machine learning competitions and may consist of many distinct individual models
    that have been combined by hand or using another machine learning algorithm.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法是最成功的机器学习算法之一，特别适用于标准的数值数据。大型集成在机器学习竞赛中非常成功，可能由许多不同的个体模型组成，这些模型已经手工组合或使用另一个机器学习算法组合起来。
- en: There are several disadvantages to combining predictions made by different models.
    These include reduced interpretability and higher complexity and cost of training,
    prediction, and model maintenance. As a result, in practice (outside of competitions),
    the small gains in accuracy from large-scale ensembling may not be worth the added
    costs.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 将不同模型的预测组合起来存在几个缺点。这些包括降低了解释性以及训练、预测和模型维护的复杂性和成本。因此，在实践中（不考虑竞赛），从大规模集成中获得的准确度微小增益可能不值得额外的成本。
- en: 'There are two groups of ensemble methods that are typically distinguished between,
    depending on how they optimize the constituent models and then integrate the results
    for a single ensemble prediction:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 根据它们如何优化组成模型然后将结果集成为单个集成预测，通常可以区分两组集成方法：
- en: '**Averaging methods** train several base estimators independently and then
    average their predictions. If the base models are not biased and make different
    prediction errors that are not highly correlated, then the combined prediction
    may have lower variance and can be more reliable. This resembles the construction
    of a portfolio from assets with uncorrelated returns to reduce the volatility
    without sacrificing the return.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均方法**独立地训练多个基本估计器，然后对它们的预测进行平均。如果基本模型没有偏差，并且产生不高度相关的不同预测误差，那么合并的预测可能具有较低的方差，更可靠。这类似于从具有不相关回报的资产构建组合以减少波动性而不牺牲回报。'
- en: '**Boosting methods**, in contrast, train base estimators sequentially with
    the specific goal of reducing the bias of the combined estimator. The motivation
    is to combine several weak models into a powerful ensemble.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升方法**相反，按顺序训练基本估计器，其特定目标是减少组合估计器的偏差。其动机是将几个弱模型组合成一个强大的集成。'
- en: We will focus on automatic averaging methods in the remainder of this chapter
    and boosting methods in *Chapter 12*, *Boosting Your Trading Strategy*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分我们将专注于自动平均方法，而在*第12章*《提升你的交易策略》中关注提升方法。
- en: Bootstrap aggregation
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自助聚合
- en: We saw that decision trees are likely to make poor predictions due to high variance,
    which implies that the tree structure is quite sensitive to the available training
    sample. We have also seen that a model with low variance, such as linear regression,
    produces similar estimates, despite different training samples, as long as there
    are sufficient samples given the number of features.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现决策树很可能由于高方差而做出糟糕的预测，这意味着树结构对可用训练样本非常敏感。我们还看到，方差较低的模型，如线性回归，产生类似的估计值，尽管训练样本不同，只要给定特征数足够多。
- en: For a given a set of independent observations, each with a variance of ![](img/B15439_11_003.png),
    the standard error of the sample mean is given by ![](img/B15439_11_004.png).
    In other words, averaging over a larger set of observations reduces the variance.
    A natural way to reduce the variance of a model and its generalization error would,
    thus, be to collect many training sets from the population, train a different
    model on each dataset, and average the resulting predictions.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定一组独立观察值，每个观察值的方差为 ![](img/B15439_11_003.png)，样本均值的标准误差由 ![](img/B15439_11_004.png)
    给出。换句话说，对更大的观察集进行平均会减小方差。因此，减少模型方差和泛化误差的一种自然方法是从总体中收集许多训练集，在每个数据集上训练不同的模型，然后对结果预测进行平均。
- en: In practice, we do not typically have the luxury of many different training
    sets. This is where **bagging**, short for **bootstrap aggregation**, comes in.
    Bagging is a general-purpose method that's used to reduce the variance of a machine
    learning model, which is particularly useful and popular when applied to decision
    trees.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们通常没有许多不同的训练集。这就是**装袋法**的用武之地，即**自助聚合**。装袋法是一种通用方法，用于降低机器学习模型的方差，当应用于决策树时特别有用且受欢迎。
- en: We will first explain how this technique mitigates overfitting and then show
    how to apply it to decision trees.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先解释这种技术如何缓解过拟合，然后展示如何将其应用于决策树。
- en: How bagging lowers model variance
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何通过装袋法降低模型的方差
- en: Bagging refers to the aggregation of bootstrap samples, which are random samples
    with replacement. Such a random sample has the same number of observations as
    the original dataset but may contain duplicates due to replacement.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋指的是自助采样的聚合，这些自助采样是带替换的随机采样。这样的随机样本与原始数据集具有相同数量的观察值，但由于替换可能包含重复观察值。
- en: Bagging increases predictive accuracy but decreases model interpretability because
    it's no longer possible to visualize the tree to understand the importance of
    each feature. As an ensemble algorithm, bagging methods train a given number of
    base estimators on these bootstrapped samples and then aggregate their predictions
    into a final ensemble prediction.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋法提高了预测准确性，但降低了模型的可解释性，因为不再能够可视化树以了解每个特征的重要性。作为一个集成算法，装袋方法在这些自助采样样本上训练给定数量的基本估计器，然后将它们的预测聚合成最终的集成预测。
- en: 'Bagging reduces the variance of the base estimators to reduce their generalization
    error by:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋法通过以下方式降低基本估计器的方差，从而降低它们的泛化误差：
- en: Randomizing how each tree is grown
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机化每棵树的生长方式
- en: Averaging their predictions
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对它们的预测进行平均
- en: It is often a straightforward approach to improve on a given model without the
    need to change the underlying algorithm. This technique works best with **complex
    models that have low bias and high variance**, such as deep decision trees, because
    its goal is to limit overfitting. Boosting methods, in contrast, work best with
    weak models, such as shallow decision trees.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是一种简单的方法来改进给定模型，而无需更改基础算法。这种技术对于具有低偏差和高方差的**复杂模型**特别有效，例如深层次的决策树，因为其目标是限制过拟合。相比之下，提升方法在弱模型（例如浅决策树）上效果最佳。
- en: 'There are several bagging methods that differ by the random sampling process
    they apply to the training set:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种bagging方法的不同之处在于它们应用于训练集的随机抽样过程：
- en: '**Pasting** draws random samples from the training data without replacement,
    whereas bagging samples with replacement.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**粘贴**从训练数据中无替换地随机抽取样本，而bagging则使用替换抽样。'
- en: '**Random subspaces** randomly sample from the features (that is, the columns)
    without replacement.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机子空间**随机从特征（即列）中抽取样本，不进行替换。'
- en: '**Random patches** train base estimators by randomly sampling both observations
    and features.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机补丁**通过随机抽样观察值和特征来训练基本估算器。'
- en: Bagged decision trees
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bagged决策树
- en: To apply bagging to decision trees, we create bootstrap samples from our training
    data by repeatedly sampling with replacement. Then, we train one decision tree
    on each of these samples and create an ensemble prediction by averaging over the
    predictions of the different trees. You can find the code for this example in
    the notebook `bagged_decision_trees`, unless otherwise noted.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要将bagging应用于决策树，我们通过反复抽样替换来创建从训练数据中生成bootstrap样本。然后，我们在每个样本上训练一个决策树，并通过对不同树的预测进行平均来创建一个集成预测。您可以在笔记本`bagged_decision_trees`中找到此示例的代码，除非另有说明。
- en: Bagged decision trees are usually grown large, that is, they have many levels
    and leaf nodes and are not pruned so that each tree has a low bias but high variance.
    The effect of averaging their predictions then aims to reduce their variance.
    Bagging has been shown to substantially improve predictive performance by constructing
    ensembles that combine hundreds or even thousands of trees trained on bootstrap
    samples.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Bagged决策树通常生长较大，即它们具有许多层和叶子节点，并且不进行修剪，以使每棵树的偏差低而方差高。然后，对其预测进行平均的效果旨在减少其方差。通过构建在bootstrap样本上训练的数百甚至数千棵树的集成，已经证明了bagging能够显著提高预测性能。
- en: 'To illustrate the effect of bagging on the variance of a regression tree, we
    can use the `BaggingRegressor` meta-estimator provided by scikit-learn. It trains
    a user-defined base estimator based on parameters that specify the sampling strategy:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 要说明bagging对回归树方差的影响，我们可以使用scikit-learn提供的`BaggingRegressor`元估计器。它基于指定抽样策略的参数来训练用户定义的基础估算器。
- en: '`max_samples` and `max_features` control the size of the subsets drawn from
    the rows and the columns, respectively.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_samples`和`max_features`分别控制从行和列中绘制的子集的大小。'
- en: '`bootstrap` and `bootstrap_features` determine whether each of these samples
    is drawn with or without replacement.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bootstrap`和`bootstrap_features`确定是否使用或不使用替换来绘制这些样本。'
- en: The following example uses an exponential function to generate training samples
    for a single `DecisionTreeRegressor` and a `BaggingRegressor` ensemble that consists
    of 10 trees, each grown 10 levels deep. Both models are trained on the random
    samples and predict outcomes for the actual function with added noise.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用指数函数生成单个`DecisionTreeRegressor`和由10棵树组成的`BaggingRegressor`集成的训练样本，每棵树都生长了10层深度。这两个模型都在随机样本上进行训练，并为添加了噪声的实际函数预测结果。
- en: 'Since we know the true function, we can decompose the mean-squared error into
    bias, variance, and noise, and compare the relative size of these components for
    both models according to the following breakdown:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道真实函数，因此我们可以将均方误差分解为偏差、方差和噪声，并根据以下分解比较这些组件的相对大小：
- en: '![](img/B15439_11_005.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_005.png)'
- en: 'We will draw 100 random samples of 250 training and 500 test observations each
    to train each model and collect the predictions:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分别从训练集和测试集中随机抽取100组样本，每组样本包括250个训练观察和500个测试观察，以训练每个模型并收集预测结果：
- en: '[PRE11]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For each model, the plots in *Figure 11.11* show:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型，在*图11.11*中显示的绘图如下：
- en: The mean prediction and a band of two standard deviations around the mean (upper
    panel)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均预测值和平均值周围两个标准偏差的带状区间（上部面板）。
- en: The bias-variance-noise breakdown based on the values for the true function
    (bottom panel)
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据真实函数的值（底部面板），进行偏差-方差-噪声分解。
- en: 'We find that the variance of the predictions of the individual decision tree
    (left side) is almost twice as high as that for the small ensemble of 10 bagged
    trees, based on bootstrapped samples:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，个体决策树预测的方差（左侧）几乎是基于自助样本的小集成的两倍高：
- en: '![](img/B15439_11_11.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_11.png)'
- en: 'Figure 11.11: Bias-variance breakdown for individual and bagged decision trees'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11：个体和袋装决策树的偏差-方差分解
- en: See the notebook `bagged_decision_trees` for implementation details.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅笔记本 `bagged_decision_trees` 以获取实现细节。
- en: How to build a random forest
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何构建随机森林
- en: The random forest algorithm builds on the randomization introduced by bagging
    to further reduce variance and improve predictive performance.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法基于装袋引入的随机化来进一步减少方差并提高预测性能。
- en: In addition to training each ensemble member on bootstrapped training data,
    random forests also randomly sample from the features used in the model (without
    replacement). Depending on the implementation, the random samples can be drawn
    for each tree or each split. As a result, the algorithm faces different options
    when learning new rules, either at the level of a tree or for each split.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在自助训练数据上训练每个集合成员外，随机森林还会随机从模型中使用的特征中进行抽样（不重复）。根据实现方式，随机样本可以为每棵树或每次分裂进行抽取。因此，算法在学习新规则时面临不同选择，无论是在树的级别还是在每次分裂时。
- en: 'The **sample size for the features** differs between regression and classification
    trees:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 特征的**样本大小**在回归和分类树之间有所不同：
- en: For **classification**, the sample size is typically the square root of the
    number of features.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于**分类**，样本大小通常是特征数量的平方根。
- en: For **regression**, it can be anywhere from one-third to all features and should
    be selected based on cross-validation.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于**回归**，特征数量可以是从三分之一到所有特征，并且应基于交叉验证进行选择。
- en: 'The following diagram illustrates how random forests randomize the training
    of individual trees and then aggregate their predictions into an ensemble prediction:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了随机森林如何随机化个体树的训练，然后将它们的预测汇总成集成预测：
- en: '![](img/B15439_11_12.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_12.png)'
- en: 'Figure 11.12: How a random forest grows individual trees'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12：随机森林如何生长个体树
- en: The goal of randomizing the features in addition to the training observations
    is to further **decorrelate the prediction errors** of the individual trees. All
    features are not created equal, and a small number of highly relevant features
    will be selected much more frequently and earlier in the tree-construction process,
    making decision trees more alike across the ensemble. However, the less the generalization
    errors of individual trees correlate, the more the overall variance will be reduced.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 除了训练观察结果外，随机化特征的目标是进一步**使个体树的预测误差失相关**。所有特征并非同等重要，一小部分高度相关的特征将在树构建过程中更频繁且更早地被选择，使得整个集成中的决策树更加相似。然而，个体树的泛化误差之间的相关性越小，整体方差就会减小得越多。
- en: How to train and tune a random forest
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何训练和调整随机森林
- en: 'The key configuration parameters include the various hyperparameters for the
    individual decision trees introduced in the *How to tune the hyperparameters*
    section. The following table lists additional options for the two `RandomForest`
    classes:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的配置参数包括介绍的各个决策树的超参数，在*如何调整超参数*部分。以下表格列出了两个`RandomForest`类的其他选项：
- en: '| Keyword | Default | Description |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 关键字 | 默认值 | 描述 |'
- en: '| `bootstrap` | `TRUE` | Bootstrap samples during training |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `bootstrap` | `TRUE` | 训练期间启用自助采样 |'
- en: '| `n_estimators` | `10` | Number of trees in the forest |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `n_estimators` | `10` | 森林中的树的数量 |'
- en: '| `oob_score` | `FALSE` | Uses out-of-bag samples to estimate the R2 on unseen
    data |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `oob_score` | `FALSE` | 使用袋外样本来估计未见数据上的R2 |'
- en: The `bootstrap` parameter activates the bagging algorithm just described. Bagging,
    in turn, enables the computation of the out-of-bag score (`oob_score`), which
    estimates the generalization accuracy from samples not included in the bootstrap
    sample used to train a given tree (see the *Out-of-bag testing* section).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`bootstrap`参数激活了刚才描述的装袋算法。装袋又可以启用计算袋外得分（`oob_score`），该分数估计了未包括在用于训练给定树的自助样本中的样本的泛化准确性（请参阅*袋外测试*部分）。'
- en: The parameter `n_estimators` defines the number of trees to be grown as part
    of the forest. Larger forests perform better, but also take more time to build.
    It is important to monitor the cross-validation error as the number of base learners
    grows. The goal is to identify when the rising cost of training an additional
    tree outweighs the benefit of reducing the validation error, or when the latter
    starts to increase again.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`n_estimators`定义了作为森林一部分生长的树的数量。更大的森林表现更好，但也需要更长的时间来构建。重要的是随着基学习者数量的增加来监控交叉验证误差。目标是确定在训练额外的树的成本上升超过减少验证误差的好处的时候，或者后者开始再次增加的时候。
- en: The `max_features` parameter controls the size of the randomly selected feature
    subsets available when learning a new decision rule and to split a node. A lower
    value reduces the correlation of the trees and, thus, the ensemble's variance,
    but may also increase the bias. As pointed out at the beginning of this section,
    good starting values are the number of training features for regression problems
    and the square root of this number for classification problems, but will depend
    on the relationships among features and should be optimized using cross-validation.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`max_features`控制在学习新决策规则并分割节点时可用的随机选择特征子集的大小。较低的值会降低树之间的相关性，从而减少集成的方差，但也可能增加偏差。正如本节开头所指出的，良好的起始值是回归问题的训练特征数量以及分类问题的这个数字的平方根，但会取决于特征之间的关系，并且应该使用交叉验证进行优化。
- en: Random forests are designed to contain deep fully-grown trees, which can be
    created using `max_depth=None` and `min_samples_split=2`. However, these values
    are not necessarily optimal, especially for high-dimensional data with many samples
    and, consequently, potentially very deep trees that can become very computationally,
    and memory, intensive.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林被设计为包含深度完全生长的树，可以使用`max_depth=None`和`min_samples_split=2`来创建。然而，这些值不一定是最优的，特别是对于具有许多样本和因此可能非常深的树的高维数据，可以变得非常计算和内存密集。
- en: The `RandomForest` class provided by scikit-learn supports parallel training
    and prediction by setting the `n_jobs` parameter to the *k* number of jobs to
    run on different cores. The `-1` value uses all available cores. The overhead
    of interprocess communication may limit the speedup from being linear so that
    *k* jobs may take more than 1/*k* the time of a single job. Nonetheless, the speedup
    is often quite significant for large forests or deep individual trees that may
    take a meaningful amount of time to train when the data is large, and split evaluation
    becomes costly.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 由scikit-learn提供的`RandomForest`类支持并行训练和预测，通过将`n_jobs`参数设置为要在不同核心上运行的*k*个作业数。值`-1`使用所有可用的核心。进程间通信的开销可能会限制速度提升呈线性增长，因此*k*个作业可能需要超过单个作业的1/*k*时间。尽管如此，对于数据大、分裂评估变得昂贵的大型森林或深层个体树，速度提升通常是相当显著的。
- en: As always, the best parameter configuration should be identified using cross-validation.
    The following steps illustrate the process. The code for this example is in the
    notebook `random_forest_tuning`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 始终应该使用交叉验证来识别最佳的参数配置。以下步骤说明了这个过程。本示例的代码在笔记本`random_forest_tuning`中。
- en: 'We will use `GridSearchCV` to identify an optimal set of parameters for an
    ensemble of classification trees:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`GridSearchCV`来识别一组最佳参数，用于分类树的集成：
- en: '[PRE12]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We use the same 10-fold custom cross-validation as in the decision tree example
    previously and populate the parameter grid with values for the key configuration
    settings:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与以前决策树示例中相同的10折自定义交叉验证，并使用关键配置设置的值填充参数网格：
- en: '[PRE13]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Configure `GridSearchCV` using the preceding as input:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述内容配置`GridSearchCV`：
- en: '[PRE14]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We run our grid search as before and find the following result for the best-performing
    regression and classification models. A random forest regression model does better
    with shallower trees compared to the classifier but otherwise uses the same settings:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像以前一样运行网格搜索，并找到最佳的回归和分类模型的以下结果。与分类器相比，随机森林回归模型对较浅的树表现更好，但在其他方面使用相同的设置：
- en: '| Parameter | Regression | Classification |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Parameter | Regression | Classification |'
- en: '| max_depth | 5 | 15 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| max_depth | 5 | 15 |'
- en: '| min_samples_leaf | 5 | 5 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| min_samples_leaf | 5 | 5 |'
- en: '| n_estimators | 100 | 100 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| n_estimators | 100 | 100 |'
- en: '| Score | 0.0435 | 0.5205 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Score | 0.0435 | 0.5205 |'
- en: However, both models underperform their individual decision tree counterparts,
    highlighting that more complex models do not necessarily outperform simpler approaches,
    especially when the data is noisy and the risk of overfitting is high.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这两个模型都不及它们各自的单独决策树模型表现好，这凸显了在数据噪声较大且过拟合风险较高时，更复杂的模型并不一定优于更简单的方法。
- en: Feature importance for random forests
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林的特征重要性
- en: A random forest ensemble may contain hundreds of individual trees, but it is
    still possible to obtain an overall summary measure of feature importance from
    bagged models.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林集成可能包含数百棵单独的树，但仍然可以从袋装模型中获得特征重要性的总体摘要指标。
- en: For a given feature, the **importance score** is the total reduction in the
    objective function's value due to splits on this feature and is averaged over
    all trees. Since the objective function takes into account how many features are
    affected by a split, features used near the top of a tree will get higher scores
    due to the larger number of observations contained in the smaller number of available
    nodes. By averaging over many trees grown in a randomized fashion, the feature
    importance estimate loses some variance and becomes more accurate.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的特征，**重要性评分**是由于对该特征进行拆分而导致的目标函数值的总减少量，并且在所有树上进行了平均。由于目标函数考虑了拆分影响的特征数量，因此在树的顶部附近使用的特征将获得较高的分数，因为更小数量的可用节点中包含了更多的观测数据。通过对以随机方式生长的许多树进行平均，特征重要性估计失去了一些方差，并变得更加准确。
- en: The score is measured in terms of the mean-squared error for regression trees
    and the Gini impurity or entropy for classification trees. scikit-learn further
    normalizes feature importance so that it sums up to 1\. Feature importance thus
    computed is also popular for feature selection as an alternative to the mutual
    information measures we saw in *Chapter 6*, *The Machine Learning Process* (see
    `SelectFromModel` in the `sklearn.feature_selection` module).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 分数以回归树的均方误差和分类树的基尼不纯度或熵来衡量。 scikit-learn 进一步规范化特征重要性，使其总和为1。因此，计算得到的特征重要性也很受欢迎，作为特征选择的替代方法，与我们在*第6章*“机器学习流程”中看到的互信息度量相比（参见
    `sklearn.feature_selection` 模块中的 `SelectFromModel`）。
- en: '*Figure 11.13* shows the values for the top 15 features for both models. The
    regression model relies much more on time periods than the better-performing decision
    tree:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.13*显示了两个模型的前15个特征的值。回归模型更依赖于时间段，而性能更好的决策树：'
- en: '![](img/B15439_11_13.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_13.png)'
- en: 'Figure 11.13: Random forest feature importance'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13：随机森林特征重要性
- en: Out-of-bag testing
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 袋外测试
- en: Random forests offer the benefit of built-in cross-validation because individual
    trees are trained on bootstrapped versions of the training data. As a result,
    each tree uses, on average, only two-thirds of the available observations. To
    see why, consider that a bootstrap sample has the same size, *n*, as the original
    sample, and each observation has the same probability, 1/*n*, to be drawn. Hence,
    the probability of not entering a bootstrap sample at all is (1-1/*n*)*n*, which
    converges (quickly) to 1/*e*, or roughly one third.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林提供了内置交叉验证的好处，因为个别树是在训练数据的自助版本上训练的。因此，每棵树平均只使用了可用观测数据的三分之二。为了理解原因，考虑到自助样本的大小与原始样本相同，每个观测数据被抽取的概率是相同的，即1/*n*。因此，根本不进入自助样本的概率是(1-1/*n*)*n*，它迅速收敛到1/*e*，或者大约三分之一。
- en: This remaining one-third of the observations that are not included in the training
    set is used to grow a bagged tree called **out-of-bag** (**OOB**) observations,
    and can serve as a validation set. Just as with cross-validation, we predict the
    response for an OOB sample for each tree built without this observation, and then
    average the predicted responses (if regression is the goal) or take a majority
    vote or predicted probability (if classification is the goal) for a single ensemble
    prediction for each OOB sample. These predictions produce an unbiased estimate
    of the generalization error, which is conveniently computed during training.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这剩余的三分之一未包含在训练集中的观测数据用于生成一个称为**袋外** (**OOB**) 观测数据的装袋树，并可以作为验证集。正如交叉验证一样，我们为每棵树构建的不包含此观测数据的装袋样本预测响应，然后对每个
    OOB 样本的单个集成预测进行平均（如果回归是目标，则为预测响应的平均值，如果分类是目标，则为多数投票或预测概率）。这些预测产生了对泛化误差的无偏估计，方便在训练期间计算。
- en: The resulting OOB error is a valid estimate of the generalization error for
    this observation. This is because the prediction is produced using decision rules
    learned in the absence of this observation. Once the random forest is sufficiently
    large, the OOB error closely approximates the leave-one-out cross-validation error.
    The OOB approach to estimate the test error is very efficient for large datasets
    where cross-validation can be computationally costly.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 由此产生的OOB误差是对该观测的泛化误差的有效估计。这是因为预测是使用在缺乏该观测时学到的决策规则产生的。一旦随机森林足够大，OOB误差会接近于留一交叉验证误差。对于大型数据集，OOB方法估计测试错误非常高效，而交叉验证可能计算成本很高。
- en: 'However, the same caveats apply as for cross-validation: you need to take care
    to avoid a lookahead bias that would ensue if OOB observations could be selected
    *out-of-order*. In practice, this makes it very difficult to use OOB testing with
    time-series data, where the validation set needs to be selected subject to the
    sequential nature of the data.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与交叉验证相同的警告适用：您需要注意避免“超前展望偏差”，如果OOB观测可以*无序*选择，则会发生这种偏差。在实践中，这使得在时间序列数据中使用OOB测试非常困难，因为验证集需要根据数据的顺序性选择。
- en: Pros and cons of random forests
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林的优缺点
- en: Bagged ensemble models have both advantages and disadvantages.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装集成模型既有优点又有缺点。
- en: 'The **advantages** of random forests include:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的**优点**包括：
- en: Depending on the use case, a random forest can perform on par with the best
    supervised learning algorithms.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据用例，随机森林可以与最佳监督学习算法表现相当。
- en: Random forests provide a reliable feature importance estimate.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林提供可靠的特征重要性估计。
- en: They offer efficient estimates of the test error without incurring the cost
    of repeated model training associated with cross-validation.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们提供了测试错误的有效估计，而不会产生与交叉验证相关的重复模型训练成本。
- en: 'On the other hand, the **disadvantages** of random forests include:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，随机森林的**缺点**包括：
- en: An ensemble model is inherently less interpretable than an individual decision
    tree.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成模型本质上比单个决策树不太可解释。
- en: Training a large number of deep trees can have high computational costs (but
    can be parallelized) and use a lot of memory.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练大量深树可能会产生高计算成本（但可以并行化）并使用大量内存。
- en: Predictions are slower, which may create challenges for applications that require
    low latency.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测速度较慢，这可能对需要低延迟的应用程序产生挑战。
- en: Let's now take a look at how we can use a random forest for a trading strategy.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用随机森林进行交易策略。
- en: Long-short signals for Japanese stocks
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日本股票的多空信号
- en: In *Chapter 9*, *Time-Series Models for Volatility Forecasts and Statistical
    Arbitrage*, we used cointegration tests to identify pairs of stocks with a long-term
    equilibrium relationship in the form of a common trend to which their prices revert.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第9章*，*用于波动率预测和统计套利的时间序列模型*中，我们使用协整检验来识别具有长期均衡关系的股票对，其价格呈现共同趋势，并回归到该趋势。
- en: In this chapter, we will use the predictions of a machine learning model to
    identify assets that are likely to go up or down so we can enter market-neutral
    long and short positions, accordingly. The approach is similar to our initial
    trading strategy that used linear regression in *Chapter 7*, *Linear Models –
    From Risk Factors to Return Forecasts*, and *Chapter 8*, *The ML4T Workflow –
    From Model to Strategy Backtesting*.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将利用机器学习模型的预测结果来识别可能上涨或下跌的资产，以便我们可以相应地进行市场中性的做多和做空操作。这种方法类似于我们最初的交易策略，该策略在*第7章*，*线性模型
    - 从风险因素到收益预测*，以及*第8章*，*ML4T工作流程 - 从模型到策略回测*中使用了线性回归。
- en: Instead of the scikit-learn random forest implementation, we will use the LightGBM
    package, which has been primarily designed for gradient boosting. One of several
    advantages is LightGBM's ability to efficiently encode categorical variables as
    numeric features rather than using one-hot dummy encoding (Fisher 1958). We'll
    provide a more detailed introduction in the next chapter, but the code samples
    should be easy to follow as the logic is similar to the scikit-learn version.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用LightGBM软件包而不是scikit-learn随机森林实现，后者主要设计用于梯度增强。 LightGBM的几个优点之一是其能够有效地将分类变量编码为数值特征，而不是使用独热编码（Fisher
    1958）。我们将在下一章中提供更详细的介绍，但是代码示例应该很容易理解，因为逻辑与scikit-learn版本相似。
- en: The data – Japanese equities
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据 - 日本股票
- en: We are going to design a strategy for a universe of Japanese stocks, using data
    provided by Stooq, a Polish data provider that currently offers interesting datasets
    for various asset classes, markets, and frequencies, which we also relied upon
    in *Chapter 9*, *Time-Series Models for Volatility Forecasts and Statistical Arbitrage*.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将设计一个关于日本股票宇宙的策略，使用由 Stooq 提供的数据，这是一家波兰数据提供商，目前提供了各种资产类别、市场和频率的有趣数据集，我们在 *第九章*，*用于波动率预测和统计套利的时间序列模型*
    中也依赖了这些数据。
- en: While there is little transparency regarding the sourcing and quality of the
    data, it has the powerful advantage of currently being free of charge. In other
    words, we get to experiment with data on stocks, bonds, commodities, and FX at
    daily, hourly, and 5-minute frequencies, but should take the results with a large
    grain of salt.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管关于数据的来源和质量几乎没有透明度，但它目前的免费使用具有强大的优势。换句话说，我们可以使用每日、每小时和每 5 分钟的股票、债券、商品和外汇数据进行实验，但应该对结果持谨慎态度。
- en: The `create_datasets` notebook in the data directory of this book's GitHub repository
    contains instructions for downloading the data and storing them in HDF5 format.
    For this example, we are using price data on some 3,000 Japanese stocks for the
    2010-2019 period. The last 2 years will serve as the out-of-sample test period,
    while the prior years will serve as our cross-validation sample for model selection.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 本书 GitHub 存储库的 data 目录中的 `create_datasets` 笔记本包含了下载数据并将其存储为 HDF5 格式的说明。对于本例子，我们使用了
    2010 年至 2019 年间约 3000 支日本股票的价格数据。最后的两年将作为样本外测试期，而之前的几年将作为我们的模型选择的交叉验证样本。
- en: Please refer to the notebook `japanese_equity_features` for the code samples
    in this section. We remove tickers with more than five consecutive missing values
    and only keep the 1,000 most-traded stocks.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅笔记本 `japanese_equity_features` 中的代码示例。我们移除了连续五个以上缺失值的股票代码，并仅保留了 1000 支交易量最高的股票。
- en: The features – lagged returns and technical indicators
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征 - 滞后回报和技术指标
- en: We'll keep it relatively simple and combine historical returns for 1, 5, 10,
    21, and 63 trading days with several technical indicators provided by TA-Lib (see
    *Chapter 4*, *Financial Feature Engineering – How to Research Alpha Factors*).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会保持相对简单，将历史回报与 1、5、10、21 和 63 个交易日的几个技术指标结合起来，这些指标由 TA-Lib 提供（见*第四章*，*金融特征工程
    - 如何研究阿尔法因子*）。
- en: 'More specifically, we compute for each stock:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们为每支股票计算：
- en: '**Percentage price oscillator** (**PPO**): A normalized version of the **moving
    average convergence/divergence** (**MACD**) indicator that measures the difference
    between the 14-day and the 26-day exponential moving average to capture differences
    in momentum across assets.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**百分比价格振荡器**（**PPO**）：是**移动平均线收敛/发散**（**MACD**）指标的归一化版本，用于衡量14天和26天指数移动平均线之间的差异，以捕捉不同资产间的动量差异。'
- en: '**Normalized average true range** (**NATR**): Measures price volatility in
    a way that can be compared across assets.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化平均真实波幅**（**NATR**）：以一种可比较各资产的方式来衡量价格波动。'
- en: '**Relative strength index** (**RSI**): Another popular momentum indicator (see
    *Chapter 4*, *Financial Feature Engineering – How to Research Alpha Factors* for
    details).'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相对强度指数**（**RSI**）：另一个流行的动量指标（详见*第四章*，*金融特征工程 - 如何研究阿尔法因子*）。'
- en: '**Bollinger Bands**: Ratios of the moving average to the moving standard deviations
    used to identify opportunities for mean reversion.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**布林带**：移动平均线与移动标准差的比率，用于识别均值回归的机会。'
- en: We will also include markers for the time periods year, month, and weekday,
    and rank stocks on a scale from 1 to 20 with respect to their latest return for
    each of the six intervals on each trading day.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将包括标记以表示年、月和星期，以及根据每个交易日的六个间隔的最新回报对股票进行排名，排名从 1 到 20。
- en: The outcomes – forward returns for different horizons
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果 - 不同视角的前瞻回报
- en: To test the predictive ability of a random forest given these features, we generate
    forward returns for the same intervals up to 21 trading days (1 month).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试随机森林在给定这些特征情况下的预测能力，我们生成了相同间隔的前瞻回报，最多到 21 个交易日（1 个月）。
- en: The leads and lags implied by the historical and forward returns cause some
    loss of data that increases with the investment horizon. We end up with 2.3 million
    observations on 18 features and 4 outcomes for 941 stocks.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 历史和前瞻回报所暗示的滞后和领先导致了一些数据的丢失，该丢失随着投资视角的增加而增加。我们最终得到了 941 支股票的 18 个特征和 4 个结果的 230
    万个观察值。
- en: The ML4T workflow with LightGBM
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于 LightGBM 的 ML4T 工作流程
- en: We will now embark on selecting a random forest model that produces tradeable
    signals. Several studies have done so successfully; see, for instance, Krauss,
    Do, and Huck (2017) and Rasekhschaffe and Jones (2019) and the resources referenced
    there.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将选择一个能产生可交易信号的随机森林模型。有几项研究已经成功地做到了，例如 Krauss、Do 和 Huck（2017）以及 Rasekhschaffe
    和 Jones（2019）等，以及其中引用的资源。
- en: We will use the fast and memory-efficient LightGBM implementation that's open
    sourced by Microsoft and most popular for gradient boosting, which is the topic
    of the next chapter, where we will take a closer look at the various LightGBM
    features.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用微软开源的快速且内存高效的 LightGBM 实现，它是最受欢迎的梯度提升方法，也是下一章节的主题，我们将更详细地了解各种 LightGBM
    特性。
- en: We will begin by discussing key experimental design decisions, then build and
    evaluate a predictive model whose signals will drive the trading strategy that
    we will design and evaluate in the final step. Please refer to the notebook `random_forest_return_signals`
    for the code samples in this section, unless otherwise stated.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从讨论关键实验设计决策开始，然后构建和评估一个预测模型，其信号将驱动我们将在最后一步设计和评估的交易策略。除非另有说明，请参考笔记本 `random_forest_return_signals`
    中的代码示例。
- en: From universe selection to hyperparameter tuning
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从宇宙选择到超参数调整
- en: 'To develop a trading strategy that uses a machine learning model, we need to
    make several decisions on the scope and design of the model, including:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 要开发使用机器学习模型的交易策略，我们需要就模型的范围和设计做出几项决策，包括：
- en: '**Lookback period**: How many historical trading days to use for training'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回溯期**：用于训练的历史交易日数'
- en: '**Lookahead period**: How many days into the future to predict returns'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前瞻期**：预测收益的未来天数'
- en: '**Test period**: For how many consecutive days to make predictions with the
    same model'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试期间**：使用相同模型进行连续预测的天数'
- en: '**Hyperparameters**: Which parameters and configurations to evaluate'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数**：要评估的参数和配置'
- en: '**Ensembling**: Whether to rely on a single model or some combination of multiple models'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成**：是否依赖于单一模型或多个模型的组合'
- en: To evaluate the options of interest, we also need to select a **universe** and
    **time period** for cross-validation, as well as an out-of-sample test period
    and universe. More specifically, we cross-validate several options for the period
    up to 2017 on a subset of our sample of Japanese stocks.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估感兴趣的选项，我们还需要选择交叉验证的**宇宙**和**时间段**，以及一个样本外测试期和宇宙。更具体地说，我们在我们的日本股票样本的子集上交叉验证了截至2017年的几个选项。
- en: Once we've settled on a model, we'll define trading rules and backtest the strategy
    that uses the signals of our model **out-of-sample** over the last 2 years on
    the complete universe to validate its performance.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了一个模型，我们将定义交易规则，并在完整宇宙上过去两年的样本外使用我们模型的信号回测策略，以验证其性能。
- en: 'For the time-series cross-validation, we''ll rely on the `MultipleTimeSeriesCV`
    that we developed in *Chapter 7*, *Linear Models – From Risk Factors to Return
    Forecasts*, to parameterize the length of the training and test period while avoiding
    lookahead bias. This custom CV class permits us to:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 对于时间序列交叉验证，我们将依赖于我们在*第7章*《线性模型 - 从风险因素到收益预测》中开发的 `MultipleTimeSeriesCV`，以对训练和测试期的长度进行参数化，同时避免前瞻性偏差。这个自定义的
    CV 类允许我们：
- en: Train the model on a consecutive sample containing `train_length` days for each
    ticker.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个 ticker 进行连续包含 `train_length` 天的样本训练模型。
- en: Validate its performance during a subsequent period containing `test_length`
    days and `lookahead` number of days, apart from the training period, to avoid
    data leakage.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在后续包含 `test_length` 天和 `lookahead` 天数的期间内验证其性能，除了训练期外，以避免数据泄漏。
- en: Repeat for a given number of `n_splits` while rolling the train and validation
    periods forward for `test_length` number of days each time.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次将训练和验证期向前滚动 `test_length` 天时，重复 `n_splits` 次。
- en: We'll work on the model selection step in this section and on strategy backtesting
    in the following one.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进行模型选择步骤，并在接下来的一节中进行策略回测。
- en: Sampling tickers to speed up cross-validation
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对股票进行抽样以加快交叉验证的速度
- en: Training a random forest takes quite a bit longer than linear regression and
    depends on the configuration, where the number of trees and their depth are the
    main drivers.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 训练随机森林所需的时间比线性回归要长得多，这取决于配置，树的数量和深度是主要驱动因素。
- en: 'To keep our experiments manageable, we''ll select the 250 most-traded stocks
    over the 2010-17 period to evaluate the performance of different outcomes and
    model configurations, as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的实验易于管理，我们将选择 2010-17 年期间交易量最大的 250 支股票，以评估不同结果和模型配置的性能，如下所示：
- en: '[PRE15]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Defining lookback, lookahead, and roll-forward periods
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义回顾、展望和向前滚动期间
- en: Running our strategy requires training models on a rolling basis, using a certain
    number of trading days (the lookback period) from our universe to learn the model
    parameters and predict the outcome for a certain number of future days. In our
    example, we'll consider 63, 126, 252, 756, and 1,260 trading days for training
    while rolling forward and predicting for 5, 21, or 63 days during each iteration.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 运行我们的策略需要以滚动方式训练模型，使用一定数量的交易日（回顾期）从我们的宇宙中学习模型参数，并预测一定数量的未来日子的结果。在我们的示例中，我们将考虑在每次迭代期间进行
    63、126、252、756 和 1,260 个交易日的训练，并向前滚动和预测 5、21 或 63 天。
- en: 'We will pair the parameters in a list for easy iteration and optional sampling
    and/or shuffling, as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把参数配对成一个列表，以便轻松迭代和可选的采样和/或洗牌，如下所示：
- en: '[PRE16]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Hyperparameter tuning with LightGBM
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用LightGBM进行超参数调整
- en: 'The LightGBM model accepts a large number of parameters, as the documentation
    explains in detail (see [https://lightgbm.readthedocs.io/](https://lightgbm.readthedocs.io/)
    and the next chapter). For our purposes, we just need to enable the random forest
    algorithm by defining `boosting_type`, setting `bagging_freq` to a positive number,
    and setting `objective` to `regression`:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM模型接受大量参数，如文档详细说明（参见[https://lightgbm.readthedocs.io/](https://lightgbm.readthedocs.io/)和下一章）。对于我们的目的，我们只需要通过定义`boosting_type`启用随机森林算法，将`bagging_freq`设置为正数，并将`objective`设置为`regression`：
- en: '[PRE17]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we select the hyperparameters most likely to affect the predictive accuracy,
    namely:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择最有可能影响预测准确性的超参数，即：
- en: The number of trees to grow for the model (`num_boost_round`)
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为模型生长的树的数量（`num_boost_round`）
- en: The share of rows (`bagging_fraction`) and columns (`feature_fraction`) used
    for bagging
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于装袋的行（`bagging_fraction`）和列（`feature_fraction`）的份额
- en: The minimum number of samples required in a leaf (`min_data_in_leaf`) to control
    for overfitting
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在叶子中需要的最小样本数（`min_data_in_leaf`）以控制过拟合
- en: Another benefit of LightGBM is that we can evaluate a trained model for a subset
    of its trees (or continue training after a certain number of evaluations), which
    allows us to test multiple `num_iteration` values during a single training session.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM的另一个好处是，我们可以评估训练模型的子集（或在一定数量的评估之后继续训练），这使我们能够在单个训练会话中测试多个`num_iteration`值。
- en: Alternatively, you can enable `early_stopping` to interrupt training when the
    loss metric for a validation set no longer improves. However, the cross-validation
    performance estimates will be biased upward as the model uses information on the
    outcome that will not be available under realistic circumstances.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以启用`early_stopping`在验证集的损失指标不再改善时中断训练。但是，由于模型使用了在实际情况下不可用的结果信息，交叉验证性能估计将被偏向上。
- en: 'We''ll use the following values for the hyperparameters, which control the
    bagging method and tree growth:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下值来控制装袋方法和树生长的超参数：
- en: '[PRE18]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Cross-validating signals over various horizons
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在各种视野上交叉验证信号
- en: To evaluate a model for a given set of hyperparameters, we will generate predictions
    using the lookback, lookahead, and roll-forward periods.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 要评估给定一组超参数的模型，我们将使用回顾、展望和向前滚动期间生成预测。
- en: 'First, we will identify categorical variables because LightGBM does not require
    one-hot encoding; instead, it sorts the categories according to the outcome, which
    delivers better results for regression trees, according to Fisher (1958). We''ll
    create variables to identify different periods:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将识别分类变量，因为LightGBM不需要独热编码；而是根据结果对类别进行排序，这样对回归树的效果更好，根据费舍尔（1958年）的说法。我们将创建变量来识别不同的时期：
- en: '[PRE19]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To this end, we will create the binary LightGBM Dataset and configure `MultipleTimeSeriesCV`
    using the given `train_length` and `test_length`, which determine the number of
    splits for our 2-year validation period:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将创建二进制的LightGBM数据集，并使用给定的`train_length`和`test_length`配置`MultipleTimeSeriesCV`，这确定了我们
    2 年验证期间的拆分数量：
- en: '[PRE20]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we take the following steps:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们采取以下步骤：
- en: Select the hyperparameters for this iteration.
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为这次迭代选择超参数。
- en: Slice the binary LightGM Dataset we just created into train and test sets.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们刚刚创建的二进制LightGM数据集切分为训练集和测试集。
- en: Train the model.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: 'Generate predictions for the validation set for a range of `num_iteration`
    settings:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为一系列`num_iteration`设置生成验证集的预测：
- en: '[PRE21]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To evaluate the validation performance, we compute the IC for the complete
    set of predictions, as well as on a daily basis, for a range of numbers of iterations:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了评估验证性能，我们计算了完整预测集的IC，以及每日的IC，对一系列迭代次数进行了评估：
- en: '[PRE22]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now, we need to assess the signal content of the predictions to select a model
    for our trading strategy.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要评估预测的信号内容以选择我们的交易策略的模型。
- en: Analyzing cross-validation performance
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析交叉验证性能
- en: First, we'll take a look at the distribution of the IC for the various train
    and test windows, as well as prediction horizons across all hyperparameter settings.
    Then, we'll take a closer look at the impact of the hyperparameter settings on
    the model's predictive accuracy.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看各种训练和测试窗口以及各种超参数设置在所有超参数设置中的IC分布。然后，我们将更详细地研究超参数设置对模型预测准确性的影响。
- en: IC for different lookback, roll-forward, and lookahead periods
  id: totrans-355
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同回溯、滚动向前和展望期的IC
- en: 'The following image illustrates the distribution and quantiles of the daily
    mean IC for four prediction horizons and five training windows, as well as the
    best-performing 21-day test window. Unfortunately, it does not yield conclusive
    insights into whether shorter or longer windows do better, but rather illustrates
    the degree of noise in the data due to the range of model configurations we tested
    and the resulting lack of consistency in outcomes:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了每日平均IC的分布和分位数，针对四个预测时段和五个训练窗口，以及最佳的21天测试窗口。不幸的是，它并没有给出关于较短或较长窗口哪个效果更好的确切见解，而是说明了由于我们测试的模型配置范围以及由此产生的结果缺乏一致性而导致的数据噪声程度：
- en: '![](img/B15439_11_14.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_14.png)'
- en: 'Figure 11.14: Distribution of the daily mean information coefficient for various
    model configurations'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14：各种模型配置的每日平均信息系数的分布
- en: OLS regression of random forest configuration parameters
  id: totrans-359
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用OLS回归分析随机森林配置参数
- en: To understand in more detail how the parameters of our experiment affect the
    outcome, we can run an OLS regression of these parameters on the daily mean IC.
    *Figure 11.15* shows the coefficients and confidence intervals for the 1- and
    5-day lookahead periods.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更详细地了解我们实验的参数如何影响结果，我们可以对这些参数在每日平均IC上进行OLS回归分析。*图11.15*显示了1天和5天展望期的系数和置信区间。
- en: 'All variables are one-hot encoded and can be interpreted relative to the smallest
    category of each that is captured by the constant. The results differ across the
    horizons; the longest training period works best for the 1-day prediction but
    yields the worst performance for 5 days, with no clear patterns. Longer training
    appears to improve the 1-day model up to a certain point, but this is less clear
    for the 5-day model. The only somewhat consistent result seems to suggest a lower
    bagging fraction and higher minimum sample settings:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 所有变量都经过one-hot编码，并且可以相对于每个变量的最小类别进行解释，该最小类别由常数捕获。结果在不同时段间有所不同；对于1天的预测，最长的训练期效果最好，但对于5天的预测效果最差，没有明显的模式。较长的训练似乎在一定程度上改善了1天模型，但对于5天模型来说这一点不太明确。唯一比较一致的结果似乎表明较低的装袋分数和较高的最小样本设置：
- en: '![](img/B15439_11_15.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_15.png)'
- en: 'Figure 11.15: OLS coefficients and confidence intervals for the various random
    forest configuration parameters'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15：各种随机森林配置参数的OLS系数和置信区间
- en: Ensembling forecasts – signal analysis using Alphalens
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成预测 - 使用Alphalens进行信号分析
- en: Ultimately, we care about the signal content of the model predictions regarding
    our investment universe and holding period. To this end, we'll evaluate the return
    spread produced by equal-weighted portfolios invested in different quantiles of
    the predicted returns using Alphalens.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们关心模型预测关于我们的投资范围和持有期的信号内容。为此，我们将使用Alphalens评估等权重组合投资于预测收益不同分位数产生的收益差异。
- en: As discussed in *Chapter 4*, *Financial Feature Engineering – How to Research
    Alpha Factors*, Alphalens computes and visualizes various metrics that summarize
    the predictive performance of an Alpha Factor. The notebook `alphalens_signals_quality`
    illustrates how to combine the model predictions with price data in the appropriate
    format using the utility function `get_clean_factor_and_forward_returns`.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 如*第4章*，*金融特征工程 - 如何研究 Alpha 因子*中讨论的那样，Alphalens 计算并可视化了各种摘要统计量，总结了 Alpha 因子的预测性能。笔记本`alphalens_signals_quality`说明了如何使用实用函数`get_clean_factor_and_forward_returns`将模型预测与价格数据以适当的格式结合起来。
- en: To address some of the noise inherent in the CV predictions, we select the top
    three 1-day models according to their mean daily IC and average their results.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决 CV 预测中固有噪声的一些问题，我们根据其平均每日 IC 选择了前三个 1 天模型，并平均了它们的结果。
- en: 'When we provide the resulting signal to Alphalens, we find the following for
    a 1-day holding period:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将结果信号提供给 Alphalens 时，我们发现了以下结果，持续 1 天：
- en: Annualized alpha of 0.081 and beta of 0.083
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年化 alpha 为 0.081，beta 为 0.083
- en: A mean period-wise spread between top and bottom quintile returns of 5.16 basis points
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前五分位数收益之间的平均期间差距为 5.16 基点
- en: 'The following image visualizes the mean period-wise returns by factor quintile
    and the cumulative daily forward returns associated with the stocks in each quintile:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像可视化了按因子五分位数的平均期间收益和与每个分位数中的股票相关的累积每日正向收益：
- en: '![](img/B15439_11_16.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_16.png)'
- en: 'Figure 11.16: Alphalens factor signal evaluation'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.16：Alphalens 因子信号评估
- en: The preceding image shows that the 1-day ahead predictions appear to contain
    useful trading signals over a short horizon based on the return spread of the
    top and bottom quintiles. We'll now move on and develop and backtest a strategy
    that uses predictions generated by the top ten 1-day lookahead models that produced
    the results shown here for the validation period.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像显示，1天预测似乎包含了基于前五分位数的收益差异的有用交易信号。我们现在将继续开发并回测一种策略，该策略使用了在验证期间产生了这些结果的前十个1天前瞻模型的预测。
- en: The strategy – backtest with Zipline
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 - 使用 Zipline 进行回测
- en: To design and backtest a trading strategy using Zipline, we need to generate
    predictions for our universe for the test period, ingest the Japanese equity data
    and load the signal into Zipline, set up a pipeline, and define rebalancing rules
    to trigger trades accordingly.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 要设计并使用 Zipline 回测交易策略，我们需要为测试期间的我们的股票池生成预测，摄入日本股票数据并将信号加载到 Zipline 中，设置一个 pipeline，并定义重新平衡规则以相应地触发交易。
- en: Ingesting Japanese Equities into Zipline
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将日本股票纳入 Zipline
- en: We follow the process described in *Chapter 8*, *The ML4T Workflow – From Model
    to Strategy Backtesting*, to convert our Stooq equity OHLCV data into a Zipline
    bundle. The directory `custom_bundle` contains the preprocessing module that creates
    the asset IDs and metadata, defines an ingest function that does the heavy lifting,
    and registers the bundle with an extension.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循*第8章*，*ML4T 工作流程 - 从模型到策略回测*中描述的流程，将我们的 Stooq 股票 OHLCV 数据转换为 Zipline bundle。目录`custom_bundle`包含了创建资产
    ID 和元数据的预处理模块，定义了一个执行繁重任务的摄入函数，并使用扩展注册了 bundle。
- en: The folder contains a `README` with additional instructions.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件夹包含一个带有额外说明的 `README`。
- en: Running an in- and out-of-sample strategy backtest
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行样本内外策略的回测
- en: The notebook `random_forest_return_signals` shows how to select the hyperparameters
    that produced the best validation IC performance and generate forecasts accordingly.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本 `random_forest_return_signals` 显示了如何选择产生最佳验证 IC 性能的超参数，并相应地生成预测。
- en: 'We will use our 1-day model predictions and apply some simple logic: we will
    enter long and short positions for the 25 assets with the highest positive and
    lowest negative predicted returns. We will trade every day, as long as there are
    at least 15 candidates on either side, and close out all positions that are not
    among the current top forecasts.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们的 1 天模型预测，并应用一些简单的逻辑：对于具有最高正预期收益和最低负预期收益的 25 个资产，我们将建立多头和空头头寸。只要每边至少有
    15 个候选资产，我们每天都会交易，并清除所有不在当前最佳预测中的头寸。
- en: This time, we will also include a small trading commission of $0.05 per share
    but will not use slippage since we are trading the most liquid Japanese stocks
    with a relatively modest capital base.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们还将包括每股 $0.05 的小额交易佣金，但不会使用滑点，因为我们正在以相对较小的资本基础交易日本最流动的股票。
- en: The results – evaluation with pyfolio
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果 - 使用 pyfolio 进行评估
- en: The left panel shown in *Figure 11.17* shows the in-sample (2016-17) and out-of-sample
    (2018-19) performance of the strategy relative to the Nikkei 225, which was mostly
    flat throughout the period.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.17*中的左侧面板显示了该策略相对于日经225指数的样本内（2016-17年）和样本外（2018-19年）表现，而日经225指数在整个时期基本持平。'
- en: The strategy earns 10.4 percent for in-sample and 5.5 percent for out-of-sample
    on an annualized basis.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略在样本内年化基础上获得了10.4%的收益率，在样本外年化基础上获得了5.5%的收益率。
- en: 'The right panel shows the 3-month rolling Sharpe ratio, which reaches 0.96
    in-sample and 0.61 out-of-sample:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧面板显示了3个月滚动夏普比率，在样本内达到0.96，在样本外达到0.61：
- en: '![](img/B15439_11_17.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_17.png)'
- en: 'Figure 11.17: Pyfolio strategy evaluation'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.17：Pyfolio策略评估
- en: 'The overall performance statistics highlight cumulative returns of 36.6 percent
    after the (low) transaction costs of $0.05 cent per share, implying an out-of-sample
    alpha of 0.06 and a beta of 0.08 (relative to the NIKKEI 225). The maximum drawdown
    was 11.0 percent in-sample and 8.7 percent out-of-sample:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 总体表现统计数据突显了交易成本（每股0.05美分）后的36.6%累计收益率，意味着样本外Alpha为0.06，Beta为0.08（相对于日经225指数）。样本内最大回撤为11.0%，样本外最大回撤为8.7%。
- en: '|  | All | In-sample | Out-of-sample |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  | 全部 | 样本内 | 样本外 |'
- en: '| # Months | 48 | 25 | 23 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| # 月份 | 48 | 25 | 23 |'
- en: '| Annual return | 8.00% | 10.40% | 5.50% |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 年化收益率 | 8.00% | 10.40% | 5.50% |'
- en: '| Cumulative returns | 36.60% | 22.80% | 11.20% |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 累积收益率 | 36.60% | 22.80% | 11.20% |'
- en: '| Annual volatility | 10.20% | 10.90% | 9.60% |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 年化波动率 | 10.20% | 10.90% | 9.60% |'
- en: '| Sharpe ratio | 0.8 | 0.96 | 0.61 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 夏普比率 | 0.8 | 0.96 | 0.61 |'
- en: '| Calmar ratio | 0.72 | 0.94 | 0.63 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| Calmar 比率 | 0.72 | 0.94 | 0.63 |'
- en: '| Stability | 0.82 | 0.82 | 0.64 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 稳定性 | 0.82 | 0.82 | 0.64 |'
- en: '| Max drawdown | -11.00% | -11.00% | -8.70% |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 最大回撤 | -11.00% | -11.00% | -8.70% |'
- en: '| Sortino ratio | 1.26 | 1.53 | 0.95 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| Sortino 比率 | 1.26 | 1.53 | 0.95 |'
- en: '| Daily value at risk | -1.30% | -1.30% | -1.20% |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 日度风险价值 | -1.30% | -1.30% | -1.20% |'
- en: '| Alpha | 0.08 | 0.11 | 0.06 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| Alpha | 0.08 | 0.11 | 0.06 |'
- en: '| Beta | 0.06 | 0.04 | 0.08 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| Beta | 0.06 | 0.04 | 0.08 |'
- en: The pyfolio tearsheets contain lots of additional details regarding exposure,
    risk profile, and other aspects.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: pyfolio tearsheets包含许多有关暴露、风险配置和其他方面的额外细节。
- en: Summary
  id: totrans-405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about a new class of model capable of capturing
    a non-linear relationship, in contrast to the classical linear models we had explored
    so far. We saw how decision trees learn rules to partition the feature space into
    regions that yield predictions, and thus segment the input data into specific
    regions.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了一种能够捕捉非线性关系的新型模型，与我们迄今为止探索的经典线性模型形成对比。我们看到了决策树如何学习规则来将特征空间划分为产生预测的区域，从而将输入数据分段成特定区域。
- en: Decision trees are very useful because they provide unique insights into the
    relationships between features and target variables, and we saw how to visualize
    the sequence of decision rules encoded in the tree structure.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树非常有用，因为它们提供了关于特征和目标变量之间关系的独特见解，我们看到了如何可视化树结构中编码的决策规则序列。
- en: Unfortunately, a decision tree is prone to overfitting. We learned that ensemble
    models and the bootstrap aggregation method manage to overcome some of the shortcomings
    of decision trees and render them useful as components of much more powerful composite
    models.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，决策树容易过拟合。我们了解到集成模型和自举聚合方法成功地克服了决策树的一些缺点，并将它们作为更强大的复合模型的组成部分变得有用。
- en: In the next chapter, we will explore another ensemble model, boosting, which
    has come to be considered one of the most important machine learning algorithms.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨另一个集成模型，即提升（boosting），它已被认为是最重要的机器学习算法之一。
