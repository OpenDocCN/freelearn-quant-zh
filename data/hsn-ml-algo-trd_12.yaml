- en: Unsupervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In [Chapter 6](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml), *Machine Learning
    Process*, we discussed how unsupervised learning adds value by uncovering structures
    in the data without an outcome variable, such as a teacher, to guide the search
    process. This task contrasts with the setting for supervised learning that we
    focused on in the last several chapters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml)，*机器学习流程*中，我们讨论了无监督学习如何通过发现数据中的结构而增加价值，而无需一个指导搜索过程的结果变量，例如老师。这个任务与我们在过去几章中专注于的监督学习的设置形成对比。
- en: Unsupervised learning algorithms can be useful when a dataset contains only
    features and no measurement of the outcome, or when we want to extract information
    independent of the outcome. Instead of predicting future outcomes, the goal is
    to study an informative representation of the data that is useful for solving
    another task, including the exploration of a dataset.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集只包含特征而没有结果的测量值，或者当我们想要提取与结果无关的信息时，无监督学习算法可以很有用。目标不是预测未来的结果，而是研究对解决另一个任务有用的数据的信息表示，包括探索数据集。
- en: Examples include identifying topics to summarize documents (see [Chapter 14](beb6fa08-c790-47d5-82ef-f48a81dcf3d1.xhtml),
    *Topic Modeling*), reducing the number of features to reduce the risk of overfitting
    and the computational cost for supervised learning, or grouping similar observations,
    as illustrated by the use of clustering for asset allocation at the end of this
    chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，识别主题以总结文档（参见[第14章](beb6fa08-c790-47d5-82ef-f48a81dcf3d1.xhtml)，*主题建模*）、减少特征数量以减少过度拟合和监督学习的计算成本，或者分组相似的观察，如本章末尾资产配置的聚类使用所示。
- en: 'Dimensionality reduction and clustering are the main tasks for unsupervised
    learning:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 降维和聚类是无监督学习的主要任务：
- en: '**Dimensionality reduction** transforms the existing features into a new, smaller
    set, while minimizing the loss of information. A broad range of algorithms exists
    that differ only in how they measure the loss of information, whether they apply
    linear or non-linear transformations, or the constraints they impose on the new
    feature set.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**将现有特征转换为一个新的、更小的集合，同时尽量减少信息的损失。存在广泛的算法，它们仅在如何衡量信息损失、是否应用线性或非线性变换或对新特征集施加的约束方面有所不同。'
- en: '**Clustering algorithms** identify and group similar observations or features
    instead of identifying new features. Algorithms differ in how they define the
    similarity of observations and their assumptions about the resulting groups.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类算法**不是识别新特征，而是识别和分组相似的观察或特征。不同的算法在如何定义观察的相似性以及对结果群组的假设上有所不同。'
- en: 'More specifically, this chapter covers the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，本章涵盖以下内容：
- en: How **Principal Component Analysis** (**PCA**) and **Independent Component Analysis** (**ICA**)
    perform linear dimensionality reduction
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析（**PCA**）和独立成分分析（**ICA**）如何进行线性降维
- en: How to apply PCA to identify risk factors and eigen portfolios from asset returns
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何应用PCA来识别资产回报的风险因素和特征组合
- en: How to use non-linear manifold learning to summarize high-dimensional data for
    effective visualization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用非线性流形学习来总结高维数据以进行有效的可视化
- en: How to use t-SNE and UMAP to explore high-dimensional alternative image data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用t-SNE和UMAP来探索高维替代图像数据
- en: How k-Means, hierarchical, and density-based clustering algorithms work
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用k-Means、分层和基于密度的聚类算法
- en: How to use agglomerative clustering to build robust portfolios according to
    hierarchical risk parity
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用凝聚聚类构建根据层次风险平衡构建健壮的投资组合
- en: The code samples for each section are in the directory of the online GitHub
    repository for this chapter at [https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每个部分的代码示例都在本章的在线GitHub存储库的目录中，网址为[https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading)。
- en: Dimensionality reduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: In linear algebra terms, the features of a dataset create a **vector space**
    whose dimensionality corresponds to the number of linearly independent columns
    (assuming there are more observations than features). Two columns are linearly
    dependent when they are perfectly correlated so that one can be computed from
    the other using the linear operations of addition and multiplication.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从线性代数的角度来看，数据集的特征创建了一个**向量空间**，其维数对应于线性独立列的数量（假设观测数量多于特征数量）。当两列线性相关时，它们完全相关，因此可以使用加法和乘法的线性运算从其中一个计算另一个。
- en: In other words, they are parallel vectors that represent the same rather than
    different directions or axes and only constitute a single dimension. Similarly,
    if one variable is a linear combination of several others, then it is an element
    of the vector space created by those columns, rather than adding a new dimension
    of its own.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它们是表示相同而不是不同方向或轴的平行向量，并且只构成一个单一的维度。类似地，如果一个变量是其他几个变量的线性组合，则它是由这些列创建的向量空间的元素，而不是增加自己的新维度。
- en: 'The number of dimensions of a dataset matter because each new dimension can
    add a signal concerning an outcome. However, there is also a downside known as
    the **curse of dimensionality**: as the number of independent features grows while
    the number of observations remains constant, the average distance between data
    points also grows, and the density of the feature space drops exponentially.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的维数很重要，因为每个新维度都可以添加关于结果的信号。但是，还存在一个被称为**维度诅咒**的负面影响：随着独立特征数量的增加，同时保持观测数量不变，数据点之间的平均距离也会增加，并且特征空间的密度呈指数级下降。
- en: The implications for machine learning are dramatic because prediction becomes
    much harder when observations are more distant; that is, different to each other.
    The next section addresses the resulting challenges.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习而言，当观察结果彼此之间更远、即相异时，预测变得更加困难；下一节将讨论由此产生的挑战。
- en: Dimensionality reduction seeks to represent the information in the data more
    efficiently by using fewer features. To this end, algorithms project the data
    to a lower-dimensional space while discarding variation in the data that is not
    informative, or by identifying a lower-dimensional subspace or manifold on or
    near which the data lives.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 降维旨在通过使用较少的特征更有效地表示数据中的信息。为此，算法将数据投影到一个低维空间，同时舍弃不具信息性的数据变异，或者通过确定数据所在的或接近的低维子空间或流形。
- en: A manifold is a space that locally resembles Euclidean space. One-dimensional
    manifolds include lines and circles (but not screenshots of eight, due to the
    crossing point). The manifold hypothesis maintains that high-dimensional data
    often resides in a lower-dimensional space that, if identified, permits a faithful
    representation of the data in this subspace.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 流形是在局部类似于欧几里得空间的空间。一维流形包括线和圆（但不包括八的截屏，因为存在交点）。流形假设认为高维数据通常位于一个低维空间中，如果确定了这个空间，就可以在这个子空间中对数据进行忠实的表示。
- en: Dimensionality reduction thus compresses the data by finding a different, smaller
    set of variables that capture what matters most in the original features to minimize
    the loss of information. Compression helps counter the curse of dimensionality,
    economizes on memory, and permits the visualization of salient aspects of higher-dimensional
    data that is otherwise very difficult to explore.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，降维通过找到一组不同的、较小的变量来捕获原始特征中最重要的内容，从而最小化信息损失。压缩有助于对抗维度诅咒，节省内存，并允许可视化高维数据的显著特征，否则这些特征很难探索。
- en: Linear and non-linear algorithms
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性和非线性算法
- en: 'Dimensionality reduction algorithms differ in the constraints they impose on
    the new variables and how they aim to minimize the loss of information:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 降维算法在对新变量施加的约束以及它们如何旨在最小化信息损失方面存在差异：
- en: '**Linear algorithms** such as PCA and ICA constrain the new variables to be
    linear combinations of the original features; that is, hyperplanes in a lower-dimensional
    space. Whereas PCA requires the new features to be uncorrelated, ICA goes further
    and imposes statistical independence—the absence of both linear and non-linear
    relationships. The following screenshot illustrates how PCA projects three-dimensional
    features into a two-dimensional space:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性算法**，如PCA和ICA，将新变量限制为原始特征的线性组合；也就是说，在较低维度空间中的超平面。而PCA要求新特征是不相关的，ICA进一步要求统计独立性——即线性和非线性关系的缺失。以下截图说明了PCA如何将三维特征投影到二维空间中：'
- en: '![](img/07053c5f-7c75-45e0-be31-03b59be1d355.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07053c5f-7c75-45e0-be31-03b59be1d355.png)'
- en: 'Non-linear algorithms are not restricted to hyperplanes and can capture more
    complex structure in the data. However, given the infinite number of options,
    the algorithms still need to make assumptions to arrive at a solution. In this
    section, we show how **t-distributed Stochastic Neighbor Embedding** (**t-SNE**)
    and **Uniform Manifold Approximation and Projection** (**UMAP**) are very useful
    for visualizing higher-dimensional data. The following screenshot illustrates
    how manifold learning identifies a two-dimensional sub-space in the three-dimensional
    feature space (the `manifold_learning` notebook illustrates the use of additional
    algorithms, including local linear embedding):'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性算法不局限于超平面，并且可以捕捉数据中更复杂的结构。然而，由于选项的无限数量，算法仍然需要做出假设来得出解决方案。在本节中，我们展示了**t-分布随机邻居嵌入**（**t-SNE**）和**均匀流形近似和投影**（**UMAP**）对于可视化高维数据非常有用。以下截图说明了流形学习如何在三维特征空间中识别二维子空间（`manifold_learning`笔记本展示了使用其他算法的情况，包括局部线性嵌入）：
- en: '![](img/34edbd7d-ac2c-47e3-9e30-353957b438cc.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34edbd7d-ac2c-47e3-9e30-353957b438cc.png)'
- en: The curse of dimensionality
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度的诅咒
- en: An increase in the number of dimensions of a dataset means there are more entries
    in the vector of features that represents each observation in the corresponding
    Euclidean space. We measure the distance in a vector space using Euclidean distance,
    also known as the **L2 norm**, which we applied to the vector of linear regression
    coefficients to train a regularized Ridge Regression model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集维度的增加意味着表示对应欧几里得空间中每个观测的特征向量中的条目更多。我们使用欧几里得距离，也称为**L2范数**，来测量向量空间中的距离，我们将其应用于线性回归系数向量以训练正则化的岭回归模型。
- en: 'The Euclidean distance between two *n*-dimensional vectors with Cartesian coordinates
    *p = (p[1], p[2], ..., p[n])* and *q = (q[1], q[2], ..., q[n])* is computed using
    the familiar formula developed by Pythagoras:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 两个具有笛卡尔坐标*p = (p[1], p[2], ..., p[n])*和*q = (q[1], q[2], ..., q[n])*的*n*维向量之间的欧几里得距离是用毕达哥拉斯所发展的熟知公式计算的：
- en: '![](img/1bdfbf33-f078-47cb-98a7-36996940182d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1bdfbf33-f078-47cb-98a7-36996940182d.png)'
- en: Hence, each new dimension adds a non-negative term to the sum, so that the distance
    increases with the number of dimensions for distinct vectors. In other words,
    as the number of features grows for a given number of observations, the feature
    space becomes increasingly sparse; that is, less dense or emptier. On the flip
    side, the lower data density requires more observations to keep the average distance
    between data points the same.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个新的维度都会向总和中添加一个非负项，以使距离随着不同向量的维数增加而增加。换句话说，随着给定观测数量的特征数量增加，特征空间变得越来越稀疏；也就是说，越来越少或者更为空。
- en: 'The following chart shows how many data points we need to maintain the average
    distance of 10 observations uniformly distributed on a line. It increases exponentially
    from 10¹ in a single dimension to 10² in two and 10³ in three dimensions, as the
    data needs to expand by a factor of 10 each time we add a new dimension:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了我们需要多少数据点来维持均匀分布在一条线上的10个观测的平均距离。随着数据在每次添加新维度时需要扩展10倍，从一维中的10¹增加到二维中的10²和三维中的10³，呈指数增长：
- en: '![](img/fc8d524c-d176-474d-a115-5a48dac9902f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc8d524c-d176-474d-a115-5a48dac9902f.png)'
- en: 'The `curse_of_dimensionality` notebook in the GitHub repo folder for this section
    simulates how the average and minimum distances between data points increase as
    the number of dimensions grows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub存储库文件夹中的`curse_of_dimensionality`笔记本模拟了随着维度数量增加，数据点之间的平均距离和最小距离如何增加：
- en: '![](img/cf927dcd-7464-4663-903d-2d3ef684379f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf927dcd-7464-4663-903d-2d3ef684379f.png)'
- en: The simulation draws features in the range [0, 1] from uncorrelated uniform
    or correlated normal distributions, and gradually increases the number of features
    to 2,500\. The average distance between data points increases to over 11 times
    the feature range for features drawn from the normal distribution, and to over
    20 times in the (extreme) case of uncorrelated uniform distribution.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 该模拟从不相关的均匀分布或相关的正态分布中绘制特征的范围为[0, 1]，并逐渐增加特征数量到2,500。对于从正态分布中绘制的特征，数据点之间的平均距离增加到特征范围的11倍以上，对于从不相关的均匀分布中绘制的特征，在(极端)情况下增加到20倍以上。
- en: When the distance between observations grows, supervised machine learning becomes
    more difficult because predictions for new samples are less likely to be based
    on learning from similar training features. Put differently, the number of possible
    unique rows grows exponentially as the number of features increases, which makes
    it so much harder to efficiently sample the space.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当观察之间的距离增加时，监督机器学习变得更加困难，因为对新样本的预测不太可能是基于对相似训练特征的学习。换句话说，随着特征数量的增加，可能的唯一行数呈指数增长，这使得从空间中高效采样变得更加困难。
- en: Similarly, the complexity of the functions learned by flexible algorithms that
    make fewer assumptions about the actual relationship grows exponentially with
    the number of dimensions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，由灵活算法学到的函数的复杂度随着维度数量的增加呈指数增长，这些算法对实际关系的假设较少。
- en: Flexible algorithms include the tree-based models we saw in [Chapter 10](7d7aa662-362a-4c3a-acda-a18fb1bad6e7.xhtml), *Decision
    Trees and Random Forests*, and [Chapter 11](2fbfa6b5-87f3-49c3-b13a-5ead63471370.xhtml),
    *Gradient Boosting Machines*, and the deep neural networks that we will cover
    from Chapter 17, *Deep Learning* onward. The variance of these algorithms increases
    as they get more opportunity to overfit to noise in more dimensions, resulting
    in poor generalization performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 灵活算法包括我们在[第10章](7d7aa662-362a-4c3a-acda-a18fb1bad6e7.xhtml)，*决策树和随机森林*，以及[第11章](2fbfa6b5-87f3-49c3-b13a-5ead63471370.xhtml)，*梯度提升机*
    中看到的基于树的模型，以及从第17章 *深度学习* 开始我们将介绍的深度神经网络。这些算法的方差随着它们有更多机会在更多维度上过拟合噪声而增加，导致泛化性能较差。
- en: In practice, features are correlated, often substantially so, or do not exhibit
    much variation. For these reasons, dimensionality reduction helps to compress
    the data without losing much of the signal, and combat the curse while also economizing
    on memory. In these cases, it complements the use of regularization to manage
    prediction error due to variance and model complexity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，特征之间存在相关性，通常相当显著，或者不具有太多变化。出于这些原因，降维有助于压缩数据而几乎不损失信号，并且在节约内存的同时抵消了维数灾难。在这些情况下，它补充了使用正则化来管理由于方差和模型复杂度而产生的预测误差。
- en: 'The critical question that we take on in the following section then becomes:
    what are the best ways to find a lower-dimensional representation of the data
    that loses as little information as possible?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要解决的关键问题是：找到一种尽可能减少信息损失的数据的低维表示的最佳方法是什么？
- en: Linear dimensionality reduction
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性降维
- en: Linear dimensionality reduction algorithms compute linear combinations that
    translate, rotate, and rescale the original features to capture significant variation
    in the data, subject to constraints on the characteristics of the new features.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 线性降维算法计算线性组合，将原始特征进行平移、旋转和重新缩放，以捕捉数据中的显著变化，同时受到新特征特性的约束。
- en: '**Principal Component Analysis** (**PCA**), invented in 1901 by Karl Pearson,
    finds new features that reflect directions of maximal variance in the data while
    being mutually uncorrelated, or orthogonal.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析** (**PCA**)，由卡尔·皮尔逊于1901年发明，找到了反映数据中最大方差方向的新特征，同时这些特征相互不相关，或者正交。'
- en: '**Independent Component Analysis** (**ICA**), in contrast, originated in signal
    processing in the 1980s, with the goal of separating different signals while imposing
    the stronger constraint of statistical independence.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**独立成分分析** (**ICA**) 相反，起源于20世纪80年代的信号处理领域，其目标是在强加统计独立性的更严格约束条件下分离不同的信号。'
- en: This section introduces these two algorithms and then illustrates how to apply
    PCA to asset returns to learn risk factors from the data, and to build so-called
    eigen portfolios for systematic trading strategies.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了这两个算法，然后说明了如何将PCA应用于资产收益，以从数据中学习风险因素，并构建所谓的特征投资组合来进行系统性交易策略。
- en: Principal Component Analysis
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: PCA finds principal components as linear combinations of the existing features
    and uses these components to represent the original data. The number of components
    is a hyperparameter that determines the target dimensionality and needs to be
    equal to or smaller than the number of observations or columns, whichever is smaller.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: PCA将主成分视为现有特征的线性组合，并使用这些成分来表示原始数据。成分的数量是一个超参数，确定了目标维度，需要等于或小于观测或列的数量，以较小者为准。
- en: PCA aims to capture most of the variance in the data, to make it easy to recover
    the original features and so that each component adds information. It reduces
    dimensionality by projecting the original data into the principal component space.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: PCA旨在捕捉数据中的大部分方差，以便轻松恢复原始特征，并使每个成分都添加信息。它通过将原始数据投影到主成分空间来降低维度。
- en: The PCA algorithm works by identifying a sequence of principal components, each
    of which aligns with the direction of maximum variance in the data after accounting
    for variation captured by previously-computed components. The sequential optimization
    also ensures that new components are not correlated with existing components so
    that the resulting set constitutes an orthogonal basis for a vector space.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PCA算法通过识别一系列主成分来工作，每个主成分都与数据中最大方差的方向对齐，考虑了先前计算的成分捕获的变化后。顺序优化还确保新成分与现有成分不相关，使得结果集构成向量空间的正交基础。
- en: This new basis corresponds to a rotated version of the original basis so that
    the new axis point in the direction of successively decreasing variance. The decline
    in the amount of variance of the original data explained by each principal component
    reflects the extent of correlation among the original features.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新基础对应于原始基础的旋转版本，使得新轴指向连续减小的方差方向。每个主成分解释的原始数据方差量的下降反映了原始特征之间相关性的程度。
- en: The number of components that capture, for example, 95% of the original variation
    relative to the total number of features provides an insight into the linearly-independent
    information in the original data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获例如95%的原始变化相对于总特征数的成分数量为原始数据中线性独立信息的洞察提供了一个见解。
- en: Visualizing PCA in 2D
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在2D中可视化PCA
- en: 'The following screenshot illustrates several aspects of PCA for a two-dimensional
    random dataset (see the `pca_key_ideas` notebook):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图说明了一个二维随机数据集的PCA的几个方面（请参见`pca_key_ideas`笔记本）：
- en: '![](img/f122db9e-99db-4235-b0d3-2f84dbaac1c1.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f122db9e-99db-4235-b0d3-2f84dbaac1c1.png)'
- en: The left panel shows how the first and second principal components align with
    the directions of maximum variance while being orthogonal.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左侧面板显示了第一和第二主成分如何与最大方差的方向对齐，同时又是正交的。
- en: The central panel shows how the first principal component minimizes the reconstruction
    error, measured as the sum of the distances between the data points and the new
    axis.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中心面板显示了第一主成分如何最小化重构误差，重构误差定义为数据点与新轴之间的距离之和。
- en: Finally, the right panel illustrates supervised OLS, which approximates the
    outcome variable (here we choose x[2]) by a (one-dimensional) hyperplane computed
    from the (single) feature. The vertical lines highlight how OLS minimizes the
    distance along the outcome axis, in contrast with PCA, which minimizes the distances
    orthogonal to the hyperplane.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，右侧面板说明了监督OLS，它通过从（单个）特征计算的（一维）超平面近似结果变量（这里我们选择x[2]）。垂直线突出显示OLS如何最小化沿结果轴的距离，与PCA相反，后者最小化与超平面正交的距离。
- en: The assumptions made by PCA
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA所做的假设
- en: 'PCA makes several assumptions that are important to keep in mind. These include
    the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PCA做出了几个重要的假设需要牢记。这些包括以下内容：
- en: High variance implies a high signal-to-noise ratio
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高方差意味着高信噪比
- en: The data is standardized so that the variance is comparable across features
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据被标准化，使得各个特征的方差可比较
- en: Linear transformations capture the relevant aspects of the data
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性变换捕捉了数据的相关方面
- en: Higher-order statistics beyond the first and second moment do not matter, which
    implies that the data has a normal distribution
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超过第一和第二时刻的高阶统计量不重要，这意味着数据具有正态分布
- en: The emphasis on the first and second moments aligns with standard risk/return
    metrics, but the normality assumption may conflict with the characteristics of
    market data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对第一和第二时刻的强调与标准风险/回报度量相一致，但正态性假设可能与市场数据的特征相冲突。
- en: How the PCA algorithm works
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA算法的工作原理
- en: The algorithm finds vectors to create a hyperplane of target dimensionality
    that minimizes the reconstruction error, measured as the sum of the squared distances
    of the data points to the plane. As illustrated above, this goal corresponds to
    finding a sequence of vectors that align with directions of maximum retained variance
    given the other components while ensuring all principal components are mutually
    orthogonal.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法找到向量以创建目标维度的超平面，最小化重构误差，重构误差被测量为数据点到平面的平方距离之和。正如上面所示，这个目标对应于找到一个序列的向量，这些向量与给定其他分量的最大保留方差的方向对齐，同时确保所有主要成分都是相互正交的。
- en: In practice, the algorithm solves the problem either by computing the eigenvectors
    of the covariance matrix or using the singular value decomposition.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，该算法通过计算协方差矩阵的特征向量或使用奇异值分解来解决问题。
- en: 'We illustrate the computation using a randomly generated three-dimensional
    ellipse with 100 data points, shown in the left panel of the following screenshot,
    including the two-dimensional hyperplane defined by the first two principal components
    (see the `the_math_behind_pca` notebook for the following code samples):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个随机生成的三维椭圆来说明计算过程，其中包含100个数据点，显示在以下屏幕截图的左侧面板中，包括由前两个主成分定义的二维超平面（参见`the_math_behind_pca`笔记本中的以下代码示例）：
- en: '![](img/0622eac2-a2c5-49dc-8d25-6e178b44e0ae.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0622eac2-a2c5-49dc-8d25-6e178b44e0ae.png)'
- en: Three-dimensional ellipse and two-dimensional hyperplane
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 三维椭圆和二维超平面
- en: PCA based on the covariance matrix
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于协方差矩阵的PCA
- en: 'We first compute the principal components using the square covariance matrix
    with the pairwise sample covariances for the features *x[i], x[j],* *i*, *j* =
    1, ..., *n* as entries in row *i* and column *j*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用特征*x[i], x[j]*的成对样本协方差作为行*i*和列*j*的输入来计算主成分：
- en: '![](img/33024c77-1334-4fa2-a548-3a2738854670.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/33024c77-1334-4fa2-a548-3a2738854670.png)'
- en: 'For a square matrix *M* of *n* dimensions, we define the eigenvectors *ω[i]*
    and eigenvalues *λ[i]*, *i*=1, ..., *n* as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个*n*维的方阵*M*，我们定义如下的特征向量*ω[i]*和特征值*λ[i]*，*i*=1, ..., *n*：
- en: '![](img/18c48888-f82f-4ec6-8b71-e22db24b4c18.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/18c48888-f82f-4ec6-8b71-e22db24b4c18.png)'
- en: 'Hence, we can represent the matrix *M* using eigenvectors and eigenvalues,
    where *W* is a matrix that contains the eigenvectors as column vectors, and *L*
    is a matrix that contains the λ[i] as diagonal entries (and 0s otherwise). We
    define the eigendecomposition as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用特征向量和特征值表示矩阵*M*，其中*W*是一个包含特征向量的矩阵，*L*是一个包含*λ[i]*作为对角元素（其他元素为0）的矩阵。我们将特征值分解定义如下：
- en: '![](img/5efee6d1-23ff-4b2d-861a-4689996908ce.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5efee6d1-23ff-4b2d-861a-4689996908ce.png)'
- en: 'Using NumPy, we implement this as follows, where the pandas DataFrame contains
    the 100 data points of the ellipse:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NumPy，我们按如下方式实现这一点，其中pandas DataFrame包含椭圆的100个数据点：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we calculate the eigenvectors and eigenvalues of the covariance matrix.
    The eigenvectors contain the principal components (where the sign is arbitrary):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算协方差矩阵的特征向量和特征值。特征向量包含主成分（符号是任意的）：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can compare the result with the result obtained from sklearn, and find that
    they match in absolute terms:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将结果与从sklearn获得的结果进行比较，发现它们在绝对意义上是匹配的：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also verify the eigendecomposition, starting with the diagonal matrix
    *L* that contains the eigenvalues:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以验证特征值分解，从包含特征值的对角矩阵*L*开始：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We find that the result does indeed hold:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现结果确实成立：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: PCA using Singular Value Decomposition
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用奇异值分解的PCA
- en: Next, we'll look at the alternative computation using **Singular Value Decomposition**
    (**SVD**). This algorithm is slower when the number of observations is greater
    than the number of features (the typical case), but yields better numerical stability,
    especially when some of the features are strongly correlated (often the reason
    to use PCA in the first place).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看使用**奇异值分解**(**SVD**)进行的备用计算。当观测次数大于特征数时（典型情况），该算法速度较慢，但在一些特征强相关（通常是使用PCA的原因）的情况下，可以获得更好的数值稳定性。
- en: 'SVD generalizes the eigendecomposition that we just applied to the square and
    symmetric covariance matrix to the more general case of *m* x *n* rectangular
    matrices. It has the form shown at the center of the following diagram. The diagonal
    values of *Σ* are the singular values, and the transpose of *V** contains the
    principal components as column vectors:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: SVD将我们刚刚应用于方形对称协方差矩阵的特征分解推广到更一般的*m* x *n*矩形矩阵。其形式如下图中心所示。 *Σ* 的对角值是奇异值，*V* 的转置包含主成分作为列向量：
- en: '![](img/a28fb8b0-14c4-4a60-a026-247d71cd6af9.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a28fb8b0-14c4-4a60-a026-247d71cd6af9.png)'
- en: 'In this case, we need to make sure our data is centered with mean zero (the
    computation of the covariance preceding took care of this):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要确保我们的数据中心化，均值为零（在计算协方差之前已经处理了这一点）：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We find that the decomposition does indeed reproduce the standardized data:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，分解确实重新生成了标准化数据：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Lastly, we confirm that the columns of the transpose of *V** contain the principal
    components:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们确认*V*的转置的列包含主成分：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the next section, we show how sklearn implements PCA.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将展示sklearn如何实现PCA。
- en: PCA with sklearn
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: sklearn中的PCA
- en: The `sklearn.decomposition.PCA` implementation follows the standard API based
    on the `fit()` and `transform()` methods, which compute the desired number of
    principal components and project the data into the component space, respectively.
    The convenience method `fit_transform()` accomplishes this in a single step.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.decomposition.PCA`的实现遵循基于`fit()`和`transform()`方法的标准API，这些方法分别计算所需数量的主成分并将数据投影到组件空间。便捷方法`fit_transform()`在一步中完成此操作。'
- en: 'PCA offers three different algorithms that can be specified using the `svd_solver`
    parameter:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: PCA提供了三种不同的算法，可以使用`svd_solver`参数指定：
- en: '`Full` computes the exact SVD using the LAPACK solver provided by SciPy'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Full` 使用SciPy提供的LAPACK求解器计算精确的SVD'
- en: '`Arpack` runs a truncated version suitable for computing less than the full
    number of components'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Arpack` 运行适用于计算不到全部组件的截断版本'
- en: '`Randomized` uses a sampling-based algorithm that is more efficient when the
    dataset has more than 500 observations and features, and the goal is to compute
    less than 80% of the components'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Randomized` 使用基于抽样的算法，当数据集具有500个以上观测值和特征，并且目标是计算少于80%的组件时，效率更高'
- en: '`Auto` uses randomized where most efficient, otherwise, it uses the full SVD'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Auto` 使用随机化算法在效率最高的地方，否则使用完整的SVD'
- en: See references on GitHub for algorithmic implementation details.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有关算法实现细节的参考请参见GitHub上的参考资料。
- en: 'Other key configuration parameters of the PCA object are as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: PCA对象的其他关键配置参数如下：
- en: '`n_components`: These compute all principal components by passing `None` (the
    default), or limit the number to `int`. For `svd_solver=full`, there are two additional
    options: a float in the interval [0, 1] computes the number of components required
    to retain the corresponding share of the variance in the data, and the `mle` option
    estimates the number of dimensions using maximum likelihood.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_components`：通过传递`None`（默认）来计算所有主成分，或者限制数量为`int`。对于`svd_solver=full`，还有两个额外的选项：在[0,1]区间内的浮点数计算需要保留数据方差的相应份额的组件数量，`mle`选项使用最大似然估计来估计维度数量。'
- en: '`whiten`: If `True`, it standardizes the component vectors to unit variance
    that, in some cases, can be useful for use in a predictive model (the default
    is `False`).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`whiten`：如果为`True`，则将组件向量标准化为单位方差，在某些情况下，这可能对在预测模型中使用有用（默认为`False`）。'
- en: 'To compute the first two principal components of the three-dimensional ellipsis
    and project the data into the new space, use `fit_transform()` as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算三维椭圆的前两个主成分并将数据投影到新空间中，可以使用`fit_transform()`如下所示：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The explained variance of the first two components is very close to 100%:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个成分的解释方差非常接近100%：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The screenshot at the beginning of this section shows the projection of the
    data into the new two-dimensional space.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本节开始处的截图显示了数据投影到新的二维空间中。
- en: Independent Component Analysis
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独立成分分析
- en: '**Independent Component Analysis** (**ICA**) is another linear algorithm that
    identifies a new basis on which to represent the original data, but pursues a
    different objective to PCA.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**独立成分分析**（**ICA**）是另一种线性算法，它确定了一种新的基础，用于表示原始数据，但追求的目标不同于PCA。'
- en: ICA emerged in signal processing, and the problem it aims to solve is called
    **blind source separation**. It is typically framed as the cocktail party problem,
    in which a given number of guests are speaking at the same time so that a single
    microphone would record overlapping signals. ICA assumes there are as many different
    microphones as there are speakers, each placed at different locations so as to
    record a different mix of the signals. ICA then aims to recover the individual
    signals from the different recordings.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ICA 起源于信号处理，其旨在解决的问题称为**盲源分离**。通常将其描述为鸡尾酒会问题，即同时有多位嘉宾讲话，以至于单个麦克风会记录重叠的信号。ICA
    假设存在与说话者数量相同的不同麦克风，每个麦克风放置在不同的位置，以记录不同混合信号。然后，ICA 旨在从不同的录音中恢复个别信号。
- en: 'In other words, there are *n* original signals and an unknown square mixing
    matrix *A* that produces an *n*-dimensional set of *m* observations, so that:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，有 *n* 个原始信号和一个未知的方形混合矩阵 *A*，产生一个 *n* 维 *m* 观测值集合，使得：
- en: '![](img/e242918a-364d-4f7b-8061-8faaf7bb772e.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e242918a-364d-4f7b-8061-8faaf7bb772e.png)'
- en: The goal is to find the matrix *W=A^(-1)* that untangles the mixed signals to
    recover the sources.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是找到矩阵*W=A^(-1)*，以解开混合信号以恢复源信号。
- en: The ability to uniquely determine the matrix *W* hinges on the non-Gaussian
    distribution of the data. Otherwise, *W* could be rotated arbitrarily given the
    multivariate normal distribution's symmetry under rotation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一确定矩阵 *W* 的能力取决于数据的非高斯分布。否则，*W* 可以在多元正态分布在旋转下的对称性的情况下任意旋转。
- en: Furthermore, ICA assumes the mixed signal is the sum of its components and is
    unable to identify Gaussian components because their sum is also normally distributed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ICA 假设混合信号是其组成部分的和，并且无法识别高斯分量，因为它们的和也是正态分布的。
- en: ICA assumptions
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ICA 假设
- en: 'ICA makes the following critical assumptions:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ICA 做出以下关键假设：
- en: The sources of the signals are statistically independent
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号的来源是统计独立的
- en: Linear transformations are sufficient to capture the relevant information
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性变换足以捕获相关信息
- en: The independent components do not have a normal distribution
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立分量没有正态分布
- en: The mixing matrix A can be inverted
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合矩阵 A 可以被反转
- en: ICA also requires the data to be centered and whitened; that is, to be mutually
    uncorrelated with unit variance. Preprocessing the data using PCA as outlined
    above achieves the required transformations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ICA 还要求数据被居中和白化；也就是说，彼此不相关且方差为单位。使用 PCA 对数据进行预处理如上所述可以实现所需的转换。
- en: The ICA algorithm
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ICA 算法
- en: FastICA, used by sklearn, is a fixed-point algorithm that uses higher-order
    statistics to recover the independent sources. In particular, it maximizes the
    distance to a normal distribution for each component as a proxy for independence.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn 使用的 FastICA 是一种固定点算法，它使用高阶统计量来恢复独立源。特别是，它最大化了每个分量到正态分布的距离，作为独立性的代理。
- en: An alternative algorithm called **InfoMax** minimizes the mutual information
    between components as a measure of statistical independence.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 名为**InfoMax**的替代算法最小化组件之间的互信息作为统计独立性的度量。
- en: ICA with sklearn
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 sklearn 进行 ICA
- en: The ICA implementation by sklearn uses the same interface as PCA, so there is
    little to add. Note that there is no measure of explained variance because ICA
    does not compute components successively. Instead, each component aims to capture
    independent aspects of the data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn 的 ICA 实现使用与 PCA 相同的接口，因此没有什么需要额外添加的。请注意，没有解释方差的度量，因为 ICA 不会逐步计算组件。相反，每个组件旨在捕获数据的独立方面。
- en: PCA for algorithmic trading
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于算法交易的 PCA
- en: PCA is useful for algorithmic trading in several respects. These include the
    data-driven derivation of risk factors by applying PCA to asset returns, and the
    construction of uncorrelated portfolios based on the principal components of the
    correlation matrix of asset returns.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 在算法交易中有几方面的用途。这些包括将 PCA 应用于资产收益，通过数据驱动的方法导出风险因素，并基于资产收益的相关矩阵的主成分构建不相关的投资组合。
- en: Data-driven risk factors
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据驱动的风险因素
- en: In [Chapter 7](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml), *Linear Models*,
    we explored risk factor models used in quantitative finance to capture the main
    drivers of returns. These models explain differences in returns on assets based
    on their exposure to systematic risk factors and the rewards associated with these
    factors.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 7 章](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml) *Linear Models* 中，我们探讨了量化金融中用于捕捉回报主要驱动因素的风险因素模型。这些模型根据资产暴露于系统性风险因素的程度以及与这些因素相关的回报来解释资产回报的差异。
- en: In particular, we explored the Fama-French approach, which specifies factors
    based on prior knowledge about the empirical behavior of average returns, treats
    these factors as observable, and then estimates risk model coefficients using
    linear regression. An alternative approach treats risk factors as latent variables
    and uses factor analytic techniques such as PCA to simultaneously estimate the
    factors and how they drive returns from historical returns.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们探索了法玛-法rench 方法，该方法根据有关平均收益的经验行为的先验知识指定因素，将这些因素视为可观察的，然后使用线性回归估计风险模型系数。另一种方法将风险因素视为潜在变量，并使用因子分析技术（如
    PCA）同时估计因素以及它们如何从历史回报中驱动回报。
- en: In this section, we will review how this method derives factors in a purely
    statistical or data-driven way, with the advantage of not requiring ex-ante knowledge
    of the behavior of asset returns (see the `pca` and `risk_factor` notebook models
    for details).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将审查这种方法如何以纯粹的统计或数据驱动方式推导因素，其优势在于不需要对资产回报行为的 ex-ante 知识（有关详细信息，请参见`pca`
    和 `risk_factor` 笔记本模型）。
- en: 'We will use the Quandl stock price data and select the daily adjusted close
    prices of the 500 stocks with the largest market capitalization and data for the
    2010-18 period. We then compute the daily returns as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Quandl 股价数据，并选择市值最大的 500 支股票在 2010-18 期间的每日调整收盘价。然后，我们按以下方式计算每日回报：
- en: '[PRE10]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We obtain `351` stocks and returns for over 2,000 trading days:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了 `351` 支股票和超过 2000 个交易日的收益：
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'PCA is sensitive to outliers, so we winsorize the data at the 2.5% and 97.5%
    quantiles:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 对异常值很敏感，因此我们将数据在 2.5% 和 97.5% 分位数处进行了 winsorize 处理：
- en: '[PRE12]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'PCA does not permit missing data, so we will remove stocks that do not have
    data for at least 95% of the time period, and in a second step, remove trading
    days that do not have observations on at least 95% of the remaining stocks:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 不允许缺失数据，因此我们将删除至少在 95% 的时间段内没有数据的股票，并在第二步中删除至少在剩余股票中至少 95% 的交易日没有观察到的日子：
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We are left with `314` equity return series covering a similar period:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们剩下了覆盖类似时间段的 `314` 个股票收益系列：
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We impute any remaining missing values using the average return for any given
    trading day:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用给定交易日的平均收益来填补任何剩余的缺失值：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we are ready to fit the principal components model to the asset returns
    using default parameters to compute all components using the full SVD algorithm:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备使用默认参数将主成分模型拟合到资产回报上，以使用全 SVD 算法计算所有成分：
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We find that the most important factor explains around 40% of the daily return
    variation. The dominant factor is usually interpreted as the market, whereas the
    remaining factors can be interpreted as industry or style factors, in line with
    our discussion in [Chapter 5](1de6a332-69f8-4530-8d18-1007d0a3eb7e.xhtml), *Strategy
    Evaluation*, and [Chapter 7](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml), *Linear
    Models*, depending on the results of closer inspection (see the next example).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现最重要的因素解释了大约 40% 的日收益变化。主要因素通常被解释为市场，而剩余的因素可以被解释为行业或风格因素，符合我们在[第 5 章](1de6a332-69f8-4530-8d18-1007d0a3eb7e.xhtml)
    *Strategy Evaluation* 和[第 7 章](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml) *Linear
    Models* 中的讨论，具体取决于更近距离观察的结果（见下一个示例）。
- en: 'The plot on the right shows the cumulative explained variance, and indicates
    that around 10 factors explain 60% of the returns of this large cross-section
    of stocks:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的图显示了累积解释方差，并表明大约 10 个因素解释了这一大型股票横截面的 60% 的回报：
- en: '![](img/a2d2b23c-65a0-4a9f-a1ea-c48d9c225b8b.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2d2b23c-65a0-4a9f-a1ea-c48d9c225b8b.png)'
- en: The notebook contains a simulation for a broader cross-section of stocks and
    the longer 2000-18 time period. It finds that, on average, the first three components
    explained 25%, 10%, and 5% of 500 randomly selected stocks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本包含对更广泛的股票横截面和更长的 2000-18 时间段的模拟。它发现，平均而言，500 支随机选择的股票的前三个成分解释了 25%，10% 和
    5%。
- en: The cumulative plot shows a typical elbow pattern that can help identify a suitable
    target dimensionality because it indicates that additional components add less
    explanatory value.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 累积图显示了典型的拐点模式，可以帮助确定适当的目标维度，因为它表明附加组件增加的解释价值较少。
- en: 'We can select the top two principal components to verify that they are indeed
    uncorrelated:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择前两个主成分来验证它们确实是不相关的：
- en: '[PRE17]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Moreover, we can plot the time series to highlight how each factor captures
    different volatility patterns:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以绘制时间序列以突出每个因子捕捉不同波动性模式的方式：
- en: '![](img/a88573bd-0658-466e-8330-5af9cdf17d86.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a88573bd-0658-466e-8330-5af9cdf17d86.png)'
- en: A risk factor model would employ a subset of the principal components as features
    to predict future returns, similar to our approach in [Chapter 7](https://cdp.packtpub.com/hands_on_machine_learning_for_algorithmic_trading/wp-admin/post.php?post=572&action=edit#post_28), *Linear
    Models – Regression and Classification*.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 风险因子模型将采用主成分的子集作为特征来预测未来的回报，类似于我们在[第7章](https://cdp.packtpub.com/hands_on_machine_learning_for_algorithmic_trading/wp-admin/post.php?post=572&action=edit#post_28)，*线性模型
    - 回归与分类* 中的方法。
- en: Eigen portfolios
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征组合
- en: Another application of PCA involves the covariance matrix of the normalized
    returns. The principal components of the correlation matrix capture most of the
    covariation among assets in descending order and are mutually uncorrelated. Moreover,
    we can use standardized principal components as portfolio weights.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的另一个应用涉及归一化回报的协方差矩阵。相关矩阵的主要成分以降序捕获资产之间的大部分协变异，并且彼此不相关。此外，我们可以使用标准化的主成分作为投资组合权重。
- en: 'Let''s use the 30 largest stocks with data for the 2010-2018 period to facilitate
    the exposition:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用2010年至2018年期间具有数据的30个最大股票来便于表述：
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We again winsorize and also normalize the returns:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次修剪并对回报进行归一化：
- en: '[PRE19]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After dropping assets and trading days as in the previous example, we are left
    with 23 assets and over 2,000 trading days. We estimate all principal components,
    and find that the two largest explain 55.9% and 15.5% of the covariation, respectively:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的例子一样，删除资产和交易日后，我们剩下23个资产和超过2,000个交易日。我们估计所有主成分，并发现前两个分别解释了55.9%和15.5%的协变异：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we select and normalize the four largest components so that they sum
    to `1` and we can use them as weights for portfolios that we can compare to an
    equal-weighted portfolio formed from all stocks:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择并归一化四个最大的组件，使它们总和为`1`，我们可以将它们用作与由所有股票形成的等权重投资组合进行比较的投资组合的权重：
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The weights show distinct emphasis—for example, **Portfolio 3** puts large
    weights on Mastercard and Visa, the two payment processors in the sample, whereas
    **Portfolio 2** has more exposure to technology companies:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 权重显示出明显的重点 - 例如，**投资组合3** 重点关注样本中的两个支付处理器 Mastercard 和 Visa，而 **投资组合2** 则更多地暴露于科技公司：
- en: '![](img/a89efa65-3d1b-4201-848c-18967d6e4a53.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a89efa65-3d1b-4201-848c-18967d6e4a53.png)'
- en: 'When comparing the performance of each portfolio over the sample period to
    The Market consisting of our small sample, we find that portfolio 1 performs very
    similarly, whereas the other portfolios capture different return patterns:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个投资组合在样本期间的表现与我们的小样本组成的市场进行比较时，我们发现投资组合1的表现非常相似，而其他投资组合捕捉到不同的回报模式：
- en: '![](img/e5475635-cc5e-4952-8b51-69e94163a966.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5475635-cc5e-4952-8b51-69e94163a966.png)'
- en: Comparing performances of each portfolio
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 比较每个投资组合的表现
- en: Manifold learning
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流形学习
- en: Linear dimensionality reduction projects the original data onto a lower-dimensional
    hyperplane that aligns with informative directions in the data. The focus on linear
    transformations simplifies the computation and echoes common financial metrics,
    such as PCA's goal to capture the maximum variance.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 线性维度降低将原始数据投影到与数据中的信息方向一致的低维超平面上。专注于线性变换简化了计算，并呼应了常见的财务指标，例如PCA旨在捕获最大的方差。
- en: However, linear approaches will naturally ignore signal reflected in non-linear
    relationships in the data. Such relationships are very important in alternative
    datasets containing, for example, image or text data. Detecting such relationships
    during exploratory analysis can provide important clues about the data's potential
    signal content.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，线性方法自然会忽略数据中非线性关系反映的信号。在包含图像或文本数据的替代数据集中，这样的关系非常重要。在探索性分析期间检测到这样的关系可以提供关于数据潜在信号内容的重要线索。
- en: In contrast, the manifold hypothesis emphasizes that high-dimensional data often
    lies on or near a lower-dimensional non-linear manifold that is embedded in the
    higher dimensional space. The two-dimensional swiss roll displayed in the screenshot
    at the beginning of this section illustrates such a topological structure.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，流形假设强调高维数据通常位于或接近嵌入在高维空间中的低维非线性流形上。本节开头的屏幕截图中显示的二维瑞士卷就是这样一种拓扑结构的示例。
- en: Manifold learning aims to find the manifold of intrinsic dimensionality and
    then represent the data in this subspace. A simplified example uses a road as
    one-dimensional manifolds in a three-dimensional space and identifies data points
    using house numbers as local coordinates.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 流形学习旨在找到内在维度的流形，然后在这个子空间中表示数据。一个简化的例子是将道路视为三维空间中的一维流形，并使用房子编号作为局部坐标来标识数据点。
- en: Several techniques approximate a lower dimensional manifold. One example is
    **locally-linear embedding** (**LLE**), which was developed in 2000 by Sam Roweis
    and Lawrence Saul and used to unroll the swiss roll in the previous screenshot
    (see examples in the `manifold_learning_lle` notebook).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 几种技术近似一个较低维度的流形。一个例子是**局部线性嵌入**（**LLE**），它是由 Sam Roweis 和 Lawrence Saul 于 2000
    年开发的，用于展开上一个屏幕截图中的瑞士卷（参见`manifold_learning_lle`笔记本中的示例）。
- en: For each data point, LLE identifies a given number of nearest neighbors and
    computes weights that represent each point as a linear combination of its neighbors.
    It finds a lower-dimensional embedding by linearly projecting each neighborhood
    onto global internal coordinates on the lower-dimensional manifold, and can be
    thought of as a sequence of PCA applications.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个数据点，LLE 确定一定数量的最近邻居，并计算代表每个点的权重，使其表示为其邻居的线性组合。它通过将每个邻域线性投影到较低维度流形上的全局内部坐标上来找到较低维度的嵌入，并且可以被认为是一系列PCA应用。
- en: Visualization requires the reduction to at least three dimensions, possibly
    below the intrinsic dimensionality, and poses the challenge of faithfully representing
    local and global structure. This challenge relates to the increasing distance
    associated with the curse of dimensionality. While the volume of a sphere expands
    exponentially with the number of dimensions, the space in lower dimensions available
    to represent high-dimensional data is much more limited.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化需要降至至少三个维度，可能低于内在维度，并且面临着忠实地表示局部和全局结构的挑战。这个挑战与与维度诅咒相关。虽然球体的体积随着维度数量的增加而呈指数级增长，但用于表示高维数据的较低维度空间却受到了极大的限制。
- en: 'For example, in 12 dimensions, there can be 13 equidistant points, but in two
    dimensions there can only be three that form a triangle with sides of equal length.
    Hence, accurately reflecting the distance of one point to its high-dimensional
    neighbors in lower dimensions risks distorting the relations among all other points.
    The result is the crowding problem: to maintain global distances, local points
    may need to be placed too closely together, and vice versa.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 12 维中，可能有 13 个等距离点，但在两个维度中只有三个点可以形成边长相等的三角形。因此，在较低维度准确反映一个点与其高维邻居的距离的情况下，可能会扭曲所有其他点之间的关系。结果就是拥挤问题：为了保持全局距离，局部点可能需要被放置得太近，反之亦然。
- en: 'The following two sections cover techniques that have made progress in addressing
    the crowding problem for the visualization of complex datasets. We will use the
    fashion `MNIST` dataset, a more sophisticated alternative to the classic handwritten
    digit MNIST benchmark data used for computer vision. It contains 60,000 train
    and 10,000 test images of fashion objects in 10 classes (see following samples):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个部分介绍了一些技术，这些技术在解决复杂数据集的可视化拥挤问题方面取得了进展。我们将使用时尚`MNIST`数据集，这是经典手写数字MNIST基准数据的一个更复杂的替代方案，用于计算机视觉。它包含了
    60,000 张训练图像和 10,000 张测试图像，涵盖了 10 个类别的时尚物品（见下面的示例）：
- en: '![](img/aaba9ade-5f81-43d1-abad-f3d1034a6f1d.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aaba9ade-5f81-43d1-abad-f3d1034a6f1d.png)'
- en: The goal of a manifold learning algorithm for this data is to detect whether
    the classes lie on distinct manifolds, to facilitate their recognition and differentiation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据的流形学习算法的目标是检测类别是否位于不同的流形上，以便促进它们的识别和区分。
- en: t-SNE
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: t-SNE
- en: The t-distributed stochastic neighbor embedding is an award-winning algorithm
    developed in 2010 by Laurens van der Maaten and Geoff Hinton to detect patterns
    in high-dimensional data. It takes a probabilistic, non-linear approach to locating
    data on several different but related low-dimensional manifolds.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: t-分布随机邻域嵌入是由劳伦斯·范德马滕（Laurens van der Maaten）和杰夫·辛顿（Geoff Hinton）于2010年开发的获奖算法，用于检测高维数据中的模式。它采用概率、非线性方法来定位位于几个不同但相关的低维流形上的数据。
- en: The algorithm emphasizes keeping similar points together in low dimensions,
    as opposed to maintaining the distance between points that are apart in high dimensions,
    which results from algorithms such as PCA that minimize squared distances.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法强调将低维度中的相似点放在一起，而不是保持高维度中相距较远的点之间的距离，这是由最小化平方距离的算法（如PCA）产生的结果。
- en: The algorithm proceeds by converting high-dimensional distances to (conditional)
    probabilities, where high probabilities imply low distance and reflect the likelihood
    of sampling two points based on similarity. It accomplishes this by positioning
    a normal distribution over each point and computing the density for a point and
    each neighbor, where the perplexity parameter controls the effective number of
    neighbors.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通过将高维距离转换为（条件）概率来进行，其中高概率意味着低距离，并反映了基于相似性对两点进行抽样的可能性。它通过在每个点上定位一个正态分布并计算点和每个邻居的密度来实现这一点，其中困惑度参数控制有效邻居的数量。
- en: In a second step, it arranges points in low dimensions and uses similarly computed
    low-dimensional probabilities to match the high-dimensional distribution. It measures
    the difference between the distributions using the Kullback-Leibler divergence,
    which puts a high penalty on misplacing similar points in low dimensions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，它将点排列在低维中，并使用类似计算的低维概率来匹配高维度分布。它通过使用Kullback-Leibler散度来衡量分布之间的差异，该散度对于在低维中错误放置相似点施加了很大的惩罚。
- en: The low-dimensional probabilities use a Student's t-distribution with one degree
    of freedom, as it has fatter tails that reduce the penalty of misplacing points
    that are more distant in high dimensions, to manage the crowding problem.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 低维概率使用具有一个自由度的学生t分布，因为它具有更胖的尾部，减少了在高维度中更远的点的错误放置的惩罚，以解决拥挤问题。
- en: 'The upper panels of the following chart show how t-SNE is able to differentiate
    between the image classes. A higher perplexity value increases the number of neighbors
    used to compute local structure, and gradually results in more emphasis on global
    relationships:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表的上面板显示了t-SNE如何区分图像类别。更高的困惑度值增加了用于计算局部结构的邻居数量，并逐渐更加强调全局关系：
- en: '![](img/644b48d6-0a84-43cc-9e00-5f6a0d374c1f.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/644b48d6-0a84-43cc-9e00-5f6a0d374c1f.png)'
- en: t-SNE is currently the state-of-the-art in high-dimensional data visualization.
    Weaknesses include the computational complexity that scales quadratically in the
    number *n* of points because it evaluates all pairwise distances, but a subsequent
    tree-based implementation has reduced the cost to *n log n*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE目前是高维数据可视化的最新技术。缺点包括计算复杂度随着点数*n*的平方增长，因为它评估所有成对距离，但随后基于树的实现已将成本降低至*n log
    n*。
- en: t-SNE does not facilitate the projection of new data points into the low-dimensional
    space. The compressed output is not a very useful input for distance-based or
    density-based cluster algorithms, because t-SNE treats small and large distances
    differently.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE不利于将新数据点投影到低维空间中。压缩输出对于基于距离或基于密度的聚类算法并不是一个非常有用的输入，因为t-SNE对待小距离和大距离的方式不同。
- en: UMAP
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UMAP
- en: Uniform Manifold Approximation and Projection is a more recent algorithm for
    visualization and general dimensionality reduction. It assumes the data is uniformly
    distributed on a locally-connected manifold and looks for the closest low-dimensional
    equivalent using fuzzy topology. It uses a neighbors parameter that impacts the
    result similarly as perplexity above.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 统一流形逼近和投影是用于可视化和一般降维的较新算法。它假设数据在局部连接的流形上均匀分布，并使用模糊拓扑来寻找最接近的低维等效物。它使用一个neighbors参数，与上面的困惑度类似地影响结果。
- en: It is faster, and hence scales better to large datasets than t-SNE, and sometimes
    preserves global structure than better than t-SNE. It can also work with different
    distance functions, including, for example, cosine similarity, which is used to
    measure the distance between word count vectors.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 它比 t-SNE 更快，因此在大型数据集上的扩展性更好，并且有时比 t-SNE 更好地保留全局结构。它还可以使用不同的距离函数，包括例如余弦相似度，用于衡量词数向量之间的距离。
- en: The four charts in the bottom row of the previous figure illustrates how UMAP
    does indeed move the different clusters further apart, whereas t-SNE provides
    more granular insight into the local structure.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 前一图的底部一行中的四个图表说明了 UMAP 确实将不同的群集分开得更远，而 t-SNE 则提供了更细粒度的对局部结构的了解。
- en: The notebook also contains interactive Plotly visualizations for each algorithm,
    which permit the exploration of the labels and identify which objects are placed
    close to each other.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本还包含每个算法的交互式 Plotly 可视化，允许探索标签并确定彼此靠近的对象。
- en: Clustering
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: Both clustering and dimensionality reduction summarize the data. As just discussed
    in detail, dimensionality reduction compresses the data by representing it using
    new, fewer features that capture the most relevant information. Clustering algorithms,
    by contrast, assign existing observations to subgroups that consist of similar
    data points.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类和降维都对数据进行总结。正如刚刚详细讨论的那样，降维通过使用捕捉最相关信息的新特征来表示数据，从而压缩数据。相比之下，聚类算法将现有观察结果分配给由相似数据点组成的子组。
- en: Clustering can serve to better understand the data through the lens of categories
    learned from continuous variables. It also permits automatically categorizing
    new objects according to the learned criteria. Examples of related applications
    include hierarchical taxonomies, medical diagnostics, and customer segmentation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可以通过从连续变量学习的类别的角度更好地理解数据。它还允许根据学习的标准自动对新对象进行分类。相关应用的示例包括层次分类法、医学诊断和客户细分。
- en: Alternatively, clusters can be used to represent groups as prototypes, using
    (for example) the midpoint of a cluster as the best representative of learned
    grouping. An example application includes image compression.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以使用集群表示组，例如使用集群的中点作为学习组的最佳代表。示例应用包括图像压缩。
- en: 'Clustering algorithms differ with respect to their strategies for identifying
    groupings:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法在识别分组方面的策略不同：
- en: Combinatorial algorithms select the most coherent of different groupings of
    observations
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合算法选择最一致的观察组合
- en: Probabilistic modeling estimates distributions that most likely generated the
    clusters
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率建模估计最可能生成群集的分布
- en: Hierarchical clustering finds a sequence of nested clusters that optimizes coherence
    at any given stage
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层聚类找到一系列嵌套群集，优化任何给定阶段的一致性
- en: 'Algorithms also differ in their notion of what constitutes a useful collection
    of objects, which needs to match the data characteristics, domain, and the goal
    of the applications. Types of groupings include the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 算法还在于什么构成一个有用的对象集的概念上有所不同，这需要与数据特征、领域和应用目标相匹配。分组类型包括以下内容：
- en: Clearly separated groups of various shapes
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明显分离的各种形状的组
- en: Prototype-based or center-based compact clusters
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于原型或基于中心的紧凑群集
- en: Density-based clusters of arbitrary shape
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密度为基础的任意形状的群集
- en: Connectivity-based or graph-based clusters
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于连接性或基于图的群集
- en: 'Important additional aspects of a clustering algorithm include the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法的重要附加方面包括以下内容：
- en: Whether it requires exclusive cluster membership
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否需要独占的群集成员身份
- en: Whether it makes hard (binary) or soft (probabilistic) assignment
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否进行硬（二进制）或软（概率）分配
- en: Whether it is complete and assigns all data points to clusters
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否完整并将所有数据点分配给群集
- en: The following sections introduce key algorithms, including k-Means, hierarchical,
    and density-based clustering, as well as Gaussian mixture models. The `clustering_algos`
    notebook compares the performance of these algorithms on different, labeled datasets
    to highlight their strengths and weaknesses. It uses mutual information (see [Chapter
    6](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml), *The* *Machine Learning Process*)
    to measure the congruence of cluster assignments and labels.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节介绍了关键算法，包括 k-Means、层次和基于密度的聚类，以及高斯混合模型。`clustering_algos` 笔记本在不同的标记数据集上比较了这些算法的性能，以突出它们的优缺点。它使用互信息（参见[第
    6 章](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml)，*机器学习过程*）来衡量聚类分配和标签的一致性。
- en: k-Means clustering
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-Means 聚类
- en: k-Means is the most well-known clustering algorithm and was first proposed by
    Stuart Lloyd at Bell Labs in 1957.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: k-Means 是最著名的聚类算法，最早由贝尔实验室的 Stuart Lloyd 在 1957 年提出。
- en: The algorithm finds *K* centroids and assigns each data point to exactly one
    cluster with the goal of minimizing the within-cluster variance (called inertia).
    It typically uses Euclidean distance, but other metrics can also be used. k-Means
    assumes that clusters are spherical and of equal size, and ignores the covariance
    among features.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法找到 *K* 个质心，并将每个数据点分配给恰好一个聚类，目标是最小化聚类内变异（称为惯性）。通常使用欧几里德距离，但也可以使用其他度量。k-Means
    假设聚类是球形的且大小相等，并忽略特征之间的协方差。
- en: 'The problem is computationally difficult (np-hard) because there are *K^N*
    ways to partition the *N* observations into *K* clusters. The standard iterative
    algorithm delivers a local optimum for a given *K* and proceeds as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 该问题在计算上很困难（np-hard），因为有 *K^N* 种方式将 *N* 个观察结果划分为 *K* 个聚类。标准的迭代算法为给定的 *K* 交付了一个局部最优解，并按以下方式进行：
- en: Randomly define *K* cluster centers and assign points to nearest centroid.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机定义 *K* 个聚类中心并将点分配给最近的质心。
- en: 'Repeat as follows:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复如下步骤：
- en: For each cluster, compute the centroid as the average of the features
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个聚类，将质心计算为特征的平均值
- en: Assign each observation to the closest centroid
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个观察分配给最近的质心
- en: 'Convergence: assignments (or within-cluster variation) don''t change.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收敛：分配（或聚类内变异）不再改变。
- en: 'The `kmeans_implementation` notebook shows how to code the algorithm using
    Python, and visualizes the algorithm''s iterative optimization. The following
    screenshot highlights how the resulting centroids partition the feature space
    into areas called **Voronoi** which delineate the clusters:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`kmeans_implementation` 笔记本展示了如何使用 Python 编写该算法，并可视化算法的迭代优化。以下截图突出显示了结果质心如何将特征空间划分为称为**Voronoi**的区域，这些区域划分了聚类：'
- en: '![](img/5d174126-9251-4803-970b-0d1369da0989.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d174126-9251-4803-970b-0d1369da0989.png)'
- en: The result is optimal for the given initialization, but alternative starting
    positions will produce different results. Hence, we compute multiple clusterings
    from different initial values and select the solution that minimizes within-cluster
    variance.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 结果对于给定的初始化是最优的，但是不同的起始位置将产生不同的结果。因此，我们从不同的初始值计算多个聚类，并选择最小化聚类内变异的解决方案。
- en: k-Means requires continuous or one-hot encoded categorical variables. Distance
    metrics are typically sensitive to scale so that standardizing features is necessary
    to make sure they have equal weight.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: k-Means 需要连续或独热编码的分类变量。距离度量通常对比例尺敏感，因此标准化特征是必要的，以确保它们具有相等的权重。
- en: The strengths of k-Means include its wide range of applicability, fast convergence,
    and linear scalability to large data while producing clusters of even size.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: k-Means 的优点包括其广泛的适用性，快速收敛以及对大型数据的线性可伸缩性，同时产生大小均匀的聚类。
- en: 'The weaknesses include:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点包括：
- en: The need to tune the hyperparameter *k*
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要调整超参数 *k*
- en: The lack of a guarantee to find a global optimum
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法保证找到全局最优解
- en: Restrictive assumption that clusters are spheres and features are not correlated
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制性假设，即聚类是球形的且特征不相关
- en: Sensitivity to outliers
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对异常值的敏感性
- en: Evaluating cluster quality
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估聚类质量
- en: 'Cluster quality metrics help select among alternative clustering results. The `kmeans_evaluation`
    notebook illustrates the following options:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类质量度量有助于在替代聚类结果中进行选择。`kmeans_evaluation` 笔记本说明了以下选项：
- en: The k-Means objective function suggests we compare the evolution of the inertia
    or within-cluster variance.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: k-Means 目标函数建议我们比较惯性或聚类内变异性的演变。
- en: Initially, additional centroids decrease the inertia sharply because new clusters
    improve the overall fit.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初，额外的质心会急剧减少惯性，因为新的聚类提高了整体拟合度。
- en: Once an appropriate number of clusters has been found (assuming it exists),
    new centroids reduce the within-cluster variance by much less as they tend to
    split natural groupings.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦找到了合适数量的聚类（假设存在），新的质心减少了集群内的方差，因为它们倾向于分裂自然的分组。
- en: 'Hence, when k-Means finds a good cluster representation of the data, the inertia
    tends to follow an elbow-shaped path similar to the explained variance ratio for
    PCA, as shown in the following screenshot (see notebook for implementation details):'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，当k均值发现数据的良好聚类表示时，惯性往往会遵循类似于PCA的解释方差比的拐点形状的路径，如下面的屏幕截图所示（请参阅实现细节的笔记本）：
- en: '![](img/c58c7f92-24fd-4c80-84e9-47374cf599f0.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c58c7f92-24fd-4c80-84e9-47374cf599f0.png)'
- en: 'The silhouette coefficient provides a more detailed picture of cluster quality.
    It answers the question: how far are the points in the nearest cluster, relative
    to the points in the assigned cluster?'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Silhouette系数提供了对聚类质量的更详细的图像。它回答了一个问题：最近集群中的点与分配的集群中的点有多远？
- en: 'To this end, it compares the mean intra-cluster distance (*a*) to the mean
    distance of the nearest cluster (*b*) and computes the following score *s*:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，它比较了簇内平均距离（*a*）与最近簇的平均距离（*b*），并计算了以下分数*s*：
- en: '![](img/de29d1c9-9959-45c3-b16b-3397a9f7e7ce.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de29d1c9-9959-45c3-b16b-3397a9f7e7ce.png)'
- en: The score can vary from between *-1* and *1*, but negative values are unlikely
    in practice because they imply that the majority of points are assigned to the
    wrong cluster. A useful visualization of the silhouette score compares the values
    for each data point to the global average because it highlights the coherence
    of each cluster relative to the global configuration. The rule of thumb is to
    avoid clusters with mean scores below the average for all samples.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 分数可以从*-1*到*1*之间变化，但在实践中负值不太可能出现，因为它们意味着大多数点被分配到错误的聚类中。轮廓分数的一个有用的可视化将每个数据点的值与全局平均值进行比较，因为它突出显示了每个聚类相对于全局配置的一致性。经验法则是避免具有平均样本以下分数的簇。
- en: 'The following screenshot shows an excerpt from the silhouette plot for three
    and four clusters, where the former highlights the poor fit of cluster *1* by
    sub-par contributions to the global silhouette score, whereas all of the four
    clusters have some values that exhibit above average scores:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了三个和四个聚类的轮廓图节选，前者突出了聚类*1*的不良拟合，因为对全局轮廓分数的次优贡献，而所有四个聚类都有一些值表现出高于平均水平的分数：
- en: '![](img/b5293c0c-9b6d-4722-bff9-aeeb1d7fc14b.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5293c0c-9b6d-4722-bff9-aeeb1d7fc14b.png)'
- en: In sum, given the usually unsupervised nature, it is necessary to vary the hyperparameters
    of the cluster algorithms and evaluate the different results. It is also important
    to calibrate the scale of the features, in particular when some should be given
    a higher weight and should thus be measured on a larger scale.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，考虑到通常是无监督的性质，有必要变化聚类算法的超参数并评估不同的结果。调整特征的尺度也很重要，特别是当一些特征应该被赋予更高的权重并且因此应该在较大的尺度上进行测量时。
- en: Finally, to validate the robustness of the results, use subsets of data to identify
    whether particular patterns emerge consistently.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了验证结果的稳健性，使用数据子集识别是否出现特定模式是一致的。
- en: Hierarchical clustering
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: Hierarchical clustering avoids the need to specify a target number of clusters
    because it assumes that data can successively be merged into increasingly dissimilar
    clusters. It does not pursue a global objective but decides incrementally how
    to produce a sequence of nested clusters that range from a single cluster to clusters
    consisting of the individual data points.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类避免了需要指定目标聚类数量的需求，因为它假设数据可以成功地被合并成越来越不相似的簇。它不追求全局目标，而是逐渐决定如何产生一系列从单一簇到由单个数据点组成的簇的嵌套簇。
- en: 'There are two approaches:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法：
- en: '**Agglomerative clustering** proceeds bottom-up, sequentially merging two of
    the remaining groups based on similarity'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**凝聚聚类** 自底向上进行，根据相似性顺序合并剩余的两个组'
- en: '**Divisive clustering** works top-down and sequentially splits the remaining
    clusters to produce the most distinct subgroups'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分裂聚类** 自顶向下工作，并顺序地分裂剩余的集群以产生最独特的子群'
- en: Both groups produce *N*-1 hierarchical levels and facilitate the selection of
    a clustering at the level that best partitions data into homogenous groups. We
    will focus on the more common agglomerative clustering approach.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 两个组都生成*N*-1个层次级别，并促进在最佳分割数据成同质组的级别上选择聚类。我们将专注于更常见的聚合聚类方法。
- en: The agglomerative clustering algorithm departs from the individual data points
    and computes a similarity matrix containing all mutual distances. It then takes
    *N*-1 steps until there are no more distinct clusters, and each time updates the
    similarity matrix to substitute elements that have been merged by the new cluster
    so that the matrix progressively shrinks.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合聚类算法偏离了个别数据点，并计算包含所有相互距离的相似性矩阵。然后，它经过*N*-1步，直到不再存在不同的聚类，并且每次更新相似度矩阵以替换已被新聚类合并的元素，以便矩阵逐渐缩小。
- en: 'While hierarchical clustering does not have hyperparameters like k-Means, the
    measure of dissimilarity between clusters (as opposed to individual data points)
    has an important impact on the clustering result. The options differ as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分层聚类没有像k-Means那样的超参数，但是聚类之间（而不是个别数据点之间）的不相似度度量对聚类结果有重要影响。选项如下所示：
- en: '**Single-link**: the distance between nearest neighbors of two clusters'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单链接：**两个聚类最近邻之间的距离'
- en: '**Complete link**: the maximum distance between respective cluster members'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全连接：**各个集群成员之间的最大距离'
- en: '**Group average:** the distance between averages for each group'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组平均：**每组平均值之间的距离'
- en: '**Ward''s method**: minimizes within-cluster variance'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**沃德方法：**最小化簇内方差'
- en: Visualization – dendrograms
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化 - 树状图
- en: Hierarchical clustering provides insight into degrees of similarity among observations
    as it continues to merge data. A significant change in the similarity metric from
    one merge to the next suggests a natural clustering existed prior to this point.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 分层聚类提供了对观察之间相似程度的洞察，因为它不断合并数据。从一个合并到下一个的相似度度量的显著变化表明在此之前存在自然的聚类。
- en: The dendrogram visualizes the successive merges as a binary tree, displaying
    the individual data points as leaves and the final merge as the root of the tree.
    It also shows how the similarity monotonically decreases from bottom to top. Hence,
    it is natural to select a clustering by cutting the dendrogram.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 树状图将连续的合并可视化为二叉树，将个别数据点显示为叶子，将最终合并显示为树的根。它还显示了相似度如何从底部向顶部单调减少。因此，通过切割树状图选择聚类是自然的。
- en: 'The following screenshot (see the `hierarchical_clustering` notebook for implementation
    details) illustrates the dendrogram for the classic Iris dataset with four classes
    and three features, using the four different distance metrics introduced precedingly:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图（有关实现细节，请参阅`hierarchical_clustering`笔记本）说明了具有四个类别和三个特征的经典鸢尾花数据集的树状图，使用了先前介绍的四种不同距离度量：
- en: '![](img/000bcf25-596d-4af8-b8e8-02cab8bab63d.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/000bcf25-596d-4af8-b8e8-02cab8bab63d.png)'
- en: It evaluates the fit of the hierarchical clustering using the cophenetic correlation
    coefficient, which compares the pairwise distances among points and the cluster
    similarity metric at which a pairwise merge occurred. A coefficient of 1 implies
    that closer points always merge earlier.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用科菲尼特相关性系数评估层次聚类的拟合度，该系数比较了点之间的成对距离和在哪个聚类相似度度量中成对合并发生了。系数为1意味着更接近的点总是更早合并。
- en: Different linkage methods produce dendrograms of different appearance, so we
    cannot use this visualization to compare results across methods. In addition,
    Ward's method, which minimizes within-cluster variance, may not properly reflect
    the change in variance, but rather the total variance, which may be misleading.
    Instead, other quality metrics such as cophenetic correlation, or measures such
    as inertia (if aligned with the overall goal), may be more appropriate.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的链接方法产生不同外观的树状图，因此我们不能使用此可视化工具跨方法比较结果。此外，沃德方法，它最小化簇内方差，可能不能恰当地反映方差的变化，而是总方差，这可能会误导。相反，其他质量度量，如科菲尼特相关性，或者与总体目标对齐的惯性等度量，可能更合适。
- en: 'The strengths of clustering include:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的优势包括：
- en: You do not need to specify the number of clusters
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您无需指定聚类的数量
- en: It offers insight about potential clustering by means of an intuitive visualization
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过直观的可视化提供了潜在聚类的洞察
- en: It produces a hierarchy of clusters that can serve as taxonomy
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它产生可以用作分类学的聚类层次结构
- en: It can be combined with k-Means to reduce the number of items at the start of
    the agglomerative process
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以与k-Means结合使用，以减少聚合过程开始时的项目数量
- en: 'Weaknesses of hierarchical clustering include:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 分层聚类的弱点包括：
- en: The high cost in terms of computation and memory because of the numerous similarity
    matrix updates
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于大量相似性矩阵更新而导致的计算和内存成本高昂
- en: It does not achieve the global optimum because all merges are final
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它无法达到全局最优，因为所有合并都是最终的
- en: The curse of dimensionality leads to difficulties with noisy, high-dimensional
    data
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度诅咒导致噪声大、高维数据困难
- en: Density-based clustering
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于密度的聚类
- en: Density-based clustering algorithms assign cluster membership based on proximity
    to other cluster members. They pursue the goal of identifying dense regions of
    arbitrary shapes and sizes. They do not require the specification of a certain
    number of clusters but instead rely on parameters that define the size of a neighborhood
    and a density threshold (see the `density_based_clustering` notebook for relevant
    code samples).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的聚类算法根据与其他集群成员的接近程度来分配集群成员资格。它们追求识别任意形状和大小的密集区域的目标。它们不需要指定一定数量的集群，而是依赖于定义邻域大小和密度阈值的参数（请参阅相关代码示例的`density_based_clustering`笔记本）。
- en: DBSCAN
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBSCAN
- en: '**Density-based spatial clustering of applications with noise** (**DBSCAN**)
    was developed in 1996, and received the *Test of Time* award at the 2014 KDD conference
    because of the attention it has received in both theory and practice.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**具有噪声的基于密度的空间聚类应用**（**DBSCAN**）于1996年开发，并且由于在理论和实践中受到的关注，在2014年KDD会议上获得了*时间测试*奖。'
- en: It aims to identify core and non-core samples, where the former extend a cluster
    and the latter are part of a cluster but do not have sufficient nearby neighbors
    to further grow the cluster. Other samples are outliers and not assigned to any
    cluster.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 它旨在识别核心和非核心样本，其中前者扩展一个集群，后者是集群的一部分，但没有足够的附近邻居进一步扩展集群。其他样本是异常值，不分配给任何集群。
- en: It uses an `eps` parameter for the radius of the neighborhood and `min_samples`
    for the number of members required for core samples. It is deterministic and exclusive
    and has difficulties with clusters of different density and high-dimensional data.
    It can be challenging to tune the parameters to the requisite density, especially
    as it is often not constant.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用`eps`参数表示邻域半径和`min_samples`表示核心样本所需的成员数量。它是确定性的和排他的，并且在不同密度的集群和高维数据中存在困难。调整参数以满足必要密度可能具有挑战性，尤其是因为它通常不是恒定的。
- en: Hierarchical DBSCAN
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层DBSCAN
- en: Hierarchical DBSCAN is a more recent development that assumes clusters are islands
    of potentially differing density, to overcome the DBSCAN challenges just mentioned.
    It also aims to identify the core and non-core samples. It uses the `min_cluster_
    size` and `min_samples` parameters to select a neighborhood and extend a cluster.
    The algorithm iterates over multiple `eps` values and chooses the most stable
    clustering.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 分层DBSCAN是一个更近期的发展，它假设集群是潜在密度不同的岛屿，以克服刚才提到的DBSCAN挑战。它还旨在识别核心和非核心样本。它使用`min_cluster_size`和`min_samples`参数来选择邻域并扩展集群。该算法在多个`eps`值上进行迭代，并选择最稳定的聚类。
- en: In addition to identifying clusters of varying density, it provides insight
    into the density and hierarchical structure of the data.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 除了识别不同密度的集群外，它还提供了数据的密度和层次结构的见解。
- en: 'The following screenshots show how DBSCAN and HDBSCAN are able to identify
    very differently shaped clusters:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了DBSCAN和HDBSCAN如何能够识别非常不同形状的集群：
- en: '![](img/8667993d-ea61-4372-b107-a000c8036cdf.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8667993d-ea61-4372-b107-a000c8036cdf.png)'
- en: Gaussian mixture models
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: A **Gaussian mixture model** (**GMM**) is a generative model that assumes the
    data has been generated by a mix of various multivariate normal distributions.
    The algorithm aims to estimate the mean and covariance matrices of these distributions.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '**高斯混合模型**（**GMM**）是一种生成模型，假设数据是由各种多元正态分布的混合生成的。该算法旨在估计这些分布的均值和协方差矩阵。'
- en: 'It generalizes the k-Means algorithm: it adds covariance among features so
    that clusters can be ellipsoids rather than spheres, while the centroids are represented
    by the means of each distribution. The GMM algorithm performs soft assignments
    because each point has the probability to be a member of any cluster.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 它推广了k-Means算法：它添加了特征之间的协方差，以便聚类可以是椭圆而不是球体，而质心由每个分布的平均值表示。GMM算法执行软分配，因为每个点都有可能是任何群集的成员。
- en: The expectation-maximization algorithm
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 期望最大化算法
- en: GMM uses the expectation-maximization algorithm to identify the components of
    the mixture of Gaussian distributions. The goal is to learn the probability distribution
    parameters from unlabeled data.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: GMM使用期望最大化算法来识别高斯分布混合的组成成分。目标是从未标记的数据中学习概率分布参数。
- en: 'The algorithm proceeds iteratively as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法按以下方式迭代进行：
- en: Initialization—Assume random centroids (for example, using k-Means)
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化——假设随机质心（例如，使用k-Means）
- en: 'Repeat the following steps until convergence (that is, changes in assignments
    drop below the threshold):'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复以下步骤，直到收敛（即，分配的更改下降到阈值以下）：
- en: '**Expectation step**: Soft assignment—compute probabilities for each point
    from each distribution'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**期望步骤**：软分配——为每个点从每个分布计算概率'
- en: '**Maximization step**: Adjust normal-distribution parameters to make data points
    most likely'
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大化步骤**：调整正态分布参数以使数据点最有可能'
- en: 'The following screenshot shows the GMM cluster membership probabilities for
    the Iris dataset as contour lines:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了Iris数据集的GMM集群成员概率作为等高线：
- en: '![](img/ffc6ce3d-4da8-4d41-8a8a-a9d68111943c.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ffc6ce3d-4da8-4d41-8a8a-a9d68111943c.png)'
- en: Hierarchical risk parity
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层风险平价
- en: The key idea of hierarchical risk parity is to use hierarchical clustering on
    the covariance matrix in order to be able to group assets with similar correlations
    together, and reduce the number of degrees of freedom by only considering similar
    assets as substitutes when constructing the portfolio (see notebook and Python
    files in the `hierarchical_risk_parity` subfolder for details).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 分层风险平价的关键思想是使用协方差矩阵上的分层聚类，以便能够将具有相似相关性的资产组合在一起，并通过在构建投资组合时仅考虑相似资产作为替代品来减少自由度的数量（有关详细信息，请参阅笔记本和`hierarchical_risk_parity`子文件夹中的Python文件）。
- en: 'The first step is to compute a distance matrix that represents proximity for
    correlated assets and meets distance metric requirements. The resulting matrix
    becomes an input to the SciPy hierarchical clustering function which computes
    the successive clusters using one of several available methods discussed so far:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是计算代表相关资产接近度并满足距离度量要求的距离矩阵。结果矩阵成为SciPy分层聚类函数的输入，该函数使用到目前为止讨论过的几种可用方法计算连续的群集：
- en: '[PRE22]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `linkage_matrix` can be used as input to the `seaborn.clustermap` function
    to visualize the resulting hierarchical clustering. The dendrogram displayed by
    `seaborn` shows how individual assets and clusters of assets are merged based
    on their relative distances:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '`linkage_matrix`可用作`seaborn.clustermap`函数的输入，以可视化结果的分层聚类。`seaborn`显示的树状图显示了基于相对距离合并单个资产和资产集群的方式：'
- en: '[PRE23]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](img/d243557e-7c8a-4cbc-83c5-222cd87119ad.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d243557e-7c8a-4cbc-83c5-222cd87119ad.png)'
- en: Heatmap
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 热图
- en: Compared to a `seaborn.heatmap` of the original correlation matrix, there is
    now significantly more structure in the sorted data (right panel).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始相关矩阵的`seaborn.heatmap`相比，现在在排序数据中有更多结构（右侧面板）。
- en: 'Using the tickers sorted according to the hierarchy induced by the clustering
    algorithm, HRP now proceeds to compute a top-down inverse-variance allocation
    that successively adjusts weights depending on the variance of the subclusters
    further down the tree:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 使用由聚类算法引起的层次结构排序的票据，HRP现在继续计算一个自上而下的逆方差分配，根据树下的子群的方差依次调整权重：
- en: '[PRE24]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To this end, the algorithm uses bisectional search to allocate the variance
    of a cluster to its elements based on their relative riskiness:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，该算法使用二分搜索将群集的方差分配给其元素，基于它们的相对风险性：
- en: '[PRE25]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The resulting portfolio allocation produces weights that sum to `1` and reflect
    the structure present in the correlation matrix (see notebook for details).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的投资组合分配产生总和为`1`的权重，并反映出相关矩阵中存在的结构（有关详细信息，请参阅笔记本）。
- en: Summary
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored unsupervised learning methods that allow us to
    extract valuable signal from our data, without relying on the help of outcome
    information provided by labels.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了无监督学习方法，它们允许我们从数据中提取有价值的信号，而无需依赖标签提供的结果信息的帮助。
- en: We saw how we can use linear dimensionality reduction methods, such as PCA and
    ICA, to extract uncorrelated or independent components from the data that can
    serve as risk factors or portfolio weights. We also covered advanced non-linear
    manifold learning techniques that produce state-of-the-art visualizations of complex
    alternative datasets.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了如何使用线性降维方法，如PCA和ICA，从数据中提取不相关或独立的组件，这些组件可以作为风险因子或投资组合权重。我们还涵盖了产生复杂替代数据的最先进可视化的高级非线性流形学习技术。
- en: In the second part, we covered several clustering methods that produce data-driven
    groupings under various assumptions. These groupings can be useful, for example,
    to construct portfolios that apply risk-parity principles to assets that have
    been clustered hierarchically.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分中，我们涵盖了几种根据不同假设生成数据驱动分组的聚类方法。这些分组可以很有用，例如，用于构建将风险平价原则应用于已经按层次聚类的资产的投资组合。
- en: In the next three chapters, we will learn about various ML techniques for a
    key source of alternative data, namely, natural language processing for text documents.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三章中，我们将学习关于一种替代数据的关键来源的各种机器学习技术，即文本文档的自然语言处理。
