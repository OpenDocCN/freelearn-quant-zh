- en: '12'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '12'
- en: The Power of Parameterised Quantum Circuits
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化量子电路的力量
- en: 'As we have seen in the previous chapters, there is a wide range of QML models
    based on parameterised quantum circuits. One reason for this is their tolerance
    to noise  [[222](Biblography.xhtml#XNguyen2020)], which is important when we work
    with the NISQ hardware. However, this does not fully explain the popularity of
    PQCs or why they are considered strong competitors to classical ML models. There
    must be some fundamental properties of PQCs that make them superior to their classical
    counterparts. In this chapter, we discuss two such properties: resistance to overfitting
    and larger expressive power.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几章中看到的，基于参数化量子电路的量子机器学习（QML）模型种类繁多。其原因之一是它们对噪声的耐受性[[222](Biblography.xhtml#XNguyen2020)]，这在我们使用NISQ硬件时尤为重要。然而，这并不能完全解释PQC的流行，或它们为何被认为是经典机器学习模型的强劲竞争者。PQC必定有一些基本特性，使其优于经典对等模型。在本章中，我们讨论了两种这样的特性：抗过拟合能力和更强的表达能力。
- en: Resistance to overfitting is a direct consequence of the fact that a typical
    PQC – one without mid-circuit measurement – can be represented by a linear unitary
    operator. Linear models impose strong regularisation, thus preventing overfitting.
    At the same time, the model remains powerful due to the mapping of the input into
    the higher-dimensional Hilbert space where it may be easier to perform classification
    if the PQC is trained as a discriminative model (QNN).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 抗过拟合能力是一个直接后果，因为典型的PQC——没有中途测量的PQC——可以通过线性单位ary算符来表示。线性模型强烈施加正则化，从而防止过拟合。同时，由于输入被映射到更高维的希尔伯特空间，模型仍然保持强大，如果PQC作为判别模型（QNN）进行训练，则在该空间中可能更容易执行分类。
- en: Expressive power is related to the model’s ability to express different relationships
    between variables, i.e., its ability to learn complex data structures. It appears
    that PQCs trained as generative models (QCBM) have strictly larger expressive
    power than their equivalent classical versions (such as RBM).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 表达能力与模型表达变量之间不同关系的能力相关，即其学习复杂数据结构的能力。看来，作为生成模型（QCBM）训练的参数化量子电路（PQC）相比于其对应的经典版本（如RBM），具有严格更大的表达能力。
- en: 12.1 Strong Regularisation
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1 强正则化
- en: 'Parameterised quantum circuits trained as classifiers face the same challenge
    as classical models: the need to generalise well to unseen data points. Classically,
    we have a wide range of supervised learning models and regularisation techniques
    to choose from. These regularisation techniques that fight overfitting are model
    specific. For example, we can try to restrict the depth of the decision trees
    or to impose a penalty term in the cost function when training neural networks.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 作为分类器训练的参数化量子电路面临着与经典模型相同的挑战：需要良好地推广到未见过的数据点。经典上，我们有许多监督学习模型和正则化技术可供选择。这些应对过拟合的正则化技术是模型特定的。例如，我们可以尝试限制决策树的深度，或在训练神经网络时在代价函数中施加惩罚项。
- en: Consider a conventional feedforward neural network as, arguably, the most direct
    classical counterpart of a quantum classifier. In both classical and quantum cases,
    the signal travels through the network in one direction and the layers of quantum
    gates can be compared to the layers of classical activation units. Regardless
    of whether we apply *L*[1] (Lasso) or *L*[2] (Ridge) penalty terms, or use dropout
    techniques, we would like to have a measure of regularisation present in the network.
    This is an interesting theoretical problem as well as an important practical task
    that allows us to develop an optimal strategy for fighting overfitting. Ideally,
    such a measure should be applicable to both classical and quantum neural networks
    to provide a meaningful comparison of their respective regularisation properties.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个传统的前馈神经网络，可以说它是量子分类器最直接的经典对等物。在经典和量子两种情况下，信号都通过网络单向传递，量子门层可以与经典激活单元的层进行比较。无论我们是应用*L*[1]（Lasso）或*L*[2]（Ridge）惩罚项，还是使用dropout技术，我们都希望网络中存在某种正则化的度量。这是一个有趣的理论问题，同时也是一个重要的实践任务，它让我们能够制定应对过拟合的最佳策略。理想情况下，这种度量应适用于经典和量子神经网络，以提供对比它们各自正则化特性的有意义比较。
- en: Very often, relatively small network weights are associated with a high degree
    of regularisation and relatively high network weights are symptoms of overfitting.
    However, it would be highly desirable to have a formal mathematical tool for quantifying
    the network capacity to overfit. One such possible well-defined measure that captures
    the degree of regularisation is the Lipschitz constant.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，相对较小的网络权重与较高程度的正则化相关联，而较高的网络权重则是过拟合的症状。然而，拥有一个正式的数学工具来量化网络的过拟合能力是非常理想的。Lipschitz常数就是一种可能的、能够捕捉正则化程度的明确量度。
- en: 12.1.1 Lipschitz constant
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.1 Lipschitz常数
- en: 'Following Gouk  [[115](Biblography.xhtml#XGouk2018)], given two metric spaces
    (𝒳*,d*[𝒳]) and (*,d*[) a function *f* : 𝒳 →, is said to be Lipschitz continuous
    if there exists a constant *k* ≥ 0 such that]'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '根据Gouk [[115](Biblography.xhtml#XGouk2018)]，给定两个度量空间 (𝒳*,d*[𝒳]) 和 (*,d*[)，如果存在一个常数*k*
    ≥ 0，使得一个函数*f* : 𝒳 → 被称为Lipschitz连续的，则满足：'
- en: '| ![d (f (x1),f(x2)) ≤ kd𝒳(x1,x2), for all x1,x2 ∈ 𝒳 . ](img/file1184.jpg)
    |  |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| ![d (f (x1),f(x2)) ≤ kd𝒳(x1,x2), for all x1,x2 ∈ 𝒳 . ](img/file1184.jpg)
    |  |'
- en: The value of *k* is known as the Lipschitz constant, and the function is referred
    to as *k*-Lipschitz. We are interested in the smallest possible Lipschitz constant
    or, at least, its upper bound. To obtain the upper bound estimate, we should note
    some useful properties of feedforward neural networks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*的值被称为Lipschitz常数，且该函数被称为*k*-Lipschitz函数。我们关注的是最小的Lipschitz常数，或者至少是其上界。为了获得上界估计，我们应当注意前馈神经网络的一些有用性质。'
- en: In the case of a *j*-th layer of a feedforward neural network, x[1] and x[2]
    are the *n*-dimensional sample outputs of the previous layer, *j* − 1, and *f*(x[1])
    and *f*(x[2]) are the *m*-dimensional outputs of layer *j*. The metrics *d*[𝒳]
    and *d* [can be, for example, *L*[1] or *L*[2] norms.]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在前馈神经网络的*j*层中，x[1]和x[2]是前一层*j* − 1的*n*维样本输出，*f*(x[1])和*f*(x[2])是第*j*层的*m*维输出。度量*d*[𝒳]和*d*可以是*L*[1]或*L*[2]范数。
- en: 'A feedforward neural network consisting of *l* fully connected layers can be
    expressed as a series of function compositions:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由*l*个完全连接层组成的前馈神经网络可以表示为一系列函数组合：
- en: '| ![f(x) = (ϕl ∘ ϕl− 1 ∘...∘ ϕ1)(x), ](img/file1185.jpg) |  |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| ![f(x) = (ϕl ∘ ϕl− 1 ∘...∘ ϕ1)(x), ](img/file1185.jpg) |  |'
- en: 'where each *ϕ*[j] implements the *j*-th layer affine transformation of the
    *n*-dimensional input x, parameterised by an *m* × *n* weight matrix W[j] and
    an *m*-dimensional bias vector b[j]:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，每个*ϕ*[j]实现了*x*的第*j*层仿射变换，该变换由*m* × *n*的权重矩阵W[j]和*m*维的偏置向量b[j]来参数化：
- en: '| ![ϕj(x) = Wjx + bj. ](img/file1186.jpg) |  |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| ![ϕj(x) = Wjx + bj. ](img/file1186.jpg) |  |'
- en: The composition of a *k*[1]-Lipschitz function with a *k*[2]-Lipschitz function
    is a *k*[1]*k*[2]-Lipschitz function  [[115](Biblography.xhtml#XGouk2018)]. Therefore,
    we can compute the Lipschitz constants for each layer separately and combine them
    together to obtain an upper bound on the Lipschitz constant for the entire network.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*k*[1]-Lipschitz函数与一个*k*[2]-Lipschitz函数的组合是一个*k*[1]*k*[2]-Lipschitz函数 [[115](Biblography.xhtml#XGouk2018)]。因此，我们可以分别计算每一层的Lipschitz常数，并将它们组合在一起，从而获得整个网络Lipschitz常数的上界。
- en: Choose *d*[𝒳] and *d* [to be the *L*[2] norms ∥⋅∥[2]. In this case, we obtain
    the following relationship from the definition of Lipschitz continuity for the
    fully connected network layer *j*:]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 选择*d*[𝒳]和*d*为*L*[2]范数∥⋅∥[2]。在这种情况下，我们从Lipschitz连续性的定义中得到以下关系，对于完全连接网络层*j*：
- en: '| ![∥(Wjx1 + bj)− (Wjx2 + bj)∥2 ≤ k∥x1 − x2∥2\. ](img/file1187.jpg) |  |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| ![∥(Wjx1 + bj)− (Wjx2 + bj)∥2 ≤ k∥x1 − x2∥2\. ](img/file1187.jpg) |  |'
- en: Introducing a = x[1] − x[2] and assuming that x[1]*≠*x[2] we arrive at the estimate
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 引入a = x[1] − x[2]并假设x[1] *≠* x[2]，我们得到估计
- en: '| ![∥Wja-∥2-≤ k. ∥a∥2 ](img/file1188.jpg) |  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| ![∥Wja-∥2-≤ k. ∥a∥2 ](img/file1188.jpg) |  |'
- en: 'The smallest Lipschitz constant of the fully connected network layer, *L*(*ϕ*[j]),
    is equal to the supremum of the left-hand side of inequality ([12.1.1](#x1-2270001)):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '完全连接网络层的最小Lipschitz常数，*L*(*ϕ*[j])，等于不等式左侧的上确界 ([12.1.1](#x1-2270001)):'
- en: '| ![ ∥W a∥ L (ϕj) := sup ---j--2-. a⁄=0 ∥a∥2 ](img/file1189.jpg) |  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∥W a∥ L (ϕj) := sup ---j--2-. a⁄=0 ∥a∥2 ](img/file1189.jpg) |  |'
- en: The operator norm ([12.1.1](#x1-2270001)) is given by the largest singular value
    of the weight matrix W[j], which corresponds to the spectral norm – the maximum
    scale by which the matrix can stretch a vector. It is straightforward to calculate
    using any of the suitable open-source packages, for example sklearn.decomposition.TruncatedSVD
    from the `scikit-learn` package.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 操作符范数([12.1.1](#x1-2270001))由权重矩阵W[j]的最大奇异值给出，它对应于谱范数——矩阵拉伸向量的最大比例。可以通过任何适用的开源包轻松计算，例如来自`scikit-learn`包的sklearn.decomposition.TruncatedSVD。
- en: In the case of quantum neural networks, any parameterised quantum circuit operating
    on *n* qubits, regardless how complex and deep, can be represented by a 2^n ×
    2^n unitary matrix. Since all singular values of the unitary matrix are equal
    to one, this gives us a natural benchmark for comparison of regularisation capabilities
    of various networks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在量子神经网络的情况下，任何在*n*个量子比特上操作的参数化量子电路，无论其多么复杂和深度，都可以通过一个2^n × 2^n的幺正矩阵表示。由于幺正矩阵的所有奇异值都等于1，这为比较不同网络的正则化能力提供了一个自然的基准。
- en: 12.1.2 Regularisation example
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.2 正则化示例
- en: The Australian Credit Approval (ACA) dataset  [[241](Biblography.xhtml#XUCI_ACA), [242](Biblography.xhtml#XQuinlan1987)]
    we analysed in Chapter [8](Chapter_8.xhtml#x1-1620008) can serve as a good illustrative
    example. We can compare the performance of classical and quantum neural networks
    while monitoring regularisation as measured by the Lipschitz constant.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[8章](Chapter_8.xhtml#x1-1620008)分析的澳大利亚信用审批（ACA）数据集[[241](Biblography.xhtml#XUCI_ACA)，[242](Biblography.xhtml#XQuinlan1987)]可以作为一个很好的示例。我们可以比较经典和量子神经网络的性能，同时监控由Lipschitz常数衡量的正则化。
- en: 'The classical neural network is an MLP Classifier with two hidden layers. Each
    hidden layer holds the same number of activation units as the number of features
    in the ACA dataset (14), so that we have to calculate the largest singular values
    for two 14 × 14 square matrices. The features are standardised with `sklearn.preprocessing.StandardScaler`.
    We also use `sklearn.neural_network.MLPClassifier` to construct the classifier
    with the set of hyperparameters shown in Table [12.1](#x1-228002r1):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 经典神经网络是一个具有两层隐藏层的MLP分类器。每一层隐藏层的激活单元数量与ACA数据集中的特征数量（14）相同，因此我们需要为两个14 × 14的方阵计算最大的奇异值。这些特征使用`sklearn.preprocessing.StandardScaler`进行了标准化。我们还使用`sklearn.neural_network.MLPClassifier`来构建具有表格[12.1](#x1-228002r1)中所示超参数集的分类器：
- en: '| Hyperparameter | Value |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| Number of hidden layers: | 2 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏层数量： | 2 |'
- en: '| Number of activation units in each layer: | 14 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 每层的激活单元数量： | 14 |'
- en: '| Activation function: | tanh |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数： | tanh |'
- en: '| Solver: | adam |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 求解器： | adam |'
- en: '| Intial learning rate: | 0.01 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 初始学习率： | 0.01 |'
- en: '| Number of iterations: | 5000 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 迭代次数： | 5000 |'
- en: '| Random state: | 0 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 随机状态： | 0 |'
- en: '| Regularisation parameter, *α*: | variable |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 正则化参数，*α*： | 可变 |'
- en: 'Table 12.1: MLP Classifier hyperparameters.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表格12.1：MLP分类器超参数。
- en: 'The MLP Classifier regularisation parameter *α* is our control variable. It
    controls the *L*[2] regularisation term in the network cost function: the larger
    this parameter, the more large network weights are penalised. All other parameters
    were set at their default values.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: MLP分类器的正则化参数*α*是我们的控制变量。它控制网络代价函数中的*L*[2]正则化项：这个参数越大，越多的大网络权重会受到惩罚。所有其他参数都设置为默认值。
- en: The quantum neural network is shown in Figure [8.5](Chapter_8.xhtml#8.5). The
    parameterised quantum circuit consists of just 7 fixed two-qubit gates (CZ) and
    15 adjustable one-qubit gates (R[X] and R[Y]). Table [12.2](#x1-228005r2) compares
    the MLP and the QNN classifiers on the in-sample and out-of-sample datasets (the
    ACA dataset was split 50:50 into training and test datasets using sklearn.preprocessing.StandardScaler).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 量子神经网络如图[8.5](Chapter_8.xhtml#8.5)所示。该参数化量子电路仅由7个固定的两量子比特门（CZ）和15个可调的一量子比特门（R[X]和R[Y]）组成。表格[12.2](#x1-228005r2)比较了MLP和QNN分类器在样本内和样本外数据集上的表现（ACA数据集使用`sklearn.preprocessing.StandardScaler`被分割成50:50的训练集和测试集）。
- en: We observe that QNN provides strong regularisation with similar performance
    on the in-sample and out-of-sample datasets as expected from the network represented
    by the unitary matrix.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到QNN提供了强大的正则化，且在样本内和样本外数据集上的表现如预期一致，这与由幺正矩阵表示的网络相符。
- en: '| Classifier | Average *F*[1] score | Average *F*[1] score | Lipschitz Constant
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 分类器 | 平均*F*[1]得分 | 平均*F*[1]得分 | Lipschitz常数 |'
- en: '|  | (in-sample) | (out-of-sample) | (upper bound) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | （样本内） | （样本外） | （上限） |'
- en: '| MLP, *α* = 0*.*001 | 1.00 | 0.78 | 36.2 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *α* = 0*.*001 | 1.00 | 0.78 | 36.2 |'
- en: '| MLP, *α* = 0*.*01 | 1.00 | 0.79 | 33.5 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *α* = 0*.*01 | 1.00 | 0.79 | 33.5 |'
- en: '| MLP, *α* = 0*.*1 | 1.00 | 0.80 | 18.6 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *α* = 0*.*1 | 1.00 | 0.80 | 18.6 |'
- en: '| MLP, *α* = 1 | 0.99 | 0.83 | 7.4 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *α* = 1 | 0.99 | 0.83 | 7.4 |'
- en: '| MLP, *α* = 10 | 0.90 | 0.86 | 1.3 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *α* = 10 | 0.90 | 0.86 | 1.3 |'
- en: '| MLP, *α* = 40 | 0.85 | 0.86 | 0.5 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *α* = 40 | 0.85 | 0.86 | 0.5 |'
- en: '| MLP, *α* = 50 | 0.35 | 0.37 | 1e-05 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *α* = 50 | 0.35 | 0.37 | 1e-05 |'
- en: '| QNN | 0.86 | 0.85 | 1.0 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| QNN | 0.86 | 0.85 | 1.0 |'
- en: 'Table 12.2: F[1] scores and Lipschitz constants for MLP and QNN classifiers
    trained on the ACA dataset.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.2：MLP和QNN分类器在ACA数据集上训练的F[1]得分和Lipschitz常数。
- en: Further, we observe that the equivalent degree of regularisation can be achieved
    by MLP only with exceptionally large values of the regularisation parameter *α*.
    Making *α* any larger completely destroys the learning abilities of the network.
    For the chosen MLP configuration, the critical value of *α* is between 40 and 50.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们观察到，等效的正则化程度只能通过MLP在正则化参数*α*取极大值时才能实现。将*α*进一步增大将完全破坏网络的学习能力。对于所选的MLP配置，*α*的临界值在40到50之间。
- en: Parameterised quantum circuits can be represented as (high-dimensional) norm-preserving
    unitary matrices. This ensures strong regularisation properties of the quantum
    neural networks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化量子电路可以表示为（高维）保持范数的单位ary矩阵。这确保了量子神经网络的强正则化特性。
- en: 'Now we can move to the next feature of the parameterised quantum circuits:
    their expressive power. We can define the expressivity of a PQC as the circuit’s
    ability to generate pure quantum states that are well representative of the Hilbert
    space  [[266](Biblography.xhtml#XSim2019)]. In other words, from the QML point
    of view, the expressive power of a PQC is its ability to learn ("express") complex
    data structures. In the following section, we will try to quantify the degree
    of expressivity inherent in different PQC types.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以进入参数化量子电路的下一个特性：它们的表达能力。我们可以将PQC的表达能力定义为电路生成纯量子态的能力，这些量子态能够很好地代表希尔伯特空间[[266](Biblography.xhtml#XSim2019)]。换句话说，从QML的角度来看，PQC的表达能力是它学习（“表达”）复杂数据结构的能力。在接下来的章节中，我们将尝试量化不同类型PQC固有的表达能力。
- en: 12.2 Expressive Power
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2 表达能力
- en: 'We saw in previous chapters how PQCs can be applied to solving optimisation
    problems (QAOA and VQE) as well as to various machine learning tasks covering
    both discriminative (QNN classifier) and generative (QCBM market generator) use
    cases. In general, the PQCs we used for quantum machine learning tasks can be
    divided into two types  [[88](Biblography.xhtml#XDu2018)]: tensor network PQC
    (similar to the QNN circuit in Figure [8.4](Chapter_8.xhtml#8.4)) and multilayer
    PQC (similar to the QCBM circuit in Figure [9.1](Chapter_9.xhtml#9.1)). What is
    their expressive power and how can we rank them? Before trying to answer this
    question, let us have a look at a simple illustrative example: quantum circuits
    specified on a single quantum register.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的章节中看到，PQC可以应用于解决优化问题（QAOA和VQE），以及各种机器学习任务，包括判别性（QNN分类器）和生成性（QCBM市场生成器）用例。一般来说，我们用于量子机器学习任务的PQC可以分为两类[[88](Biblography.xhtml#XDu2018)]：张量网络PQC（类似于图[8.4](Chapter_8.xhtml#8.4)中的QNN电路）和多层PQC（类似于图[9.1](Chapter_9.xhtml#9.1)中的QCBM电路）。它们的表达能力如何，我们又该如何对它们进行排名呢？在尝试回答这个问题之前，让我们先来看一个简单的示例：在单一量子寄存器上指定的量子电路。
- en: '![Figure 12.1: PQCs with different expressive powers. ](img/file1190.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图12.1：具有不同表达能力的PQC。 ](img/file1190.jpg)'
- en: 'Figure 12.1: PQCs with different expressive powers.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：具有不同表达能力的PQC。
- en: Figure [12.1](#12.1) displays four one-qubit circuits with dramatically different
    expressive powers, where *U*[−*π,π*] denotes the Uniform distribution over the
    closed interval [−*π,π*]. Let us go through them one by one.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图[12.1](#12.1)展示了四个具有显著不同表达能力的单量子比特电路，其中*U*[−*π,π*]表示闭区间[−*π,π*]上的均匀分布。让我们逐个分析它们。
- en: PQC A starts with the qubit state initialised as ![|0⟩](img/file1191.jpg) –
    North Pole on the Bloch sphere (Figure [7.2](Chapter_7.xhtml#x1-1520002)). The
    only gate is the Hadamard gate H that moves ![|0⟩](img/file1192.jpg) to (![|0⟩](img/file1193.jpg)
    + ![|1⟩](img/file1194.jpg))*∕*![√ -- 2](img/file1195.jpg). Thus, state ![|ψA⟩](img/file1196.jpg)
    can only be a single point on the Bloch sphere.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PQC A以量子比特状态初始化开始，如![|0⟩](img/file1191.jpg) – Bloch球上的北极（图[7.2](Chapter_7.xhtml#x1-1520002)）。唯一的门是Hadamard门H，它将![|0⟩](img/file1192.jpg)变换为
    (![|0⟩](img/file1193.jpg) + ![|1⟩](img/file1194.jpg))*∕*![√ -- 2](img/file1195.jpg)。因此，状态![|ψA⟩](img/file1196.jpg)只能是Bloch球上的一个单点。
- en: PQC B also starts with the qubit state initialised as ![|0⟩](img/file1197.jpg)
    and applies the Hadamard gate transforming the initial state into (![|0⟩](img/file1198.jpg)
    + ![|1⟩](img/file1199.jpg))*∕*![√ -- 2](img/file1200.jpg) before applying the
    rotation R[Z] around the *z*-axis by an angle *𝜃*[z] drawn from the Uniform distribution
    on [−*π,π*]. The final state ![|ψB ⟩](img/file1201.jpg) can be any point on the
    equator, all reached with equal probability.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PQC B也从量子比特状态初始化为![|0⟩](img/file1197.jpg)开始，应用Hadamard门将初始状态转化为 (![|0⟩](img/file1198.jpg)
    + ![|1⟩](img/file1199.jpg))*∕*![√ -- 2](img/file1200.jpg)，然后围绕*z*-轴应用旋转R[Z]，角度*𝜃*[z]从区间[−*π,π*]上的均匀分布中抽取。最终状态![|ψB
    ⟩](img/file1201.jpg)可以是赤道上的任何一点，且所有这些点的概率相等。
- en: PQC C adds a rotation R[X] to PQC B, by an angle *𝜃*[x] drawn from the Uniform
    distribution on [−*π,π*]. With two rotations around two orthogonal axes we can
    reach any point on the Bloch sphere. However, with angles *𝜃*[z] and *𝜃*[x] drawn
    from the Uniform distribution on [−*π,π*] we do not have a Uniform distribution
    of points on the Bloch sphere for state ![|ψ ⟩ C](img/file1202.jpg). We observe
    the highest density around points (![|0⟩](img/file1203.jpg) + ![|1⟩](img/file1204.jpg))*∕*![√--
    2](img/file1205.jpg) and (![|0⟩](img/file1206.jpg)−![|1⟩](img/file1207.jpg))*∕*![√
    -- 2](img/file1208.jpg), which are the points where the equator crosses the 0^∘
    and 180^∘ meridians, and the lowest density along the 90^∘ and 270^∘ meridians.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: PQC C通过旋转R[X]向PQC B添加一个旋转角度*𝜃*[x]，该角度从区间[−*π,π*]上的均匀分布中抽取。通过围绕两个正交轴的两次旋转，我们可以到达Bloch球上的任何点。然而，若角度*𝜃*[z]和*𝜃*[x]从区间[−*π,π*]上的均匀分布中抽取，则在状态![|ψ
    ⟩ C](img/file1202.jpg)下，Bloch球上的点并不呈均匀分布。我们观察到，在点 (![|0⟩](img/file1203.jpg) +
    ![|1⟩](img/file1204.jpg))*∕*![√-- 2](img/file1205.jpg) 和 (![|0⟩](img/file1206.jpg)−![|1⟩](img/file1207.jpg))*∕*![√
    -- 2](img/file1208.jpg) 附近，密度最高，而在90^∘和270^∘经线沿线，密度最低。
- en: Finally, PQC D adds one more rotation R[Y] around the *y*-axis by an angle *𝜃*[y]
    drawn from the Uniform distribution on [−*π,π*]. This rotation results in spreading
    the previously clustered points more evenly around the Bloch sphere, thus making
    all points on the Bloch sphere equally accessible.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，PQC D在*y*轴上通过一个旋转R[Y]添加了一个角度*𝜃*[y]，该角度从区间[−*π,π*]的均匀分布中抽取。这个旋转使得之前聚集的点在Bloch球上更加均匀地分布，从而使Bloch球上的所有点都能被平等地访问。
- en: 'Therefore, in terms of our ability to explore the Hilbert space, we have the
    following hierarchy of the expressive power of the PQCs introduced above:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，关于我们探索希尔伯特空间的能力，我们可以将上述PQC的表达能力层次分为以下几类：
- en: '![PQC D > PQC C > PQC B > PQC A. ](img/file1209.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![PQC D > PQC C > PQC B > PQC A. ](img/file1209.jpg)'
- en: We can now return to the PQCs developed in the previous chapters.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以回到前几章开发的PQC。
- en: 12.2.1 Multilayer PQC
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.1 多层PQC
- en: A multilayer PQC (MPQC) consists of multiple blocks of quantum circuits in which
    the arrangement of quantum gates in each block is identical  [[28](Biblography.xhtml#XBenedetti2018), [189](Biblography.xhtml#XLiuWang2018)].
    Figure [12.2](#12.2) shows a schematic representation of the MPQC.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 多层量子电路（MPQC）由多个量子电路块组成，每个块中的量子门排列是相同的[[28](Biblography.xhtml#XBenedetti2018)，[189](Biblography.xhtml#XLiuWang2018)]。图[12.2](#12.2)展示了MPQC的示意图。
- en: '![Figure 12.2: Schematic representation of a multilayer PQC. ](img/file1210.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图12.2：多层PQC的示意图。](img/file1210.jpg)'
- en: 'Figure 12.2: Schematic representation of a multilayer PQC.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：多层PQC的示意图。
- en: The following mathematical formalism can be used to describe MPQC. The input
    *n*-qubit quantum state with all qubits initialised as ![|0⟩](img/file1211.jpg)
    in the computational basis is ![|0⟩](img/file1212.jpg)^(⊗n), the total number
    of blocks is denoted *l*, and the *i*-th block is denoted U(𝜃^i), where the number
    of parameters is proportional to the number of qubits, and *n* is logarithmically
    proportional to the dimension of the generated data (this reflects our assumption
    about the data encoding scheme). The generated output state of the circuit thus
    reads
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下数学形式可以用来描述MPQC。输入的*n*量子比特量子态，所有量子比特初始化为![|0⟩](img/file1211.jpg)的计算基态，为![|0⟩](img/file1212.jpg)^(⊗n)，总共的电路块数为*l*，第*i*个块表示为U(𝜃^i)，其中参数的数量与量子比特数成正比，*n*与生成数据的维度呈对数关系（这反映了我们关于数据编码方案的假设）。因此，电路生成的输出态为
- en: '| ![ ∏ l i ⊗n &#124;ψ ⟩ = U (𝜃 )&#124;0⟩ . i=1 ](img/file1213.jpg) |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∏ l i ⊗n &#124;ψ ⟩ = U (𝜃 )&#124;0⟩ . i=1 ](img/file1213.jpg) |  |'
- en: 12.2.2 Tensor network PQC
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.2 张量网络PQC
- en: A tensor network PQC (TPQC) treats each block as a local tensor. The arrangement
    of the blocks follows a particular network structure, such as matrix product states
    or tree tensor networks  [[144](Biblography.xhtml#XHuggins2018)]. A schematic
    representation of TPQC is shown in Figure [12.3](#12.3).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 张量网络PQC（TPQC）将每个块视为局部张量。这些块的排列遵循特定的网络结构，如矩阵积态或树形张量网络[[144](Biblography.xhtml#XHuggins2018)]。图[12.3](#12.3)展示了TPQC的示意图。
- en: '![Figure 12.3: Schematic representation of a tensor network PQC. ](img/file1214.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图12.3：张量网络PQC的示意图。](img/file1214.jpg)'
- en: 'Figure 12.3: Schematic representation of a tensor network PQC.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3：张量网络PQC的示意图。
- en: Mathematically, the *i*-th block U(𝜃^i) is composed of *M*[i] local tensor blocks,
    with *M*[i] ∝ *n∕*2^i, denoted as U(𝜃^i) = ⊗ [j=1]^(M[i])U(𝜃[j]^i). Note that
    many of these tensor blocks may be identity operators. The generated state is
    thus of the form
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，第*i*块U(𝜃^i)由*M*[i]个局部张量块组成，其中*M*[i] ∝ *n∕*2^i，表示为U(𝜃^i) = ⊗ [j=1]^(M[i])U(𝜃[j]^i)。请注意，这些张量块中的许多可能是恒等算符。因此，生成的状态呈现以下形式：
- en: '| ![ M ∏l ⊗ i i ⊗n &#124;ψ⟩ = U(𝜃j)&#124;0⟩ . i=1 j=1 ](img/file1215.jpg) |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ![ M ∏l ⊗ i i ⊗n &#124;ψ⟩ = U(𝜃j)&#124;0⟩ . i=1 j=1 ](img/file1215.jpg) |  |'
- en: 12.2.3 Measures of expressive power
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.3 表达能力的度量
- en: The main question to answer is whether MPQC and TPQC have larger expressive
    power in comparison with their classical counterparts, such as classical neural
    networks. The expressive power of a model can be defined in many different ways,
    for example, as a model capacity to express different relationships between variables  [[22](Biblography.xhtml#XBaldi2019)].
    Deep neural networks serve as a good example of powerful models capable of learning
    complex data structures  [[94](Biblography.xhtml#XDziugaite2017)]. Therefore,
    the power of a model can be quantified by its complexity, with the *Vapnik-Chervonenkis*
    *dimension* being a complexity measure of choice  [[293](Biblography.xhtml#XVapnik1971)].
    The objective is to provide an estimate of how well a model generalises to the
    unseen data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 主要问题是回答MPQC和TPQC与其经典对应物（如经典神经网络）相比，是否具有更大的表达能力。模型的表达能力可以通过多种方式定义，例如作为模型表达不同变量之间关系的能力[[22](Biblography.xhtml#XBaldi2019)]。深度神经网络是能够学习复杂数据结构的强大模型的一个很好的例子[[94](Biblography.xhtml#XDziugaite2017)]。因此，模型的能力可以通过其复杂性量化，而*Vapnik-Chervonenkis*
    *维度*是衡量复杂性的选择度量[[293](Biblography.xhtml#XVapnik1971)]。目标是提供一个关于模型如何泛化到未见数据的估计。
- en: Another popular approach is the *Fisher information*, which describes the geometry
    of a model parameter space  [[247](Biblography.xhtml#XRissanen1996)]. Arguably,
    the *effective dimension* based on Fisher information, rather than Vapnik-Chervonenkis
    dimension, is a better measure to study the power of quantum and classical neural
    networks  [[1](Biblography.xhtml#XAbbas2020)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的方法是*费舍尔信息*，它描述了模型参数空间的几何结构[[247](Biblography.xhtml#XRissanen1996)]。可以说，基于费舍尔信息的*有效维度*，而非
    Vapnik-Chervonenkis维度，是研究量子和经典神经网络能力的更好度量[[1](Biblography.xhtml#XAbbas2020)]。
- en: However, one of the most natural metrics of expressive power is *entanglement*
    *entropy*, which allows us to establish a well-defined ranking of quantum and
    classical machine learning models. In this chapter, we will present the expressive
    power estimates obtained in  [[88](Biblography.xhtml#XDu2018)] for TPQC and MPQC
    based on entanglement entropy.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最自然的表达能力度量之一是*纠缠*熵，它使我们能够为量子和经典机器学习模型建立一个明确的排名。在本章中，我们将展示基于纠缠熵在[[88](Biblography.xhtml#XDu2018)]中为TPQC和MPQC获得的表达能力估计。
- en: 'Let us recall the definitions of entropy in statistical mechanics (the Gibbs
    entropy *S*) and in information theory (the Shannon entropy H) introduced in Chapter [6](Chapter_6.xhtml#x1-1190006):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下在统计力学中引入的熵的定义（吉布斯熵*S*）和在信息论中引入的熵的定义（香农熵H），详见第[6](Chapter_6.xhtml#x1-1190006)章：
- en: '| ![ ∑ ∑ S := − kB pilog(pi) and H := − pilog2(pi). i i ](img/file1216.jpg)
    |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑ ∑ S := − kB pilog(pi) 和 H := − pilog2(pi). i i ](img/file1216.jpg) |  |'
- en: Here, *p*[i] is the probability that the microstate *i* is taken from an equilibrium
    ensemble in the case of the Gibbs entropy, and that the message *i* is picked
    from the message space in the case of the Shannon entropy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*p*[i]是吉布斯熵情况下从平衡集中取出微观状态*i*的概率，而在香农熵的情况下，它是从消息空间中选择消息*i*的概率。
- en: 'These definitions of entropy can be extended to the quantum case. In Chapter [1](Chapter_1.xhtml#x1-220001),
    we introduced the density matrix as a universal tool for describing pure and mixed
    quantum states:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些熵的定义可以扩展到量子情况。在第[1章](Chapter_1.xhtml#x1-220001)中，我们介绍了密度矩阵作为描述纯态和混合量子态的通用工具：
- en: '![ N N ∑ ∑ ρ := ρij |i⟩⟨j|, i=1 j=1 ](img/file1217.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![ N N ∑ ∑ ρ := ρij |i⟩⟨j|, i=1 j=1 ](img/file1217.jpg)'
- en: where (![|i⟩](img/file1218.jpg))[i=1,…,N] are the basis vectors of a given quantum
    system. The *von* *Neumann entropy* 𝒮 is then defined as
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 (![|i⟩](img/file1218.jpg))[i=1,…,N] 是给定量子系统的基向量。*冯·诺依曼熵* 𝒮 定义为
- en: '| ![𝒮(ρ) := − Tr(ρlog(ρ)). ](img/file1219.jpg) |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ![𝒮(ρ) := − Tr(ρlog(ρ)). ](img/file1219.jpg) |  |'
- en: Since the density matrix is Hermitian, it is *diagonalisable*, so that there
    exists a basis (![|k⟩](img/file1220.jpg))[k=1,…,N] such that
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于密度矩阵是厄米的，它是*可对角化*的，因此存在一个基 (![|k⟩](img/file1220.jpg))[k=1,…,N]，使得
- en: '![ N N N ρ = ∑ ρ |k⟩ ⟨k| =:∑ p |k⟩⟨k|, where ∑ p = 1\. kk k k k=1 k=1 k=1 ](img/file1221.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![ N N N ρ = ∑ ρ |k⟩ ⟨k| =:∑ p |k⟩⟨k|, where ∑ p = 1\. kk k k k=1 k=1 k=1 ](img/file1221.jpg)'
- en: 'The eigenvalues of the operator *ρ*log(*ρ*) are thus (*p*[k] log(*p*[k]))[k=1,…,N],
    and we obtain the following expression for the von Neumann entropy:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 算符 *ρ*log(*ρ*) 的特征值为 (*p*[k] log(*p*[k]))[k=1,…,N]，因此我们可以得到冯·诺依曼熵的以下表达式：
- en: '| ![ ∑ 𝒮(ρ) = − Tr (ρ log(ρ)) = − pklog(pk). k ](img/file1222.jpg) |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑ 𝒮(ρ) = − Tr (ρ log(ρ)) = − pklog(pk). k ](img/file1222.jpg) |  |'
- en: From ([12.2.3](#x1-2320003)) and ([12.2.3](#x1-2320003)) we see that for the
    orthogonal mixture of quantum states, the quantum and classical entropies coincide.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 从 ([12.2.3](#x1-2320003)) 和 ([12.2.3](#x1-2320003)) 可以看出，对于量子态的正交混合，量子熵和经典熵是相同的。
- en: If the system has two component parts, *A* and *B*, we can define the *reduced*
    *density matrix* as the *partial trace* of the density matrix over the subspace
    of the Hilbert space we are not interested in. Let (![|a⟩ i](img/file1223.jpg))[i=1,...,N]
    be the standard orthonormal basis of the Hilbert space ℍ[A] of system *A*, and
    (![|b ⟩ j](img/file1224.jpg))[j=1,...,M] be the standard orthonormal basis of
    the Hilbert space ℍ[B] of system *B*. The density matrix *ρ*[AB] of the bipartite
    system *AB* on the tensor product Hilbert space ℍ[A] ⊗ℍ[B] can then be represented
    as
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统有两个组成部分，*A* 和 *B*，我们可以将*约化* *密度矩阵* 定义为对不感兴趣的希尔伯特空间子空间上的密度矩阵的*部分迹*。令 (![|a⟩
    i](img/file1223.jpg))[i=1,...,N] 为系统 *A* 的希尔伯特空间 ℍ[A] 的标准正交基，且 (![|b ⟩ j](img/file1224.jpg))[j=1,...,M]
    为系统 *B* 的希尔伯特空间 ℍ[B] 的标准正交基。则双体系统 *AB* 在张量积希尔伯特空间 ℍ[A] ⊗ℍ[B] 上的密度矩阵 *ρ*[AB] 可表示为
- en: '| ![ ∑N ∑M N∑ ∑M ρAB = cijkl &#124;ai⟩⟨ak&#124;⊗ &#124;bj⟩⟨bl&#124;, i=1j=1
    k=1l=1 ](img/file1225.jpg) |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑N ∑M N∑ ∑M ρAB = cijkl &#124;ai⟩⟨ak&#124;⊗ &#124;bj⟩⟨bl&#124;, i=1j=1
    k=1l=1 ](img/file1225.jpg) |  |'
- en: for some coefficients *c*[ijkl]. The partial traces then read
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对某些系数 *c*[ijkl]，部分迹为
- en: '| ![ ∑N M∑ ∑N ∑M TrB(ρAB ) = cijkl &#124;ai⟩⟨ak&#124;⟨bl&#124;bj⟩, i=1 j=1
    k=1 l=1 ](img/file1226.jpg) |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑N M∑ ∑N ∑M TrB(ρAB ) = cijkl &#124;ai⟩⟨ak&#124;⟨bl&#124;bj⟩, i=1 j=1
    k=1 l=1 ](img/file1226.jpg) |  |'
- en: which is a reduced density matrix *ρ*[A] on ℍ[A], and
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种在 ℍ[A] 上的约化密度矩阵 *ρ*[A]，并且
- en: '| ![ N M N M Tr (ρ ) = ∑ ∑ ∑ ∑ c &#124;b ⟩⟨b &#124;⟨a &#124;a ⟩, A AB i=1 j=1
    ijkl j l k i k=1 l=1 ](img/file1227.jpg) |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ![ N M N M Tr (ρ ) = ∑ ∑ ∑ ∑ c &#124;b ⟩⟨b &#124;⟨a &#124;a ⟩, A AB i=1 j=1
    ijkl j l k i k=1 l=1 ](img/file1227.jpg) |  |'
- en: which is a reduced density matrix *ρ*[B] on ℍ[B]. Note that Tr(![|ai⟩](img/file1228.jpg)⟨*a*[k]|)
    = ![⟨ak|ai⟩](img/file1229.jpg) and Tr(![|b ⟩ j](img/file1230.jpg)⟨*b*[l]|) = ![⟨b|b
    ⟩ l j](img/file1231.jpg).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种在 ℍ[B] 上的约化密度矩阵 *ρ*[B]。请注意，Tr(![|ai⟩](img/file1228.jpg)⟨*a*[k]|) = ![⟨ak|ai⟩](img/file1229.jpg)
    和 Tr(![|b ⟩ j](img/file1230.jpg)⟨*b*[l]|) = ![⟨b|b ⟩ l j](img/file1231.jpg)。
- en: '**Example:** Consider the two-qubit system in the state'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 考虑处于状态中的两比特系统'
- en: '![|ψ ⟩ = 1√--(|01⟩ + |10 ⟩), 2 ](img/file1232.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![|ψ ⟩ = 1√--(|01⟩ + |10 ⟩), 2 ](img/file1232.jpg)'
- en: 'which is one of the four maximally entangled Bell states (Section [6.5.2](Chapter_6.xhtml#x1-1360002)).
    We assume that the first qubit is system *A* and the second qubit is system *B*.
    This state corresponds to the following density matrix:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是四种最大纠缠贝尔态之一（见第[6.5.2节](Chapter_6.xhtml#x1-1360002)）。我们假设第一个量子比特是系统 *A*，第二个量子比特是系统
    *B*。该状态对应以下密度矩阵：
- en: '![ ( ) ρ := |ψ ⟩⟨ψ| = 1- |01⟩⟨01|+ |01⟩⟨10|+ |10⟩⟨01|+ |10⟩⟨10| . AB 2 ](img/file1233.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![ ( ) ρ := |ψ ⟩⟨ψ| = 1- |01⟩⟨01|+ |01⟩⟨10|+ |10⟩⟨01|+ |10⟩⟨10| . AB 2 ](img/file1233.jpg)'
- en: 'Let us now act on this state with the partial trace Tr[B](⋅):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对这个状态施加部分迹 Tr[B](⋅)：
- en: '| ![ 1 ( ) ρA := TrB(ρAB ) =-- &#124;0⟩⟨0&#124;⟨1&#124;1 ⟩+ &#124;0⟩⟨1&#124;⟨0&#124;1⟩+
    &#124;1⟩ ⟨0&#124;⟨1&#124;0⟩+ &#124;1⟩⟨1&#124;⟨0&#124;0⟩ 2 ⌊ ⌋ 1 ( ) 1 1 0 = 2-
    &#124;0⟩⟨0&#124;+ &#124;1⟩⟨1&#124; = 2-⌈ ⌉ . 0 1 ](img/file1234.jpg) |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ![ 1 ( ) ρA := TrB(ρAB ) =-- &#124;0⟩⟨0&#124;⟨1&#124;1 ⟩+ &#124;0⟩⟨1&#124;⟨0&#124;1⟩+
    &#124;1⟩ ⟨0&#124;⟨1&#124;0⟩+ &#124;1⟩⟨1&#124;⟨0&#124;0⟩ 2 ⌊ ⌋ 1 ( ) 1 1 0 = 2-
    &#124;0⟩⟨0&#124;+ &#124;1⟩⟨1&#124; = 2-⌈ ⌉ . 0 1 ](img/file1234.jpg) |  |'
- en: The reduced density matrix *ρ*[A] in ([12.2.3](#x1-2320003)) is the same as
    the density matrix *ρ* in ([1.3.3](Chapter_1.xhtml#x1-44003r3)), which describes
    a statistical ensemble of states ![|0⟩](img/file1235.jpg) and ![|1⟩](img/file1236.jpg)
    (mixed state), i.e., a physical system prepared to be either in state ![|0⟩](img/file1237.jpg)
    or state ![|1⟩](img/file1238.jpg) with equal probability.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在([12.2.3](#x1-2320003))中的约化密度矩阵*ρ*[A]与([1.3.3](Chapter_1.xhtml#x1-44003r3))中的密度矩阵*ρ*相同，后者描述了一个统计集合，其中包括状态![|0⟩](img/file1235.jpg)和![|1⟩](img/file1236.jpg)（混合态），即，物理系统以相等概率准备处于状态![|0⟩](img/file1237.jpg)或状态![|1⟩](img/file1238.jpg)。
- en: The *entanglement entropy* of a bipartite system *AB* is then defined as
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 双体系统*AB*的*纠缠熵*定义为
- en: '| ![𝒮(ρA) := − Tr(ρA log(ρA )) = − Tr(ρB log(ρB)) =: 𝒮 (ρB), ](img/file1239.jpg)
    |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ![𝒮(ρA) := − Tr(ρA log(ρA )) = − Tr(ρB log(ρB)) =: 𝒮 (ρB), ](img/file1239.jpg)
    |  |'
- en: and can be used as a measure of expressive power of a model in the following
    way. First, note that TPQC, MPQC and classical neural networks have a close connection
    with *tensor networks*, such as matrix product states (MPS)  [[88](Biblography.xhtml#XDu2018)].
    The key question is then whether the given quantum system can be efficiently represented
    by the MPS.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 并且可以作为衡量模型表现力的度量，方法如下。首先，注意到TPQC、MPQC和经典神经网络与*张量网络*（如矩阵积态MPS）有着密切的联系[[88](Biblography.xhtml#XDu2018)]。关键问题是，给定的量子系统是否可以通过MPS高效表示。
- en: A quantum system that satisfies the *area law* (its entanglement entropy grows
    proportionally with the boundary area) has an efficient MPS representation. At
    the same time, a quantum system that satisfies the *volume law* (its entanglement
    entropy grows proportionally with the volume) cannot be efficiently represented
    by MPS  [[88](Biblography.xhtml#XDu2018)].
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 满足*面积定律*（其纠缠熵与边界面积成比例增长）的量子系统具有高效的矩阵积态（MPS）表示。同时，满足*体积定律*（其纠缠熵与体积成比例增长）的量子系统不能通过MPS有效表示[[88](Biblography.xhtml#XDu2018)]。
- en: 12.2.4 Expressive power of PQC
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.4 PQC的表现力
- en: 'In Chapter [5](Chapter_5.xhtml#x1-960005), we introduced the Restricted Boltzmann
    Machine (RBM) – a neural network operating on stochastic binary activation units,
    which is a natural classical counterpart of parameterised quantum circuits. We
    considered two types of RBM:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[5](Chapter_5.xhtml#x1-960005)章中，我们介绍了受限玻尔兹曼机（RBM）——一种操作在随机二进制激活单元上的神经网络，是参数化量子电路的自然经典对应物。我们考虑了两种类型的RBM：
- en: a shallow two-layer network where the activation units in the visible layer
    are connected to the activation units in the hidden layer with no connections
    between the activation units within the same layer;
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个浅层的两层网络，其中可见层的激活单元与隐藏层的激活单元相连，但同一层内的激活单元之间没有连接；
- en: a deeper multi-layer network of stacked RBMs where the hidden layer of the *k*-th
    RBM serves as the visible layer of the (*k* + 1)-th RBM. Such stacked RBMs (trained
    sequentially) are called Deep Boltzmann Machines (DBMs).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更深层次的多层堆叠式受限玻尔兹曼机（RBM）网络，其中第*k*层RBM的隐藏层作为第(*k* + 1)层RBM的可见层。这种堆叠式RBM（按顺序训练）称为深度玻尔兹曼机（DBM）。
- en: It is also possible to impose further restrictions on the connections between
    the RBM layers. In *short-range* RBMs, we restrict the connectivity of the hidden
    layer activation units such that they are allowed to connect to the limited number
    of activation units in the visible layer that are in close proximity to each other
    (local connectivity)  [[84](Biblography.xhtml#XDeng2017)]. In *long-range* RBMs,
    we allow connections between the hidden layer activation units and the visible
    layer activation units that are not necessarily local.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以对RBM层之间的连接施加进一步的限制。在*短程*RBM中，我们限制隐藏层激活单元的连接性，使它们只能连接到可见层中相互接近的少数激活单元（局部连接）[[84](Biblography.xhtml#XDeng2017)]。在*长程*RBM中，我们允许隐藏层激活单元与可见层激活单元之间的连接不一定是局部的。
- en: It has been established by Deng, Li, and Sarma  [[85](Biblography.xhtml#XDeng2017X)]
    that the entanglement entropy of all short-range RBM states satisfies an area
    law for arbitrary dimensions and bipartition geometry. For long-range RBM states,
    such states could exhibit volume law entanglement. Therefore long-range RBMs are
    capable of representing quantum states with large entanglement.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 邓、李和萨尔马已经确定了所有短程RBM态的纠缠熵满足任意维度和二分几何的面积定律[[85](Biblography.xhtml#XDeng2017X)]。对于长程RBM态，这些态可能表现出体积定律的纠缠。因此，长程RBM能够表示具有大纠缠的量子态。
- en: 'It is probably not surprising that a DBM would have even larger expressive
    power than a single RBM. However, using entanglement entropy as a measure of expressive
    power, Du, Hsieh, Liu, and Tao have proven in  [[88](Biblography.xhtml#XDu2018)]
    that MPQC have strictly larger expressive power than DBM. The main result can
    be formulated as the following theorem:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 可能并不令人惊讶的是，DBM的表达能力甚至比单一的RBM更强。然而，利用纠缠熵作为表达能力的度量，Du、Hsieh、Liu和Tao在[[88](Biblography.xhtml#XDu2018)]中证明了MPQC的表达能力严格大于DBM。主要结果可以表述为以下定理：
- en: '**Theorem 10** (Expressive Power Theorem)**.** *The expressive power of MPQC
    and* *TPQC with* 𝒪(*poly*(*n*)) *single qubit gates and* CNOT *gates, and classical
    neural* *networks with* 𝒪(*poly*(*n*)) *trainable parameters, where* *n* *refers
    to the number of* *qubits or visible units, can be ordered as*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理10**（表达能力定理）**。** *MPQC和* *TPQC的表达能力，使用* 𝒪(*poly*(*n*)) *个单量子比特门和* CNOT
    *门，以及具有* 𝒪(*poly*(*n*)) *可训练参数的经典神经网络，其中* *n* *表示量子比特或可见单元的数量，可以排序为*'
- en: '| ![MPQC > DBM > long-range RBM > TPQC > short-range RBM. ](img/file1240.jpg)
    |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ![MPQC > DBM > long-range RBM > TPQC > short-range RBM.](img/file1240.jpg)
    |  |'
- en: Theorem [10](#x1-233002r10) provides a solid theoretical foundation for experimental
    works aimed at establishing quantum advantage of PQC-based QML models. The larger
    expressive power of PQCs in comparison with their classical counterparts prompted
    the development of many such models in recent years. For example, a hybrid quantum-classical
    approach, suitable for NISQ devices and harnessing the greater expressive power
    of quantum entanglement, was proposed in  [[59](Biblography.xhtml#XChen2020S)].
    It was shown through numerical simulations that the Quantum Long Short Term Memory
    (QLSTM) model learns faster than the equivalent classical LSTM with a similar
    number of network parameters. In addition, the convergence of QLSTM was shown
    to be more stable than that of its classical counterpart. A Quantum Convolutional
    Neural Network (QCNN) was proposed in  [[58](Biblography.xhtml#XChen2020D)] which,
    due to its larger expressive power, achieved greater test accuracy compared to
    classical CNNs. The source of expressive power was the replacement of the classical
    convolutional filters with quantum convolutional kernels based on variational
    quantum circuits.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 定理[10](#x1-233002r10)为旨在确立基于PQC的QML模型量子优势的实验工作提供了坚实的理论基础。与经典模型相比，PQC更强的表达能力促使了近年来许多此类模型的发展。例如，针对NISQ设备并利用量子纠缠更强表达能力的混合量子-经典方法，已在[[59](Biblography.xhtml#XChen2020S)]中提出。通过数值模拟表明，量子长短期记忆（QLSTM）模型比具有相似网络参数的经典LSTM学习得更快。此外，QLSTM的收敛性比其经典对手更加稳定。在[[58](Biblography.xhtml#XChen2020D)]中提出了量子卷积神经网络（QCNN），由于其更强的表达能力，达到了比经典CNN更高的测试精度。其表达能力的来源是用基于变分量子电路的量子卷积核替代了经典的卷积滤波器。
- en: Multi-layer parameterised quantum circuits such as QCBM have strictly more expressive
    power than classical models such as RBM when only a polynomial number of parameters
    is allowed. For systems that exhibit quantum supremacy, a classical model cannot
    learn to reproduce the statistics unless it uses exponentially scaling resources  [[29](Biblography.xhtml#XBenedetti2019)].
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 多层参数化量子电路，如QCBM，在只允许多项式数量的参数时，比经典模型如RBM具有更强的表达能力。对于表现出量子优势的系统，经典模型无法学习重现统计数据，除非它使用指数级的资源[[29](Biblography.xhtml#XBenedetti2019)]。
- en: Summary
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned where the power of parameterised quantum circuits
    comes from. We started with the observation that quantum neural networks enjoy
    strong regularisation inherent in their architecture. This is due to the fact
    that any PQC, however wide and deep, is a unitary linear operator.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了参数化量子电路的力量来自何处。我们从观察到量子神经网络在其架构中具有强大的正则化开始。这是因为任何PQC，无论多么宽广和深度，都是一个单位线性算符。
- en: Next, we considered the expressive power of parameterised quantum circuits and
    established the concept of the expressive power hierarchy. The main result (Theorem [10](#x1-233002r10))
    supports the experimental findings indicating the presence of the elements of
    quantum advantage in various QML models compatible with the main characteristics
    of NISQ devices.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们考虑了参数化量子电路的表达能力，并建立了表达能力层级的概念。主要结果（定理[10](#x1-233002r10)）支持了实验结果，这些实验表明在与NISQ设备主要特性兼容的各种QML模型中存在量子优势的元素。
- en: In the next chapter, we will go deeper into the less explored territory of new
    quantum algorithms, an area of very active research.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨新量子算法这一较少研究的领域，这是一个非常活跃的研究方向。
- en: Join our book’s Discord space
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/quantum](https://packt.link/quantum)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，与志同道合的人一起学习，和超过2000名成员共同探讨：[https://packt.link/quantum](https://packt.link/quantum)
- en: '![PIC](img/file1.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1.png)'
