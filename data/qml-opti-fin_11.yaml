- en: '12'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '12'
- en: The Power of Parameterised Quantum Circuits
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°åŒ–é‡å­ç”µè·¯çš„åŠ›é‡
- en: 'As we have seen in the previous chapters, there is a wide range of QML models
    based on parameterised quantum circuits. One reason for this is their tolerance
    to noiseÂ Â [[222](Biblography.xhtml#XNguyen2020)], which is important when we work
    with the NISQ hardware. However, this does not fully explain the popularity of
    PQCs or why they are considered strong competitors to classical ML models. There
    must be some fundamental properties of PQCs that make them superior to their classical
    counterparts. In this chapter, we discuss two such properties: resistance to overfitting
    and larger expressive power.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨å‰å‡ ç« ä¸­çœ‹åˆ°çš„ï¼ŒåŸºäºå‚æ•°åŒ–é‡å­ç”µè·¯çš„é‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰æ¨¡å‹ç§ç±»ç¹å¤šã€‚å…¶åŸå› ä¹‹ä¸€æ˜¯å®ƒä»¬å¯¹å™ªå£°çš„è€å—æ€§[[222](Biblography.xhtml#XNguyen2020)]ï¼Œè¿™åœ¨æˆ‘ä»¬ä½¿ç”¨NISQç¡¬ä»¶æ—¶å°¤ä¸ºé‡è¦ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸èƒ½å®Œå…¨è§£é‡ŠPQCçš„æµè¡Œï¼Œæˆ–å®ƒä»¬ä¸ºä½•è¢«è®¤ä¸ºæ˜¯ç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¼ºåŠ²ç«äº‰è€…ã€‚PQCå¿…å®šæœ‰ä¸€äº›åŸºæœ¬ç‰¹æ€§ï¼Œä½¿å…¶ä¼˜äºç»å…¸å¯¹ç­‰æ¨¡å‹ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†ä¸¤ç§è¿™æ ·çš„ç‰¹æ€§ï¼šæŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›å’Œæ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚
- en: Resistance to overfitting is a direct consequence of the fact that a typical
    PQC â€“ one without mid-circuit measurement â€“ can be represented by a linear unitary
    operator. Linear models impose strong regularisation, thus preventing overfitting.
    At the same time, the model remains powerful due to the mapping of the input into
    the higher-dimensional Hilbert space where it may be easier to perform classification
    if the PQC is trained as a discriminative model (QNN).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›æ˜¯ä¸€ä¸ªç›´æ¥åæœï¼Œå› ä¸ºå…¸å‹çš„PQCâ€”â€”æ²¡æœ‰ä¸­é€”æµ‹é‡çš„PQCâ€”â€”å¯ä»¥é€šè¿‡çº¿æ€§å•ä½aryç®—ç¬¦æ¥è¡¨ç¤ºã€‚çº¿æ€§æ¨¡å‹å¼ºçƒˆæ–½åŠ æ­£åˆ™åŒ–ï¼Œä»è€Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚åŒæ—¶ï¼Œç”±äºè¾“å…¥è¢«æ˜ å°„åˆ°æ›´é«˜ç»´çš„å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼Œæ¨¡å‹ä»ç„¶ä¿æŒå¼ºå¤§ï¼Œå¦‚æœPQCä½œä¸ºåˆ¤åˆ«æ¨¡å‹ï¼ˆQNNï¼‰è¿›è¡Œè®­ç»ƒï¼Œåˆ™åœ¨è¯¥ç©ºé—´ä¸­å¯èƒ½æ›´å®¹æ˜“æ‰§è¡Œåˆ†ç±»ã€‚
- en: Expressive power is related to the modelâ€™s ability to express different relationships
    between variables, i.e., its ability to learn complex data structures. It appears
    that PQCs trained as generative models (QCBM) have strictly larger expressive
    power than their equivalent classical versions (such as RBM).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨è¾¾èƒ½åŠ›ä¸æ¨¡å‹è¡¨è¾¾å˜é‡ä¹‹é—´ä¸åŒå…³ç³»çš„èƒ½åŠ›ç›¸å…³ï¼Œå³å…¶å­¦ä¹ å¤æ‚æ•°æ®ç»“æ„çš„èƒ½åŠ›ã€‚çœ‹æ¥ï¼Œä½œä¸ºç”Ÿæˆæ¨¡å‹ï¼ˆQCBMï¼‰è®­ç»ƒçš„å‚æ•°åŒ–é‡å­ç”µè·¯ï¼ˆPQCï¼‰ç›¸æ¯”äºå…¶å¯¹åº”çš„ç»å…¸ç‰ˆæœ¬ï¼ˆå¦‚RBMï¼‰ï¼Œå…·æœ‰ä¸¥æ ¼æ›´å¤§çš„è¡¨è¾¾èƒ½åŠ›ã€‚
- en: 12.1 Strong Regularisation
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1 å¼ºæ­£åˆ™åŒ–
- en: 'Parameterised quantum circuits trained as classifiers face the same challenge
    as classical models: the need to generalise well to unseen data points. Classically,
    we have a wide range of supervised learning models and regularisation techniques
    to choose from. These regularisation techniques that fight overfitting are model
    specific. For example, we can try to restrict the depth of the decision trees
    or to impose a penalty term in the cost function when training neural networks.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºåˆ†ç±»å™¨è®­ç»ƒçš„å‚æ•°åŒ–é‡å­ç”µè·¯é¢ä¸´ç€ä¸ç»å…¸æ¨¡å‹ç›¸åŒçš„æŒ‘æˆ˜ï¼šéœ€è¦è‰¯å¥½åœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„æ•°æ®ç‚¹ã€‚ç»å…¸ä¸Šï¼Œæˆ‘ä»¬æœ‰è®¸å¤šç›‘ç£å­¦ä¹ æ¨¡å‹å’Œæ­£åˆ™åŒ–æŠ€æœ¯å¯ä¾›é€‰æ‹©ã€‚è¿™äº›åº”å¯¹è¿‡æ‹Ÿåˆçš„æ­£åˆ™åŒ–æŠ€æœ¯æ˜¯æ¨¡å‹ç‰¹å®šçš„ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•é™åˆ¶å†³ç­–æ ‘çš„æ·±åº¦ï¼Œæˆ–åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶åœ¨ä»£ä»·å‡½æ•°ä¸­æ–½åŠ æƒ©ç½šé¡¹ã€‚
- en: Consider a conventional feedforward neural network as, arguably, the most direct
    classical counterpart of a quantum classifier. In both classical and quantum cases,
    the signal travels through the network in one direction and the layers of quantum
    gates can be compared to the layers of classical activation units. Regardless
    of whether we applyÂ *L*[1] (Lasso) orÂ *L*[2] (Ridge) penalty terms, or use dropout
    techniques, we would like to have a measure of regularisation present in the network.
    This is an interesting theoretical problem as well as an important practical task
    that allows us to develop an optimal strategy for fighting overfitting. Ideally,
    such a measure should be applicable to both classical and quantum neural networks
    to provide a meaningful comparison of their respective regularisation properties.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸€ä¸ªä¼ ç»Ÿçš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå¯ä»¥è¯´å®ƒæ˜¯é‡å­åˆ†ç±»å™¨æœ€ç›´æ¥çš„ç»å…¸å¯¹ç­‰ç‰©ã€‚åœ¨ç»å…¸å’Œé‡å­ä¸¤ç§æƒ…å†µä¸‹ï¼Œä¿¡å·éƒ½é€šè¿‡ç½‘ç»œå•å‘ä¼ é€’ï¼Œé‡å­é—¨å±‚å¯ä»¥ä¸ç»å…¸æ¿€æ´»å•å…ƒçš„å±‚è¿›è¡Œæ¯”è¾ƒã€‚æ— è®ºæˆ‘ä»¬æ˜¯åº”ç”¨*L*[1]ï¼ˆLassoï¼‰æˆ–*L*[2]ï¼ˆRidgeï¼‰æƒ©ç½šé¡¹ï¼Œè¿˜æ˜¯ä½¿ç”¨dropoutæŠ€æœ¯ï¼Œæˆ‘ä»¬éƒ½å¸Œæœ›ç½‘ç»œä¸­å­˜åœ¨æŸç§æ­£åˆ™åŒ–çš„åº¦é‡ã€‚è¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„ç†è®ºé—®é¢˜ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„å®è·µä»»åŠ¡ï¼Œå®ƒè®©æˆ‘ä»¬èƒ½å¤Ÿåˆ¶å®šåº”å¯¹è¿‡æ‹Ÿåˆçš„æœ€ä½³ç­–ç•¥ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œè¿™ç§åº¦é‡åº”é€‚ç”¨äºç»å…¸å’Œé‡å­ç¥ç»ç½‘ç»œï¼Œä»¥æä¾›å¯¹æ¯”å®ƒä»¬å„è‡ªæ­£åˆ™åŒ–ç‰¹æ€§çš„æœ‰æ„ä¹‰æ¯”è¾ƒã€‚
- en: Very often, relatively small network weights are associated with a high degree
    of regularisation and relatively high network weights are symptoms of overfitting.
    However, it would be highly desirable to have a formal mathematical tool for quantifying
    the network capacity to overfit. One such possible well-defined measure that captures
    the degree of regularisation is the Lipschitz constant.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œç›¸å¯¹è¾ƒå°çš„ç½‘ç»œæƒé‡ä¸è¾ƒé«˜ç¨‹åº¦çš„æ­£åˆ™åŒ–ç›¸å…³è”ï¼Œè€Œè¾ƒé«˜çš„ç½‘ç»œæƒé‡åˆ™æ˜¯è¿‡æ‹Ÿåˆçš„ç—‡çŠ¶ã€‚ç„¶è€Œï¼Œæ‹¥æœ‰ä¸€ä¸ªæ­£å¼çš„æ•°å­¦å·¥å…·æ¥é‡åŒ–ç½‘ç»œçš„è¿‡æ‹Ÿåˆèƒ½åŠ›æ˜¯éå¸¸ç†æƒ³çš„ã€‚Lipschitzå¸¸æ•°å°±æ˜¯ä¸€ç§å¯èƒ½çš„ã€èƒ½å¤Ÿæ•æ‰æ­£åˆ™åŒ–ç¨‹åº¦çš„æ˜ç¡®é‡åº¦ã€‚
- en: 12.1.1 Lipschitz constant
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.1 Lipschitzå¸¸æ•°
- en: 'Following GoukÂ Â [[115](Biblography.xhtml#XGouk2018)], given two metric spaces
    (ğ’³*,d*[ğ’³]) and (*,d*[) a function *f* : ğ’³ â†’, is said to be Lipschitz continuous
    if there exists a constant *k* â‰¥ 0 such that]'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ ¹æ®Gouk [[115](Biblography.xhtml#XGouk2018)]ï¼Œç»™å®šä¸¤ä¸ªåº¦é‡ç©ºé—´ (ğ’³*,d*[ğ’³]) å’Œ (*,d*[)ï¼Œå¦‚æœå­˜åœ¨ä¸€ä¸ªå¸¸æ•°*k*
    â‰¥ 0ï¼Œä½¿å¾—ä¸€ä¸ªå‡½æ•°*f* : ğ’³ â†’ è¢«ç§°ä¸ºLipschitzè¿ç»­çš„ï¼Œåˆ™æ»¡è¶³ï¼š'
- en: '| ![d (f (x1),f(x2)) â‰¤ kdğ’³(x1,x2), for all x1,x2 âˆˆ ğ’³ . ](img/file1184.jpg)
    |  |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| ![d (f (x1),f(x2)) â‰¤ kdğ’³(x1,x2), for all x1,x2 âˆˆ ğ’³ . ](img/file1184.jpg)
    |  |'
- en: The value ofÂ *k* is known as the Lipschitz constant, and the function is referred
    to as *k*-Lipschitz. We are interested in the smallest possible Lipschitz constant
    or, at least, its upper bound. To obtain the upper bound estimate, we should note
    some useful properties of feedforward neural networks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*çš„å€¼è¢«ç§°ä¸ºLipschitzå¸¸æ•°ï¼Œä¸”è¯¥å‡½æ•°è¢«ç§°ä¸º*k*-Lipschitzå‡½æ•°ã€‚æˆ‘ä»¬å…³æ³¨çš„æ˜¯æœ€å°çš„Lipschitzå¸¸æ•°ï¼Œæˆ–è€…è‡³å°‘æ˜¯å…¶ä¸Šç•Œã€‚ä¸ºäº†è·å¾—ä¸Šç•Œä¼°è®¡ï¼Œæˆ‘ä»¬åº”å½“æ³¨æ„å‰é¦ˆç¥ç»ç½‘ç»œçš„ä¸€äº›æœ‰ç”¨æ€§è´¨ã€‚'
- en: In the case of a *j*-th layer of a feedforward neural network, x[1] and x[2]
    are the *n*-dimensional sample outputs of the previous layer, *j* âˆ’ 1, and *f*(x[1])
    and *f*(x[2]) are the *m*-dimensional outputs of layerÂ *j*. The metricsÂ *d*[ğ’³]
    andÂ *d* [can be, for example,Â *L*[1] orÂ *L*[2] norms.]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰é¦ˆç¥ç»ç½‘ç»œçš„*j*å±‚ä¸­ï¼Œx[1]å’Œx[2]æ˜¯å‰ä¸€å±‚*j* âˆ’ 1çš„*n*ç»´æ ·æœ¬è¾“å‡ºï¼Œ*f*(x[1])å’Œ*f*(x[2])æ˜¯ç¬¬*j*å±‚çš„*m*ç»´è¾“å‡ºã€‚åº¦é‡*d*[ğ’³]å’Œ*d*å¯ä»¥æ˜¯*L*[1]æˆ–*L*[2]èŒƒæ•°ã€‚
- en: 'A feedforward neural network consisting ofÂ *l* fully connected layers can be
    expressed as a series of function compositions:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±*l*ä¸ªå®Œå…¨è¿æ¥å±‚ç»„æˆçš„å‰é¦ˆç¥ç»ç½‘ç»œå¯ä»¥è¡¨ç¤ºä¸ºä¸€ç³»åˆ—å‡½æ•°ç»„åˆï¼š
- en: '| ![f(x) = (Ï•l âˆ˜ Ï•lâˆ’ 1 âˆ˜...âˆ˜ Ï•1)(x), ](img/file1185.jpg) |  |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| ![f(x) = (Ï•l âˆ˜ Ï•lâˆ’ 1 âˆ˜...âˆ˜ Ï•1)(x), ](img/file1185.jpg) |  |'
- en: 'where each *Ï•*[j] implements the *j*-th layer affine transformation of the
    *n*-dimensional inputÂ x, parameterised by an *m* Ã— *n* weight matrixÂ W[j] and
    an *m*-dimensional bias vectorÂ b[j]:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œæ¯ä¸ª*Ï•*[j]å®ç°äº†*x*çš„ç¬¬*j*å±‚ä»¿å°„å˜æ¢ï¼Œè¯¥å˜æ¢ç”±*m* Ã— *n*çš„æƒé‡çŸ©é˜µW[j]å’Œ*m*ç»´çš„åç½®å‘é‡b[j]æ¥å‚æ•°åŒ–ï¼š
- en: '| ![Ï•j(x) = Wjx + bj. ](img/file1186.jpg) |  |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| ![Ï•j(x) = Wjx + bj. ](img/file1186.jpg) |  |'
- en: The composition of a *k*[1]-Lipschitz function with a *k*[2]-Lipschitz function
    is a *k*[1]*k*[2]-Lipschitz functionÂ Â [[115](Biblography.xhtml#XGouk2018)]. Therefore,
    we can compute the Lipschitz constants for each layer separately and combine them
    together to obtain an upper bound on the Lipschitz constant for the entire network.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª*k*[1]-Lipschitzå‡½æ•°ä¸ä¸€ä¸ª*k*[2]-Lipschitzå‡½æ•°çš„ç»„åˆæ˜¯ä¸€ä¸ª*k*[1]*k*[2]-Lipschitzå‡½æ•° [[115](Biblography.xhtml#XGouk2018)]ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†åˆ«è®¡ç®—æ¯ä¸€å±‚çš„Lipschitzå¸¸æ•°ï¼Œå¹¶å°†å®ƒä»¬ç»„åˆåœ¨ä¸€èµ·ï¼Œä»è€Œè·å¾—æ•´ä¸ªç½‘ç»œLipschitzå¸¸æ•°çš„ä¸Šç•Œã€‚
- en: ChooseÂ *d*[ğ’³] andÂ *d* [to be theÂ *L*[2] normsÂ âˆ¥â‹…âˆ¥[2]. In this case, we obtain
    the following relationship from the definition of Lipschitz continuity for the
    fully connected network layerÂ *j*:]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©*d*[ğ’³]å’Œ*d*ä¸º*L*[2]èŒƒæ•°âˆ¥â‹…âˆ¥[2]ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä»Lipschitzè¿ç»­æ€§çš„å®šä¹‰ä¸­å¾—åˆ°ä»¥ä¸‹å…³ç³»ï¼Œå¯¹äºå®Œå…¨è¿æ¥ç½‘ç»œå±‚*j*ï¼š
- en: '| ![âˆ¥(Wjx1 + bj)âˆ’ (Wjx2 + bj)âˆ¥2 â‰¤ kâˆ¥x1 âˆ’ x2âˆ¥2\. ](img/file1187.jpg) |  |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| ![âˆ¥(Wjx1 + bj)âˆ’ (Wjx2 + bj)âˆ¥2 â‰¤ kâˆ¥x1 âˆ’ x2âˆ¥2\. ](img/file1187.jpg) |  |'
- en: Introducing a = x[1] âˆ’ x[2] and assuming that x[1]*â‰ *x[2] we arrive at the estimate
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¼•å…¥a = x[1] âˆ’ x[2]å¹¶å‡è®¾x[1] *â‰ * x[2]ï¼Œæˆ‘ä»¬å¾—åˆ°ä¼°è®¡
- en: '| ![âˆ¥Wja-âˆ¥2-â‰¤ k. âˆ¥aâˆ¥2 ](img/file1188.jpg) |  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| ![âˆ¥Wja-âˆ¥2-â‰¤ k. âˆ¥aâˆ¥2 ](img/file1188.jpg) |  |'
- en: 'The smallest Lipschitz constant of the fully connected network layer, *L*(*Ï•*[j]),
    is equal to the supremum of the left-hand side of inequalityÂ ([12.1.1](#x1-2270001)):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'å®Œå…¨è¿æ¥ç½‘ç»œå±‚çš„æœ€å°Lipschitzå¸¸æ•°ï¼Œ*L*(*Ï•*[j])ï¼Œç­‰äºä¸ç­‰å¼å·¦ä¾§çš„ä¸Šç¡®ç•Œ ([12.1.1](#x1-2270001)):'
- en: '| ![ âˆ¥W aâˆ¥ L (Ï•j) := sup ---j--2-. aâ„=0 âˆ¥aâˆ¥2 ](img/file1189.jpg) |  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ¥W aâˆ¥ L (Ï•j) := sup ---j--2-. aâ„=0 âˆ¥aâˆ¥2 ](img/file1189.jpg) |  |'
- en: The operator normÂ ([12.1.1](#x1-2270001)) is given by the largest singular value
    of the weight matrixÂ W[j], which corresponds to the spectral norm â€“ the maximum
    scale by which the matrix can stretch a vector. It is straightforward to calculate
    using any of the suitable open-source packages, for example sklearn.decomposition.TruncatedSVD
    from the `scikit-learn` package.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ“ä½œç¬¦èŒƒæ•°([12.1.1](#x1-2270001))ç”±æƒé‡çŸ©é˜µW[j]çš„æœ€å¤§å¥‡å¼‚å€¼ç»™å‡ºï¼Œå®ƒå¯¹åº”äºè°±èŒƒæ•°â€”â€”çŸ©é˜µæ‹‰ä¼¸å‘é‡çš„æœ€å¤§æ¯”ä¾‹ã€‚å¯ä»¥é€šè¿‡ä»»ä½•é€‚ç”¨çš„å¼€æºåŒ…è½»æ¾è®¡ç®—ï¼Œä¾‹å¦‚æ¥è‡ª`scikit-learn`åŒ…çš„sklearn.decomposition.TruncatedSVDã€‚
- en: In the case of quantum neural networks, any parameterised quantum circuit operating
    onÂ *n* qubits, regardless how complex and deep, can be represented by a 2^n Ã—
    2^n unitary matrix. Since all singular values of the unitary matrix are equal
    to one, this gives us a natural benchmark for comparison of regularisation capabilities
    of various networks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é‡å­ç¥ç»ç½‘ç»œçš„æƒ…å†µä¸‹ï¼Œä»»ä½•åœ¨*n*ä¸ªé‡å­æ¯”ç‰¹ä¸Šæ“ä½œçš„å‚æ•°åŒ–é‡å­ç”µè·¯ï¼Œæ— è®ºå…¶å¤šä¹ˆå¤æ‚å’Œæ·±åº¦ï¼Œéƒ½å¯ä»¥é€šè¿‡ä¸€ä¸ª2^n Ã— 2^nçš„å¹ºæ­£çŸ©é˜µè¡¨ç¤ºã€‚ç”±äºå¹ºæ­£çŸ©é˜µçš„æ‰€æœ‰å¥‡å¼‚å€¼éƒ½ç­‰äº1ï¼Œè¿™ä¸ºæ¯”è¾ƒä¸åŒç½‘ç»œçš„æ­£åˆ™åŒ–èƒ½åŠ›æä¾›äº†ä¸€ä¸ªè‡ªç„¶çš„åŸºå‡†ã€‚
- en: 12.1.2 Regularisation example
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.2 æ­£åˆ™åŒ–ç¤ºä¾‹
- en: The Australian Credit Approval (ACA) datasetÂ Â [[241](Biblography.xhtml#XUCI_ACA),Â [242](Biblography.xhtml#XQuinlan1987)]
    we analysed in ChapterÂ [8](Chapter_8.xhtml#x1-1620008) can serve as a good illustrative
    example. We can compare the performance of classical and quantum neural networks
    while monitoring regularisation as measured by the Lipschitz constant.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ç¬¬[8ç« ](Chapter_8.xhtml#x1-1620008)åˆ†æçš„æ¾³å¤§åˆ©äºšä¿¡ç”¨å®¡æ‰¹ï¼ˆACAï¼‰æ•°æ®é›†[[241](Biblography.xhtml#XUCI_ACA)ï¼Œ[242](Biblography.xhtml#XQuinlan1987)]å¯ä»¥ä½œä¸ºä¸€ä¸ªå¾ˆå¥½çš„ç¤ºä¾‹ã€‚æˆ‘ä»¬å¯ä»¥æ¯”è¾ƒç»å…¸å’Œé‡å­ç¥ç»ç½‘ç»œçš„æ€§èƒ½ï¼ŒåŒæ—¶ç›‘æ§ç”±Lipschitzå¸¸æ•°è¡¡é‡çš„æ­£åˆ™åŒ–ã€‚
- en: 'The classical neural network is an MLP Classifier with two hidden layers. Each
    hidden layer holds the same number of activation units as the number of features
    in the ACA dataset (14), so that we have to calculate the largest singular values
    for two 14 Ã— 14 square matrices. The features are standardised with `sklearn.preprocessing.StandardScaler`.
    We also use `sklearn.neural_network.MLPClassifier` to construct the classifier
    with the set of hyperparameters shown in TableÂ [12.1](#x1-228002r1):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç»å…¸ç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ªå…·æœ‰ä¸¤å±‚éšè—å±‚çš„MLPåˆ†ç±»å™¨ã€‚æ¯ä¸€å±‚éšè—å±‚çš„æ¿€æ´»å•å…ƒæ•°é‡ä¸ACAæ•°æ®é›†ä¸­çš„ç‰¹å¾æ•°é‡ï¼ˆ14ï¼‰ç›¸åŒï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä¸ºä¸¤ä¸ª14 Ã— 14çš„æ–¹é˜µè®¡ç®—æœ€å¤§çš„å¥‡å¼‚å€¼ã€‚è¿™äº›ç‰¹å¾ä½¿ç”¨`sklearn.preprocessing.StandardScaler`è¿›è¡Œäº†æ ‡å‡†åŒ–ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨`sklearn.neural_network.MLPClassifier`æ¥æ„å»ºå…·æœ‰è¡¨æ ¼[12.1](#x1-228002r1)ä¸­æ‰€ç¤ºè¶…å‚æ•°é›†çš„åˆ†ç±»å™¨ï¼š
- en: '| Hyperparameter | Value |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| è¶…å‚æ•° | å€¼ |'
- en: '| Number of hidden layers: | 2 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| éšè—å±‚æ•°é‡ï¼š | 2 |'
- en: '| Number of activation units in each layer: | 14 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| æ¯å±‚çš„æ¿€æ´»å•å…ƒæ•°é‡ï¼š | 14 |'
- en: '| Activation function: | tanh |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| æ¿€æ´»å‡½æ•°ï¼š | tanh |'
- en: '| Solver: | adam |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| æ±‚è§£å™¨ï¼š | adam |'
- en: '| Intial learning rate: | 0.01 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| åˆå§‹å­¦ä¹ ç‡ï¼š | 0.01 |'
- en: '| Number of iterations: | 5000 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| è¿­ä»£æ¬¡æ•°ï¼š | 5000 |'
- en: '| Random state: | 0 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| éšæœºçŠ¶æ€ï¼š | 0 |'
- en: '| Regularisation parameter, *Î±*: | variable |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| æ­£åˆ™åŒ–å‚æ•°ï¼Œ*Î±*ï¼š | å¯å˜ |'
- en: 'TableÂ 12.1: MLP Classifier hyperparameters.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æ ¼12.1ï¼šMLPåˆ†ç±»å™¨è¶…å‚æ•°ã€‚
- en: 'The MLP Classifier regularisation parameterÂ *Î±* is our control variable. It
    controls the *L*[2] regularisation term in the network cost function: the larger
    this parameter, the more large network weights are penalised. All other parameters
    were set at their default values.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: MLPåˆ†ç±»å™¨çš„æ­£åˆ™åŒ–å‚æ•°*Î±*æ˜¯æˆ‘ä»¬çš„æ§åˆ¶å˜é‡ã€‚å®ƒæ§åˆ¶ç½‘ç»œä»£ä»·å‡½æ•°ä¸­çš„*L*[2]æ­£åˆ™åŒ–é¡¹ï¼šè¿™ä¸ªå‚æ•°è¶Šå¤§ï¼Œè¶Šå¤šçš„å¤§ç½‘ç»œæƒé‡ä¼šå—åˆ°æƒ©ç½šã€‚æ‰€æœ‰å…¶ä»–å‚æ•°éƒ½è®¾ç½®ä¸ºé»˜è®¤å€¼ã€‚
- en: The quantum neural network is shown in FigureÂ [8.5](Chapter_8.xhtml#8.5). The
    parameterised quantum circuit consists of just 7 fixed two-qubit gates (CZ) and
    15 adjustable one-qubit gates (R[X] andÂ R[Y]). TableÂ [12.2](#x1-228005r2) compares
    the MLP and the QNN classifiers on the in-sample and out-of-sample datasets (the
    ACA dataset was split 50:50 into training and test datasets using sklearn.preprocessing.StandardScaler).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­ç¥ç»ç½‘ç»œå¦‚å›¾[8.5](Chapter_8.xhtml#8.5)æ‰€ç¤ºã€‚è¯¥å‚æ•°åŒ–é‡å­ç”µè·¯ä»…ç”±7ä¸ªå›ºå®šçš„ä¸¤é‡å­æ¯”ç‰¹é—¨ï¼ˆCZï¼‰å’Œ15ä¸ªå¯è°ƒçš„ä¸€é‡å­æ¯”ç‰¹é—¨ï¼ˆR[X]å’ŒR[Y]ï¼‰ç»„æˆã€‚è¡¨æ ¼[12.2](#x1-228005r2)æ¯”è¾ƒäº†MLPå’ŒQNNåˆ†ç±»å™¨åœ¨æ ·æœ¬å†…å’Œæ ·æœ¬å¤–æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ˆACAæ•°æ®é›†ä½¿ç”¨`sklearn.preprocessing.StandardScaler`è¢«åˆ†å‰²æˆ50:50çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼‰ã€‚
- en: We observe that QNN provides strong regularisation with similar performance
    on the in-sample and out-of-sample datasets as expected from the network represented
    by the unitary matrix.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è§‚å¯Ÿåˆ°QNNæä¾›äº†å¼ºå¤§çš„æ­£åˆ™åŒ–ï¼Œä¸”åœ¨æ ·æœ¬å†…å’Œæ ·æœ¬å¤–æ•°æ®é›†ä¸Šçš„è¡¨ç°å¦‚é¢„æœŸä¸€è‡´ï¼Œè¿™ä¸ç”±å¹ºæ­£çŸ©é˜µè¡¨ç¤ºçš„ç½‘ç»œç›¸ç¬¦ã€‚
- en: '| Classifier | Average *F*[1] score | Average *F*[1] score | Lipschitz Constant
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| åˆ†ç±»å™¨ | å¹³å‡*F*[1]å¾—åˆ† | å¹³å‡*F*[1]å¾—åˆ† | Lipschitzå¸¸æ•° |'
- en: '|  | (in-sample) | (out-of-sample) | (upper bound) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | ï¼ˆæ ·æœ¬å†…ï¼‰ | ï¼ˆæ ·æœ¬å¤–ï¼‰ | ï¼ˆä¸Šé™ï¼‰ |'
- en: '| MLP, *Î±* = 0*.*001 | 1.00 | 0.78 | 36.2 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *Î±* = 0*.*001 | 1.00 | 0.78 | 36.2 |'
- en: '| MLP, *Î±* = 0*.*01 | 1.00 | 0.79 | 33.5 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *Î±* = 0*.*01 | 1.00 | 0.79 | 33.5 |'
- en: '| MLP, *Î±* = 0*.*1 | 1.00 | 0.80 | 18.6 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *Î±* = 0*.*1 | 1.00 | 0.80 | 18.6 |'
- en: '| MLP, *Î±* = 1 | 0.99 | 0.83 | 7.4 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *Î±* = 1 | 0.99 | 0.83 | 7.4 |'
- en: '| MLP, *Î±* = 10 | 0.90 | 0.86 | 1.3 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *Î±* = 10 | 0.90 | 0.86 | 1.3 |'
- en: '| MLP, *Î±* = 40 | 0.85 | 0.86 | 0.5 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *Î±* = 40 | 0.85 | 0.86 | 0.5 |'
- en: '| MLP, *Î±* = 50 | 0.35 | 0.37 | 1e-05 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| MLP, *Î±* = 50 | 0.35 | 0.37 | 1e-05 |'
- en: '| QNN | 0.86 | 0.85 | 1.0 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| QNN | 0.86 | 0.85 | 1.0 |'
- en: 'TableÂ 12.2: F[1] scores and Lipschitz constants for MLP and QNN classifiers
    trained on the ACA dataset.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨12.2ï¼šMLPå’ŒQNNåˆ†ç±»å™¨åœ¨ACAæ•°æ®é›†ä¸Šè®­ç»ƒçš„F[1]å¾—åˆ†å’ŒLipschitzå¸¸æ•°ã€‚
- en: Further, we observe that the equivalent degree of regularisation can be achieved
    by MLP only with exceptionally large values of the regularisation parameterÂ *Î±*.
    MakingÂ *Î±* any larger completely destroys the learning abilities of the network.
    For the chosen MLP configuration, the critical value ofÂ *Î±* is betweenÂ 40 andÂ 50.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œç­‰æ•ˆçš„æ­£åˆ™åŒ–ç¨‹åº¦åªèƒ½é€šè¿‡MLPåœ¨æ­£åˆ™åŒ–å‚æ•°*Î±*å–æå¤§å€¼æ—¶æ‰èƒ½å®ç°ã€‚å°†*Î±*è¿›ä¸€æ­¥å¢å¤§å°†å®Œå…¨ç ´åç½‘ç»œçš„å­¦ä¹ èƒ½åŠ›ã€‚å¯¹äºæ‰€é€‰çš„MLPé…ç½®ï¼Œ*Î±*çš„ä¸´ç•Œå€¼åœ¨40åˆ°50ä¹‹é—´ã€‚
- en: Parameterised quantum circuits can be represented as (high-dimensional) norm-preserving
    unitary matrices. This ensures strong regularisation properties of the quantum
    neural networks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°åŒ–é‡å­ç”µè·¯å¯ä»¥è¡¨ç¤ºä¸ºï¼ˆé«˜ç»´ï¼‰ä¿æŒèŒƒæ•°çš„å•ä½aryçŸ©é˜µã€‚è¿™ç¡®ä¿äº†é‡å­ç¥ç»ç½‘ç»œçš„å¼ºæ­£åˆ™åŒ–ç‰¹æ€§ã€‚
- en: 'Now we can move to the next feature of the parameterised quantum circuits:
    their expressive power. We can define the expressivity of a PQC as the circuitâ€™s
    ability to generate pure quantum states that are well representative of the Hilbert
    spaceÂ Â [[266](Biblography.xhtml#XSim2019)]. In other words, from the QML point
    of view, the expressive power of a PQC is its ability to learn ("express") complex
    data structures. In the following section, we will try to quantify the degree
    of expressivity inherent in different PQC types.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿›å…¥å‚æ•°åŒ–é‡å­ç”µè·¯çš„ä¸‹ä¸€ä¸ªç‰¹æ€§ï¼šå®ƒä»¬çš„è¡¨è¾¾èƒ½åŠ›ã€‚æˆ‘ä»¬å¯ä»¥å°†PQCçš„è¡¨è¾¾èƒ½åŠ›å®šä¹‰ä¸ºç”µè·¯ç”Ÿæˆçº¯é‡å­æ€çš„èƒ½åŠ›ï¼Œè¿™äº›é‡å­æ€èƒ½å¤Ÿå¾ˆå¥½åœ°ä»£è¡¨å¸Œå°”ä¼¯ç‰¹ç©ºé—´[[266](Biblography.xhtml#XSim2019)]ã€‚æ¢å¥è¯è¯´ï¼Œä»QMLçš„è§’åº¦æ¥çœ‹ï¼ŒPQCçš„è¡¨è¾¾èƒ½åŠ›æ˜¯å®ƒå­¦ä¹ ï¼ˆâ€œè¡¨è¾¾â€ï¼‰å¤æ‚æ•°æ®ç»“æ„çš„èƒ½åŠ›ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å°è¯•é‡åŒ–ä¸åŒç±»å‹PQCå›ºæœ‰çš„è¡¨è¾¾èƒ½åŠ›ã€‚
- en: 12.2 Expressive Power
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2 è¡¨è¾¾èƒ½åŠ›
- en: 'We saw in previous chapters how PQCs can be applied to solving optimisation
    problems (QAOA and VQE) as well as to various machine learning tasks covering
    both discriminative (QNN classifier) and generative (QCBM market generator) use
    cases. In general, the PQCs we used for quantum machine learning tasks can be
    divided into two typesÂ Â [[88](Biblography.xhtml#XDu2018)]: tensor network PQC
    (similar to the QNN circuit in FigureÂ [8.4](Chapter_8.xhtml#8.4)) and multilayer
    PQC (similar to the QCBM circuit in FigureÂ [9.1](Chapter_9.xhtml#9.1)). What is
    their expressive power and how can we rank them? Before trying to answer this
    question, let us have a look at a simple illustrative example: quantum circuits
    specified on a single quantum register.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨å‰é¢çš„ç« èŠ‚ä¸­çœ‹åˆ°ï¼ŒPQCå¯ä»¥åº”ç”¨äºè§£å†³ä¼˜åŒ–é—®é¢˜ï¼ˆQAOAå’ŒVQEï¼‰ï¼Œä»¥åŠå„ç§æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ¤åˆ«æ€§ï¼ˆQNNåˆ†ç±»å™¨ï¼‰å’Œç”Ÿæˆæ€§ï¼ˆQCBMå¸‚åœºç”Ÿæˆå™¨ï¼‰ç”¨ä¾‹ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬ç”¨äºé‡å­æœºå™¨å­¦ä¹ ä»»åŠ¡çš„PQCå¯ä»¥åˆ†ä¸ºä¸¤ç±»[[88](Biblography.xhtml#XDu2018)]ï¼šå¼ é‡ç½‘ç»œPQCï¼ˆç±»ä¼¼äºå›¾[8.4](Chapter_8.xhtml#8.4)ä¸­çš„QNNç”µè·¯ï¼‰å’Œå¤šå±‚PQCï¼ˆç±»ä¼¼äºå›¾[9.1](Chapter_9.xhtml#9.1)ä¸­çš„QCBMç”µè·¯ï¼‰ã€‚å®ƒä»¬çš„è¡¨è¾¾èƒ½åŠ›å¦‚ä½•ï¼Œæˆ‘ä»¬åˆè¯¥å¦‚ä½•å¯¹å®ƒä»¬è¿›è¡Œæ’åå‘¢ï¼Ÿåœ¨å°è¯•å›ç­”è¿™ä¸ªé—®é¢˜ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆæ¥çœ‹ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼šåœ¨å•ä¸€é‡å­å¯„å­˜å™¨ä¸ŠæŒ‡å®šçš„é‡å­ç”µè·¯ã€‚
- en: '![FigureÂ 12.1: PQCs with different expressive powers. ](img/file1190.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾12.1ï¼šå…·æœ‰ä¸åŒè¡¨è¾¾èƒ½åŠ›çš„PQCã€‚ ](img/file1190.jpg)'
- en: 'FigureÂ 12.1: PQCs with different expressive powers.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾12.1ï¼šå…·æœ‰ä¸åŒè¡¨è¾¾èƒ½åŠ›çš„PQCã€‚
- en: FigureÂ [12.1](#12.1) displays four one-qubit circuits with dramatically different
    expressive powers, where *U*[âˆ’*Ï€,Ï€*] denotes the Uniform distribution over the
    closed interval [âˆ’*Ï€,Ï€*]. Let us go through them one by one.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[12.1](#12.1)å±•ç¤ºäº†å››ä¸ªå…·æœ‰æ˜¾è‘—ä¸åŒè¡¨è¾¾èƒ½åŠ›çš„å•é‡å­æ¯”ç‰¹ç”µè·¯ï¼Œå…¶ä¸­*U*[âˆ’*Ï€,Ï€*]è¡¨ç¤ºé—­åŒºé—´[âˆ’*Ï€,Ï€*]ä¸Šçš„å‡åŒ€åˆ†å¸ƒã€‚è®©æˆ‘ä»¬é€ä¸ªåˆ†æå®ƒä»¬ã€‚
- en: PQCÂ A starts with the qubit state initialised asÂ ![|0âŸ©](img/file1191.jpg) â€“
    North Pole on the Bloch sphere (FigureÂ [7.2](Chapter_7.xhtml#x1-1520002)). The
    only gate is the Hadamard gateÂ H that movesÂ ![|0âŸ©](img/file1192.jpg) to (![|0âŸ©](img/file1193.jpg)
    + ![|1âŸ©](img/file1194.jpg))*âˆ•*![âˆš -- 2](img/file1195.jpg). Thus, state ![|ÏˆAâŸ©](img/file1196.jpg)
    can only be a single point on the Bloch sphere.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PQC Aä»¥é‡å­æ¯”ç‰¹çŠ¶æ€åˆå§‹åŒ–å¼€å§‹ï¼Œå¦‚![|0âŸ©](img/file1191.jpg) â€“ Blochçƒä¸Šçš„åŒ—æï¼ˆå›¾[7.2](Chapter_7.xhtml#x1-1520002)ï¼‰ã€‚å”¯ä¸€çš„é—¨æ˜¯Hadamardé—¨Hï¼Œå®ƒå°†![|0âŸ©](img/file1192.jpg)å˜æ¢ä¸º
    (![|0âŸ©](img/file1193.jpg) + ![|1âŸ©](img/file1194.jpg))*âˆ•*![âˆš -- 2](img/file1195.jpg)ã€‚å› æ­¤ï¼ŒçŠ¶æ€![|ÏˆAâŸ©](img/file1196.jpg)åªèƒ½æ˜¯Blochçƒä¸Šçš„ä¸€ä¸ªå•ç‚¹ã€‚
- en: PQCÂ B also starts with the qubit state initialised asÂ ![|0âŸ©](img/file1197.jpg)
    and applies the Hadamard gate transforming the initial state into (![|0âŸ©](img/file1198.jpg)
    + ![|1âŸ©](img/file1199.jpg))*âˆ•*![âˆš -- 2](img/file1200.jpg) before applying the
    rotationÂ R[Z] around the *z*-axis by an angleÂ *ğœƒ*[z] drawn from the Uniform distribution
    on [âˆ’*Ï€,Ï€*]. The final stateÂ ![|ÏˆB âŸ©](img/file1201.jpg) can be any point on the
    equator, all reached with equal probability.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PQC Bä¹Ÿä»é‡å­æ¯”ç‰¹çŠ¶æ€åˆå§‹åŒ–ä¸º![|0âŸ©](img/file1197.jpg)å¼€å§‹ï¼Œåº”ç”¨Hadamardé—¨å°†åˆå§‹çŠ¶æ€è½¬åŒ–ä¸º (![|0âŸ©](img/file1198.jpg)
    + ![|1âŸ©](img/file1199.jpg))*âˆ•*![âˆš -- 2](img/file1200.jpg)ï¼Œç„¶åå›´ç»•*z*-è½´åº”ç”¨æ—‹è½¬R[Z]ï¼Œè§’åº¦*ğœƒ*[z]ä»åŒºé—´[âˆ’*Ï€,Ï€*]ä¸Šçš„å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–ã€‚æœ€ç»ˆçŠ¶æ€![|ÏˆB
    âŸ©](img/file1201.jpg)å¯ä»¥æ˜¯èµ¤é“ä¸Šçš„ä»»ä½•ä¸€ç‚¹ï¼Œä¸”æ‰€æœ‰è¿™äº›ç‚¹çš„æ¦‚ç‡ç›¸ç­‰ã€‚
- en: PQCÂ C adds a rotationÂ R[X] to PQCÂ B, by an angleÂ *ğœƒ*[x] drawn from the Uniform
    distribution on [âˆ’*Ï€,Ï€*]. With two rotations around two orthogonal axes we can
    reach any point on the Bloch sphere. However, with anglesÂ *ğœƒ*[z] andÂ *ğœƒ*[x] drawn
    from the Uniform distribution on [âˆ’*Ï€,Ï€*] we do not have a Uniform distribution
    of points on the Bloch sphere for stateÂ ![|Ïˆ âŸ© C](img/file1202.jpg). We observe
    the highest density around points (![|0âŸ©](img/file1203.jpg) + ![|1âŸ©](img/file1204.jpg))*âˆ•*![âˆš--
    2](img/file1205.jpg) and (![|0âŸ©](img/file1206.jpg)âˆ’![|1âŸ©](img/file1207.jpg))*âˆ•*![âˆš
    -- 2](img/file1208.jpg), which are the points where the equator crosses theÂ 0^âˆ˜
    and 180^âˆ˜ meridians, and the lowest density along the 90^âˆ˜ and 270^âˆ˜ meridians.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: PQC Cé€šè¿‡æ—‹è½¬R[X]å‘PQC Bæ·»åŠ ä¸€ä¸ªæ—‹è½¬è§’åº¦*ğœƒ*[x]ï¼Œè¯¥è§’åº¦ä»åŒºé—´[âˆ’*Ï€,Ï€*]ä¸Šçš„å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–ã€‚é€šè¿‡å›´ç»•ä¸¤ä¸ªæ­£äº¤è½´çš„ä¸¤æ¬¡æ—‹è½¬ï¼Œæˆ‘ä»¬å¯ä»¥åˆ°è¾¾Blochçƒä¸Šçš„ä»»ä½•ç‚¹ã€‚ç„¶è€Œï¼Œè‹¥è§’åº¦*ğœƒ*[z]å’Œ*ğœƒ*[x]ä»åŒºé—´[âˆ’*Ï€,Ï€*]ä¸Šçš„å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–ï¼Œåˆ™åœ¨çŠ¶æ€![|Ïˆ
    âŸ© C](img/file1202.jpg)ä¸‹ï¼ŒBlochçƒä¸Šçš„ç‚¹å¹¶ä¸å‘ˆå‡åŒ€åˆ†å¸ƒã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨ç‚¹ (![|0âŸ©](img/file1203.jpg) +
    ![|1âŸ©](img/file1204.jpg))*âˆ•*![âˆš-- 2](img/file1205.jpg) å’Œ (![|0âŸ©](img/file1206.jpg)âˆ’![|1âŸ©](img/file1207.jpg))*âˆ•*![âˆš
    -- 2](img/file1208.jpg) é™„è¿‘ï¼Œå¯†åº¦æœ€é«˜ï¼Œè€Œåœ¨90^âˆ˜å’Œ270^âˆ˜ç»çº¿æ²¿çº¿ï¼Œå¯†åº¦æœ€ä½ã€‚
- en: Finally, PQCÂ D adds one more rotationÂ R[Y] around the *y*-axis by an angleÂ *ğœƒ*[y]
    drawn from the Uniform distribution on [âˆ’*Ï€,Ï€*]. This rotation results in spreading
    the previously clustered points more evenly around the Bloch sphere, thus making
    all points on the Bloch sphere equally accessible.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼ŒPQC Dåœ¨*y*è½´ä¸Šé€šè¿‡ä¸€ä¸ªæ—‹è½¬R[Y]æ·»åŠ äº†ä¸€ä¸ªè§’åº¦*ğœƒ*[y]ï¼Œè¯¥è§’åº¦ä»åŒºé—´[âˆ’*Ï€,Ï€*]çš„å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–ã€‚è¿™ä¸ªæ—‹è½¬ä½¿å¾—ä¹‹å‰èšé›†çš„ç‚¹åœ¨Blochçƒä¸Šæ›´åŠ å‡åŒ€åœ°åˆ†å¸ƒï¼Œä»è€Œä½¿Blochçƒä¸Šçš„æ‰€æœ‰ç‚¹éƒ½èƒ½è¢«å¹³ç­‰åœ°è®¿é—®ã€‚
- en: 'Therefore, in terms of our ability to explore the Hilbert space, we have the
    following hierarchy of the expressive power of the PQCs introduced above:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå…³äºæˆ‘ä»¬æ¢ç´¢å¸Œå°”ä¼¯ç‰¹ç©ºé—´çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸Šè¿°PQCçš„è¡¨è¾¾èƒ½åŠ›å±‚æ¬¡åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š
- en: '![PQC D > PQC C > PQC B > PQC A. ](img/file1209.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![PQC D > PQC C > PQC B > PQC A. ](img/file1209.jpg)'
- en: We can now return to the PQCs developed in the previous chapters.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å›åˆ°å‰å‡ ç« å¼€å‘çš„PQCã€‚
- en: 12.2.1 Multilayer PQC
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.1 å¤šå±‚PQC
- en: A multilayer PQC (MPQC) consists of multiple blocks of quantum circuits in which
    the arrangement of quantum gates in each block is identicalÂ Â [[28](Biblography.xhtml#XBenedetti2018),Â [189](Biblography.xhtml#XLiuWang2018)].
    FigureÂ [12.2](#12.2) shows a schematic representation of the MPQC.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå±‚é‡å­ç”µè·¯ï¼ˆMPQCï¼‰ç”±å¤šä¸ªé‡å­ç”µè·¯å—ç»„æˆï¼Œæ¯ä¸ªå—ä¸­çš„é‡å­é—¨æ’åˆ—æ˜¯ç›¸åŒçš„[[28](Biblography.xhtml#XBenedetti2018)ï¼Œ[189](Biblography.xhtml#XLiuWang2018)]ã€‚å›¾[12.2](#12.2)å±•ç¤ºäº†MPQCçš„ç¤ºæ„å›¾ã€‚
- en: '![FigureÂ 12.2: Schematic representation of a multilayer PQC. ](img/file1210.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾12.2ï¼šå¤šå±‚PQCçš„ç¤ºæ„å›¾ã€‚](img/file1210.jpg)'
- en: 'FigureÂ 12.2: Schematic representation of a multilayer PQC.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾12.2ï¼šå¤šå±‚PQCçš„ç¤ºæ„å›¾ã€‚
- en: The following mathematical formalism can be used to describe MPQC. The input
    *n*-qubit quantum state with all qubits initialised as ![|0âŸ©](img/file1211.jpg)
    in the computational basis isÂ ![|0âŸ©](img/file1212.jpg)^(âŠ—n), the total number
    of blocks is denotedÂ *l*, and the *i*-th block is denoted U(ğœƒ^i), where the number
    of parameters is proportional to the number of qubits, and *n* is logarithmically
    proportional to the dimension of the generated data (this reflects our assumption
    about the data encoding scheme). The generated output state of the circuit thus
    reads
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ•°å­¦å½¢å¼å¯ä»¥ç”¨æ¥æè¿°MPQCã€‚è¾“å…¥çš„*n*é‡å­æ¯”ç‰¹é‡å­æ€ï¼Œæ‰€æœ‰é‡å­æ¯”ç‰¹åˆå§‹åŒ–ä¸º![|0âŸ©](img/file1211.jpg)çš„è®¡ç®—åŸºæ€ï¼Œä¸º![|0âŸ©](img/file1212.jpg)^(âŠ—n)ï¼Œæ€»å…±çš„ç”µè·¯å—æ•°ä¸º*l*ï¼Œç¬¬*i*ä¸ªå—è¡¨ç¤ºä¸ºU(ğœƒ^i)ï¼Œå…¶ä¸­å‚æ•°çš„æ•°é‡ä¸é‡å­æ¯”ç‰¹æ•°æˆæ­£æ¯”ï¼Œ*n*ä¸ç”Ÿæˆæ•°æ®çš„ç»´åº¦å‘ˆå¯¹æ•°å…³ç³»ï¼ˆè¿™åæ˜ äº†æˆ‘ä»¬å…³äºæ•°æ®ç¼–ç æ–¹æ¡ˆçš„å‡è®¾ï¼‰ã€‚å› æ­¤ï¼Œç”µè·¯ç”Ÿæˆçš„è¾“å‡ºæ€ä¸º
- en: '| ![ âˆ l i âŠ—n &#124;Ïˆ âŸ© = U (ğœƒ )&#124;0âŸ© . i=1 ](img/file1213.jpg) |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ l i âŠ—n &#124;Ïˆ âŸ© = U (ğœƒ )&#124;0âŸ© . i=1 ](img/file1213.jpg) |  |'
- en: 12.2.2 Tensor network PQC
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.2 å¼ é‡ç½‘ç»œPQC
- en: A tensor network PQC (TPQC) treats each block as a local tensor. The arrangement
    of the blocks follows a particular network structure, such as matrix product states
    or tree tensor networksÂ Â [[144](Biblography.xhtml#XHuggins2018)]. A schematic
    representation of TPQC is shown in FigureÂ [12.3](#12.3).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ é‡ç½‘ç»œPQCï¼ˆTPQCï¼‰å°†æ¯ä¸ªå—è§†ä¸ºå±€éƒ¨å¼ é‡ã€‚è¿™äº›å—çš„æ’åˆ—éµå¾ªç‰¹å®šçš„ç½‘ç»œç»“æ„ï¼Œå¦‚çŸ©é˜µç§¯æ€æˆ–æ ‘å½¢å¼ é‡ç½‘ç»œ[[144](Biblography.xhtml#XHuggins2018)]ã€‚å›¾[12.3](#12.3)å±•ç¤ºäº†TPQCçš„ç¤ºæ„å›¾ã€‚
- en: '![FigureÂ 12.3: Schematic representation of a tensor network PQC. ](img/file1214.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾12.3ï¼šå¼ é‡ç½‘ç»œPQCçš„ç¤ºæ„å›¾ã€‚](img/file1214.jpg)'
- en: 'FigureÂ 12.3: Schematic representation of a tensor network PQC.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾12.3ï¼šå¼ é‡ç½‘ç»œPQCçš„ç¤ºæ„å›¾ã€‚
- en: Mathematically, the *i*-th block U(ğœƒ^i) is composed ofÂ *M*[i] local tensor blocks,
    with *M*[i] âˆ *nâˆ•*2^i, denoted as U(ğœƒ^i) = âŠ— [j=1]^(M[i])U(ğœƒ[j]^i). Note that
    many of these tensor blocks may be identity operators. The generated state is
    thus of the form
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ•°å­¦ä¸Šè®²ï¼Œç¬¬*i*å—U(ğœƒ^i)ç”±*M*[i]ä¸ªå±€éƒ¨å¼ é‡å—ç»„æˆï¼Œå…¶ä¸­*M*[i] âˆ *nâˆ•*2^iï¼Œè¡¨ç¤ºä¸ºU(ğœƒ^i) = âŠ— [j=1]^(M[i])U(ğœƒ[j]^i)ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›å¼ é‡å—ä¸­çš„è®¸å¤šå¯èƒ½æ˜¯æ’ç­‰ç®—ç¬¦ã€‚å› æ­¤ï¼Œç”Ÿæˆçš„çŠ¶æ€å‘ˆç°ä»¥ä¸‹å½¢å¼ï¼š
- en: '| ![ M âˆl âŠ— i i âŠ—n &#124;ÏˆâŸ© = U(ğœƒj)&#124;0âŸ© . i=1 j=1 ](img/file1215.jpg) |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ![ M âˆl âŠ— i i âŠ—n &#124;ÏˆâŸ© = U(ğœƒj)&#124;0âŸ© . i=1 j=1 ](img/file1215.jpg) |  |'
- en: 12.2.3 Measures of expressive power
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.3 è¡¨è¾¾èƒ½åŠ›çš„åº¦é‡
- en: The main question to answer is whether MPQC and TPQC have larger expressive
    power in comparison with their classical counterparts, such as classical neural
    networks. The expressive power of a model can be defined in many different ways,
    for example, as a model capacity to express different relationships between variablesÂ Â [[22](Biblography.xhtml#XBaldi2019)].
    Deep neural networks serve as a good example of powerful models capable of learning
    complex data structuresÂ Â [[94](Biblography.xhtml#XDziugaite2017)]. Therefore,
    the power of a model can be quantified by its complexity, with the *Vapnik-Chervonenkis*
    *dimension* being a complexity measure of choiceÂ Â [[293](Biblography.xhtml#XVapnik1971)].
    The objective is to provide an estimate of how well a model generalises to the
    unseen data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»è¦é—®é¢˜æ˜¯å›ç­”MPQCå’ŒTPQCä¸å…¶ç»å…¸å¯¹åº”ç‰©ï¼ˆå¦‚ç»å…¸ç¥ç»ç½‘ç»œï¼‰ç›¸æ¯”ï¼Œæ˜¯å¦å…·æœ‰æ›´å¤§çš„è¡¨è¾¾èƒ½åŠ›ã€‚æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼å®šä¹‰ï¼Œä¾‹å¦‚ä½œä¸ºæ¨¡å‹è¡¨è¾¾ä¸åŒå˜é‡ä¹‹é—´å…³ç³»çš„èƒ½åŠ›[[22](Biblography.xhtml#XBaldi2019)]ã€‚æ·±åº¦ç¥ç»ç½‘ç»œæ˜¯èƒ½å¤Ÿå­¦ä¹ å¤æ‚æ•°æ®ç»“æ„çš„å¼ºå¤§æ¨¡å‹çš„ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­[[94](Biblography.xhtml#XDziugaite2017)]ã€‚å› æ­¤ï¼Œæ¨¡å‹çš„èƒ½åŠ›å¯ä»¥é€šè¿‡å…¶å¤æ‚æ€§é‡åŒ–ï¼Œè€Œ*Vapnik-Chervonenkis*
    *ç»´åº¦*æ˜¯è¡¡é‡å¤æ‚æ€§çš„é€‰æ‹©åº¦é‡[[293](Biblography.xhtml#XVapnik1971)]ã€‚ç›®æ ‡æ˜¯æä¾›ä¸€ä¸ªå…³äºæ¨¡å‹å¦‚ä½•æ³›åŒ–åˆ°æœªè§æ•°æ®çš„ä¼°è®¡ã€‚
- en: Another popular approach is the *Fisher information*, which describes the geometry
    of a model parameter spaceÂ Â [[247](Biblography.xhtml#XRissanen1996)]. Arguably,
    the *effective dimension* based on Fisher information, rather than Vapnik-Chervonenkis
    dimension, is a better measure to study the power of quantum and classical neural
    networksÂ Â [[1](Biblography.xhtml#XAbbas2020)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æµè¡Œçš„æ–¹æ³•æ˜¯*è´¹èˆå°”ä¿¡æ¯*ï¼Œå®ƒæè¿°äº†æ¨¡å‹å‚æ•°ç©ºé—´çš„å‡ ä½•ç»“æ„[[247](Biblography.xhtml#XRissanen1996)]ã€‚å¯ä»¥è¯´ï¼ŒåŸºäºè´¹èˆå°”ä¿¡æ¯çš„*æœ‰æ•ˆç»´åº¦*ï¼Œè€Œé
    Vapnik-Chervonenkisç»´åº¦ï¼Œæ˜¯ç ”ç©¶é‡å­å’Œç»å…¸ç¥ç»ç½‘ç»œèƒ½åŠ›çš„æ›´å¥½åº¦é‡[[1](Biblography.xhtml#XAbbas2020)]ã€‚
- en: However, one of the most natural metrics of expressive power is *entanglement*
    *entropy*, which allows us to establish a well-defined ranking of quantum and
    classical machine learning models. In this chapter, we will present the expressive
    power estimates obtained inÂ Â [[88](Biblography.xhtml#XDu2018)] for TPQC and MPQC
    based on entanglement entropy.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæœ€è‡ªç„¶çš„è¡¨è¾¾èƒ½åŠ›åº¦é‡ä¹‹ä¸€æ˜¯*çº ç¼ *ç†µï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿä¸ºé‡å­å’Œç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹å»ºç«‹ä¸€ä¸ªæ˜ç¡®çš„æ’åã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºåŸºäºçº ç¼ ç†µåœ¨[[88](Biblography.xhtml#XDu2018)]ä¸­ä¸ºTPQCå’ŒMPQCè·å¾—çš„è¡¨è¾¾èƒ½åŠ›ä¼°è®¡ã€‚
- en: 'Let us recall the definitions of entropy in statistical mechanics (the Gibbs
    entropyÂ *S*) and in information theory (the Shannon entropyÂ H) introduced in ChapterÂ [6](Chapter_6.xhtml#x1-1190006):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹åœ¨ç»Ÿè®¡åŠ›å­¦ä¸­å¼•å…¥çš„ç†µçš„å®šä¹‰ï¼ˆå‰å¸ƒæ–¯ç†µ*S*ï¼‰å’Œåœ¨ä¿¡æ¯è®ºä¸­å¼•å…¥çš„ç†µçš„å®šä¹‰ï¼ˆé¦™å†œç†µHï¼‰ï¼Œè¯¦è§ç¬¬[6](Chapter_6.xhtml#x1-1190006)ç« ï¼š
- en: '| ![ âˆ‘ âˆ‘ S := âˆ’ kB pilog(pi) and H := âˆ’ pilog2(pi). i i ](img/file1216.jpg)
    |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘ âˆ‘ S := âˆ’ kB pilog(pi) å’Œ H := âˆ’ pilog2(pi). i i ](img/file1216.jpg) |  |'
- en: Here, *p*[i] is the probability that the microstateÂ *i* is taken from an equilibrium
    ensemble in the case of the Gibbs entropy, and that the messageÂ *i* is picked
    from the message space in the case of the Shannon entropy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ*p*[i]æ˜¯å‰å¸ƒæ–¯ç†µæƒ…å†µä¸‹ä»å¹³è¡¡é›†ä¸­å–å‡ºå¾®è§‚çŠ¶æ€*i*çš„æ¦‚ç‡ï¼Œè€Œåœ¨é¦™å†œç†µçš„æƒ…å†µä¸‹ï¼Œå®ƒæ˜¯ä»æ¶ˆæ¯ç©ºé—´ä¸­é€‰æ‹©æ¶ˆæ¯*i*çš„æ¦‚ç‡ã€‚
- en: 'These definitions of entropy can be extended to the quantum case. In ChapterÂ [1](Chapter_1.xhtml#x1-220001),
    we introduced the density matrix as a universal tool for describing pure and mixed
    quantum states:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç†µçš„å®šä¹‰å¯ä»¥æ‰©å±•åˆ°é‡å­æƒ…å†µã€‚åœ¨ç¬¬[1ç« ](Chapter_1.xhtml#x1-220001)ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¯†åº¦çŸ©é˜µä½œä¸ºæè¿°çº¯æ€å’Œæ··åˆé‡å­æ€çš„é€šç”¨å·¥å…·ï¼š
- en: '![ N N âˆ‘ âˆ‘ Ï := Ïij |iâŸ©âŸ¨j|, i=1 j=1 ](img/file1217.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![ N N âˆ‘ âˆ‘ Ï := Ïij |iâŸ©âŸ¨j|, i=1 j=1 ](img/file1217.jpg)'
- en: where (![|iâŸ©](img/file1218.jpg))[i=1,â€¦,N] are the basis vectors of a given quantum
    system. The *von* *Neumann entropy*Â ğ’® is then defined as
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ (![|iâŸ©](img/file1218.jpg))[i=1,â€¦,N] æ˜¯ç»™å®šé‡å­ç³»ç»Ÿçš„åŸºå‘é‡ã€‚*å†¯Â·è¯ºä¾æ›¼ç†µ* ğ’® å®šä¹‰ä¸º
- en: '| ![ğ’®(Ï) := âˆ’ Tr(Ïlog(Ï)). ](img/file1219.jpg) |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ![ğ’®(Ï) := âˆ’ Tr(Ïlog(Ï)). ](img/file1219.jpg) |  |'
- en: Since the density matrix is Hermitian, it is *diagonalisable*, so that there
    exists a basis (![|kâŸ©](img/file1220.jpg))[k=1,â€¦,N] such that
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå¯†åº¦çŸ©é˜µæ˜¯å„ç±³çš„ï¼Œå®ƒæ˜¯*å¯å¯¹è§’åŒ–*çš„ï¼Œå› æ­¤å­˜åœ¨ä¸€ä¸ªåŸº (![|kâŸ©](img/file1220.jpg))[k=1,â€¦,N]ï¼Œä½¿å¾—
- en: '![ N N N Ï = âˆ‘ Ï |kâŸ© âŸ¨k| =:âˆ‘ p |kâŸ©âŸ¨k|, where âˆ‘ p = 1\. kk k k k=1 k=1 k=1 ](img/file1221.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![ N N N Ï = âˆ‘ Ï |kâŸ© âŸ¨k| =:âˆ‘ p |kâŸ©âŸ¨k|, where âˆ‘ p = 1\. kk k k k=1 k=1 k=1 ](img/file1221.jpg)'
- en: 'The eigenvalues of the operator *Ï*log(*Ï*) are thus (*p*[k] log(*p*[k]))[k=1,â€¦,N],
    and we obtain the following expression for the von Neumann entropy:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—ç¬¦ *Ï*log(*Ï*) çš„ç‰¹å¾å€¼ä¸º (*p*[k] log(*p*[k]))[k=1,â€¦,N]ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å¾—åˆ°å†¯Â·è¯ºä¾æ›¼ç†µçš„ä»¥ä¸‹è¡¨è¾¾å¼ï¼š
- en: '| ![ âˆ‘ ğ’®(Ï) = âˆ’ Tr (Ï log(Ï)) = âˆ’ pklog(pk). k ](img/file1222.jpg) |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘ ğ’®(Ï) = âˆ’ Tr (Ï log(Ï)) = âˆ’ pklog(pk). k ](img/file1222.jpg) |  |'
- en: FromÂ ([12.2.3](#x1-2320003)) andÂ ([12.2.3](#x1-2320003)) we see that for the
    orthogonal mixture of quantum states, the quantum and classical entropies coincide.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Â ([12.2.3](#x1-2320003)) å’ŒÂ ([12.2.3](#x1-2320003)) å¯ä»¥çœ‹å‡ºï¼Œå¯¹äºé‡å­æ€çš„æ­£äº¤æ··åˆï¼Œé‡å­ç†µå’Œç»å…¸ç†µæ˜¯ç›¸åŒçš„ã€‚
- en: If the system has two component parts, *A* and *B*, we can define the *reduced*
    *density matrix* as the *partial trace* of the density matrix over the subspace
    of the Hilbert space we are not interested in. Let (![|aâŸ© i](img/file1223.jpg))[i=1,...,N]
    be the standard orthonormal basis of the Hilbert spaceÂ â„[A] of systemÂ *A*, and
    (![|b âŸ© j](img/file1224.jpg))[j=1,...,M] be the standard orthonormal basis of
    the Hilbert spaceÂ â„[B] of systemÂ *B*. The density matrixÂ *Ï*[AB] of the bipartite
    systemÂ *AB* on the tensor product Hilbert space â„[A] âŠ—â„[B] can then be represented
    as
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœç³»ç»Ÿæœ‰ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼Œ*A* å’Œ *B*ï¼Œæˆ‘ä»¬å¯ä»¥å°†*çº¦åŒ–* *å¯†åº¦çŸ©é˜µ* å®šä¹‰ä¸ºå¯¹ä¸æ„Ÿå…´è¶£çš„å¸Œå°”ä¼¯ç‰¹ç©ºé—´å­ç©ºé—´ä¸Šçš„å¯†åº¦çŸ©é˜µçš„*éƒ¨åˆ†è¿¹*ã€‚ä»¤ (![|aâŸ©
    i](img/file1223.jpg))[i=1,...,N] ä¸ºç³»ç»Ÿ *A* çš„å¸Œå°”ä¼¯ç‰¹ç©ºé—´ â„[A] çš„æ ‡å‡†æ­£äº¤åŸºï¼Œä¸” (![|b âŸ© j](img/file1224.jpg))[j=1,...,M]
    ä¸ºç³»ç»Ÿ *B* çš„å¸Œå°”ä¼¯ç‰¹ç©ºé—´ â„[B] çš„æ ‡å‡†æ­£äº¤åŸºã€‚åˆ™åŒä½“ç³»ç»Ÿ *AB* åœ¨å¼ é‡ç§¯å¸Œå°”ä¼¯ç‰¹ç©ºé—´ â„[A] âŠ—â„[B] ä¸Šçš„å¯†åº¦çŸ©é˜µ *Ï*[AB] å¯è¡¨ç¤ºä¸º
- en: '| ![ âˆ‘N âˆ‘M Nâˆ‘ âˆ‘M ÏAB = cijkl &#124;aiâŸ©âŸ¨ak&#124;âŠ— &#124;bjâŸ©âŸ¨bl&#124;, i=1j=1
    k=1l=1 ](img/file1225.jpg) |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘N âˆ‘M Nâˆ‘ âˆ‘M ÏAB = cijkl &#124;aiâŸ©âŸ¨ak&#124;âŠ— &#124;bjâŸ©âŸ¨bl&#124;, i=1j=1
    k=1l=1 ](img/file1225.jpg) |  |'
- en: for some coefficients *c*[ijkl]. The partial traces then read
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æŸäº›ç³»æ•° *c*[ijkl]ï¼Œéƒ¨åˆ†è¿¹ä¸º
- en: '| ![ âˆ‘N Mâˆ‘ âˆ‘N âˆ‘M TrB(ÏAB ) = cijkl &#124;aiâŸ©âŸ¨ak&#124;âŸ¨bl&#124;bjâŸ©, i=1 j=1
    k=1 l=1 ](img/file1226.jpg) |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘N Mâˆ‘ âˆ‘N âˆ‘M TrB(ÏAB ) = cijkl &#124;aiâŸ©âŸ¨ak&#124;âŸ¨bl&#124;bjâŸ©, i=1 j=1
    k=1 l=1 ](img/file1226.jpg) |  |'
- en: which is a reduced density matrixÂ *Ï*[A] onÂ â„[A], and
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§åœ¨ â„[A] ä¸Šçš„çº¦åŒ–å¯†åº¦çŸ©é˜µ *Ï*[A]ï¼Œå¹¶ä¸”
- en: '| ![ N M N M Tr (Ï ) = âˆ‘ âˆ‘ âˆ‘ âˆ‘ c &#124;b âŸ©âŸ¨b &#124;âŸ¨a &#124;a âŸ©, A AB i=1 j=1
    ijkl j l k i k=1 l=1 ](img/file1227.jpg) |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ![ N M N M Tr (Ï ) = âˆ‘ âˆ‘ âˆ‘ âˆ‘ c &#124;b âŸ©âŸ¨b &#124;âŸ¨a &#124;a âŸ©, A AB i=1 j=1
    ijkl j l k i k=1 l=1 ](img/file1227.jpg) |  |'
- en: which is a reduced density matrixÂ *Ï*[B] onÂ â„[B]. Note that Tr(![|aiâŸ©](img/file1228.jpg)âŸ¨*a*[k]|)
    = ![âŸ¨ak|aiâŸ©](img/file1229.jpg) and Tr(![|b âŸ© j](img/file1230.jpg)âŸ¨*b*[l]|) = ![âŸ¨b|b
    âŸ© l j](img/file1231.jpg).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§åœ¨ â„[B] ä¸Šçš„çº¦åŒ–å¯†åº¦çŸ©é˜µ *Ï*[B]ã€‚è¯·æ³¨æ„ï¼ŒTr(![|aiâŸ©](img/file1228.jpg)âŸ¨*a*[k]|) = ![âŸ¨ak|aiâŸ©](img/file1229.jpg)
    å’Œ Tr(![|b âŸ© j](img/file1230.jpg)âŸ¨*b*[l]|) = ![âŸ¨b|b âŸ© l j](img/file1231.jpg)ã€‚
- en: '**Example:** Consider the two-qubit system in the state'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¤ºä¾‹ï¼š** è€ƒè™‘å¤„äºçŠ¶æ€ä¸­çš„ä¸¤æ¯”ç‰¹ç³»ç»Ÿ'
- en: '![|Ïˆ âŸ© = 1âˆš--(|01âŸ© + |10 âŸ©), 2 ](img/file1232.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![|Ïˆ âŸ© = 1âˆš--(|01âŸ© + |10 âŸ©), 2 ](img/file1232.jpg)'
- en: 'which is one of the four maximally entangled Bell states (SectionÂ [6.5.2](Chapter_6.xhtml#x1-1360002)).
    We assume that the first qubit is system *A* and the second qubit is system *B*.
    This state corresponds to the following density matrix:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å››ç§æœ€å¤§çº ç¼ è´å°”æ€ä¹‹ä¸€ï¼ˆè§ç¬¬[6.5.2èŠ‚](Chapter_6.xhtml#x1-1360002)ï¼‰ã€‚æˆ‘ä»¬å‡è®¾ç¬¬ä¸€ä¸ªé‡å­æ¯”ç‰¹æ˜¯ç³»ç»Ÿ *A*ï¼Œç¬¬äºŒä¸ªé‡å­æ¯”ç‰¹æ˜¯ç³»ç»Ÿ
    *B*ã€‚è¯¥çŠ¶æ€å¯¹åº”ä»¥ä¸‹å¯†åº¦çŸ©é˜µï¼š
- en: '![ ( ) Ï := |Ïˆ âŸ©âŸ¨Ïˆ| = 1- |01âŸ©âŸ¨01|+ |01âŸ©âŸ¨10|+ |10âŸ©âŸ¨01|+ |10âŸ©âŸ¨10| . AB 2 ](img/file1233.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![ ( ) Ï := |Ïˆ âŸ©âŸ¨Ïˆ| = 1- |01âŸ©âŸ¨01|+ |01âŸ©âŸ¨10|+ |10âŸ©âŸ¨01|+ |10âŸ©âŸ¨10| . AB 2 ](img/file1233.jpg)'
- en: 'Let us now act on this state with the partial trace Tr[B](â‹…):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å¯¹è¿™ä¸ªçŠ¶æ€æ–½åŠ éƒ¨åˆ†è¿¹ Tr[B](â‹…)ï¼š
- en: '| ![ 1 ( ) ÏA := TrB(ÏAB ) =-- &#124;0âŸ©âŸ¨0&#124;âŸ¨1&#124;1 âŸ©+ &#124;0âŸ©âŸ¨1&#124;âŸ¨0&#124;1âŸ©+
    &#124;1âŸ© âŸ¨0&#124;âŸ¨1&#124;0âŸ©+ &#124;1âŸ©âŸ¨1&#124;âŸ¨0&#124;0âŸ© 2 âŒŠ âŒ‹ 1 ( ) 1 1 0 = 2-
    &#124;0âŸ©âŸ¨0&#124;+ &#124;1âŸ©âŸ¨1&#124; = 2-âŒˆ âŒ‰ . 0 1 ](img/file1234.jpg) |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ![ 1 ( ) ÏA := TrB(ÏAB ) =-- &#124;0âŸ©âŸ¨0&#124;âŸ¨1&#124;1 âŸ©+ &#124;0âŸ©âŸ¨1&#124;âŸ¨0&#124;1âŸ©+
    &#124;1âŸ© âŸ¨0&#124;âŸ¨1&#124;0âŸ©+ &#124;1âŸ©âŸ¨1&#124;âŸ¨0&#124;0âŸ© 2 âŒŠ âŒ‹ 1 ( ) 1 1 0 = 2-
    &#124;0âŸ©âŸ¨0&#124;+ &#124;1âŸ©âŸ¨1&#124; = 2-âŒˆ âŒ‰ . 0 1 ](img/file1234.jpg) |  |'
- en: The reduced density matrix *Ï*[A] inÂ ([12.2.3](#x1-2320003)) is the same as
    the density matrix *Ï* inÂ ([1.3.3](Chapter_1.xhtml#x1-44003r3)), which describes
    a statistical ensemble of statesÂ ![|0âŸ©](img/file1235.jpg) andÂ ![|1âŸ©](img/file1236.jpg)
    (mixed state), i.e., a physical system prepared to be either in stateÂ ![|0âŸ©](img/file1237.jpg)
    or stateÂ ![|1âŸ©](img/file1238.jpg) with equal probability.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨([12.2.3](#x1-2320003))ä¸­çš„çº¦åŒ–å¯†åº¦çŸ©é˜µ*Ï*[A]ä¸([1.3.3](Chapter_1.xhtml#x1-44003r3))ä¸­çš„å¯†åº¦çŸ©é˜µ*Ï*ç›¸åŒï¼Œåè€…æè¿°äº†ä¸€ä¸ªç»Ÿè®¡é›†åˆï¼Œå…¶ä¸­åŒ…æ‹¬çŠ¶æ€![|0âŸ©](img/file1235.jpg)å’Œ![|1âŸ©](img/file1236.jpg)ï¼ˆæ··åˆæ€ï¼‰ï¼Œå³ï¼Œç‰©ç†ç³»ç»Ÿä»¥ç›¸ç­‰æ¦‚ç‡å‡†å¤‡å¤„äºçŠ¶æ€![|0âŸ©](img/file1237.jpg)æˆ–çŠ¶æ€![|1âŸ©](img/file1238.jpg)ã€‚
- en: The *entanglement entropy* of a bipartite system *AB* is then defined as
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åŒä½“ç³»ç»Ÿ*AB*çš„*çº ç¼ ç†µ*å®šä¹‰ä¸º
- en: '| ![ğ’®(ÏA) := âˆ’ Tr(ÏA log(ÏA )) = âˆ’ Tr(ÏB log(ÏB)) =: ğ’® (ÏB), ](img/file1239.jpg)
    |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ![ğ’®(ÏA) := âˆ’ Tr(ÏA log(ÏA )) = âˆ’ Tr(ÏB log(ÏB)) =: ğ’® (ÏB), ](img/file1239.jpg)
    |  |'
- en: and can be used as a measure of expressive power of a model in the following
    way. First, note that TPQC, MPQC and classical neural networks have a close connection
    with *tensor networks*, such as matrix product states (MPS)Â Â [[88](Biblography.xhtml#XDu2018)].
    The key question is then whether the given quantum system can be efficiently represented
    by the MPS.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”å¯ä»¥ä½œä¸ºè¡¡é‡æ¨¡å‹è¡¨ç°åŠ›çš„åº¦é‡ï¼Œæ–¹æ³•å¦‚ä¸‹ã€‚é¦–å…ˆï¼Œæ³¨æ„åˆ°TPQCã€MPQCå’Œç»å…¸ç¥ç»ç½‘ç»œä¸*å¼ é‡ç½‘ç»œ*ï¼ˆå¦‚çŸ©é˜µç§¯æ€MPSï¼‰æœ‰ç€å¯†åˆ‡çš„è”ç³»[[88](Biblography.xhtml#XDu2018)]ã€‚å…³é”®é—®é¢˜æ˜¯ï¼Œç»™å®šçš„é‡å­ç³»ç»Ÿæ˜¯å¦å¯ä»¥é€šè¿‡MPSé«˜æ•ˆè¡¨ç¤ºã€‚
- en: A quantum system that satisfies the *area law* (its entanglement entropy grows
    proportionally with the boundary area) has an efficient MPS representation. At
    the same time, a quantum system that satisfies the *volume law* (its entanglement
    entropy grows proportionally with the volume) cannot be efficiently represented
    by MPSÂ Â [[88](Biblography.xhtml#XDu2018)].
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æ»¡è¶³*é¢ç§¯å®šå¾‹*ï¼ˆå…¶çº ç¼ ç†µä¸è¾¹ç•Œé¢ç§¯æˆæ¯”ä¾‹å¢é•¿ï¼‰çš„é‡å­ç³»ç»Ÿå…·æœ‰é«˜æ•ˆçš„çŸ©é˜µç§¯æ€ï¼ˆMPSï¼‰è¡¨ç¤ºã€‚åŒæ—¶ï¼Œæ»¡è¶³*ä½“ç§¯å®šå¾‹*ï¼ˆå…¶çº ç¼ ç†µä¸ä½“ç§¯æˆæ¯”ä¾‹å¢é•¿ï¼‰çš„é‡å­ç³»ç»Ÿä¸èƒ½é€šè¿‡MPSæœ‰æ•ˆè¡¨ç¤º[[88](Biblography.xhtml#XDu2018)]ã€‚
- en: 12.2.4 Expressive power of PQC
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.4 PQCçš„è¡¨ç°åŠ›
- en: 'In ChapterÂ [5](Chapter_5.xhtml#x1-960005), we introduced the Restricted Boltzmann
    Machine (RBM) â€“ a neural network operating on stochastic binary activation units,
    which is a natural classical counterpart of parameterised quantum circuits. We
    considered two types of RBM:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬[5](Chapter_5.xhtml#x1-960005)ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å—é™ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰â€”â€”ä¸€ç§æ“ä½œåœ¨éšæœºäºŒè¿›åˆ¶æ¿€æ´»å•å…ƒä¸Šçš„ç¥ç»ç½‘ç»œï¼Œæ˜¯å‚æ•°åŒ–é‡å­ç”µè·¯çš„è‡ªç„¶ç»å…¸å¯¹åº”ç‰©ã€‚æˆ‘ä»¬è€ƒè™‘äº†ä¸¤ç§ç±»å‹çš„RBMï¼š
- en: a shallow two-layer network where the activation units in the visible layer
    are connected to the activation units in the hidden layer with no connections
    between the activation units within the same layer;
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæµ…å±‚çš„ä¸¤å±‚ç½‘ç»œï¼Œå…¶ä¸­å¯è§å±‚çš„æ¿€æ´»å•å…ƒä¸éšè—å±‚çš„æ¿€æ´»å•å…ƒç›¸è¿ï¼Œä½†åŒä¸€å±‚å†…çš„æ¿€æ´»å•å…ƒä¹‹é—´æ²¡æœ‰è¿æ¥ï¼›
- en: a deeper multi-layer network of stacked RBMs where the hidden layer of the *k*-th
    RBM serves as the visible layer of the (*k* + 1)-th RBM. Such stacked RBMs (trained
    sequentially) are called Deep Boltzmann Machines (DBMs).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´æ·±å±‚æ¬¡çš„å¤šå±‚å †å å¼å—é™ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰ç½‘ç»œï¼Œå…¶ä¸­ç¬¬*k*å±‚RBMçš„éšè—å±‚ä½œä¸ºç¬¬(*k* + 1)å±‚RBMçš„å¯è§å±‚ã€‚è¿™ç§å †å å¼RBMï¼ˆæŒ‰é¡ºåºè®­ç»ƒï¼‰ç§°ä¸ºæ·±åº¦ç»å°”å…¹æ›¼æœºï¼ˆDBMï¼‰ã€‚
- en: It is also possible to impose further restrictions on the connections between
    the RBM layers. In *short-range* RBMs, we restrict the connectivity of the hidden
    layer activation units such that they are allowed to connect to the limited number
    of activation units in the visible layer that are in close proximity to each other
    (local connectivity)Â Â [[84](Biblography.xhtml#XDeng2017)]. In *long-range* RBMs,
    we allow connections between the hidden layer activation units and the visible
    layer activation units that are not necessarily local.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜å¯ä»¥å¯¹RBMå±‚ä¹‹é—´çš„è¿æ¥æ–½åŠ è¿›ä¸€æ­¥çš„é™åˆ¶ã€‚åœ¨*çŸ­ç¨‹*RBMä¸­ï¼Œæˆ‘ä»¬é™åˆ¶éšè—å±‚æ¿€æ´»å•å…ƒçš„è¿æ¥æ€§ï¼Œä½¿å®ƒä»¬åªèƒ½è¿æ¥åˆ°å¯è§å±‚ä¸­ç›¸äº’æ¥è¿‘çš„å°‘æ•°æ¿€æ´»å•å…ƒï¼ˆå±€éƒ¨è¿æ¥ï¼‰[[84](Biblography.xhtml#XDeng2017)]ã€‚åœ¨*é•¿ç¨‹*RBMä¸­ï¼Œæˆ‘ä»¬å…è®¸éšè—å±‚æ¿€æ´»å•å…ƒä¸å¯è§å±‚æ¿€æ´»å•å…ƒä¹‹é—´çš„è¿æ¥ä¸ä¸€å®šæ˜¯å±€éƒ¨çš„ã€‚
- en: It has been established by Deng, Li, and SarmaÂ Â [[85](Biblography.xhtml#XDeng2017X)]
    that the entanglement entropy of all short-range RBM states satisfies an area
    law for arbitrary dimensions and bipartition geometry. For long-range RBM states,
    such states could exhibit volume law entanglement. Therefore long-range RBMs are
    capable of representing quantum states with large entanglement.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: é‚“ã€æå’Œè¨å°”é©¬å·²ç»ç¡®å®šäº†æ‰€æœ‰çŸ­ç¨‹RBMæ€çš„çº ç¼ ç†µæ»¡è¶³ä»»æ„ç»´åº¦å’ŒäºŒåˆ†å‡ ä½•çš„é¢ç§¯å®šå¾‹[[85](Biblography.xhtml#XDeng2017X)]ã€‚å¯¹äºé•¿ç¨‹RBMæ€ï¼Œè¿™äº›æ€å¯èƒ½è¡¨ç°å‡ºä½“ç§¯å®šå¾‹çš„çº ç¼ ã€‚å› æ­¤ï¼Œé•¿ç¨‹RBMèƒ½å¤Ÿè¡¨ç¤ºå…·æœ‰å¤§çº ç¼ çš„é‡å­æ€ã€‚
- en: 'It is probably not surprising that a DBM would have even larger expressive
    power than a single RBM. However, using entanglement entropy as a measure of expressive
    power, Du, Hsieh, Liu, and Tao have proven inÂ Â [[88](Biblography.xhtml#XDu2018)]
    that MPQC have strictly larger expressive power than DBM. The main result can
    be formulated as the following theorem:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½å¹¶ä¸ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒDBMçš„è¡¨è¾¾èƒ½åŠ›ç”šè‡³æ¯”å•ä¸€çš„RBMæ›´å¼ºã€‚ç„¶è€Œï¼Œåˆ©ç”¨çº ç¼ ç†µä½œä¸ºè¡¨è¾¾èƒ½åŠ›çš„åº¦é‡ï¼ŒDuã€Hsiehã€Liuå’ŒTaoåœ¨[[88](Biblography.xhtml#XDu2018)]ä¸­è¯æ˜äº†MPQCçš„è¡¨è¾¾èƒ½åŠ›ä¸¥æ ¼å¤§äºDBMã€‚ä¸»è¦ç»“æœå¯ä»¥è¡¨è¿°ä¸ºä»¥ä¸‹å®šç†ï¼š
- en: '**Theorem 10** (Expressive Power Theorem)**.** *The expressive power of MPQC
    and* *TPQC with* ğ’ª(*poly*(*n*)) *single qubit gates and* CNOT *gates, and classical
    neural* *networks with* ğ’ª(*poly*(*n*)) *trainable parameters, where* *n* *refers
    to the number of* *qubits or visible units, can be ordered as*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®šç†10**ï¼ˆè¡¨è¾¾èƒ½åŠ›å®šç†ï¼‰**ã€‚** *MPQCå’Œ* *TPQCçš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½¿ç”¨* ğ’ª(*poly*(*n*)) *ä¸ªå•é‡å­æ¯”ç‰¹é—¨å’Œ* CNOT
    *é—¨ï¼Œä»¥åŠå…·æœ‰* ğ’ª(*poly*(*n*)) *å¯è®­ç»ƒå‚æ•°çš„ç»å…¸ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­* *n* *è¡¨ç¤ºé‡å­æ¯”ç‰¹æˆ–å¯è§å•å…ƒçš„æ•°é‡ï¼Œå¯ä»¥æ’åºä¸º*'
- en: '| ![MPQC > DBM > long-range RBM > TPQC > short-range RBM. ](img/file1240.jpg)
    |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ![MPQC > DBM > long-range RBM > TPQC > short-range RBM.](img/file1240.jpg)
    |  |'
- en: TheoremÂ [10](#x1-233002r10) provides a solid theoretical foundation for experimental
    works aimed at establishing quantum advantage of PQC-based QML models. The larger
    expressive power of PQCs in comparison with their classical counterparts prompted
    the development of many such models in recent years. For example, a hybrid quantum-classical
    approach, suitable for NISQ devices and harnessing the greater expressive power
    of quantum entanglement, was proposed inÂ Â [[59](Biblography.xhtml#XChen2020S)].
    It was shown through numerical simulations that the Quantum Long Short Term Memory
    (QLSTM) model learns faster than the equivalent classical LSTM with a similar
    number of network parameters. In addition, the convergence of QLSTM was shown
    to be more stable than that of its classical counterpart. A Quantum Convolutional
    Neural Network (QCNN) was proposed inÂ Â [[58](Biblography.xhtml#XChen2020D)] which,
    due to its larger expressive power, achieved greater test accuracy compared to
    classical CNNs. The source of expressive power was the replacement of the classical
    convolutional filters with quantum convolutional kernels based on variational
    quantum circuits.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å®šç†[10](#x1-233002r10)ä¸ºæ—¨åœ¨ç¡®ç«‹åŸºäºPQCçš„QMLæ¨¡å‹é‡å­ä¼˜åŠ¿çš„å®éªŒå·¥ä½œæä¾›äº†åšå®çš„ç†è®ºåŸºç¡€ã€‚ä¸ç»å…¸æ¨¡å‹ç›¸æ¯”ï¼ŒPQCæ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ä¿ƒä½¿äº†è¿‘å¹´æ¥è®¸å¤šæ­¤ç±»æ¨¡å‹çš„å‘å±•ã€‚ä¾‹å¦‚ï¼Œé’ˆå¯¹NISQè®¾å¤‡å¹¶åˆ©ç”¨é‡å­çº ç¼ æ›´å¼ºè¡¨è¾¾èƒ½åŠ›çš„æ··åˆé‡å­-ç»å…¸æ–¹æ³•ï¼Œå·²åœ¨[[59](Biblography.xhtml#XChen2020S)]ä¸­æå‡ºã€‚é€šè¿‡æ•°å€¼æ¨¡æ‹Ÿè¡¨æ˜ï¼Œé‡å­é•¿çŸ­æœŸè®°å¿†ï¼ˆQLSTMï¼‰æ¨¡å‹æ¯”å…·æœ‰ç›¸ä¼¼ç½‘ç»œå‚æ•°çš„ç»å…¸LSTMå­¦ä¹ å¾—æ›´å¿«ã€‚æ­¤å¤–ï¼ŒQLSTMçš„æ”¶æ•›æ€§æ¯”å…¶ç»å…¸å¯¹æ‰‹æ›´åŠ ç¨³å®šã€‚åœ¨[[58](Biblography.xhtml#XChen2020D)]ä¸­æå‡ºäº†é‡å­å·ç§¯ç¥ç»ç½‘ç»œï¼ˆQCNNï¼‰ï¼Œç”±äºå…¶æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æ¯”ç»å…¸CNNæ›´é«˜çš„æµ‹è¯•ç²¾åº¦ã€‚å…¶è¡¨è¾¾èƒ½åŠ›çš„æ¥æºæ˜¯ç”¨åŸºäºå˜åˆ†é‡å­ç”µè·¯çš„é‡å­å·ç§¯æ ¸æ›¿ä»£äº†ç»å…¸çš„å·ç§¯æ»¤æ³¢å™¨ã€‚
- en: Multi-layer parameterised quantum circuits such as QCBM have strictly more expressive
    power than classical models such as RBM when only a polynomial number of parameters
    is allowed. For systems that exhibit quantum supremacy, a classical model cannot
    learn to reproduce the statistics unless it uses exponentially scaling resourcesÂ Â [[29](Biblography.xhtml#XBenedetti2019)].
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå±‚å‚æ•°åŒ–é‡å­ç”µè·¯ï¼Œå¦‚QCBMï¼Œåœ¨åªå…è®¸å¤šé¡¹å¼æ•°é‡çš„å‚æ•°æ—¶ï¼Œæ¯”ç»å…¸æ¨¡å‹å¦‚RBMå…·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚å¯¹äºè¡¨ç°å‡ºé‡å­ä¼˜åŠ¿çš„ç³»ç»Ÿï¼Œç»å…¸æ¨¡å‹æ— æ³•å­¦ä¹ é‡ç°ç»Ÿè®¡æ•°æ®ï¼Œé™¤éå®ƒä½¿ç”¨æŒ‡æ•°çº§çš„èµ„æº[[29](Biblography.xhtml#XBenedetti2019)]ã€‚
- en: Summary
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: In this chapter, we learned where the power of parameterised quantum circuits
    comes from. We started with the observation that quantum neural networks enjoy
    strong regularisation inherent in their architecture. This is due to the fact
    that any PQC, however wide and deep, is a unitary linear operator.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å‚æ•°åŒ–é‡å­ç”µè·¯çš„åŠ›é‡æ¥è‡ªä½•å¤„ã€‚æˆ‘ä»¬ä»è§‚å¯Ÿåˆ°é‡å­ç¥ç»ç½‘ç»œåœ¨å…¶æ¶æ„ä¸­å…·æœ‰å¼ºå¤§çš„æ­£åˆ™åŒ–å¼€å§‹ã€‚è¿™æ˜¯å› ä¸ºä»»ä½•PQCï¼Œæ— è®ºå¤šä¹ˆå®½å¹¿å’Œæ·±åº¦ï¼Œéƒ½æ˜¯ä¸€ä¸ªå•ä½çº¿æ€§ç®—ç¬¦ã€‚
- en: Next, we considered the expressive power of parameterised quantum circuits and
    established the concept of the expressive power hierarchy. The main result (TheoremÂ [10](#x1-233002r10))
    supports the experimental findings indicating the presence of the elements of
    quantum advantage in various QML models compatible with the main characteristics
    of NISQ devices.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è€ƒè™‘äº†å‚æ•°åŒ–é‡å­ç”µè·¯çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶å»ºç«‹äº†è¡¨è¾¾èƒ½åŠ›å±‚çº§çš„æ¦‚å¿µã€‚ä¸»è¦ç»“æœï¼ˆå®šç†[10](#x1-233002r10)ï¼‰æ”¯æŒäº†å®éªŒç»“æœï¼Œè¿™äº›å®éªŒè¡¨æ˜åœ¨ä¸NISQè®¾å¤‡ä¸»è¦ç‰¹æ€§å…¼å®¹çš„å„ç§QMLæ¨¡å‹ä¸­å­˜åœ¨é‡å­ä¼˜åŠ¿çš„å…ƒç´ ã€‚
- en: In the next chapter, we will go deeper into the less explored territory of new
    quantum algorithms, an area of very active research.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨æ–°é‡å­ç®—æ³•è¿™ä¸€è¾ƒå°‘ç ”ç©¶çš„é¢†åŸŸï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸æ´»è·ƒçš„ç ”ç©¶æ–¹å‘ã€‚
- en: Join our bookâ€™s Discord space
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬ä¹¦ç±çš„Discordç©ºé—´
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/quantum](https://packt.link/quantum)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬çš„Discordç¤¾åŒºï¼Œä¸å¿—åŒé“åˆçš„äººä¸€èµ·å­¦ä¹ ï¼Œå’Œè¶…è¿‡2000åæˆå‘˜å…±åŒæ¢è®¨ï¼š[https://packt.link/quantum](https://packt.link/quantum)
- en: '![PIC](img/file1.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file1.png)'
