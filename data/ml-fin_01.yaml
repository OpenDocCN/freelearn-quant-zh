- en: Chapter 1. Neural Networks and Gradient-Based Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 神经网络与基于梯度的优化
- en: The financial services industry is fundamentally an information processing industry.
    An investment fund processes information in order to evaluate investments, an insurance
    company processes information to price their insurances, while a retail bank will
    process information in order to decide which products to offer to which customers.
    It is, therefore, no accident that the financial industry was an early adopter
    of computers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 金融服务行业本质上是一个信息处理行业。投资基金处理信息以评估投资，保险公司处理信息以定价保险，而零售银行则处理信息以决定向哪些客户提供哪些产品。因此，金融行业早期采用计算机并非偶然。
- en: The first stock ticker was the printing telegraph, which was invented back in
    1867\. The first mechanical adding machine, which was directly targeted at the
    finance industry, was patented in 1885\. Then in 1971, the automatic teller banking
    machine, which allowed customers to withdraw cash using a plastic card, was patented.
    That same year, the first electronic stock exchange, the NASDAQ, opened its doors,
    and 11 years later, in 1982, the first Bloomberg Terminal was installed. The reason
    for the happy marriage between the finance sector and computers is that success
    in the industry, especially in investing, is often tied to you having an information
    advantage.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个股票行情机是印刷电报，发明于1867年。第一台专门面向金融行业的机械加法机于1885年获得专利。然后，在1971年，自动柜员机（ATM）获得专利，允许客户使用塑料卡提取现金。同年，首个电子证券交易所——纳斯达克（NASDAQ）开盘，11年后，即1982年，第一台彭博终端（Bloomberg
    Terminal）安装使用。金融行业与计算机之间的成功结合，原因在于该行业的成功，尤其是在投资领域，通常与是否拥有信息优势密切相关。
- en: In the early days of Wall Street, the legends of the gilded age made brazen
    use of private information. Jay Gould, for example, one of the richest men of
    his time, placed a mole inside the US government. The mole was to give notice
    of government gold sales and through that, tried to influence President Ulysses
    S. Grant as well as his secretary. Toward the end of the 1930s, the SEC and CFTC
    stood between investors and such information advantages.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在华尔街的早期，镀金时代的传说肆意利用私人信息。例如，杰伊·古尔德（Jay Gould），当时最富有的人之一，在美国政府内部安插了一个间谍。这个间谍负责提前通报政府的黄金销售信息，并借此试图影响总统尤利西斯·S·格兰特及其秘书。到了20世纪30年代末，证券交易委员会（SEC）和商品期货交易委员会（CFTC）成为了投资者和这些信息优势之间的屏障。
- en: As information advantages ceased to be a reliable source of above-market performance,
    clever financial modeling took its place. The term *hedge fund* was coined back
    in 1949, the Harry Markowitz model was published in 1953, and in 1973, the Black-Scholes
    formula was first published. Since then, the field has made much progress and
    has developed a wide range of financial products. However, as knowledge of these
    models becomes more widespread, the returns on using them diminish.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随着信息优势不再成为超越市场表现的可靠来源，聪明的金融建模取而代之。*对冲基金*这一术语最早出现在1949年，哈里·马科维茨（Harry Markowitz）模型于1953年发布，1973年，布莱克-斯科尔斯（Black-Scholes）公式首次发表。自那时以来，该领域取得了巨大的进展，并开发出了多种金融产品。然而，随着这些模型知识的普及，使用这些模型的回报逐渐减少。
- en: When we look at the financial industry coupled with modern computing, it's clear
    that the information advantage is back. This time not in the form of insider information
    and sleazy deals, but instead is coming from an automated analysis of the vast
    amount of public information that's out there.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看到金融行业与现代计算技术结合时，显然信息优势重新回归。这一次，它不是以内幕信息和不正当交易的形式出现，而是通过自动化分析大量公开信息的方式展现。
- en: Today's fund managers have access to more information than their forbearers
    could ever dream of. However, this is not useful on its own. For example, let's
    look at news reports. You can get them via the internet and they are easy to access,
    but to make use of them, a computer would have to read, understand, and contextualize
    them. The computer would have to know which company an article is about, whether
    it is good news or bad news that's being reported, and whether we can learn something
    about the relationship between this company and another company mentioned in the
    article. Those are just a couple of examples of contextualizing the story. Firms that
    master sourcing such **alternative data,** as it is often called, will often have an
    advantage.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的基金经理拥有比前辈们曾经梦寐以求的更多信息。然而，光有这些信息本身并不有用。例如，来看新闻报道。你可以通过互联网获取它们，而且容易访问，但要利用它们，计算机必须能够阅读、理解并将其置于上下文中。计算机必须知道一篇文章讲的是哪家公司，是好消息还是坏消息，并且是否可以从中了解这家公司与文章中提到的另一家公司之间的关系。这只是几个关于如何将故事放入上下文中的例子。那些擅长获取这种**替代数据**的公司，往往会拥有优势。
- en: But it does not stop there. Financial professionals are expensive people who
    frequently make six- to seven-figure salaries and occupy office space in some
    of the most expensive real estate in the world. This is justified as many financial
    professionals are smart, well-educated, and hard-working people that are scarce
    and for which there is a high demand. Because of this, it's thus in the interest
    of any company to maximize the productivity of these individuals. By getting more
    bang for the buck from the best employees, they will allow companies to offer
    their products cheaper or in greater variety.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 但事情并不止于此。金融专业人士是昂贵的人员，他们经常拥有六到七位数的薪水，并占据全球一些最昂贵的房地产。这是合理的，因为许多金融专业人士是聪明、受过良好教育且勤奋工作的人，他们稀缺并且需求量大。因此，任何公司都希望最大化这些个人的生产力。通过让最优秀的员工创造更多价值，公司将能够以更低的成本或更多样化的产品来提供服务。
- en: Passive investing through exchange-traded funds, for instance, requires little
    management for large sums of money. Fees for passive investment vehicles, such
    as funds that just mirror the S&P 500, are often well below one percent. But with
    the rise of modern computing technology, firms are now able to increase the productivity
    of their money managers and thus reduce their fees to stay competitive.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，通过交易所交易基金进行的被动投资，对大笔资金的管理几乎无需干预。被动投资工具的费用，例如那些仅仅跟踪标普500指数的基金，通常低于1%。但是随着现代计算技术的崛起，企业现在能够提高其资金经理的生产力，从而降低费用以保持竞争力。
- en: Our journey in this book
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书的旅程
- en: This book is not only about investing or trading in the finance sector; it's
    much more as a direct result of the love story between computers and finance.
    Investment firms have customers, often insurance firms or pension funds, and these
    firms are financial services companies themselves and, in turn, also have customers,
    everyday people that have a pension or are insured.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书不仅仅是关于金融领域的投资或交易；它更是计算机与金融之间爱情故事的直接结果。投资公司有客户，通常是保险公司或养老金基金，而这些公司本身就是金融服务公司，反过来，它们也有客户，那些拥有养老金或投保的普通人。
- en: Most bank customers are everyday people as well, and increasingly, the main
    way people are interacting with their bank, insurer, or pension is through an
    app on their mobile phone.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数银行客户也是普通人，而且越来越多的人通过手机应用与银行、保险公司或养老金机构互动。
- en: In the decades before today, retail banks relied on the fact that people would
    have to come into the branch, face-to-face, in order to withdraw cash or to make
    a transaction. While they were in the branch, their advisor could also sell them
    another product, such as a mortgage or insurance. Today's customers still want
    to buy mortgages and insurance, but they no longer have to do it in person at
    the branch. In today's world, banks tend to advise their clients online, whether
    it's through the app or their website.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天之前的几十年里，零售银行依赖于人们必须亲自到分行才能提取现金或进行交易这一事实。当人们在分行时，银行顾问还可以向他们推销其他产品，例如抵押贷款或保险。今天的客户仍然想要购买抵押贷款和保险，但他们不再需要亲自到分行办理。在今天的世界里，银行倾向于通过应用程序或网站在线为客户提供建议。
- en: This online aspect only works if the bank can understand its customers' needs
    from their data and provide tailor-made experiences online. Equally, from the
    customers, perspective, they now expect to be able to submit insurance claims
    from their phone and to get an instant response. In today's world, insurers need
    to be able to automatically assess claims and make decisions in order to fulfill
    their customers' demands.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在线模式只有在银行能够通过数据理解客户需求并提供量身定制的在线体验时才能奏效。同样，从客户的角度来看，他们现在期望能够通过手机提交保险理赔并立即得到回应。在今天的世界中，保险公司需要能够自动评估理赔并做出决策，以满足客户的需求。
- en: This book is not about how to write trading algorithms in order to make a quick
    buck. It is about leveraging the art and craft of building machine learning-driven
    systems that are useful in the financial industry.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书并非讲解如何编写交易算法以快速获利。它是关于如何利用机器学习驱动的系统艺术和技巧，在金融行业中构建有用的系统。
- en: Building anything of value requires a lot of time and effort. Right now, the
    market for building valuable things, to make an analogy to economics, is highly
    inefficient. Applications of machine learning will transform the industry over
    the next few decades, and this book will provide you with a toolbox that allows
    you to be part of the change.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 构建任何有价值的东西都需要大量的时间和精力。目前，构建有价值事物的市场，与经济学类比，是高度低效的。机器学习的应用将在未来几十年内变革这个行业，而本书将为你提供一个工具箱，帮助你成为这一变革的一部分。
- en: Many of the examples in this book use data outside the realm of "financial data."
    Stock market data is used at no time in this book, and this decision was made
    for three specific reasons.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的许多例子使用了超出“财务数据”范畴的数据。本书中没有使用股市数据，做出这一决定有三个具体原因。
- en: Firstly, the examples that are shown demonstrate techniques that can usually
    easily be applied to other datasets. Therefore, datasets were chosen that demonstrate
    some common challenges that professionals, like yourselves, will face while also
    remaining computationally tractable.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，所展示的例子演示了通常可以轻松应用于其他数据集的技术。因此，选择了能够展示一些专业人士（如你们）在实践中可能面临的常见挑战的数据集，同时这些数据集也保持了计算上的可处理性。
- en: Secondly, financial data is fundamentally time dependent. To make this book
    useful over a longer span of time, and to ensure that as machine learning becomes
    more prominent, this book remains a vital part of your toolkit, we have used some
    non-financial data so that the data discussed here will still be relevant.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，金融数据本质上是时间相关的。为了使本书在更长时间跨度内仍然有用，并确保随着机器学习的普及，本书能继续成为你工具箱中的重要组成部分，我们使用了一些非财务数据，这样本书讨论的数据将保持相关性。
- en: Finally, using alternative and non-classical data aims to inspire you to think
    about what other data you could use in your processes. Could you use drone footage
    of plants to augment your grain price models? Could you use web browsing behavior
    to offer different financial products? Thinking outside of the box is a necessary
    skill to have if you want to make use of the data that is around you.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用替代性和非传统数据旨在激发你思考在你的流程中还可以使用哪些其他数据。你是否可以利用植物的无人机影像来增强你的粮食价格模型？你是否可以利用网页浏览行为来提供不同的金融产品？如果你想利用身边的数据，跳出框框思考是一个必要的技能。
- en: What is machine learning?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: '"Machine learning is the subfield of computer science that gives computers the ability
    to learn without being explicitly programmed."'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “机器学习是计算机科学的一个子领域，它赋予计算机在没有明确编程的情况下学习的能力。”
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- Arthur Samuel, 1959'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 亚瑟·塞缪尔，1959年'
- en: What do we mean by machine learning? Most computer programs today are handcrafted
    by humans. Software engineers carefully craft every rule that governs how software
    behaves and then translate it into computer code.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的机器学习是什么意思？今天的大多数计算机程序是由人类手工编写的。软件工程师精心设计每一个规则来规范软件的行为，然后将其转化为计算机代码。
- en: If you are reading this as an eBook, take a look at your screen right now. Everything
    that you see appears there because of some rule that a software engineer somewhere
    crafted. This approach has gotten us quite far, but that's not to say there are
    no limits to it. Sometimes, there might just be too many rules for humans to write.
    We might not be able to think of rules since they are too complex for even the
    smartest developers to come up with.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在阅读这本电子书，现在可以看看你的屏幕。你所看到的一切都是因为某个软件工程师设计了一些规则。这种方法已经带我们走得很远，但这并不意味着它没有限制。有时，可能会有太多规则让人类去编写。我们可能无法想到规则，因为它们太复杂，即使是最聪明的开发人员也难以想出。
- en: As a brief exercise, take a minute to come up with a list of rules that describe
    all dogs, but clearly distinguish dogs from all other animals. Fur? Well, cats
    have fur, too. What about a dog wearing a jacket? That is still a dog, just in
    a jacket. Researchers have spent years trying to craft these rules, but they've
    had very little success.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 做一个简短的练习，花一分钟时间列出描述所有狗的规则，并且明确区分狗和所有其他动物。毛发？好吧，猫也有毛发。那如果一只狗穿着夹克呢？那还是狗，只不过穿了夹克。研究人员花了多年时间试图制定这些规则，但收效甚微。
- en: Humans don't seem to be able to perfectly tell why something is a dog, but they
    know a dog when they see a dog. As a species, we seem to detect specific, hard-to-describe
    patterns that, in aggregate, let us classify an animal as a dog. Machine learning
    attempts to do the same. Instead of handcrafting rules, we let a computer develop
    its own rules through pattern detection.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 人类似乎无法完美地解释为什么某样东西是狗，但我们一眼就能认出狗。作为一个物种，我们似乎能检测出一些难以描述的特定模式，这些模式合起来让我们能够将动物分类为狗。机器学习试图做的就是一样的事情。我们不是手工编写规则，而是让计算机通过模式检测来发展自己的规则。
- en: 'There are different ways this can work, and we''re now going to look at three
    different types of learning: supervised, unsupervised, and reinforcement learning.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法有多种实现方式，接下来我们将介绍三种不同的学习类型：监督学习、无监督学习和强化学习。
- en: Supervised learning
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: Let's go back to our dog classifier. There are in fact many such classifiers
    currently in use today. If you use Google images, for example, and search for
    "dog," it will use an image classifier to show you pictures of dogs. These classifiers
    are trained under a paradigm known as supervised learning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到狗分类器的话题。实际上，目前有很多这样的分类器正在被使用。例如，如果你使用谷歌图片搜索“狗”，它会使用图像分类器来展示狗的图片。这些分类器是在监督学习的范式下进行训练的。
- en: '![Supervised learning](img/B10354_01_01.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习](img/B10354_01_01.jpg)'
- en: Supervised learning
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习
- en: In supervised learning, we have a large number of training examples, such as
    images of animals, and labels that describe what the expected outcome for those
    training examples is. For example, the preceding figure would come with the label
    "dog," while an image of a cat would come with a label "not a dog."
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们有大量的训练样本，比如动物的图像，以及描述这些训练样本期望结果的标签。例如，上面的图像会附有标签“狗”，而猫的图像则会附有标签“不是狗”。
- en: If we have a high number of these labeled training examples, we can train a
    classifier on detecting the subtle statistical patterns that differentiate dogs
    from all other animals.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有大量带标签的训练示例，我们可以训练一个分类器，检测出区分狗和其他动物的微妙统计模式。
- en: Tip
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Note**: The classifier does not know what a dog fundamentally is. It only
    knows the statistical patterns that linked images to dogs in training.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：分类器并不知道狗的本质是什么。它只知道训练过程中将图像与狗联系起来的统计模式。'
- en: If a supervised learning classifier encounters something that's very different
    from the training data, it can often get confused and will just output nonsense.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个监督学习分类器遇到与训练数据非常不同的内容，它往往会感到困惑，并输出无意义的结果。
- en: Unsupervised learning
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: While supervised learning has made great advances over the last few years, most
    of this book will focus on working with labeled examples. However, sometimes we
    may not have labels. In this case, we can still use machine learning to find hidden
    patterns in data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管监督学习在过去几年取得了很大的进展，但本书的大部分内容将重点讨论使用带标签的示例。然而，有时我们可能没有标签。在这种情况下，我们仍然可以使用机器学习来发现数据中的隐藏模式。
- en: '![Unsupervised learning](img/B10354_01_02.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习](img/B10354_01_02.jpg)'
- en: Clustering is a common form of unsupervised learning
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是无监督学习的一种常见形式
- en: Imagine a company that has a number of customers for its products. These customers
    can probably be grouped into different market segments, but what we don't know
    is what the different market segments are. We also cannot ask customers which
    market segment they belong to because they probably don't know. Which market segment
    of the shampoo market are you? Do you even know how shampoo firms segment their customers?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，一个公司拥有若干客户群体。这些客户大致可以分为不同的市场细分，但我们不知道这些市场细分具体是什么。我们也不能直接询问客户他们属于哪个市场细分，因为他们可能并不知道。你是洗发水市场的哪一类消费者？你甚至知道洗发水公司是如何细分客户的吗？
- en: In this example, we would like an algorithm that looks at a lot of data from
    customers and groups them into segments. This is an example of unsupervised learning.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们希望有一个算法能够查看大量来自客户的数据，并将他们分为不同的群体。这是一个无监督学习的例子。
- en: This area of machine learning is far less developed than supervised learning,
    but it still holds great potential.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这一领域的机器学习远不如监督学习发展成熟，但它依然蕴藏着巨大的潜力。
- en: Reinforcement learning
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: In reinforcement learning, we train agents who take actions in an environment,
    such as a self-driving car on the road. While we do not have labels, that is,
    we cannot tell what the *correct* action is in any situation, we can assign rewards
    or punishments. For example, we could reward keeping a proper distance from the
    car in front.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，我们训练能够在环境中采取行动的智能体，例如在道路上行驶的自动驾驶汽车。尽管我们没有标签，也就是说，我们无法判断在任何情况下什么是*正确*的行动，但我们可以给予奖励或惩罚。例如，我们可以奖励保持与前车的适当距离。
- en: '![Reinforcement learning](img/B10354_01_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习](img/B10354_01_03.jpg)'
- en: Reinforcement learning
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习
- en: A driving instructor does not tell the student to "push the brake halfway down
    while moving the steering wheel two degrees to the right," but rather they tell
    the student whether they are doing well or not, while the student figures out
    the exact amount of brakes to use.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 驾驶教练不会告诉学员“在转动方向盘两度右转的同时将刹车踩下去一半”，而是告诉学员他们做得好不好，学员则自己找出该使用多少刹车。
- en: Reinforcement learning has also made some remarkable progress in the past couple of
    years and is considered by many to be a promising avenue toward general artificial
    intelligence, that being computers that are as smart as humans.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，强化学习也取得了显著进展，许多人认为它是通向通用人工智能的一个有前途的方向，也就是计算机具备与人类相当的智能。
- en: The unreasonable effectiveness of data
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据的非凡有效性
- en: In 2009, three Google engineers published a landmark paper titled *The unreasonable
    effectiveness of data*. In the paper, they described how relatively simple machine
    learning systems that had been around for a long time had exhibited much better
    performance when fed with the enormous amounts of data Google had on its servers.
    In fact, they discovered that when fed with more data, these simple systems could
    master tasks that had been thought to be impossible before.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 2009年，三位谷歌工程师发表了一篇具有里程碑意义的论文，题为*数据的非凡有效性*。在论文中，他们描述了长期以来一直存在的相对简单的机器学习系统，在接入谷歌服务器上巨量的数据后，表现出了远超预期的性能。实际上，他们发现，当这些简单的系统获得更多数据时，它们能够掌握以前被认为不可能完成的任务。
- en: From there, researchers quickly started revisiting old machine learning technologies and
    found that artificial neural networks did especially well when trained on massive
    datasets. This was around the same time that computing power became cheap and
    plentiful enough to train much bigger networks than before.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，研究人员迅速开始重新审视旧有的机器学习技术，并发现人工神经网络在大数据集上训练时表现尤其出色。大约在那个时候，计算能力变得更加便宜和丰富，足以训练比以往更大的网络。
- en: 'These bigger artificial neural networks were so effective that they got a name:
    deep neural networks, or deep learning. Deep neural networks are especially good
    at pattern detection. They can find complex patterns, such as the statistical
    pattern of light and dark that describes a face in a picture, and they can do
    so automatically given enough data.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些更大的人工神经网络如此有效，以至于它们被赋予了一个名字：深度神经网络，或深度学习。深度神经网络在模式识别方面特别擅长。它们可以找到复杂的模式，例如描述面部的光暗统计模式，且在拥有足够数据的情况下能够自动完成这一过程。
- en: Machine learning is, therefore, best understood as a paradigm change in how
    we program computers. Instead of carefully handcrafting rules, we feed the computer
    vast amounts of information and train it to craft the rules by itself.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，机器学习最好被理解为一种程序设计范式的变革。我们不再精心手工编写规则，而是向计算机提供大量信息，训练它自行制定规则。
- en: This approach is superior if there is a very large number of rules, or even
    if these rules are difficult to describe. Modern machine learning is, therefore,
    the ideal tool for combing through the huge amounts of data the financial industry
    is confronted with.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果规则的数量非常庞大，或者这些规则难以描述，那么这种方法是更优的。因此，现代机器学习是理想的工具，用于处理金融行业所面临的大量数据。
- en: All models are wrong
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所有模型都是错误的
- en: There is a saying in statistics that *all models are wrong, but some are useful*.
    Machine learning creates incredibly complex statistical models that are often,
    for example, in deep learning, not interpretable to humans. They sure are useful
    and have great value, but they are still wrong. This is because they are complex
    black boxes, and people tend to not question machine learning models, even though
    they should question them precisely because they are black boxes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学中有句名言：*所有模型都是错误的，但有些是有用的*。机器学习创造了极为复杂的统计模型，这些模型往往在深度学习中，难以被人类理解。它们确实很有用，并且具有很大的价值，但它们仍然是错误的。这是因为它们是复杂的“黑箱”，而人们往往不去质疑机器学习模型，尽管正因为它们是黑箱，我们更应该质疑它们。
- en: There will come a time when even the most sophisticated deep neural network
    will make a fundamentally wrong prediction, just as the advanced **Collateralized
    Debt Obligation** (**CDO**) models did in the financial crises of 2008\. Even
    worse, black box machine learning models, which will make millions of decisions
    on loan approval or insurance, impacting everyday people's lives, will eventually
    make wrong decisions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总有一天，即使是最先进的深度神经网络，也会做出根本错误的预测，正如2008年金融危机中的**担保债务凭证**（**CDO**）模型所做的那样。更糟糕的是，黑箱式的机器学习模型将在贷款审批或保险等方面做出数百万个决定，影响普通人的生活，最终可能做出错误的决策。
- en: Sometimes they will be biased. Machine learning is ever only as good as the
    data that we feed it, data that can often be biased in what it's showing, something
    we'll consider later on in this chapter. This is something we must pay a lot of
    time in addressing, as if we mindlessly deploy these algorithms, we will automate
    discrimination too, which has the possibility of causing another financial crisis.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候它们会有偏见。机器学习的效果永远取决于我们提供给它的数据，而这些数据常常在展示时存在偏见，这是我们将在本章稍后讨论的内容。我们必须花大量时间去解决这个问题，因为如果我们不加思考地部署这些算法，我们也会自动化歧视，进而可能引发另一场金融危机。
- en: This is especially true in the financial industry, where algorithms can often
    have a severe impact on people's lives while at the same time being kept secret.
    The unquestionable, secret black boxes that gain their acceptance through the
    heavy use of math pose a much bigger threat to society than the self-aware artificial
    intelligence taking over the world that you see in movies.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点在金融行业尤为真实，在这里，算法往往会对人们的生活产生严重影响，同时这些算法又是保密的。那些通过大量数学运算获得认可的、不容质疑的秘密黑箱，带来的社会威胁远远大于你在电影中看到的、自觉的人工智能接管世界的情节。
- en: 'While this is not an ethics book, it makes sense for any practitioner of the
    field to get familiar with the ethical implications of his or her work. In addition
    to recommending that you read Cathy O''Neil''s *Weapons of math destruction,*
    it''s also worth asking you to swear *The Modelers Hippocratic Oath*. The oath
    was developed by Emanuel Derman and Paul Wilmott, two quantitative finance researchers,
    in 2008 in the wake of the financial crisis:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不是一本伦理学书籍，但对于任何该领域的从业者来说，了解自己工作的伦理影响是有意义的。除了推荐你阅读Cathy O'Neil的*《数学毁灭武器》*，我还建议你宣誓*《建模者的希波克拉底誓言》*。这项誓言由量化金融研究人员Emanuel
    Derman和Paul Wilmott于2008年在金融危机后提出：
- en: '"I will remember that I didn''t make the world, and it doesn''t satisfy my
    equations. Though I will use models boldly to estimate value, I will not be overly
    impressed by mathematics. I will never sacrifice reality for elegance without
    explaining why I have done so. Nor will I give the people who use my model false
    comfort about its accuracy. Instead, I will make explicit its assumptions and
    oversights. I understand that my work may have enormous effects on society and
    the economy, many of them beyond my comprehension."'
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我会记得，我不是创造了这个世界，世界也不符合我的方程式。尽管我会大胆地使用模型来估计价值，但我不会过分看重数学。我绝不会为了优雅而牺牲现实，除非我能解释为什么这样做。我也不会让使用我的模型的人对其准确性产生虚假的安慰。相反，我会明确指出其假设和疏漏。我明白，我的工作可能对社会和经济产生巨大影响，其中许多影响超出了我的理解。”
- en: In recent years, machine learning has made a number of great strides, with researchers
    mastering tasks that were previously seen as unsolvable. From identifying objects
    in images to transcribing voice and playing complex board games like Go, modern
    machine learning has matched, and continues to match and even beat, human performance
    at a dazzling range of tasks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，机器学习取得了许多重大进展，研究人员已经掌握了曾经被认为无法解决的任务。从图像中的物体识别到语音转录，再到像围棋这样复杂的棋盘游戏，现代机器学习已经在众多任务上与人类表现相匹敌，并且持续超越人类表现。
- en: Interestingly, **deep learning** is the method behind all these advances. In
    fact, the bulk of advances come from a subfield of deep learning called **deep
    neural networks**. While many practitioners are familiar with standard econometric
    models, such as regression, few are familiar with this new breed of modeling.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，**深度学习**正是所有这些进展背后的方法。事实上，绝大多数的进展来自于深度学习的一个子领域——**深度神经网络**。虽然许多从业者熟悉标准的计量经济学模型，比如回归分析，但很少有人熟悉这种新型的建模方法。
- en: The bulk of this book is devoted to deep learning. This is because it is one
    of the most promising techniques for machine learning and will give anyone mastering
    it the ability to tackle tasks considered impossible before.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的大部分内容都集中在深度学习上。这是因为深度学习是机器学习中最有前景的技术之一，掌握它的人将具备解决曾经被认为不可能完成的任务的能力。
- en: In this chapter, we will explore how and why neural networks work in order to give you
    a fundamental understanding of the topic.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索神经网络如何以及为什么能够工作，以便让你对这一主题有一个基本的理解。
- en: Setting up your workspace
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置工作环境
- en: Before we can start, you will need to set up your workspace. The examples in this book
    are all meant to run in a Jupyter notebook. Jupyter notebooks are an interactive
    development environment mostly used for data-science applications and are considered
    the go-to environment to build data-driven applications in.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，你需要设置工作环境。本书中的所有示例都旨在Jupyter笔记本中运行。Jupyter笔记本是一种互动式开发环境，主要用于数据科学应用，是构建数据驱动应用程序的首选环境。
- en: You can run Jupyter notebooks either on your local machine, on a server in the
    cloud, or on a website such as Kaggle.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本地计算机上、云服务器上，或者像Kaggle这样的网站上运行Jupyter笔记本。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: All code examples for this book can be found here: [https://github.com/PacktPublishing/Machine-Learning-for-Finance](https://github.com/PacktPublishing/Machine-Learning-for-Finance)
    and for chapter 1 refer the following link: [https://www.kaggle.com/jannesklaas/machine-learning-for-finance-chapter-1-code](https://www.kaggle.com/jannesklaas/machine-learning-for-finance-chapter-1-code).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：本书的所有代码示例可以在这里找到：[https://github.com/PacktPublishing/Machine-Learning-for-Finance](https://github.com/PacktPublishing/Machine-Learning-for-Finance)，第1章的代码请参考以下链接：[https://www.kaggle.com/jannesklaas/machine-learning-for-finance-chapter-1-code](https://www.kaggle.com/jannesklaas/machine-learning-for-finance-chapter-1-code)。'
- en: Deep learning is computer intensive, and the data used in the examples throughout
    this book are frequently over a gigabyte in size. It can be accelerated by the
    use of **Graphics Processing Units** (**GPUs**), which were invented for rendering
    video and games. If you have a GPU enabled computer, you can run the examples
    locally. If you do not have such a machine, it is recommended to use a service
    such as Kaggle kernels.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习对计算机的要求很高，本书中使用的示例数据常常超过一千兆字节。通过使用**图形处理单元**（**GPU**），这类任务可以加速处理，而GPU最初是为渲染视频和游戏而发明的。如果你拥有启用了GPU的计算机，你可以在本地运行这些示例。如果没有这样的机器，建议使用像Kaggle
    Kernels这样的服务。
- en: Learning deep learning used to be an expensive endeavor because GPUs are an expensive
    piece of hardware. While there are cheaper options available, a powerful GPU can
    cost up to $10,000 if you buy it and about $0.80 an hour to rent it in the cloud.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 学习深度学习过去是一项昂贵的任务，因为 GPU 是一块昂贵的硬件。虽然现在有更便宜的选择，但如果你购买一块强大的 GPU，它的价格可以高达 $10,000，而在云端租用它的费用约为每小时
    $0.80。
- en: If you have many, long-running training jobs, it might be worth considering
    building a "deep learning" box, a desktop computer with a GPU. There are countless
    tutorials for this online and a decent box can be assembled for as little as a
    few hundred dollars all the way to $5,000.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有许多长期运行的训练任务，可能值得考虑建立一个“深度学习”工作站，即一台配备 GPU 的台式电脑。网上有无数的教程可以帮助你构建，而一台普通的工作站可以花费几百美元，最高可达
    $5,000。
- en: The examples in this book can all be run on Kaggle for free, though. In fact,
    they have been developed using this site.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书中的示例都可以在 Kaggle 上免费运行。实际上，它们是使用该网站开发的。
- en: Using Kaggle kernels
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kaggle 内核
- en: Kaggle is a popular data-science website owned by Google. It started out with
    competitions in which participants had to build machine learning models in order
    to make predictions. However, over the years, it has also had a popular forum,
    an online learning system and, most importantly for us, a hosted Jupyter service.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 是一个由 Google 拥有的流行数据科学网站。它最初通过竞赛形式运作，参赛者需要构建机器学习模型以进行预测。然而，随着时间的推移，它还拥有了一个受欢迎的论坛、一个在线学习系统，最重要的是，为我们提供了一个托管的
    Jupyter 服务。
- en: To use Kaggle, you can visit their website at [https://www.kaggle.com/](https://www.kaggle.com/).
    In order to use the site, you will be required to create an account.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Kaggle，你可以访问他们的网站 [https://www.kaggle.com/](https://www.kaggle.com/)。为了使用该网站，你需要创建一个账户。
- en: 'After you''ve created your account, you can find the **Kernels** page by clicking
    on **Kernels** located in the main menu, as seen in the following screenshot:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 创建账户后，你可以通过点击主菜单中的 **Kernels** 来访问 **Kernels** 页面，如下图所示：
- en: '![Using Kaggle kernels](img/B10354_01_04.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Kaggle 内核](img/B10354_01_04.jpg)'
- en: Public Kaggle kernels
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 公共 Kaggle 内核
- en: In the preceding screenshot, you can see a number of kernels that other people
    have both written and published. Kernels can be private, but publishing kernels
    is a good way to show skills and share knowledge.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，你可以看到其他人编写并发布的一些内核。内核可以是私有的，但发布内核是展示技能和分享知识的好方式。
- en: 'To start a new kernel, click **New Kernel**. In the dialog that follows, you
    want to select **Notebook**:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动一个新的内核，请点击 **New Kernel**。在接下来的对话框中，你需要选择 **Notebook**：
- en: '![Using Kaggle kernels](img/B10354_01_05.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Kaggle 内核](img/B10354_01_05.jpg)'
- en: The kernel editor
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 内核编辑器
- en: You will get to the kernel editor, which looks like the preceding screenshot.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你将进入内核编辑器，它看起来像前面的截图。
- en: Note that Kaggle is actively iterating on the kernel design, and so a few elements
    might be in different positions, but the basic functionality is the same. The
    most important piece of a notebook is the code cells. Here you can enter the code
    and run it by clicking the run button on the bottom left, or alternatively by
    pressing *Shift* + *Enter*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Kaggle 正在积极迭代其内核设计，因此某些元素可能会处于不同的位置，但基本功能是相同的。笔记本中最重要的部分是代码单元。在这里，你可以输入代码，并通过点击左下角的运行按钮来运行它，或者使用
    *Shift* + *Enter* 的快捷键。
- en: The variables you define in one cell become environment variables, so you can
    access them in another cell. Markdown cells allow you to write text in markdown
    format to add a description to what is going on in your code. You can upload and
    download notebooks with the little cloud buttons featured in the top-right corner.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你在一个单元中定义的变量会变成环境变量，因此可以在另一个单元中访问它们。Markdown 单元允许你使用 markdown 格式编写文本，为你的代码添加说明。你可以通过位于右上角的小云按钮上传和下载笔记本。
- en: To publish a notebook from the kernel editor, firstly you must click the **Commit
    & Run** button and then set the notebook to **Public** in the settings. To enable
    a GPU on your notebook, make sure to check the **Enable GPU** button located in
    the bottom right. It's important to remember that this will restart your notebook,
    so your environment variables will be lost.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要从内核编辑器发布笔记本，首先必须点击 **Commit & Run** 按钮，然后在设置中将笔记本设置为 **Public**。要启用笔记本中的 GPU，请确保选中右下角的
    **Enable GPU** 按钮。需要记住的是，这将重启你的笔记本，因此你的环境变量将丢失。
- en: Once you run the code, the run button turns into a stop button. If your code
    ever gets stuck, you can interrupt it by clicking that stop button. If you want
    to wipe all environment variables and begin anew, simply click the restart button
    located in the bottom-right corner.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你运行了代码，运行按钮将变为停止按钮。如果你的代码卡住了，可以通过点击停止按钮来中断它。如果你想清除所有环境变量并重新开始，只需点击右下角的重启按钮。
- en: With this system, you can connect a kernel to any dataset hosted on Kaggle,
    or alternatively you can just upload a new dataset on the fly. The notebooks belonging
    to this book already come with the data connection.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个系统，你可以将内核连接到Kaggle上托管的任何数据集，或者你也可以随时上传新的数据集。本书中的笔记本已经配置好了数据连接。
- en: Kaggle kernels come with the most frequently used packages preinstalled, so for
    most of the time you do not have to worry about installing packages.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle内核预装了最常用的包，因此大部分时间你无需担心安装包。
- en: Sometimes this book does use custom packages not installed in Kaggle by default.
    In this case, you can add custom packages at the bottom of the Settings menu.
    Instructions for installing custom packages will be provided when they are used
    in this book.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本书有时会使用Kaggle默认没有安装的自定义包。在这种情况下，你可以在设置菜单的底部添加自定义包。每当本书中使用自定义包时，会提供相关的安装说明。
- en: Kaggle kernels are free to use and can save you a lot of time and money, so
    it's recommended to run the code samples on Kaggle. To copy a notebook, go to
    the link provided at the beginning of the code section of each chapter and then
    click **Fork Notebook**. Note that Kaggle kernels can run for up to six hours.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle内核是免费的，能为你节省大量时间和金钱，因此建议在Kaggle上运行代码示例。要复制一个笔记本，请访问每章代码部分开头提供的链接，然后点击**Fork
    Notebook**。请注意，Kaggle内核最多可以运行六个小时。
- en: Running notebooks locally
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地运行笔记本
- en: If you have a machine powerful enough to run deep learning operations, you can run the
    code samples locally. In that case, it's strongly recommended to install Jupyter
    through Anaconda.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的机器足够强大，可以运行深度学习操作，那么你可以在本地运行代码示例。在这种情况下，强烈建议通过Anaconda安装Jupyter。
- en: To install Anaconda, simply visit [https://www.anaconda.com/download](https://www.anaconda.com/download)
    to download the distribution. The graphical installer will guide you through the
    steps necessary to install Anaconda on your system. When installing Anaconda,
    you'll also install a range of useful Python libraries such as NumPy and matplotlib,
    which will be used throughout this book.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Anaconda，只需访问[https://www.anaconda.com/download](https://www.anaconda.com/download)下载发行版。图形化安装程序将引导你完成安装Anaconda到系统的步骤。在安装Anaconda时，你还将安装一系列有用的Python库，如NumPy和matplotlib，这些库将在本书中使用。
- en: 'After installing Anaconda, you can start a Jupyter server locally by opening
    your machine''s Terminal and typing in the following code:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Anaconda后，你可以通过打开机器的终端并输入以下代码，来本地启动Jupyter服务器：
- en: '[PRE0]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can then visit the URL displayed in the Terminal. This will take you to
    your local notebook server.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以访问终端中显示的URL。这将带你到本地的笔记本服务器。
- en: To start a new notebook, click on **New** in the top-right corner.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动一个新的笔记本，点击右上角的**New**。
- en: All code samples in this book use Python 3, so make sure you are using Python
    3 in your local notebooks. If you are running your notebooks locally, you will
    also need to install both TensorFlow and Keras, the two deep learning libraries
    used throughout this book.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有代码示例都使用Python 3，因此请确保在本地笔记本中使用Python 3。如果你在本地运行笔记本，还需要安装TensorFlow和Keras，这两个人工智能深度学习库将在本书中使用。
- en: Installing TensorFlow
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装TensorFlow
- en: 'Before installing Keras, we need to first install TensorFlow. You can install
    TensorFlow by opening a Terminal window and entering the following command:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装Keras之前，我们需要先安装TensorFlow。你可以通过打开终端窗口并输入以下命令来安装TensorFlow：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For instructions on how to install TensorFlow with GPU support, simply click
    on this link, where you will be provided with the instructions for doing so: [https://www.tensorflow.org/](https://www.tensorflow.org/).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解如何安装支持GPU的TensorFlow，只需点击此链接，你将在这里找到安装说明：[https://www.tensorflow.org/](https://www.tensorflow.org/)。
- en: It's worth noting that you will need a CUDA-enabled GPU in order to run TensorFlow
    with CUDA. For instructions on how to install CUDA, visit [https://docs.nvidia.com/cuda/index.html](https://docs.nvidia.com/cuda/index.html).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，你需要一块支持CUDA的GPU才能运行带有CUDA的TensorFlow。有关如何安装CUDA的说明，请访问[https://docs.nvidia.com/cuda/index.html](https://docs.nvidia.com/cuda/index.html)。
- en: Installing Keras
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装Keras
- en: 'After you have installed TensorFlow, you can install Keras in the same way,
    by running the following command:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 安装TensorFlow后，你可以通过运行以下命令来安装Keras：
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Keras will now automatically use the TensorFlow backend. Note that TensorFlow
    1.7 will include Keras built in, which we'll cover this later on in this chapter.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Keras将自动使用TensorFlow作为后端。请注意，TensorFlow 1.7将内置Keras，我们将在本章稍后介绍这一点。
- en: Using data locally
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地使用数据
- en: To use the data of the book code samples locally, visit the notebooks on Kaggle
    and then download the connected datasets from there. Note that the file paths
    to the data will change depending on where you save the data, so you will need
    to replace the file paths when running notebooks locally.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要在本地使用书中的代码示例数据，请访问Kaggle上的笔记本并从那里下载相关数据集。请注意，数据的文件路径会根据你保存数据的位置而有所变化，因此在本地运行笔记本时，你需要替换文件路径。
- en: Kaggle also offers a command-line interface, which allows you to download the
    data more easily. Visit [https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api)
    for instructions on how to achieve this.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle还提供了一个命令行接口，使你能够更轻松地下载数据。访问[https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api)了解如何实现。
- en: Using the AWS deep learning AMI
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS深度学习AMI
- en: '**Amazon Web Services** (**AWS**) provides an easy-to-use, preconfigured way
    to run deep learning in the cloud.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**亚马逊云服务**（**AWS**）提供了一种易于使用、预配置的方式来在云端运行深度学习。'
- en: Visit [https://aws.amazon.com/machine-learning/amis/](https://aws.amazon.com/machine-learning/amis/)
    for instructions on how to set up an **Amazon Machine Image** (**AMI**). While
    AMIs are paid, they can run longer than Kaggle kernels. So, for big projects,
    it might be worth using an AMI instead of a kernel.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[https://aws.amazon.com/machine-learning/amis/](https://aws.amazon.com/machine-learning/amis/)了解如何设置**亚马逊机器镜像**（**AMI**）。虽然AMI是付费的，但它们的运行时间比Kaggle内核长。所以，对于大项目，使用AMI而不是内核可能更值得。
- en: To run the notebooks for this book on an AMI, first set up the AMI, then download
    the notebooks from GitHub, and then upload them to your AMI. You will have to
    download the data from Kaggle as well. See the *Using data locally* section for
    instructions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要在AMI上运行本书的笔记本，首先设置好AMI，然后从GitHub下载笔记本，之后将它们上传到你的AMI。你还需要从Kaggle下载数据。有关说明，请参见*本地使用数据*部分。
- en: Approximating functions
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逼近函数
- en: 'There are many views on how best to think about neural networks, but perhaps
    the most useful is to see them as function approximators. Functions in math relate
    some input, *x,* to some output, *y*. We can write it as the following formula:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 关于神经网络的最佳理解方式有很多种看法，但也许最有用的方式是将它们看作是函数逼近器。数学中的函数将某个输入，*x*，与某个输出，*y*，相关联。我们可以将其写成如下公式：
- en: '![Approximating functions](img/B10354_01_001.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![逼近函数](img/B10354_01_001.jpg)'
- en: 'A simple function could be like this:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的函数可能是这样的：
- en: '![Approximating functions](img/B10354_01_002.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![逼近函数](img/B10354_01_002.jpg)'
- en: 'In this case, we can give the function an input, *x*, and it would quadruple
    it:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以给函数一个输入，*x*，然后它将其四倍化：
- en: '![Approximating functions](img/B10354_01_003.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![逼近函数](img/B10354_01_003.jpg)'
- en: You might have seen functions like this in school, but functions can do more;
    as an example, they can map an element from a set (the collection of values the
    function accepts) to another element of a set. These sets can be something other
    than simple numbers.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在学校见过这样的函数，但函数能做的不止这些；举个例子，它们可以将一个集合中的元素（函数接受的值的集合）映射到另一个集合中的元素。这些集合可以是除了简单数字以外的其他内容。
- en: 'A function could, for example, also map an image to an identification of what
    is in the image:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个函数还可以将图像映射到图像中的识别内容：
- en: '![Approximating functions](img/B10354_01_004.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![逼近函数](img/B10354_01_004.jpg)'
- en: 'This function would map an image of a cat to the label "cat," as we can see
    in the following diagram:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将把一张猫的图片映射到标签“猫”，如下图所示：
- en: '![Approximating functions](img/B10354_01_06.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![逼近函数](img/B10354_01_06.jpg)'
- en: Mapping images to labels
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像映射到标签
- en: We should note that for a computer, images are matrices full of numbers and
    any description of an image's content would also be stored as a matrix of numbers.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要注意的是，对于计算机来说，图像是充满数字的矩阵，任何图像内容的描述也将以数字矩阵的形式存储。
- en: A neural network, if it is big enough, can approximate any function. It has
    been mathematically proven that an indefinitely large network could approximate
    every function. While we don't need to use an indefinitely large network, we are
    certainly using very large networks.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果神经网络足够大，它可以逼近任何函数。数学上已经证明，任何无限大的网络都可以逼近每一个函数。虽然我们不需要使用无限大的网络，但我们确实在使用非常大的网络。
- en: Modern deep learning architectures can have tens or even hundreds of layers
    and millions of parameters, so only storing the model already takes up a few gigabytes.
    This means that a neural network, if it's big enough, could also approximate our
    function, *f*, for mapping images to their content.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习架构可以有几十层甚至几百层，并且有数百万个参数，所以仅仅存储模型就需要几GB的空间。这意味着如果神经网络足够大，它也可以逼近我们的函数 *f*，用于将图像映射到其内容。
- en: The condition that the neural network has to be "big enough" explains why deep
    (big) neural networks have taken off. The fact that "big enough" neural networks
    can approximate any function means that they are useful for a large number of
    tasks.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络必须“足够大”的这个条件解释了为什么深度（大）神经网络得到了广泛应用。因为“足够大”的神经网络可以逼近任何函数，这意味着它们对于许多任务都非常有用。
- en: A forward pass
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一次前向传递
- en: Over the course of this book, we will build powerful neural networks that are
    able to approximate extremely complex functions. We will be mapping text to named
    entities, images to their content, and even news articles to their summaries.
    But for now, we will work with a simple problem that can be solved with logistic
    regression, a popular technique used in both economics and finance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的过程中，我们将构建强大的神经网络，能够逼近极其复杂的函数。我们将把文本映射到命名实体，将图像映射到其内容，甚至将新闻文章映射到其摘要。但现在，我们将处理一个可以用逻辑回归解决的简单问题，逻辑回归是一种在经济学和金融学中广泛使用的技术。
- en: We will be working with a simple problem. Given an input matrix, *X*, we want
    to output the first column of the matrix, *X[1]*. In this example, we will be
    approaching the problem from a mathematical perspective in order to gain some
    intuition for what is going on.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将处理一个简单的问题。给定一个输入矩阵 *X*，我们想要输出矩阵的第一列 *X[1]*。在这个例子中，我们将从数学的角度来解决这个问题，以便对发生的事情获得一些直观理解。
- en: 'Later on in this chapter, we will implement what we have described in Python.
    We already know that we need data to train a neural network, and so the data,
    seen here, will be our dataset for the exercise:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面，我们将用 Python 实现我们描述的内容。我们已经知道需要数据来训练神经网络，所以这里的数据显示了我们的练习数据集：
- en: '| X[1] | X[2] | X[3] | y |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| X[1] | X[2] | X[3] | y |'
- en: '| --- | --- | --- | --- |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 1 | 0 | 0 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 0 | 0 |'
- en: '| 1 | 0 | 0 | 1 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 1 |'
- en: '| 1 | 1 | 1 | 1 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 | 1 |'
- en: '| 0 | 1 | 1 | 0 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 1 | 0 |'
- en: In the dataset, each row contains an input vector, *X*, and an output, *y*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中，每一行包含一个输入向量 *X* 和一个输出 *y*。
- en: 'The data follows the formula:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 数据遵循以下公式：
- en: '![A forward pass](img/B10354_01_005.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![一次前向传递](img/B10354_01_005.jpg)'
- en: 'The function we want to approximate is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要逼近的函数如下：
- en: '![A forward pass](img/B10354_01_006.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![一次前向传递](img/B10354_01_006.jpg)'
- en: In this case, writing down the function is relatively straightforward. However,
    keep in mind that in most cases it is not possible to write down the function,
    as functions expressed by deep neural networks can become very complex.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，写下这个函数相对简单。然而，请记住，在大多数情况下，无法写出函数，因为深度神经网络表达的函数可能非常复杂。
- en: For this simple function, a shallow neural network with only one layer will
    be enough. Such shallow networks are also called logistic regressors.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个简单的函数，只有一层的浅层神经网络就足够了。这样的浅层网络也被称为逻辑回归器。
- en: A logistic regressor
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个逻辑回归器
- en: As we just explained, the simplest neural network is a logistic regressor. A
    logistic regression takes in values of any range but only outputs values between
    zero and one.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚才解释的，最简单的神经网络是逻辑回归器。逻辑回归接受任何范围的值，但只输出零和一之间的值。
- en: There is a wide range of applications where logistic regressors are suitable.
    One such example is to predict the likelihood of a homeowner defaulting on a mortgage.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归器适用于各种应用。其中一个例子是预测房主违约的可能性，特别是抵押贷款的违约。
- en: We might take all kinds of values into account when trying to predict the likelihood
    of someone defaulting on their payment, such as the debtor's salary, whether they
    have a car, the security of their job, and so on, but the likelihood will always
    be a value between zero and one. Even the worst debtor ever cannot have a default
    likelihood above 100%, and the best cannot go below 0%.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试预测某人违约的可能性时，我们可能会考虑各种因素，比如债务人的工资、是否拥有汽车、工作是否稳定等等，但违约的可能性始终是一个介于 0 和 1 之间的值。即使是最差的债务人，也不可能有超过
    100% 的违约可能性，最好的债务人也不可能低于 0%。
- en: The following diagram shows a logistic regressor. *X* is our input vector; here
    it's shown as three components, *X[1]*, *X[2]*, and *X[3]*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个逻辑回归模型。*X* 是我们的输入向量；这里它显示为三个分量，*X[1]*，*X[2]*，和 *X[3]*。
- en: '*W* is a vector of three weights. You can imagine it as the thickness of each
    of the three lines. *W* determines how much each of the values of *X* goes into
    the next layer. *b* is the bias, and it can move the output of the layer up or
    down:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*W* 是一个包含三个权重的向量。你可以把它想象成每一条线的粗细。*W* 决定了每个 *X* 值对下一层的影响。*b* 是偏差，它可以将层的输出上下移动：'
- en: '![A logistic regressor](img/B10354_01_07.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![一个逻辑回归模型](img/B10354_01_07.jpg)'
- en: Logistic regressor
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型
- en: To compute the output of the regressor, we must first do a **linear step**.
    We compute the dot product of the input, *X*, and the weights, *W*. This is the
    same as multiplying each value of *X* with its weight and then taking the sum.
    To this number, we then add the bias, *b*. Afterward, we do a **nonlinear step**.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算回归模型的输出，我们必须首先进行 **线性步骤**。我们计算输入 *X* 和权重 *W* 的点积。这相当于将每个 *X* 的值与它的权重相乘，然后求和。接着，我们在这个数值上加上偏差
    *b*。之后，我们再进行 **非线性步骤**。
- en: 'In the nonlinear step, we run the linear intermediate product, *z*, through
    an **activation function**; in this case, the sigmoid function. The sigmoid function
    squishes the input values to outputs between zero and one:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在非线性步骤中，我们将线性中间结果 *z* 通过一个 **激活函数**，在这种情况下是 Sigmoid 函数。Sigmoid 函数将输入值压缩到 0 和
    1 之间的输出：
- en: '![A logistic regressor](img/B10354_01_08.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![一个逻辑回归模型](img/B10354_01_08.jpg)'
- en: The Sigmoid function
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数
- en: Python version of our logistic regressor
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的 Python 版本的逻辑回归模型
- en: If all the preceding math was a bit too theoretical for you, rejoice! We will
    now implement the same thing, but this time with Python. In our example, we will
    be using a library called NumPy, which enables easy and fast matrix operations
    within Python.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果之前的数学有点过于理论化，别担心！现在我们将实现相同的内容，不过这次使用 Python。在我们的示例中，我们将使用一个名为 NumPy 的库，它可以让我们在
    Python 中轻松快速地进行矩阵运算。
- en: 'NumPy comes preinstalled with Anaconda and on Kaggle kernels. To ensure we
    get the same result in all of our experiments, we have to set a random seed. We
    can do this by running the following code:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 已经预安装在 Anaconda 和 Kaggle 内核中。为了确保我们在所有实验中得到相同的结果，我们需要设置一个随机种子。我们可以通过运行以下代码来实现：
- en: '[PRE3]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Since our dataset is quite small, we''ll define it manually as NumPy matrices,
    as we can see here:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集相当小，我们将手动定义它为 NumPy 矩阵，如下所示：
- en: '[PRE4]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can define the sigmoid, which squishes all the values into values between
    0 and 1, through an activation function as a Python function:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个 Python 函数定义 sigmoid 函数，它将所有值压缩到 0 和 1 之间，作为激活函数：
- en: '[PRE5]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So far, so good. We now need to initialize *W*. In this case, we actually know
    already what values *W* should have. But we cannot know about other problems where
    we do not know the function yet. So, we need to assign weights randomly.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，进展顺利。接下来我们需要初始化 *W*。在这种情况下，我们实际上已经知道 *W* 应该是什么值。但在其他问题中，我们还不知道函数时，不能直接知道。所以，我们需要随机分配权重。
- en: The weights are usually assigned randomly with a mean of zero, and the bias
    is usually set to zero by default. NumPy's `random` function expects to receive
    the shape of the random matrix to be passed on as a tuple, so `random((3,1))`
    creates a 3x1 matrix. By default, the random values generated are between 0 and
    1, with a mean of 0.5 and a standard deviation of 0.5.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 权重通常是随机分配的，平均值为零，偏差通常默认为零。NumPy 的 `random` 函数期望接收一个元组作为参数，表示随机矩阵的形状，因此 `random((3,1))`
    创建一个 3x1 的矩阵。默认情况下，生成的随机值介于 0 和 1 之间，均值为 0.5，标准差为 0.5。
- en: 'We want the random values to have a mean of 0 and a standard deviation of 1,
    so we first multiply the values generated by 2 and then subtract 1\. We can achieve
    this by running the following code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望随机值的均值为 0，标准差为 1，因此我们首先将生成的值乘以 2，再减去 1。我们可以通过运行以下代码来实现：
- en: '[PRE6]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With that done, all the variables are set. We can now move on to do the linear
    step, which is achieved with the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，所有变量都已设置好。现在我们可以进行线性步骤，方法如下：
- en: '[PRE7]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we can do the nonlinear step, which is run with the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以进行非线性步骤，方法如下：
- en: '[PRE8]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, if we print out `A`, we''ll get the following output:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们打印出`A`，我们将得到以下输出：
- en: '[PRE9]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: But wait! This output looks nothing like our desired output, *y*, at all! Clearly,
    our regressor is representing *some* function, but it's quite far away from the
    function we want.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！这个输出与我们期望的输出*y*一点也不像！显然，我们的回归模型表示的是*某个*函数，但它离我们想要的函数还很远。
- en: To better approximate our desired function, we have to tweak the weights, *W*,
    and the bias, *b*. To this end, in the next section, we will optimize the model
    parameters.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地近似我们期望的函数，我们必须调整权重*W*和偏置*b*。为此，在接下来的部分中，我们将优化模型参数。
- en: Optimizing model parameters
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化模型参数
- en: We've already seen that we need to tweak the weights and biases, collectively
    called the parameters, of our model in order to arrive at a closer approximation
    of our desired function.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，为了更接近我们期望的函数，我们需要调整模型的权重和偏置，统称为模型参数。
- en: In other words, we need to look through the space of possible functions that
    can be represented by our model in order to find a function, ![Optimizing model
    parameters](img/B10354_01_007.jpg), that matches our desired function, *f*, as
    closely as possible.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们需要在可能由我们的模型表示的函数空间中寻找一个函数，![优化模型参数](img/B10354_01_007.jpg)，使得它与我们期望的函数*f*尽可能接近。
- en: But how would we know how close we are? In fact, since we don't know *f*, we
    cannot directly know how close our hypothesis, ![Optimizing model parameters](img/B10354_01_008.jpg),
    is to *f*. But what we can do is measure how well ![Optimizing model parameters](img/B10354_01_009.jpg)'s
    outputs match the output of *f*. The expected outputs of *f* given *X* are the
    labels, *y*. So, we can try to approximate *f* by finding a function, ![Optimizing
    model parameters](img/B10354_01_010.jpg), whose outputs are also *y* given *X*.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们怎么知道自己有多接近呢？事实上，由于我们不知道*f*，我们无法直接知道我们的假设，![优化模型参数](img/B10354_01_008.jpg)，与*f*有多接近。但我们可以做的是衡量![优化模型参数](img/B10354_01_009.jpg)的输出与*f*的输出之间的匹配程度。给定*X*，*f*的期望输出是标签*y*。所以，我们可以通过找到一个函数，![优化模型参数](img/B10354_01_010.jpg)，来近似*f*，使得它给定*X*的输出也为*y*。
- en: 'We know that the following is true:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道以下结论是正确的：
- en: '![Optimizing model parameters](img/B10354_01_011.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![优化模型参数](img/B10354_01_011.jpg)'
- en: 'We also know that:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还知道：
- en: '![Optimizing model parameters](img/B10354_01_012.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![优化模型参数](img/B10354_01_012.jpg)'
- en: 'We can try to find *f* by optimizing using the following formula:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用以下公式进行优化来尝试找到*f*：
- en: '![Optimizing model parameters](img/B10354_01_013.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![优化模型参数](img/B10354_01_013.jpg)'
- en: Within this formula, ![Optimizing model parameters](img/B10354_01_029.jpg) is
    the space of functions that can be represented by our model, also called the hypothesis
    space, while *D* is the distance function, which we use to evaluate how close
    ![Optimizing model parameters](img/B10354_01_014.jpg) and *y* are.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，![优化模型参数](img/B10354_01_029.jpg)是可以由我们的模型表示的函数空间，也称为假设空间，而*D*是距离函数，我们用它来评估![优化模型参数](img/B10354_01_014.jpg)和*y*之间的距离。
- en: Note
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: This approach makes a crucial assumption that our data, *X*, and
    labels, *y*, represent our desired function, *f*. This is not always the case.
    When our data contains systematic biases, we might gain a function that fits our
    data well but is different from the one we wanted.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：这种方法做出了一个关键假设，即我们的数据*X*和标签*y*代表了我们期望的函数*f*。但这并不总是成立。当我们的数据包含系统性的偏差时，我们可能会得到一个很好地拟合数据的函数，但它与我们想要的函数不同。'
- en: An example of optimizing model parameters comes from human resource management.
    Imagine you are trying to build a model that predicts the likelihood of a debtor
    defaulting on their loan, with the intention of using this to decide who should
    get a loan.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 优化模型参数的一个例子来自人力资源管理。假设你正在尝试构建一个预测债务人违约风险的模型，目的是决定谁应该获得贷款。
- en: As training data, you can use loan decisions made by human bank managers over
    the years. However, this presents a problem as these managers might be biased.
    For instance, minorities, such as black people, have historically had a harder
    time getting a loan.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 作为训练数据，你可以使用多年来人类银行经理做出的贷款决策。然而，这也带来了一个问题，因为这些经理可能存在偏见。例如，少数族裔（如黑人）在历史上更难获得贷款。
- en: With that being said, if we used that training data, our function would also
    present that bias. You'd end up with a function mirroring or even amplifying human
    biases, rather than creating a function that is good at predicting who is a good
    debtor.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，如果我们使用这些训练数据，我们的函数也会呈现出相同的偏差。最终，你将得到一个反映甚至放大人类偏见的函数，而不是创建一个擅长预测谁是好债务人的函数。
- en: It is a commonly made mistake to believe that a neural network will find the
    intuitive function that we are looking for. It'll actually find the function that
    best fits the data with no regard for whether that is the desired function or
    not.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的错误是认为神经网络会找到我们所寻找的直观函数。实际上，它会找到最适合数据的函数，而不管该函数是否为我们期望的函数。
- en: Measuring model loss
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 衡量模型损失
- en: We saw earlier how we could optimize parameters by minimizing some distance
    function, *D*. This distance function, also called the loss function, is the performance
    measure by which we evaluate possible functions. In machine learning, a loss function
    measures how bad the model performs. A high loss function goes hand in hand with
    low accuracy, whereas if the function is low, then the model is doing well.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经看到如何通过最小化某个距离函数*D*来优化参数。这个距离函数，也叫做损失函数，是我们评估可能函数的性能标准。在机器学习中，损失函数衡量模型的表现如何。损失函数高意味着准确率低，而如果损失函数低，则表示模型表现良好。
- en: 'In this case, our issue is a binary classification problem. Because of that,
    we will be using the binary cross-entropy loss, as we can see in the following
    formula:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的问题是一个二分类问题。由于这一点，我们将使用二进制交叉熵损失，正如以下公式所示：
- en: '![Measuring model loss](img/B10354_01_015.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![衡量模型损失](img/B10354_01_015.jpg)'
- en: 'Let''s go through this formula step by step:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地分析这个公式：
- en: '*D[BCE]*: This is the distance function for binary cross entropy loss.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*D[BCE]*：这是二进制交叉熵损失的距离函数。'
- en: '![Measuring model loss](img/B10354_01_016.jpg): The loss over a batch of *N*
    examples is the average loss of all examples.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![衡量模型损失](img/B10354_01_016.jpg)：一批*N*个样本的损失是所有样本平均损失。'
- en: '![Measuring model loss](img/B10354_01_017.jpg): This part of the loss only
    comes into play if the true value, *y[i]* is 1\. If *y[i]* is 1, we want ![Measuring
    model loss](img/B10354_01_018.jpg) to be as close to 1 as possible, so we can
    achieve a low loss.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![衡量模型损失](img/B10354_01_017.jpg)：只有当真实值*y[i]*为1时，这部分损失才会起作用。如果*y[i]*为1，我们希望![衡量模型损失](img/B10354_01_018.jpg)尽可能接近1，以便我们能够实现较低的损失。'
- en: '![Measuring model loss](img/B10354_01_019.jpg): This part of the loss comes
    into play if *y[i]*, is 0\. If so, we want ![Measuring model loss](img/B10354_01_020.jpg)
    to be close to 0 as well.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![衡量模型损失](img/B10354_01_019.jpg)：当*y[i]*为0时，这部分损失才会起作用。如果是这样，我们希望![衡量模型损失](img/B10354_01_020.jpg)也尽量接近0。'
- en: 'In Python, this loss function is implemented as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，这个损失函数的实现如下：
- en: '[PRE11]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output, `A,` of our logistic regressor is equal to ![Measuring model loss](img/B10354_01_021.jpg),
    so we can calculate the binary cross-entropy loss as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的逻辑回归器的输出`A`等于![衡量模型损失](img/B10354_01_021.jpg)，所以我们可以按照以下方式计算二进制交叉熵损失：
- en: '[PRE12]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As we can see, this is quite a high loss, so we should now look at seeing how
    we can improve our model. The goal here is to bring this loss to zero, or at least
    to get closer to zero.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这是一个相当高的损失，因此我们现在应该看看如何改进我们的模型。这里的目标是将损失降到零，或者至少接近零。
- en: You can think of losses with respect to different function hypotheses as a surface,
    sometimes also called the "loss surface." The loss surface is a lot like a mountain
    range, as we have high points on the mountain tops and low points in valleys.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将与不同函数假设相关的损失看作一个表面，这个表面有时也被称为“损失面”。损失面就像一座山脉，山顶上是高点，山谷中是低点。
- en: 'Our goal is to find the absolute lowest point in the mountain range: the deepest
    valley, or the "global minimum." A global minimum is a point in the function hypothesis
    space at which the loss is at the lowest point.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到山脉中最低的点：最深的谷底，或者称为“全局最小值”。全局最小值是函数假设空间中的一个点，在该点损失达到最低。
- en: A "local minimum," by contrast, is the point at which the loss is lower than
    in the immediately surrounding space. Local minima are problematic because while
    they might seem like a good function to use at face value, there are much better
    functions available. Keep this in mind as we now walk through gradient descent,
    a method for finding a minimum in our function space.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，“局部最小值”是损失比周围空间中的值更低的点。局部最小值是问题所在，因为它们看起来可能是一个不错的函数，但实际上还有更好的函数。请记住这一点，现在我们将一起走过梯度下降，它是一种在函数空间中找到最小值的方法。
- en: Gradient descent
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Now that we know what we judge our candidate models, ![Gradient descent](img/B10354_01_022.jpg),
    by, how do we tweak the parameters to obtain better models? The most popular optimization
    algorithm for neural networks is called gradient descent. Within this method,
    we slowly move along the slope, the derivative, of the loss function.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何评判我们的候选模型，[梯度下降](img/B10354_01_022.jpg)方法，那么我们如何调整参数以获得更好的模型呢？神经网络中最流行的优化算法叫做梯度下降。在这种方法中，我们慢慢沿着损失函数的坡度，也就是导数，移动。
- en: Imagine you are in a mountain forest on a hike, and you're at a point where
    you've lost the track and are now in the woods trying to find the bottom of the
    valley. The problem here is that because there are so many trees, you cannot see
    the valley's bottom, only the ground under your feet.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你正在山林中徒步旅行，来到了一个迷失方向的地方，现在你正试图穿越树林找到山谷的底部。这里的问题是，由于树木太多，你无法看到山谷的底部，只能看到脚下的地面。
- en: 'Now ask yourself this: how would you find your way down? One sensible approach
    would be to follow the slope, and where the slope goes downwards, you go. This
    is the same approach that is taken by a gradient descent algorithm.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在问问自己：你如何找到下山的路呢？一种合理的做法是沿着坡度前进，坡度向下的地方，你就走向哪里。这就是梯度下降算法所采用的相同方法。
- en: To bring it back to our focus, in this forest situation the loss function is
    the mountain, and to get to a low loss, the algorithm follows the slope, that
    is, the derivative, of the loss function. When we walk down the mountain, we are
    updating our location coordinates.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们之前的比喻，在这个森林的情境中，损失函数就像是山，而为了降低损失，算法沿着损失函数的坡度，也就是导数，前进。当我们走下山时，我们正在更新我们的坐标位置。
- en: 'The algorithm updates the parameters of the neural network, as we are seeing
    in the following diagram:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法更新神经网络的参数，正如我们在下面的图示中看到的那样：
- en: '![Gradient descent](img/B10354_01_09.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降](img/B10354_01_09.jpg)'
- en: Gradient descent
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Gradient descent requires that the loss function has a derivative with respect
    to the parameters that we want to optimize. This will work well for most supervised
    learning problems, but things become more difficult when we want to tackle problems
    for which there is no obvious derivative.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降要求损失函数相对于我们想要优化的参数有导数。这在大多数监督学习问题中效果很好，但当我们想解决没有明显导数的问题时，情况就变得更加复杂。
- en: Gradient descent can also only optimize the parameters, weights, and biases
    of our model. What it cannot do is optimize how many layers our model has or which
    activation functions it should use, since there is no way to compute the gradient
    with respect to model topology.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降只能优化模型的参数、权重和偏差。它做不到的是优化我们的模型有多少层，或者它应该使用哪些激活函数，因为无法计算关于模型拓扑的梯度。
- en: These settings, which cannot be optimized by gradient descent, are called **hyperparameters**
    and are usually set by humans. You just saw how we gradually scale down the loss
    function, but how do we update the parameters? To this end, we're going to need
    another method called backpropagation.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置是无法通过梯度下降优化的，它们被称为**超参数**，通常由人类来设置。你刚刚看到我们如何逐渐缩小损失函数的值，但我们如何更新参数呢？为此，我们需要另一种方法，叫做反向传播。
- en: Backpropagation
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: Backpropagation allows us to apply gradient descent updates to the parameters
    of a model. To update the parameters, we need to calculate the derivative of the
    loss function with respect to the weights and biases.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播使我们能够对模型的参数应用梯度下降更新。为了更新这些参数，我们需要计算损失函数相对于权重和偏差的导数。
- en: If you imagine the parameters of our models are like the geo-coordinates in
    our mountain forest analogy, calculating the loss derivative with respect to a
    parameter is like checking the mountain slope in the direction north to see whether
    you should go north or south.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你把我们模型中的参数想象成我们在山林类比中的地理坐标，那么计算损失函数相对于某个参数的导数就像是在检查朝北的山坡，看看是否应该朝北还是朝南走。
- en: 'The following diagram shows the forward and backward pass through a logistic
    regressor:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了逻辑回归模型的前向传播和反向传播过程：
- en: '![Backpropagation](img/B10354_01_10.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/B10354_01_10.jpg)'
- en: Forward and backward pass through a logistic regressor
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型的前向传播和反向传播
- en: To keep things simple, we refer to the derivative of the loss function to any
    variable as the *d* variable. For example, we'll write the derivative of the loss
    function with respect to the weights as *dW*.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们将损失函数对任何变量的导数称为*d*变量。例如，我们将损失函数相对于权重的导数写作*dW*。
- en: 'To calculate the gradient with respect to different parameters of our model,
    we can make use of the chain rule. You might remember the chain rule as the following:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算模型中不同参数的梯度，我们可以利用链式法则。你可能还记得链式法则是这样的：
- en: '![Backpropagation](img/B10354_01_023.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/B10354_01_023.jpg)'
- en: 'This is also sometimes written as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这有时也可以写作如下：
- en: '![Backpropagation](img/B10354_01_024.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/B10354_01_024.jpg)'
- en: The chain rule basically says that if you want to take the derivative through
    a number of nested functions, you multiply the derivative of the inner function
    with the derivative of the outer function.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则基本上说的是，如果你想通过多个嵌套函数来求导，你需要将内层函数的导数与外层函数的导数相乘。
- en: This is useful because neural networks, and our logistic regressor, are nested
    functions. The input goes through the linear step, a function of input, weights,
    and biases; and the output of the linear step, *z*, goes through the activation
    function.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有用，因为神经网络以及我们的逻辑回归模型都是嵌套函数。输入通过线性步骤，这是输入、权重和偏置的一个函数；然后线性步骤的输出*z*再经过激活函数。
- en: 'So, when we compute the loss derivative with respect to weights and biases,
    we''ll first compute the loss derivative with respect to the output of the linear
    step, *z*, and use it to compute *dW*. Within the code, it looks like this:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们计算损失函数相对于权重和偏置的导数时，我们首先计算损失函数相对于线性步骤的输出*z*的导数，并使用这个结果来计算*dW*。在代码中，它看起来是这样的：
- en: '[PRE14]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameter updates
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数更新
- en: Now we have the gradients, how do we improve our model? Going back to our mountain
    analogy, now that we know that the mountain goes up in the north and east directions,
    where do we go? To the south and west, of course!
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了梯度，如何改进我们的模型？回到我们的山脉类比，既然我们知道山脉在北方和东方上升，我们该往哪里走呢？当然是朝南和朝西了！
- en: Mathematically speaking, we go in the opposite direction to the gradient. If
    the gradient is positive with respect to a parameter, that is, the slope is upward,
    then we reduce the parameter. If it is negative, that is, downward sloping, we
    increase it. When our slope is steeper, we move our gradient more.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来说，我们是沿着梯度的反方向走。如果梯度相对于某个参数是正的，也就是说，坡度是向上的，那么我们就减少该参数。如果梯度是负的，也就是说，坡度是向下的，我们就增加它。当坡度变陡时，我们的梯度也会变大。
- en: 'The update rule for a parameter, *p*, then goes like this:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 参数*p*的更新规则如下：
- en: '![Parameter updates](img/B10354_01_025.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参数更新](img/B10354_01_025.jpg)'
- en: Here *p* is a model parameter (either in weight or a bias), *dp* is the loss
    derivative with respect to *p*, and ![Parameter updates](img/B10354_01_026.jpg)
    is the learning rate.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*p*是一个模型参数（可以是权重或偏置），*dp*是损失函数相对于*p*的导数，而![参数更新](img/B10354_01_026.jpg)是学习率。
- en: The learning rate is something akin to the gas pedal within a car. It sets by
    how much we want to apply the gradient updates. It is one of those hyperparameters
    that we have to set manually, and something we will discuss in the next chapter.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率有点像汽车中的油门。它决定了我们希望应用梯度更新的幅度。它是一个需要手动设置的超参数，我们将在下一章讨论它。
- en: 'Within the code, our parameter updates look like this:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们的参数更新看起来是这样的：
- en: '[PRE15]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Putting it all together
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容汇总起来
- en: Well done! We've now looked at all the parts that are needed in order to train
    a neural network. Over the next few steps in this section, we will be training
    a one-layer neural network, which is also called a logistic regressor.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！我们现在已经查看了训练神经网络所需的所有部分。在本节的接下来的几步中，我们将训练一个单层神经网络，这也叫做逻辑回归器。
- en: 'Firstly, we''ll import numpy before we define the data. We can do this by running
    the following code:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将在定义数据之前导入 numpy。我们可以通过运行以下代码来完成这一步：
- en: '[PRE16]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The next step is for us to define the sigmoid activation function and loss
    function, which we can do with the following code:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义 sigmoid 激活函数和损失函数，我们可以通过以下代码来实现：
- en: '[PRE17]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We''ll then randomly initialize our model, which we can achieve with the following code:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将随机初始化我们的模型，可以通过以下代码实现：
- en: '[PRE18]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As part of this process, we also need to set some hyperparameters. The first
    one is alpha, which we will just set to `1` here. Alpha is best understood as
    the step size. A large alpha means that while our model will train quickly, it
    might also overshoot the target. A small alpha, in comparison, allows gradient
    descent to tread more carefully and find small valleys it would otherwise shoot
    over.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们还需要设置一些超参数。第一个是 alpha，这里我们将其设置为 `1`。Alpha 最好理解为步长。较大的 alpha 意味着虽然我们的模型训练得更快，但也可能会超过目标。相反，较小的
    alpha 允许梯度下降更小心地前进，找到它本来会跳过的小谷。
- en: 'The second one is the number of times we want to run the training process,
    also called the number of epochs we want to run. We can set the parameters with
    the following code:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数是我们希望训练过程运行的次数，也叫做我们想要运行的周期数。我们可以通过以下代码来设置这些参数：
- en: '[PRE19]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Since it is used in the training loop, it''s also useful to define the number
    of samples in our data. We''ll also define an empty array in order to keep track
    of the model''s losses over time. To achieve this, we simply run the following:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它在训练循环中使用，因此定义我们数据中的样本数量也很重要。我们还将定义一个空数组，以便跟踪模型的损失情况。为了实现这一点，我们只需要运行以下代码：
- en: '[PRE20]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we come to the main training loop:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进入主要的训练循环：
- en: '[PRE21]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As a result of running the previous code, we would get the following output:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 运行之前的代码后，我们将得到以下输出：
- en: '[PRE22]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You can see that over the course of the output, the loss steadily decreases,
    starting at `0.822322582088` and ending at `0.203407060401`.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在输出过程中，损失逐步减少，从 `0.822322582088` 开始，最终减少到 `0.203407060401`。
- en: 'We can plot the loss to a graph in order to give us a better look at it. To
    do this, we can simply run the following code:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将损失绘制成图表，以便更好地查看它。为此，我们可以简单地运行以下代码：
- en: '[PRE23]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This will then output the following chart:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这将输出以下图表：
- en: '![Putting it all together](img/B10354_01_11.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![将一切整合在一起](img/B10354_01_11.jpg)'
- en: The output of the previous code, showing loss rate improving over time
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 之前代码的输出，显示损失率随时间改善
- en: A deeper network
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更深的网络
- en: We established earlier in this chapter that in order to approximate more complex
    functions, we need bigger and deeper networks. Creating a deeper network works
    by stacking layers on top of each other.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章前面已经确定，为了近似更复杂的函数，我们需要更大更深的网络。创建一个更深的网络是通过将层堆叠在一起实现的。
- en: 'In this section, we will build a two-layer neural network like the one seen
    in the following diagram:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将构建一个两层神经网络，就像以下图所示的那样：
- en: '![A deeper network](img/B10354_01_12.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![更深的网络](img/B10354_01_12.jpg)'
- en: Sketch of a two-layer neural network
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 两层神经网络的示意图
- en: The input gets multiplied with the first set of weights, *W[1]*, producing an
    intermediate product, *z[1]*. This is then run through an activation function,
    which will produce the first layer's activations, *A[1]*.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 输入与第一组权重 *W[1]* 相乘，产生一个中间结果 *z[1]*。然后通过一个激活函数，生成第一层的激活值 *A[1]*。
- en: 'These activations then get multiplied with the second layer of weights, *W[2]*,
    producing an intermediate product, *z[2]*. This gets run through a second activation
    function, which produces the output, *A[2]*, of our neural network:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这些激活值随后会与第二层权重 *W[2]* 相乘，产生一个中间结果 *z[2]*。这个结果会通过第二个激活函数，生成我们神经网络的输出 *A[2]*：
- en: '[PRE24]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: The full code for this example can be found in the GitHub repository
    belonging to this book.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：此示例的完整代码可以在本书的 GitHub 仓库中找到。'
- en: 'As you can see, the first activation function is not a sigmoid function but
    is actually a tanh function. Tanh is a popular activation function for hidden
    layers and works a lot like sigmoid, except that it squishes values in the range
    between -1 and 1 rather than 0 and 1:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，第一个激活函数不是sigmoid函数，而实际上是tanh函数。Tanh是一个常用于隐藏层的激活函数，工作原理与sigmoid类似，唯一的区别是它将值压缩到-1到1的范围内，而不是0到1：
- en: '![A deeper network](img/B10354_01_13.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![更深的网络](img/B10354_01_13.jpg)'
- en: The tanh function
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: tanh函数
- en: 'Backpropagation through our deeper network works by the chain rule, too. We
    go back through the network and multiply the derivatives:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的更深的网络进行反向传播同样使用链式法则。我们回到网络中并乘以导数：
- en: '![A deeper network](img/B10354_01_14.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![更深的网络](img/B10354_01_14.jpg)'
- en: Forward and backward pass through a two-layer neural network
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 通过两层神经网络的前向和反向传播
- en: 'The preceding equations can be expressed as the following Python code:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程可以通过以下Python代码表示：
- en: '[PRE25]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that while the size of the inputs and outputs are determined by your problem,
    you can freely choose the size of your hidden layer. The hidden layer is another
    hyperparameter you can tweak. The bigger the hidden layer size, the more complex
    the function you can approximate. However, the flip side of this is that the model
    might overfit. That is, it may develop a complex function that fits the noise
    but not the true relationship in the data.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然输入和输出的大小由你的问题决定，但你可以自由选择隐藏层的大小。隐藏层是另一个你可以调整的超参数。隐藏层单元数越大，你可以逼近的函数就越复杂。然而，这也有一个反面，即模型可能会过拟合。也就是说，模型可能会形成一个复杂的函数，拟合噪音而不是数据中的真实关系。
- en: 'Take a look at the following chart. What we see here is the two moons dataset
    that could be clearly separated, but right now there is a lot of noise, which
    makes the separation hard to see even for humans. You can find the full code for
    the two-layer neural network as well as for the generation of these samples in
    the Chapter 1 GitHub repo:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下图表。我们看到的是两个半月数据集，原本可以明显分开，但现在有很多噪音，这使得即便是人类也很难看到分隔。你可以在第1章的GitHub仓库中找到两层神经网络的完整代码以及这些样本的生成代码：
- en: '![A deeper network](img/B10354_01_15.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![更深的网络](img/B10354_01_15.jpg)'
- en: The two moons dataset
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 两个半月数据集
- en: 'The following diagram shows a visualization of the decision boundary, that
    is, the line at which the model separates the two classes, using a hidden layer
    size of 1:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了决策边界的可视化，即模型分隔两个类别的那条线，使用的是1个隐藏层单元：
- en: '![A deeper network](img/B10354_01_16.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![更深的网络](img/B10354_01_16.jpg)'
- en: Decision boundary for hidden layer size 1
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层单元数为1的决策边界
- en: 'As you can see, the network does not capture the true relationship of the data.
    This is because it''s too simplistic. In the following diagram, you will see the
    decision boundary for a network with a hidden layer size of 500:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，网络并没有捕捉到数据的真实关系。这是因为它过于简化。在以下的图示中，你将看到一个具有500个隐藏层单元的网络的决策边界：
- en: '![A deeper network](img/B10354_01_17.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![更深的网络](img/B10354_01_17.jpg)'
- en: Decision boundary for hidden layer size 500
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层单元数为500的决策边界
- en: This model clearly fits the noise, but not the moons. In this case, the right
    hidden layer size is about 3.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型显然拟合了噪音，但并没有拟合两个半月的数据。在这种情况下，合适的隐藏层单元数大约是3。
- en: Finding the right size and the right number of hidden layers is a crucial part
    of designing effective learning models. Building models with NumPy is a bit clumsy
    and can be very easy to get wrong. Luckily, there is a much faster and easier
    tool for building neural networks, called Keras.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 找到合适的隐藏层单元数和隐藏层的数量是设计有效学习模型的关键部分。使用NumPy构建模型有点笨重，而且很容易出错。幸运的是，现在有一个更快速、更简单的工具来构建神经网络——Keras。
- en: A brief introduction to Keras
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras简介
- en: Keras is a high-level neural network API that can run on top of TensorFlow,
    a library for dataflow programming. What this means is that it can run the operations
    needed for a neural network in a highly optimized way. Therefore, it's much faster
    and easier to use than TensorFlow. Because Keras acts as an interface to TensorFlow,
    it makes it easier to build even more complex neural networks. Throughout the
    rest of the book, we will be working with the Keras library in order to build
    our neural networks.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是一个高层次的神经网络API，可以在TensorFlow之上运行，TensorFlow是一个用于数据流编程的库。这意味着它可以以高度优化的方式运行神经网络所需的操作。因此，Keras比TensorFlow更快且更易于使用。因为Keras作为TensorFlow的接口，它使得构建更复杂的神经网络变得更加简单。在本书的其余部分，我们将使用Keras库来构建我们的神经网络。
- en: Importing Keras
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入 Keras
- en: 'When importing Keras, we usually just import the modules we will use. In this
    case, we need two types of layers:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 Keras 时，我们通常只导入我们会使用的模块。在这个例子中，我们需要两种类型的层：
- en: The `Dense` layer is the plain layer that we have gotten to know in this chapter
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dense` 层是我们在本章中了解的基础层'
- en: The `Activation` layer allows us to add an activation function
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Activation` 层允许我们添加激活函数'
- en: 'We can import them simply by running the following code:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下代码轻松导入它们：
- en: '[PRE26]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Keras offers two ways to build models, through the sequential and the functional
    APIs. Because the sequential API is easier to use and allows more rapid model
    building, we will be using it for most of the book. However, in later chapters,
    we will take a look at the functional API as well.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了两种构建模型的方式，通过顺序和功能性 API。由于顺序 API 更易于使用并且能更快速地构建模型，我们将在本书的大部分内容中使用它。不过，在后面的章节中，我们也会看一下功能性
    API。
- en: 'We can access the sequential API through this code:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码访问顺序 API：
- en: '[PRE27]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: A two-layer model in Keras
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras 中的两层模型
- en: Building a neural network in the sequential API works as follows.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在顺序 API 中构建神经网络的过程如下：
- en: Stacking layers
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆叠层
- en: 'Firstly, we create an empty sequential model with no layers:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个没有层的空顺序模型：
- en: '[PRE28]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Then we can add layers to this model, just like stacking a layer cake, with
    `model.add()`.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以像堆叠蛋糕一样向这个模型添加层，使用 `model.add()`。
- en: 'For the first layer, we have to specify the input dimensions of the layer.
    In our case, the data has two features, the coordinates of the point. We can add
    a hidden layer of size 3 with the following code:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一层，我们需要指定层的输入维度。在我们的例子中，数据有两个特征，即点的坐标。我们可以使用以下代码添加一个大小为 3 的隐藏层：
- en: '[PRE29]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note how we nest the functions inside `model.add()`. We specify the `Dense`
    layer, and the positional argument is the size of the layer. This `Dense` layer
    now only does the linear step.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何将函数嵌套在 `model.add()` 中。我们指定了 `Dense` 层，而位置参数是该层的大小。这个 `Dense` 层现在只执行线性步骤。
- en: 'To add a `tanh` activation function, we call the following:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 若要添加 `tanh` 激活函数，我们调用以下代码：
- en: '[PRE30]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, we add the linear step and the activation function of the output layer
    in the same way, by calling up:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们通过调用以下代码，按相同方式添加输出层的线性步骤和激活函数：
- en: '[PRE31]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then to get an overview of all the layers we now have in our model, we can
    use the following command:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了获得当前模型中所有层的概览，我们可以使用以下命令：
- en: '[PRE32]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This yields the following overview of the model:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下模型概览：
- en: '[PRE33]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You can see the layers listed nicely, including their output shape and the number
    of parameters the layer has. `None`, located within the output shape, means that
    the layer has no fixed input size in that dimension and will accept whatever we
    feed it. In our case, it means the layer will accept any number of samples.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到层按顺序列出，包括它们的输出形状和层的参数数量。输出形状中的 `None` 表示该层在这一维度上没有固定的输入大小，将接受我们输入的任何数据。在我们的案例中，这意味着该层将接受任意数量的样本。
- en: In pretty much every network, you will see that the input dimension on the first
    dimension is variable like this in order to accommodate the different amounts
    of samples.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在几乎所有的网络中，你会看到第一维的输入维度是可变的，像这样可以容纳不同数量的样本。
- en: Compiling the model
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编译模型
- en: Before we can start training the model, we have to specify how exactly we want
    to train the model; and, more importantly, we need to specify which optimizer
    and which loss function we want to use.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练模型之前，我们必须明确指定如何训练模型；更重要的是，我们需要指定要使用的优化器和损失函数。
- en: The simple optimizer we have used so far is called the **Stochastic Gradient
    Descent**, or **SGD**. To look at more optimizers, see [Chapter 2](ch02.xhtml
    "Chapter 2. Applying Machine Learning to Structured Data"), *Applying Machine
    Learning to Structured Data*.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止使用的简单优化器叫做**随机梯度下降**，或称**SGD**。要查看更多优化器，请参见[第2章](ch02.xhtml "第2章. 将机器学习应用于结构化数据")，*将机器学习应用于结构化数据*。
- en: 'The loss function we use for this binary classification problem is called binary
    cross-entropy. We can also specify what metrics we want to track during training.
    In our case, accuracy, or just `acc` to keep it short, would be interesting to
    track:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于这个二分类问题的损失函数叫做二元交叉熵。我们还可以指定在训练过程中要跟踪哪些指标。在我们的案例中，准确度，简称 `acc`，将是我们感兴趣的跟踪指标：
- en: '[PRE34]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Training the model
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Now we are ready to run the training process, which we can do with the following
    line:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好运行训练过程，可以通过以下命令来完成：
- en: '[PRE35]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This will train the model for 900 iterations, which are also referred to as
    epochs. The output should look similar to this:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这将训练模型 900 次迭代，也称为训练轮次。输出应类似于以下内容：
- en: '[PRE36]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The full output of the training process has been truncated in the middle, this
    is to save space in the book, but you can see that the loss goes continuously
    down while accuracy goes up. In other words, success!
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程的完整输出已被截断中间，目的是节省书中的空间，但你可以看到损失值持续下降，而准确度持续上升。换句话说，成功了！
- en: 'Over the course of this book, we will be adding more bells and whistles to
    these methods. But at this moment, we have a pretty solid understanding of the
    theory of deep learning. We are just missing one building block: how does Keras
    actually work under the hood? What is TensorFlow? And why does deep learning work
    faster on a GPU?'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的过程中，我们将不断为这些方法添加更多功能。但此时，我们已经对深度学习的理论有了相当扎实的理解。我们只缺少一个构建模块：Keras 到底是如何在后台工作的？什么是
    TensorFlow？为什么深度学习在 GPU 上运算更快？
- en: We will be answering these questions in the next, and final, section of this
    chapter.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的下一节也是最后一节中回答这些问题。
- en: Keras and TensorFlow
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras 和 TensorFlow
- en: Keras is a high-level library and can be used as a simplified interface to TensorFlow.
    That means Keras does not do any computations by itself; it is just a simple way
    to interact with TensorFlow, which is running in the background.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个高级库，可以作为与 TensorFlow 的简化接口。这意味着 Keras 本身不进行任何计算；它只是与后台运行的 TensorFlow
    交互的一种简单方式。
- en: TensorFlow is a software library developed by Google and is very popular for
    deep learning. In this book, we usually try to work with TensorFlow only through
    Keras, since that is easier than working with TensorFlow directly. However, sometimes
    we might want to write a bit of TensorFlow code in order to build more advanced
    models.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是由 Google 开发的软件库，在深度学习领域非常流行。在本书中，我们通常尝试通过 Keras 来使用 TensorFlow，因为这样比直接使用
    TensorFlow 更加简便。然而，有时我们可能需要编写一些 TensorFlow 代码，以构建更高级的模型。
- en: The goal of TensorFlow is to run the computations needed for deep learning as
    quickly as possible. It does so, as the name gives away, by working with tensors
    in a data flow graph. Starting in version 1.7, Keras is now also a core part of
    TensorFlow.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的目标是尽可能快速地运行深度学习所需的计算。正如其名称所示，它通过在数据流图中处理张量来实现这一目标。从 1.7 版本开始，Keras
    现在也成为了 TensorFlow 的核心部分。
- en: 'So, we could import the Keras layers by running the following:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以通过运行以下代码导入 Keras 层：
- en: '[PRE37]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This book will treat Keras as a standalone library. However, you might want
    to use a different backend for Keras one day, as it keeps the code cleaner if
    we have shorter `import` statements.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将 Keras 视为一个独立的库。然而，未来你可能想为 Keras 使用不同的后端，因为如果我们缩短 `import` 语句，代码会更加简洁。
- en: Tensors and the computational graph
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量与计算图
- en: Tensors are arrays of numbers that transform based on specific rules. The simplest
    kind of tensor is a single number. This is also called a scalar. Scalars are sometimes
    referred to as rank-zero tensors.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是基于特定规则变换的数字数组。最简单的张量类型是一个单一的数字，这也叫做标量。标量有时被称为零阶张量。
- en: 'The next tensor is a vector, also known as a rank-one tensor. The next The
    next ones up the order are matrices, called rank-two tensors; cube matrices, called
    rank-three tensors; and so on. You can see the rankings in the following table:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个张量是向量，也叫做秩一张量。接下来的张量是矩阵，称为秩二张量；立方矩阵，称为秩三张量；以此类推。你可以在下表中看到这些秩次：
- en: '| Rank | Name | Expresses |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 秩 | 名称 | 表示 |'
- en: '| --- | --- | --- |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | Scalar | Magnitude |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 标量 | 数值大小 |'
- en: '| 1 | Vector | Magnitude and Direction |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 向量 | 数值大小和方向 |'
- en: '| 2 | Matrix | Table of numbers |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 矩阵 | 数字表 |'
- en: '| 3 | Cube Matrix | Cube of numbers |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 立方矩阵 | 数字的立方 |'
- en: '| n | n-dimensional matrix | You get the idea |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| n | n 维矩阵 | 你明白了 |'
- en: This book mostly uses the word tensor for rank-three or higher tensors.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 本书主要使用“张量”一词来指代三阶或更高阶的张量。
- en: TensorFlow and every other deep learning library perform calculations along
    a computational graph. In a computational graph, operations, such as matrix multiplication
    or an activation function, are nodes in a network. Tensors get passed along the
    edges of the graph between the different operations.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 和其他深度学习库通过计算图执行计算。在计算图中，操作（如矩阵乘法或激活函数）是网络中的节点。张量在图的边缘之间传递，连接不同的操作。
- en: 'A forward pass through our simple neural network has the following graph:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单神经网络的前向传播图如下：
- en: '![Tensors and the computational graph](img/B10354_01_18.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![张量与计算图](img/B10354_01_18.jpg)'
- en: A simple computational graph
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的计算图
- en: The advantage of structuring computations as a graph is that it's easier to
    run nodes in parallel. Through parallel computation, we do not need one very fast
    machine; we can also achieve fast computation with many slow computers that split
    up the tasks.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 将计算结构化为图的优势在于，节点可以更容易地并行运行。通过并行计算，我们不需要一台非常快的机器；我们也可以通过许多慢速计算机来实现快速计算，它们将任务分摊开来。
- en: This is the reason why GPUs are so useful for deep learning. GPUs have many
    small cores, as opposed to CPUs, which only have a few fast cores. A modern CPU
    might only have four cores, whereas a modern GPU can have hundreds or even thousands
    of cores.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么GPU在深度学习中如此有用的原因。GPU有许多小核心，而CPU只有少数几个快速核心。现代CPU可能只有四个核心，而现代GPU可以有数百甚至数千个核心。
- en: 'The entire graph of just a very simple model can look quite complex, but you
    can see the components of the dense layer. There is a **matrix multiplication**
    (**matmul**), adding bias and a ReLU activation function:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 整个图表可能看起来相当复杂，即便它只是一个非常简单的模型，但你可以看到密集层的组成部分。这里有一个**矩阵乘法**（**matmul**）、加上偏置和一个ReLU激活函数：
- en: '![Tensors and the computational graph](img/B10354_01_19.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![张量与计算图](img/B10354_01_19.jpg)'
- en: The computational graph of a single layer in TensorFlow. Screenshot from TensorBoard.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中单层计算图的示意图。截图来自TensorBoard。
- en: Another advantage of using computational graphs such as this is that TensorFlow
    and other libraries can quickly and automatically calculate derivatives along
    this graph. As we have explored throughout this chapter, calculating derivatives
    is key for training neural networks.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像这样的计算图的另一个优势是，TensorFlow和其他库可以快速且自动地沿着这个图计算导数。正如我们在本章中所探讨的，计算导数是训练神经网络的关键。
- en: Exercises
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Now that we have finished the first chapter in this exciting journey, I've got
    a challenge for you! You'll find some exercises that you can do that are all themed around
    what we've covered in this chapter!
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经完成了这段激动人心的旅程中的第一章，我给你一个挑战！你会找到一些与本章内容相关的练习，来进一步巩固你的知识！
- en: 'So, why not try to do the following:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么不尝试以下操作呢：
- en: Expand the two-layer neural network in Python to three layers.
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Python中的两层神经网络扩展为三层。
- en: Within the GitHub repository, you will find an Excel file called `1 Excel Exercise`.
    The goal is to classify three types of wine by their cultivar data. Build a logistic
    regressor to this end in Excel.
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GitHub的代码库中，你会找到一个名为`1 Excel Exercise`的Excel文件。目标是通过葡萄品种数据对三种类型的葡萄酒进行分类。为此，在Excel中构建一个逻辑回归模型。
- en: Build a two-layer neural network in Excel.
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Excel中构建一个两层神经网络。
- en: Play around with the hidden layer size and learning rate of the 2-layer neural
    network. Which options offer the lowest loss? Does the lowest loss also capture
    the true relationship?
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在2层神经网络中，调整隐藏层大小和学习率。哪些选项能够提供最低的损失？最低的损失是否也能捕捉到真实的关系？
- en: Summary
  id: totrans-388
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: And that's it! We've learned how neural networks work. Throughout the rest of
    this book, we'll look at how to build more complex neural networks that can approximate
    more complex functions.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们已经学会了神经网络是如何工作的。在接下来的章节中，我们将讨论如何构建更复杂的神经网络，这些网络能够近似更复杂的函数。
- en: 'As it turns out, there are a few tweaks to make to the basic structure for
    it to work well on specific tasks, such as image recognition. The basic ideas
    introduced in this chapter, however, stay the same:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，为了使基本结构在特定任务（如图像识别）中表现良好，需要做一些调整。然而，本章中介绍的基本思想保持不变：
- en: Neural networks function as approximators
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络作为近似器发挥作用
- en: We gauge how well our approximated function, ![Summary](img/B10354_01_027.jpg),
    performs through a loss function
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过损失函数来评估我们近似的函数， ![总结](img/B10354_01_027.jpg)，的表现。
- en: Parameters of the model are optimized by updating them in the opposite direction
    of the derivative of the loss function with respect to the parameter
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的参数通过沿损失函数相对于参数的导数的反方向进行更新，从而得到优化。
- en: The derivatives are calculated backward through the model using the chain rule
    in a process called backpropagation
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过链式法则反向计算导数，这个过程叫做反向传播。
- en: The key takeaway from this chapter is that while we are looking for function
    *f*, we can try and find it by optimizing a function to perform like *f* on a
    dataset. A subtle but important distinction is that we do not know whether ![Summary](img/B10354_01_027.jpg)
    works like *f* at all. An often-cited example is a military project that tried
    to use deep learning to spot tanks within images. The model trained well on the
    dataset, but once the Pentagon wanted to try out their new tank spotting device,
    it failed miserably.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的关键 takeaway 是，虽然我们在寻找函数 *f*，但我们可以通过优化一个函数，使其在数据集上表现得像 *f* 来尝试找到它。有一个微妙但重要的区别是，我们并不知道![Summary](img/B10354_01_027.jpg)是否真的像
    *f* 一样有效。一个经常被引用的例子是一个军事项目，尝试使用深度学习在图像中识别坦克。模型在数据集上训练得很好，但当五角大楼想要测试他们的新坦克识别设备时，却惨遭失败。
- en: In the tank example, it took the Pentagon a while to figure out that in the
    dataset they used to develop the model, all the pictures of the tanks were taken
    on a cloudy day and pictures without a tank where taken on a sunny day. Instead
    of learning to spot tanks, the model had learned to spot grey skies instead.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在坦克示例中，五角大楼花了一段时间才发现，他们用来开发模型的数据集中的所有坦克图片都是在阴天拍摄的，而没有坦克的图片则是在晴天拍摄的。模型没有学会识别坦克，而是学会了识别灰色的天空。
- en: This is just one example of how your model might work very differently to how
    you think, or even plan for it to do. Flawed data might seriously throw your model
    off track, sometimes without you even noticing. However, for every failure, there
    are plenty of success stories in deep learning. It is one of the high-impact technologies
    that will reshape the face of finance.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个示例，说明你的模型可能与你想象的或甚至计划中的工作方式大不相同。存在缺陷的数据可能会严重影响模型的表现，有时你甚至没有察觉到。然而，尽管有失败，深度学习领域却有许多成功的案例。它是将重塑金融面貌的高影响力技术之一。
- en: In the next chapter, we will get our hands dirty by jumping in and working with
    a common type of data in finance, structured tabular data. More specifically,
    we will tackle the problem of fraud, a problem that many financial institutions
    sadly have to deal with and for which modern machine learning is a handy tool.
    We will learn about preparing data and making predictions using Keras, scikit-learn,
    and XGBoost.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将亲自动手，处理金融领域中一种常见的数据类型——结构化表格数据。更具体地说，我们将解决欺诈问题，这是许多金融机构不得不面对的问题，而现代机器学习是一个非常有用的工具。我们将学习如何准备数据，并使用Keras、scikit-learn和XGBoost进行预测。
