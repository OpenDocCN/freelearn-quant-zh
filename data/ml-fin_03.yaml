- en: Chapter 3. Utilizing Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章：利用计算机视觉
- en: When Snapchat first introduced a filter featuring a breakdancing hotdog, the
    stock price of the company surged. However, investors were less interested in
    the hotdog's handstand; what actually fascinated them was that Snapchat had successfully
    built a powerful form of computer vision technology.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当Snapchat首次推出带有霹雳舞热狗的滤镜时，该公司股价飙升。然而，投资者对热狗的倒立并不感兴趣；真正让他们着迷的是Snapchat成功构建了一种强大的计算机视觉技术。
- en: The Snapchat app was now not only able to take pictures, but it was also able
    to find the surfaces within those pictures that a hotdog could breakdance on.
    Their app would then stick the hotdog there, something that could still be done
    when the user moved their phone, allowing the hotdog to keep dancing in the same
    spot.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Snapchat应用程序现在不仅能够拍照，而且能够在这些照片中找到热狗可以跳霹雳舞的表面。然后，他们的应用程序会将热狗放置在那里，即使用户移动手机，热狗仍能保持在原地跳舞。
- en: While the dancing hotdog may be one of the sillier applications of computer
    vision, it successfully showed the world the potential of the technology. In a
    world full of cameras, from the billions of smartphones, security cameras, and
    satellites in use every day, to **Internet of Things** (**IoT**) devices, being
    able to interpret images yields great benefits for both consumers and producers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然跳舞的热狗可能是计算机视觉应用中最傻的一个，但它成功地向世界展示了这项技术的潜力。在一个充满摄像头的世界里，从数十亿的智能手机、安防摄像头和卫星，每天都在使用，到**物联网**（**IoT**）设备，能够解读图像为消费者和生产者带来了巨大的好处。
- en: 'Computer vision allows us to both perceive and interpret the real world at
    scale. You can think of it like this: no analyst could ever look at millions of
    satellite images to mark mining sites and track their activity over time; it''s
    just not possible. Yet for computers, it''s not just a possibility; it''s something
    that''s a reality here and now.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉使我们能够在大规模上感知和解释现实世界。你可以这样理解：没有分析师能查看数百万张卫星图像，标记矿区并跟踪其活动；这是不可能的。然而，对于计算机来说，这不仅是可能的，而且已经成为现实。
- en: In fact, something that’s being used in the real world now, by several firms,
    is retailers counting the number of cars in their parking lot in order to estimate
    what the sales of goods will be in a given period.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，现在一些公司正在现实世界中使用的一种技术是零售商通过计算停车场中的汽车数量来估算某一时期内的商品销售量。
- en: Another important application of computer vision can be seen in finance, specifically
    in the area of insurance. For instance, insurers might use drones to fly over
    roofs in order to spot issues before they become an expensive problem. This could
    extend to them using computer vision to inspect factories and equipment they insure.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的另一个重要应用可以在金融领域中看到，特别是在保险行业。例如，保险公司可能会使用无人机飞过屋顶，发现问题并在它们变成昂贵的麻烦之前解决。这也可以扩展到他们使用计算机视觉来检查他们承保的工厂和设备。
- en: Looking at another case in the finance sector, banks needing to comply with
    **Know-Your-Customer** (**KYC**) rules are automating back-office processes and
    identity verification. In financial trading, computer vision can be applied to
    candlestick charts in order to find new patterns for technical analysis. We could
    dedicate a whole book to the practical applications of computer vision.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 看看金融领域的另一个案例，银行需要遵守**了解你的客户**（**KYC**）规则，正在自动化后台流程和身份验证。在金融交易中，计算机视觉可以应用于蜡烛图，以便发现新的技术分析模式。我们甚至可以为计算机视觉的实际应用写一本书。
- en: 'In this chapter, we will be covering the building blocks of computer vision
    models. This will include a focus on the following topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论计算机视觉模型的构建模块。内容将重点涵盖以下主题：
- en: Convolutional layers.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层。
- en: Padding.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充。
- en: Pooling.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化。
- en: Regularization to prevent overfitting.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化，以防止过拟合。
- en: Momentum-based optimization.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于动量的优化。
- en: Batch normalization.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量归一化。
- en: Advanced architectures for computer vision beyond classification.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超越分类的计算机视觉高级架构。
- en: A note on libraries.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于库的说明。
- en: 'Before we start, let''s have a look at all the different libraries we will
    be using in this chapter:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们看一下本章中将使用的所有不同库：
- en: '**Keras**: A high-level neural network library and an interface to TensorFlow.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keras**：一个高层次的神经网络库，是TensorFlow的接口。'
- en: '**TensorFlow**: A dataflow programming and machine learning library that we
    use for GPU-accelerated computation.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow**：一个数据流编程和机器学习库，我们用它进行GPU加速计算。'
- en: '**Scikit-learn**: A popular machine learning library with implementation of many
    classic algorithms as well as evaluation tools.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Scikit-learn**：一个流行的机器学习库，包含许多经典算法的实现以及评估工具。'
- en: '**OpenCV**: An image processing library that can be used for rule-based augmentation'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenCV**：一个用于基于规则的增强的图像处理库。'
- en: '**NumPy**: A library for handling matrices in Python.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NumPy**：一个用于处理Python中矩阵的库。'
- en: '**Seaborn**: A plotting library.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Seaborn**：一个绘图库。'
- en: '**tqdm**: A tool to monitor the progress of Python programs.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tqdm**：一个用于监控Python程序进度的工具。'
- en: It's worth taking a minute to note that all of these libraries, except for OpenCV,
    can be installed via `pip`; for example, `pip install keras`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 值得花点时间注意，除了OpenCV，所有这些库都可以通过`pip`安装；例如，`pip install keras`。
- en: 'OpenCV, however, will require a slightly more complex installation procedure.
    This is beyond the scope of this book, but the information is well documented
    online via OpenCV documentation, which you can view at the following URL: [https://docs.opencv.org/trunk/df/d65/tutorial_table_of_content_introduction.html](https://docs.opencv.org/trunk/df/d65/tutorial_table_of_content_introduction.html).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，OpenCV的安装过程稍微复杂一些。虽然这超出了本书的范围，但你可以通过OpenCV的官方文档在线查看详细信息，网址如下：[https://docs.opencv.org/trunk/df/d65/tutorial_table_of_content_introduction.html](https://docs.opencv.org/trunk/df/d65/tutorial_table_of_content_introduction.html)。
- en: Alternately, it's worth noting that both Kaggle and Google Colab come with OpenCV
    preinstalled. To run the examples in this chapter, make sure you have OpenCV installed
    and can import with `import cv2`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，值得注意的是，Kaggle和Google Colab都预装了OpenCV。要运行本章中的示例，请确保已安装OpenCV并能够使用`import cv2`进行导入。
- en: Convolutional Neural Networks
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络。
- en: '**Convolutional Neural Networks**, **ConvNets**, or **CNNs** for short, are
    the driving engine behind computer vision. ConvNets allow us to work with larger
    images while still keeping the network at a reasonable size.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**、**ConvNets**，或简写为**CNNs**，是计算机视觉的驱动引擎。ConvNets使我们能够处理更大的图像，同时保持网络的合理大小。'
- en: The name Convolutional Neural Network comes from the mathematical operation that
    differentiates them from regular neural networks. Convolution is the mathematically
    correct term for sliding one matrix over another matrix. We'll explore in the
    next section, *Filters on MNIST*, why this is important for ConvNets, but also
    why this is not the best name in the world for them, and why ConvNets should,
    in reality, be called **filter nets**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络的名称来源于将其与常规神经网络区分开来的数学运算。卷积是数学上正确的术语，指的是将一个矩阵滑过另一个矩阵。在接下来的章节中，我们将在*MNIST上的滤波器*部分探讨这对于ConvNets的重要性，但也会讨论为什么这不是最适合它们的名称，以及为什么ConvNets实际上应该被称为**滤波器网络**。
- en: You may be asking, "but why filter nets?" The answer is simply because what
    makes them work is the fact that they use filters.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，“那为什么是滤波器网络呢？”答案很简单，因为它们之所以有效，是因为它们使用了滤波器。
- en: In the next section, we will be working with the MNIST dataset, which is a collection
    of handwritten digits that has become a standard "Hello, World!" application for
    computer vision.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将使用MNIST数据集，它是一个手写数字的集合，已成为计算机视觉中的标准“Hello, World!”应用。
- en: Filters on MNIST
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST上的滤波器。
- en: 'What does a computer actually see when it sees an image? Well, the values of
    the pixels are stored as numbers in the computer. So, when the computer *sees*
    a black-and-white image of a seven, it actually sees something similar to the
    following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机在看到一张图片时，究竟看到的是什么？实际上，图像的像素值以数字形式存储在计算机中。所以，当计算机*看到*一张数字7的黑白图像时，它实际上看到的是类似于以下内容的东西：
- en: '![Filters on MNIST](img/B10354_03_01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![MNIST上的滤波器](img/B10354_03_01.jpg)'
- en: The number 7 from the MNIST dataset
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集中的数字7。
- en: The preceding is an example from the MNIST dataset. The handwritten number in
    the image has been highlighted to make the figure seven visible for humans, but
    for the computer, the image is really just a collection of numbers. This means
    we can perform all kinds of mathematical operations on the image.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 上述是来自MNIST数据集的一个例子。图像中的手写数字已经被突出显示，使得数字七对人类可见，但对计算机而言，图像实际上只是数字的集合。这意味着我们可以对图像进行各种数学运算。
- en: When detecting numbers, there are a few lower-level features that make a number.
    For example, in this handwritten figure 7, there's a combination of one vertical
    straight line, one horizontal line on the top, and one horizontal line through
    the middle. In contrast, a 9 is made up of four rounded lines that form a circle
    at the top and a straight, vertical line.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在检测数字时，有一些低级特征构成了一个数字。例如，在这张手写的数字 7 中，包含了一条垂直直线、顶部的一条水平线和一条穿过中间的水平线。相比之下，数字
    9 由四条圆形的线条组成，顶部形成一个圆圈，下面是一个直立的垂直线。
- en: We're now able to present the central idea behind ConvNets. We can use small
    filters that can detect a certain kind of low-level feature, such as a vertical
    line, and then slide it over the entire image to detect all the vertical lines
    in the image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够呈现卷积神经网络（ConvNets）的核心思想。我们可以使用小的滤波器，检测某种低级特征，比如垂直线，然后将其滑过整张图像，检测图像中的所有垂直线。
- en: The following screenshot shows a vertical line filter. To detect vertical lines
    in our image, we need to slide this 3x3 matrix filter over the image.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了一个垂直线滤波器。为了检测图像中的垂直线，我们需要将这个 3x3 的矩阵滤波器滑过图像。
- en: '![Filters on MNIST](img/B10354_03_02.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![MNIST上的滤波器](img/B10354_03_02.jpg)'
- en: A vertical line filter
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个垂直线滤波器
- en: Using the MNIST dataset on the following page, we start in the top-left corner
    and slice out the top-left 3x3 grid of pixels, which in this case is all zeros.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用接下来的 MNIST 数据集，我们从左上角开始，切出左上角的 3x3 像素网格，在这个例子中，网格里全是零。
- en: 'We then perform an element-wise multiplication of all the elements in the filter
    with all elements in the slice of the image. The nine products then get summed
    up, and bias is added. This value then forms the output of the filter and gets
    passed on as a new pixel to the next layer:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对滤波器中的所有元素与图像切片中的所有元素进行逐元素相乘。接着，将这九个乘积相加，并加入偏置。这个值将形成滤波器的输出，并作为新像素传递到下一层：
- en: '![Filters on MNIST](img/B10354_03_001.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![MNIST上的滤波器](img/B10354_03_001.jpg)'
- en: 'As a result, the output of our vertical line filter will look like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的垂直线滤波器的输出将如下所示：
- en: '![Filters on MNIST](img/B10354_03_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![MNIST上的滤波器](img/B10354_03_03.jpg)'
- en: The output of a vertical line filter
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直线滤波器的输出
- en: Take a minute to notice that the vertical lines are visible while the horizontal
    lines are gone. Only a few artifacts remain. Also, notice how the filter captures
    the vertical line from one side.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请花点时间注意，垂直线清晰可见，而水平线则消失了，只有一些伪影残留。同时，注意滤波器如何从一侧捕捉到垂直线。
- en: Since it responds to high pixel values on the left and low pixel values on the
    right, only the right side of the output shows strong positive values. Meanwhile,
    the left side of the line actually shows negative values. This is not a big problem
    in practice as there are usually different filters for different kinds of lines
    and directions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它对左侧的高像素值和右侧的低像素值作出反应，输出的右侧显示了强烈的正值。同时，左侧的线实际上显示了负值。在实践中，这并不是大问题，因为通常不同的滤波器用于检测不同种类的线条和方向。
- en: Adding a second filter
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加第二个滤波器
- en: Our vertical filter is working, but we've already noticed that we also need
    to filter our image for horizontal lines in order to detect a seven.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的垂直滤波器正在起作用，但我们已经注意到，我们还需要对图像进行水平线的滤波，以便检测数字7。
- en: 'Our horizontal line filter might look like this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的水平线滤波器可能如下所示：
- en: '![Adding a second filter](img/B10354_03_04.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![添加第二个滤波器](img/B10354_03_04.jpg)'
- en: A horizontal line filter
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个水平线滤波器
- en: 'Using that example, we can now slide this filter over our image in the exact
    same way we did with the vertical filter, resulting in the following output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个例子后，我们现在可以像使用垂直滤波器时那样，滑动这个滤波器在图像上，得到以下的输出：
- en: '![Adding a second filter](img/B10354_03_05.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![添加第二个滤波器](img/B10354_03_05.jpg)'
- en: The output of the vertical line filter
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直线滤波器的输出
- en: 'See how this filter removes the vertical lines and pretty much only leaves
    the horizontal lines? The question now is what do we now pass onto the next layer? Well,
    we stack the outputs of both filters on top of each other, creating a three-dimensional
    cube:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这个滤波器如何去除垂直线，并几乎只留下水平线？现在的问题是，我们该将什么传递到下一层？嗯，我们将两个滤波器的输出堆叠在一起，形成一个三维的立方体：
- en: '![Adding a second filter](img/B10354_03_06.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![添加第二个滤波器](img/B10354_03_06.jpg)'
- en: The MNIST convolution
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST卷积
- en: By adding multiple convolutional layers, our ConvNet is able to extract ever
    more complex and semantic features.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加多个卷积层，我们的卷积神经网络能够提取越来越复杂和语义化的特征。
- en: Filters on color images
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 彩色图像上的滤波器
- en: Of course, our filter technique is not only limited to black-and-white images.
    In this section we're going to have a look at color images.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的滤波器技术不仅限于黑白图像。在本节中，我们将看看彩色图像。
- en: The majority of color images consist of three layers or channels, and this is
    commonly referred to as RGB, the initialism for the three layers. They are made
    up of one red channel, one blue channel, and one green channel. When these three
    channels are laid on top of each other, they add up to create the traditional
    color image that we know.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数彩色图像由三个层或通道组成，通常称为RGB，代表这三个层的首字母。它们由一个红色通道、一个蓝色通道和一个绿色通道组成。当这三个通道叠加在一起时，它们相加形成我们所知的传统彩色图像。
- en: Taking that concept, an image is therefore not flat, but actually a cube, a
    three-dimensional matrix. Combining this idea with our objective, we want to apply
    a filter to the image, and apply it to all three channels at once. We will, therefore,
    perform an element-wise multiplication between two three-dimensional cubes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个概念，图像因此不是平的，而实际上是一个立方体，一个三维矩阵。结合这个想法和我们的目标，我们希望将滤波器应用到图像上，并一次性应用到所有三个通道。因此，我们将对两个三维立方体进行逐元素相乘。
- en: 'Our 3x3 filter now has a depth of three and thus nine parameters, plus the
    bias:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的3x3滤波器现在有三个深度，因此有九个参数，再加上偏置：
- en: '![Filters on color images](img/B10354_03_07.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![彩色图像上的滤波器](img/B10354_03_07.jpg)'
- en: An example of a filter cube or convolutional kernel
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个滤波器立方体或卷积核的示例
- en: This cube, which is referred to as a convolutional kernel, gets slid over the
    image just like the two-dimensional matrix did before. The element-wise products
    then again get summed up, the bias is added, and the outcome represents a pixel
    in the next layer.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个立方体被称为卷积核，它像之前的二维矩阵一样在图像上滑动。然后，逐元素的乘积会再次求和，添加偏置，结果表示下一层中的一个像素。
- en: Filters always capture the whole depth of the previous layer. The filters are
    moved over the width and height of the image. Likewise, filters are not moved
    across the depth, that is, the different channels, of an image. In technical terms,
    weights, the numbers that make up the filters, are shared over width and height,
    but not over different channels.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 滤波器始终捕捉前一层的整个深度。滤波器在图像的宽度和高度上滑动。同样，滤波器不会在图像的深度（即不同的通道）上滑动。用技术术语来说，权重，即构成滤波器的数字，在宽度和高度上是共享的，但在不同的通道上则不是。
- en: The building blocks of ConvNets in Keras
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中的卷积神经网络构建块
- en: In this section, we will be building a simple ConvNet that can be used for classifying
    the MNIST characters, while at the same time, learning about the different pieces
    that make up modern ConvNets.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个简单的卷积神经网络，用于分类MNIST字符，同时学习现代卷积神经网络的不同组成部分。
- en: 'We can directly import the MNIST dataset from Keras by running the following
    code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下代码直接从Keras导入MNIST数据集：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Our dataset contains 60,000 28x28-pixel images. MNIST characters are black
    and white, so the data shape usually does not include channels:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集包含60,000张28x28像素的图像。MNIST字符是黑白的，因此数据形状通常不包括通道：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will take a closer look at color channels later, but for now, let''s expand
    our data dimensions to show that we only have a one-color channel. We can achieve
    this by running the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会更详细地研究颜色通道，但现在，让我们扩展数据维度，显示我们只有一个颜色通道。我们可以通过运行以下代码来实现：
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: With the code being run, you can see that we now have a single color channel
    added.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，你可以看到我们现在添加了一个单一的颜色通道。
- en: Conv2D
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Conv2D
- en: 'Now we come to the meat and potatoes of ConvNets: using a convolutional layer
    in Keras. Conv2D is the actual convolutional layer, with one Conv2D layer housing
    several filters, as can be seen in the following code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来到了卷积神经网络的核心内容：在Keras中使用卷积层。Conv2D就是实际的卷积层，一个Conv2D层包含多个滤波器，如以下代码所示：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When creating a new Conv2D layer, we must specify the number of filters we want
    to use, and the size of each filter.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新的Conv2D层时，我们必须指定要使用的滤波器数量以及每个滤波器的大小。
- en: Kernel size
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积核大小
- en: The size of the filter is also called `kernel_size`, as the individual filters
    are sometimes called kernels. If we only specify a single number as the kernel
    size, Keras will assume that our filters are squares. In this case, for example,
    our filter would be 3x3 pixels.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 滤波器的大小也叫做`kernel_size`，因为单个滤波器有时被称为核。如果我们只指定一个数字作为卷积核大小，Keras会假设我们的滤波器是方形的。在这种情况下，例如，我们的滤波器将是3x3像素。
- en: It is possible, however, to specify non-square kernel sizes by passing a tuple
    to the `kernel_size` parameter. For example, we could choose to have a 3x4-pixel
    filter through `kernel_size = (3,4)`. However, this is very rare, and in the majority
    of cases, filters have a size of either 3x3 or 5x5\. Empirically, researchers
    have found that this is a size that yields good results.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们确实可以通过将元组传递给`kernel_size`参数来指定非正方形的卷积核大小。例如，我们可以选择使用一个3x4像素的过滤器，通过`kernel_size
    = (3,4)`来指定。然而，这种情况非常罕见，在大多数情况下，过滤器的大小要么是3x3，要么是5x5。经验表明，这种大小的过滤器能产生良好的效果。
- en: Stride size
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步幅大小
- en: The `strides` parameter specifies the step size, also called the stride size,
    with which the convolutional filter slides over the image, usually referred to
    as the feature map. In the vast majority of cases, filters move pixel by pixel,
    so their stride size is set to 1\. However, there are researchers that make more
    extensive use of larger stride sizes in order to reduce the spatial size of the
    feature map.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`strides`参数指定了步幅大小，也就是卷积过滤器滑过图像时的步长，通常称为步幅大小。在绝大多数情况下，过滤器逐像素滑动，因此它们的步幅大小被设置为1。然而，也有一些研究者使用较大的步幅大小来减少特征图的空间大小。'
- en: 'Like with `kernel_size`, Keras assumes that we use the same stride size horizontally
    and vertically if we specify only one value, and in the vast majority of cases
    that is correct. However, if we want to use a stride size of one horizontally,
    but two vertically, we can pass a tuple to the parameter as follows: `strides=(1,2)`.
    As in the case of the filter size, this is rarely done.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`kernel_size`一样，如果我们只指定一个值，Keras会假设我们在水平方向和垂直方向使用相同的步幅大小，并且在绝大多数情况下，这种假设是正确的。然而，如果我们希望水平方向使用步幅为1，而垂直方向使用步幅为2，我们可以通过以下方式将元组传递给参数：`strides=(1,2)`。就像在过滤器大小的情况一样，这种情况比较少见。
- en: Padding
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 填充
- en: Finally, we have to add `padding` to our convolutional layer. Padding adds zeros
    around our image. This can be done if we want to prevent our feature map from
    shrinking.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们必须在卷积层中添加`padding`。填充会在图像的四周添加零。如果我们不想让特征图缩小，可以使用这种方式。
- en: Let's consider a 5x5-pixel feature map and a 3x3 filter. The filter only fits
    on the feature map nine times, so we'll end up with a 3x3 output. This both reduces
    the amount of information that we can capture in the next feature map, and how
    much the outer pixels of the input feature map can contribute to the task. The
    filter never centers on them; it only goes over them once.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个5x5像素的特征图和一个3x3的过滤器。由于过滤器只能在特征图上滑动九次，因此我们最终会得到一个3x3的输出。这不仅减少了我们在下一个特征图中能够捕获的信息量，还减少了输入特征图的外部像素在任务中可能的贡献。过滤器从未以它们为中心；它只经过它们一次。
- en: 'There are three options for padding: not using padding, known as "No" padding,
    "Same" padding and "Valid" padding.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 填充有三种选择：不使用填充，称为"无"填充，"同"填充和"有效"填充。
- en: 'Let''s have a look at each of the three paddings. First, No Padding:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看三种填充方式中的每一种。首先是：不使用填充：
- en: '![Padding](img/B10354_03_08.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![Padding](img/B10354_03_08.jpg)'
- en: 'Option 1: No padding'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 选项 1：不使用填充
- en: 'Then we have Same Padding:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是同样的填充方式：
- en: '![Padding](img/B10354_03_09.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![Padding](img/B10354_03_09.jpg)'
- en: 'Option 2: Same padding'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 选项 2：同样填充
- en: 'To ensure the output has the same size as the input, we can use `same` padding.
    Keras will then add enough zeros around the input feature map so that we can preserve
    the size. The default padding setting, however, is `valid`. This padding does not
    preserve the feature map size, but only makes sure that the filter and stride size
    actually fit on the input feature map:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保输出与输入的大小相同，我们可以使用`same`填充。Keras将会在输入特征图的四周添加足够的零，以保持输入大小不变。然而，默认的填充设置是`valid`。这种填充方式并不会保持特征图的大小，仅仅是确保过滤器和步幅大小能够在输入特征图上有效应用：
- en: '![Padding](img/B10354_03_10.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![Padding](img/B10354_03_10.jpg)'
- en: 'Option 3: Valid padding'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 选项 3：有效填充
- en: Input shape
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入形状
- en: Keras requires us to specify the input shape. However, this is only required
    for the first layer. For all the following layers, Keras will infer the input
    shape from the previous layer's output shape.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Keras要求我们指定输入的形状。不过，这仅仅是第一次使用层时需要的要求。对于后续的每一层，Keras将会根据上一层的输出形状推断出输入形状。
- en: Simplified Conv2D notation
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简化的Conv2D表示法
- en: 'The preceding layer takes a 28x28x1 input and slides six filters with a 2x2 filter size
    over it, going pixel by pixel. A more common way to specify the same layer would
    be by using the following code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 前一层使用28x28x1的输入，并滑动六个2x2的过滤器逐像素处理输入。指定同一层的更常见方式是使用以下代码：
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The number of filters (here `6`) and the filter size (here `3`) are set as positional
    arguments, while `strides` and `padding` default to `1` and `valid` respectively.
    If this was a layer deeper in the network, we wouldn't even have to specify the
    input shape.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器的数量（这里是 `6`）和过滤器的大小（这里是 `3`）作为位置参数设置，而 `strides` 和 `padding` 默认分别为 `1` 和
    `valid`。如果这是网络中的更深层，我们甚至不需要指定输入形状。
- en: ReLU activation
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ReLU 激活
- en: Convolutional layers only perform a linear step. The numbers that make up the image
    get multiplied with the filter, which is a linear operation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层只执行线性步骤。构成图像的数字与过滤器相乘，这是一个线性操作。
- en: 'So, in order to approximate complex functions, we need to introduce non-linearity
    with an activation function. The most common activation function for computer
    vision is the Rectified Linear Units, or ReLU function, which we can see here:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了逼近复杂的函数，我们需要通过激活函数引入非线性。计算机视觉中最常见的激活函数是修正线性单元（ReLU），我们可以在这里看到它：
- en: '![ReLU activation](img/B10354_03_11.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![ReLU 激活](img/B10354_03_11.jpg)'
- en: The ReLU activation function
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 激活函数
- en: 'The ReLU formula, which was used to produce the above chart, can be seen below:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成上述图表的 ReLU 公式如下：
- en: '*ReLU(x) = max(x, 0)*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*ReLU(x) = max(x, 0)*'
- en: In other words, the ReLU function returns the input if the input is positive.
    If it's not, then it returns zero. This very simple function has been shown to
    be quite useful, making gradient descent converge faster.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，ReLU 函数返回输入值，如果输入值为正。如果不是，它将返回零。这个非常简单的函数已被证明非常有用，使得梯度下降收敛更快。
- en: It is often argued that ReLU is faster because the derivative for all values
    above zero is just one, and it does not become very small as the derivative for
    some extreme values does, for example, with sigmoid or tanh.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通常有人认为，ReLU 更快，因为其在零以上的所有值的导数都为 1，并且不像某些极值的导数那样变得非常小，例如在 sigmoid 或 tanh 中。
- en: ReLU is also less computationally expensive than both sigmoid and tanh. It does
    not require any computationally expensive calculations, input values below zero
    are just set to zero, and the rest is outputted. Unfortunately, though, ReLU activations
    are a bit fragile and can "die."
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 也比 sigmoid 和 tanh 的计算开销更小。它不需要任何计算开销大的计算，输入值小于零的部分直接设为零，其他部分则输出。不幸的是，ReLU
    激活函数有点脆弱，可能会“死亡”。
- en: When the gradient is very large and moves multiple weights towards a negative
    direction, then the derivative of ReLU will also always be zero, so the weights
    never get updated again. This might mean that a neuron never fires again. However,
    this can be mitigated through a smaller learning rate.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当梯度非常大并将多个权重朝负方向移动时，ReLU 的导数将始终为零，因此权重永远不会再更新。这可能意味着一个神经元永远不会再次激活。然而，这可以通过较小的学习率来缓解。
- en: 'Because ReLU is fast and computationally cheap, it has become the default activation
    function for many practitioners. To use the ReLU function in Keras, we can just
    name it as the desired activation function in the activation layer, by running
    this code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ReLU 快且计算代价低，它已成为许多从业者的默认激活函数。要在 Keras 中使用 ReLU 函数，我们只需在激活层中将其命名为所需的激活函数，运行以下代码：
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: MaxPooling2D
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MaxPooling2D
- en: It's common practice to use a pooling layer after a number of convolutional
    layers. Pooling decreases the spatial size of the feature map, which in turn reduces
    the number of parameters needed in a neural network and thus reduces overfitting.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个卷积层后使用池化层是常见做法。池化减少了特征图的空间大小，从而减少了神经网络中所需的参数数量，进而减少了过拟合。
- en: 'Below, we can see an example of Max Pooling:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们可以看到一个最大池化的示例：
- en: '![MaxPooling2D](img/B10354_03_12.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![MaxPooling2D](img/B10354_03_12.jpg)'
- en: Max pooling
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化
- en: Max pooling returns the maximum element out of a pool. This is in contrast to
    the example average of `AveragePooling2D`, which returns the average of a pool.
    Max pooling often delivers superior results to average pooling, so it is the standard
    most practitioners use.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化返回池中的最大元素。这与 `AveragePooling2D` 的平均池化相反，后者返回池的平均值。最大池化通常比平均池化提供更优的结果，因此它是大多数从业者使用的标准。
- en: 'Max pooling can be achieved by running the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化可以通过运行以下代码来实现：
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When using a max pooling layer in Keras, we have to specify the desired pool
    size. The most common value is a 2x2 pool. Just as with the `Conv2D` layer, we
    can also specify a stride size.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中使用最大池化层时，我们必须指定所需的池大小。最常见的值是 2x2 的池。与 `Conv2D` 层一样，我们也可以指定步幅大小。
- en: For pooling layers, the default stride size is `None`, in which case Keras sets
    the stride size to be the same as the pool size. In other words, pools are next
    to each other and don't overlap.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于池化层，默认的步幅大小是`None`，在这种情况下，Keras会将步幅大小设置为与池化大小相同。换句话说，池化层是彼此相邻的，不会重叠。
- en: We can also specify padding, with `valid` being the default choice. However,
    specifying `same` padding for pooling layers is extremely rare since the point
    of a pooling layer is to reduce the spatial size of the feature map.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以指定填充，其中`valid`是默认选择。然而，指定`same`填充用于池化层是极为罕见的，因为池化层的目的是减少特征图的空间大小。
- en: 'Our `MaxPooling2D` layer here takes 2x2-pixel pools next to each other with
    no overlap and returns the maximum element. A more common way of specifying the same
    layer is through the execution of the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`MaxPooling2D`层在这里取2x2像素的池化区域，彼此相邻且没有重叠，并返回最大元素。指定相同层的更常见方法是执行以下操作：
- en: '[PRE9]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this case, both `strides` and `padding` are set to their defaults, `None`
    and `valid` respectively. There is usually no activation after a pooling layer
    since the pooling layer does not perform a linear step.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`strides`和`padding`都设置为其默认值，分别是`None`和`valid`。通常，池化层后没有激活函数，因为池化层不执行线性步骤。
- en: Flatten
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flatten
- en: You might have noticed that our feature maps are three dimensional while our
    desired output is a one-dimensional vector, containing the probability of each
    of the 10 classes. So, how do we get from 3D to 1D? Well, we `Flatten` our feature
    maps.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到我们的特征图是三维的，而我们期望的输出是一个一维的向量，包含10个类别的概率。那么，我们如何从三维到一维呢？好吧，我们`Flatten`了我们的特征图。
- en: The `Flatten` operation works similar to NumPy's `flatten` operation. It takes
    in a batch of feature maps with dimensions `(batch_size, height, width, channels)`
    and returns a set of vectors with dimensions `(batch_size, height * width * channels)`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`Flatten`操作类似于NumPy的`flatten`操作。它接收一批特征图，维度为`(batch_size, height, width, channels)`，并返回一个维度为`(batch_size,
    height * width * channels)`的向量集合。'
- en: 'It performs no computation and only reshapes the matrix. There are no hyperparameters
    to be set for this operation, as you can see in the following code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 它不执行任何计算，仅对矩阵进行重塑。此操作没有超参数需要设置，如你在以下代码中所见：
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Dense
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dense
- en: ConvNets usually consist of a feature extraction part, the convolutional layers,
    as well as a classification part. The classification part is made up out of the
    simple fully connected layers that we’ve already explored in [Chapter 1](ch01.xhtml
    "Chapter 1. Neural Networks and Gradient-Based Optimization"), *Neural Networks
    and Gradient-Based Optimization*, and [Chapter 2](ch02.xhtml "Chapter 2. Applying
    Machine Learning to Structured Data"), *Applying Machine Learning to Structured
    Data*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（ConvNets）通常由特征提取部分（卷积层）和分类部分组成。分类部分由我们在[第1章](ch01.xhtml "Chapter 1. Neural
    Networks and Gradient-Based Optimization")，*神经网络与基于梯度的优化*，以及[第2章](ch02.xhtml "Chapter
    2. Applying Machine Learning to Structured Data")，*将机器学习应用于结构化数据* 中已经探讨过的简单全连接层组成。
- en: To distinguish the plain layers from all other types of layers, we refer to
    them as `Dense` layers. In a dense layer, each input neuron is connected to an
    output neuron. We only have to specify the number of output neurons we would like,
    in this case, 10.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将普通层与其他类型的层区分开，我们称它们为`Dense`层。在一个密集层中，每个输入神经元都与一个输出神经元连接。我们只需要指定所需的输出神经元数量，在这个例子中是10个。
- en: 'This can be done by running the following code:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过运行以下代码来完成：
- en: '[PRE11]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After the linear step of the dense layer, we can add a `softmax` activation
    for multi-class regression, just as we did in the first two chapters, by running
    the following code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在密集层的线性步骤之后，我们可以添加`softmax`激活函数来进行多类回归，就像我们在前两章中所做的那样，通过运行以下代码：
- en: '[PRE12]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Training MNIST
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练MNIST
- en: Let's now put all of these elements together so we can train a ConvNet on the
    MNIST dataset.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将所有这些元素组合起来，这样我们就可以在MNIST数据集上训练卷积神经网络了。
- en: The model
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型
- en: 'First, we must specify the model, which we can do with the following code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须指定模型，可以使用以下代码来完成：
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the following code, you can see the general structure of a typical ConvNet:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，你可以看到典型卷积神经网络的一般结构：
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The convolution and pooling layers are often used together in these blocks;
    you can find neural networks that repeat the `Conv2D`, `MaxPool2D` combination
    tens of times.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层和池化层通常在这些模块中一起使用；你可以找到一些神经网络，重复使用`Conv2D`和`MaxPool2D`的组合多次。
- en: 'We can get an overview of our model with the following command:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下命令获得模型的概览：
- en: '[PRE15]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Which will give us the following output:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE16]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this summary, you can clearly see how the pooling layers reduce the size
    of the feature map. It's a little bit less obvious from the summary alone, but
    you can see how the output of the first `Conv2D` layer is 26x26 pixels, while
    the input images are 28x28 pixels.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个总结中，你可以清楚地看到池化层是如何减少特征图的大小的。仅从总结中看这点不太明显，但你可以看到第一个`Conv2D`层的输出是26x26像素，而输入图像是28x28像素。
- en: By using `valid` padding, `Conv2D` also reduces the size of the feature map,
    although only by a small amount. The same happens for the second `Conv2D` layer,
    which shrinks the feature map from 13x13 pixels to 11x11 pixels.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`valid`填充，`Conv2D`也减少了特征图的大小，尽管只是减少了少量。第二个`Conv2D`层也发生了同样的事情，它将特征图从13x13像素缩小到11x11像素。
- en: You can also see how the first convolutional layer only has 60 parameters, while
    the `Dense` layer has 3,010, over 50 times as many parameters. Convolutional layers
    usually achieve surprising feats with very few parameters, which is why they are
    so popular. The total number of parameters in a network can often be significantly
    reduced by convolutional and pooling layers.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以看到，第一个卷积层只有60个参数，而`Dense`层有3,010个参数，超过了卷积层的50倍。卷积层通常用非常少的参数就能实现惊人的效果，这也是它们如此受欢迎的原因。通过卷积层和池化层，网络中的总参数数目通常能显著减少。
- en: Loading the data
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载数据
- en: The MNIST dataset we are using comes preinstalled with Keras. When loading the
    data, make sure you have an internet connection if you want to use the dataset
    directly via Keras, as Keras has to download it first.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的MNIST数据集已预装在Keras中。加载数据时，如果你想直接通过Keras使用数据集，请确保你的设备已连接互联网，因为Keras需要先下载该数据集。
- en: 'You can import the dataset with the following code:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下代码导入数据集：
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As explained at the beginning of the chapter, we want to reshape the dataset
    so that it can have a channel dimension as well. The dataset as it comes does
    not have a channel dimension yet, but this is something we can do:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章开始时所解释的，我们希望将数据集重塑，使其也能具有通道维度。原始数据集本身还没有通道维度，但这是我们可以做的：
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'So, we add a channel dimension with NumPy, with the following code:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们使用NumPy添加一个通道维度，代码如下：
- en: '[PRE20]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now there is a channel dimension, as we can see here:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有了一个通道维度，正如我们在这里看到的：
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Compiling and training
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编译和训练
- en: 'In the previous chapters, we have used one-hot encoded targets for multiclass
    regression. While we have reshaped the data, the targets are still in their original
    form. They are a flat vector containing the numerical data representation for
    each handwritten figure. Remember that we have 60,000 of these in the MNIST dataset:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们使用了独热编码的目标进行多类回归。虽然我们已经重新塑形了数据，但目标仍然保持原始形式。它们是一个扁平的向量，包含每个手写数字的数值数据表示。记住，MNIST数据集中有60,000个这样的数据：
- en: '[PRE23]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Transforming targets through one-hot encoding is a frequent and annoying task,
    so Keras allows us to just specify a loss function that converts targets to one-hot
    on the fly. This loss function is called `sparse_categorical_crossentropy`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 通过独热编码转换目标是一个常见且令人烦恼的任务，因此Keras允许我们直接指定一个损失函数，在运行时将目标转换为独热编码。这个损失函数叫做`sparse_categorical_crossentropy`。
- en: It's the same as the categorical cross-entropy loss used in earlier chapters,
    the only difference is that this uses sparse, that is, not one-hot encoded, targets.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 它与前面章节中使用的分类交叉熵损失函数相同，唯一的区别是它使用的是稀疏目标，即不是独热编码的目标。
- en: Just as before, you still need to make sure that your network output has as
    many dimensions as there are classes.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，你仍然需要确保你的网络输出的维度与类别数相匹配。
- en: 'We''re now at a point where we can compile the model, which we can do with
    the following code:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经到达了可以编译模型的阶段，我们可以使用以下代码来进行编译：
- en: '[PRE25]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you can see, we are using an Adam optimizer. The exact workings of Adam are
    explained in the next section, *More bells and whistles for our neural network*,
    but for now, you can just think of it as a more sophisticated version of stochastic
    gradient descent.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们正在使用Adam优化器。Adam的具体工作原理将在下一节中解释，*为我们的神经网络增添更多功能*，但现在，你可以将它理解为一种比随机梯度下降更复杂的版本。
- en: 'When training, we can directly specify a validation set in Keras by running
    the following code:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时，我们可以通过运行以下代码直接指定验证集：
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once we have successfully run that code, we''ll get the following output:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们成功运行了这段代码，我们将得到以下输出：
- en: '[PRE27]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To better see what is going on, we can plot the progress of training with the
    following code:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解正在发生的情况，我们可以使用以下代码绘制训练进度：
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This will give us the following chart:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供以下图表：
- en: '![Compiling and training](img/B10354_03_13.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![编译与训练](img/B10354_03_13.jpg)'
- en: The visualized output of validation and training accuracy
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化的验证和训练准确度输出
- en: As you can see in the preceding chart, the model achieves about 98% validation
    accuracy, which is pretty nice!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的图表中看到的，模型达到了大约98%的验证准确率，这非常不错！
- en: More bells and whistles for our neural network
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为我们的神经网络添加更多功能
- en: Let's take a minute to look at some of the other elements of our neural network.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一点时间来看看我们神经网络的其他一些元素。
- en: Momentum
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动量
- en: In previous chapters we've explained gradient descent in terms of someone trying
    to find the way down a mountain by just following the slope of the floor. Momentum
    can be explained with an analogy to physics, where a ball is rolling down the
    same hill. A small bump in the hill would not make the ball roll in a completely
    different direction. The ball already has some momentum, meaning that its movement
    gets influenced by its previous movement.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们通过某人试图仅通过跟随地面坡度找到山下的路径来解释梯度下降。动量可以通过与物理学的类比来解释，其中一个小球沿着同样的山坡滚下。山坡上的小颠簸不会使小球滚向完全不同的方向。小球已经有了一些动量，这意味着它的运动会受到之前运动的影响。
- en: 'Instead of directly updating the model parameters with their gradient, we update
    them with the exponentially weighted moving average. We update our parameter with
    an outlier gradient, then we take the moving average, which will smoothen out
    outliers and capture the general direction of the gradient, as we can see in the
    following diagram:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是直接用梯度更新模型参数，而是使用指数加权的移动平均来更新它们。我们用一个离群点梯度更新参数，然后取移动平均，这将平滑掉离群点，并捕捉梯度的整体方向，正如我们在下面的图示中看到的那样：
- en: '![Momentum](img/B10354_03_14.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![动量](img/B10354_03_14.jpg)'
- en: How momentum smoothens gradient updates
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 动量如何平滑梯度更新
- en: 'The exponentially weighted moving average is a clever mathematical trick used
    to compute a moving average without having to memorize a set of previous values.
    The exponentially weighted average, *V*, of some value, ![Momentum](img/B10354_03_002.jpg),
    would be as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 指数加权移动平均是一个巧妙的数学技巧，用于计算移动平均，而无需记住一组之前的值。某个值的指数加权平均*V*，![动量](img/B10354_03_002.jpg)，将如下所示：
- en: '![Momentum](img/B10354_03_003.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![动量](img/B10354_03_003.jpg)'
- en: A beta value of 0.9 would mean that 90% of the mean would come from the previous
    moving average, ![Momentum](img/B10354_03_004.jpg), and 10% would come from the
    new value, ![Momentum](img/B10354_03_005.jpg).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: beta值为0.9意味着90%的均值将来自先前的移动平均，![动量](img/B10354_03_004.jpg)，10%将来自新的值，![动量](img/B10354_03_005.jpg)。
- en: Using momentum makes learning more robust against gradient descent pitfalls
    such as outlier gradients, local minima, and saddle points.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 使用动量使得学习对梯度下降中的陷阱（如离群梯度、局部最小值和鞍点）更具鲁棒性。
- en: 'We can augment the standard stochastic gradient descent optimizer in Keras
    with momentum by setting a value for beta, which we do in the following code:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在Keras中为beta设置一个值来增加标准的随机梯度下降优化器的动量，这就是我们在下面的代码中所做的：
- en: '[PRE29]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This little code snippet creates a stochastic gradient descent optimizer with
    a learning rate of 0.01 and a beta value of 0.9\. We can use it when we compile
    our model, as we''ll now do with this:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这段小代码片段创建了一个具有学习率0.01和beta值为0.9的随机梯度下降优化器。当我们编译模型时，可以使用它，就像我们现在要做的这样：
- en: '[PRE30]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The Adam optimizer
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Adam优化器
- en: Back in 2015, Diederik P. Kingma and Jimmy Ba created the **Adam** (**Adaptive
    Momentum Estimation**) optimizer. This is another way to make gradient descent
    work more efficiently. Over the past few years, this method has shown very good
    results and has, therefore, become a standard choice for many practitioners. For
    example, we've used it with the MNIST dataset.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在2015年，Diederik P. Kingma 和 Jimmy Ba 创建了**Adam**（**自适应动量估计**）优化器。这是另一种使梯度下降更加高效的方法。在过去的几年里，这种方法显示出了非常好的效果，因此，已成为许多实践者的标准选择。例如，我们已经在MNIST数据集上使用过它。
- en: 'First, the Adam optimizer computes the exponentially weighted average of the
    gradients, just like a momentum optimizer does. It achieves this with the following
    formula:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Adam优化器像动量优化器一样计算梯度的指数加权平均。它通过以下公式来实现这一点：
- en: '![The Adam optimizer](img/B10354_03_006.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![Adam优化器](img/B10354_03_006.jpg)'
- en: 'It then also computes the exponentially weighted average of the squared gradients:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它还会计算平方梯度的指数加权平均值：
- en: '![The Adam optimizer](img/B10354_03_007.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![Adam优化器](img/B10354_03_007.jpg)'
- en: 'It then updates the model parameters like this:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它会像这样更新模型参数：
- en: '![The Adam optimizer](img/B10354_03_008.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![Adam优化器](img/B10354_03_008.jpg)'
- en: Here ![The Adam optimizer](img/B10354_03_009.jpg) is a very small number to
    avoid division by zero.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的![Adam优化器](img/B10354_03_009.jpg)是一个非常小的数字，用于避免除以零的情况。
- en: This division by the root of squared gradients reduces the update speed when
    gradients are very large. It also stabilizes learning as the learning algorithm
    does not get thrown off track by outliers as much.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对平方梯度的根进行除法操作，会在梯度非常大的时候减慢更新速度。它还通过防止学习算法受到异常值的干扰，稳定了学习过程。
- en: Using Adam, we have a new hyperparameter. Instead of having just one momentum
    factor, ![The Adam optimizer](img/B10354_03_010.jpg), we now have two, ![The Adam
    optimizer](img/B10354_03_011.jpg) and ![The Adam optimizer](img/B10354_03_012.jpg).
    The recommended values for
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Adam时，我们有一个新的超参数。除了只有一个动量因子![Adam优化器](img/B10354_03_010.jpg)外，我们现在有两个，![Adam优化器](img/B10354_03_011.jpg)和![Adam优化器](img/B10354_03_012.jpg)。对于这些超参数的推荐值是
- en: '![The Adam optimizer](img/B10354_03_013.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![Adam优化器](img/B10354_03_013.jpg)'
- en: and ![The Adam optimizer](img/B10354_03_014.jpg) are 0.9 and 0.999 respectively.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 和![Adam优化器](img/B10354_03_014.jpg)分别是0.9和0.999。
- en: 'We can use Adam in Keras like this:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像这样在Keras中使用Adam：
- en: '[PRE31]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As you have seen earlier in this chapter, we can also compile the model just
    by passing the `adam` string as an optimizer. In this case, Keras will create
    an Adam optimizer for us and choose the recommended values.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本章前面看到的那样，我们也可以通过仅传递`adam`字符串作为优化器来编译模型。在这种情况下，Keras会为我们创建一个Adam优化器，并选择推荐的值。
- en: Regularization
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: Regularization is a technique used to avoid overfitting. Overfitting is when
    the model fits the training data too well, and as a result, it does not generalize
    well to either development or test data. You may see that overfitting is sometimes
    also referred to as "high variance," while underfitting, obtaining poor results
    on training, development, and test data, is referred to as "high bias."
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是一种用于防止过拟合的技术。过拟合是指模型对训练数据拟合得过于完美，导致它不能很好地泛化到开发数据或测试数据上。你可能会看到，过拟合有时也被称为“高方差”，而欠拟合，即在训练、开发和测试数据上都得到较差的结果，则被称为“高偏差”。
- en: In classical statistical learning, there is a lot of focus on the bias-variance
    tradeoff. The argument that is made is that a model that fits very well to the
    training set is likely to be overfitting and that some amount of underfitting
    (bias) has to be accepted in order to obtain good outcomes. In classical statistical
    learning, the hyperparameters that prevent overfitting also often prevent the
    training set fitting well.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的统计学习中，很多重点都放在了偏差-方差权衡上。观点是，一个非常符合训练集的模型很可能会过拟合，因此必须接受一定程度的欠拟合（偏差），以便获得良好的结果。在经典统计学习中，防止过拟合的超参数往往也会防止训练集拟合得很好。
- en: Regularization in neural networks, as it is presented here, is largely borrowed
    from classical learning algorithms. Yet, modern machine learning research is starting
    to embrace the concept of "orthogonality," the idea that different hyperparameters
    influence bias and variance.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的正则化，如这里所介绍的，主要借鉴了经典学习算法。然而，现代机器学习研究开始接受“正交性”这一概念，认为不同的超参数会影响偏差和方差。
- en: By separating those hyperparameters, the bias-variance tradeoff can be broken,
    and we can find models that generalize well and deliver accurate predictions.
    However, so far these efforts have only yielded small rewards, as low-bias and
    low-variance models require large amounts of training data.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分离这些超参数，可以打破偏差-方差权衡，从而找到能够很好地泛化并提供准确预测的模型。然而，到目前为止，这些努力仅仅取得了小小的成果，因为低偏差和低方差的模型需要大量的训练数据。
- en: L2 regularization
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L2正则化
- en: 'One popular technique to counter overfitting is L2 regularization. L2 regularization
    adds the sum of squared weights to the loss function. We can see an example of
    this in the formula below:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常用的防止过拟合的技术是L2正则化。L2正则化将权重平方的和添加到损失函数中。我们可以在下面的公式中看到这个例子：
- en: '![L2 regularization](img/B10354_03_015.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![L2正则化](img/B10354_03_015.jpg)'
- en: Here *N* is the number of training examples and ![L2 regularization](img/B10354_03_016.jpg)
    is the regularization hyperparameter, which determines how much we want to regularize,
    with a common value being around 0.01.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*N*是训练示例的数量，而![L2正则化](img/B10354_03_016.jpg)是正则化超参数，它决定了我们希望进行多大程度的正则化，常见值大约为0.01。
- en: Adding this regularization to the loss function means that high weights increase
    losses and the algorithm is incentivized to reduce the weights. Small weights,
    those around zero, mean that the neural network will rely less on them.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 将此正则化添加到损失函数中意味着较高的权重会增加损失，从而促使算法减少权重。较小的权重，接近零的权重，意味着神经网络会减少对它们的依赖。
- en: Therefore, a regularized algorithm will rely less on every single feature and
    every single node activation, and instead will have a more holistic view, taking
    into account many features and activations. This will prevent the algorithm from overfitting.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个正则化的算法会减少对每个特征和每个节点激活的依赖，而是更全面地看待问题，考虑多个特征和激活。这将防止算法发生过拟合。
- en: L1 regularization
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L1正则化
- en: 'L1 regularization is very similar to L2 regularization, but instead of adding
    the sum of squares, it adds the sum of absolute values, as we can see in this
    formula:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: L1正则化与L2正则化非常相似，但它不是加和平方，而是加和绝对值，如我们在这个公式中看到的：
- en: '![L1 regularization](img/B10354_03_018.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![L1正则化](img/B10354_03_018.jpg)'
- en: In practice, it is often a bit uncertain as to which of the two will work best,
    but the difference between the two is not very large.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，通常不确定哪种方法效果最佳，但两者之间的差异并不大。
- en: Regularization in Keras
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Keras中的正则化
- en: In Keras, regularizers that are applied to the weights are called **kernel_regularizer**,
    and regularizers that are applied to the bias are called **bias_regularizer**.
    You can also apply regularization directly to the activation of the nodes to prevent
    them from being activated very strongly with **activity_regularizer**.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，应用于权重的正则化器称为**kernel_regularizer**，应用于偏置的正则化器称为**bias_regularizer**。你还可以直接将正则化应用于节点的激活，以防止它们被过度激活，使用**activity_regularizer**。
- en: 'For now, let''s add some L2 regularization to our network. To do this, we need
    to run the following code:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在网络中添加一些L2正则化。为此，我们需要运行以下代码：
- en: '[PRE32]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Setting `kernel_regularizer` as done in the first convolutional layer in Keras
    means regularizing weights. Setting `bias_regularizer` regularizes the bias, and setting
    `activity_regularizer` regularizes the output activations of a layer.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras的第一个卷积层中将`kernel_regularizer`设置为正则化权重。将`bias_regularizer`设置为正则化偏置，设置`activity_regularizer`则是正则化一个层的输出激活。
- en: In this following example, the regularizers are set to be shown off, but here
    they actually harm the performance to our network. As you can see from the preceding
    training results, our network is not actually overfitting, so setting regularizers
    harms performance here, and as a result, the model underfits.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，正则化器被设置为启用，但在这里它们实际上会损害我们网络的性能。从前面的训练结果可以看出，我们的网络并没有发生过拟合，因此启用正则化器会损害性能，导致模型出现欠拟合。
- en: 'As we can see in the following output, in this case, the model reaches about
    87% validation accuracy:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在以下输出中看到的，在这种情况下，模型达到了大约87%的验证准确度：
- en: '[PRE33]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You'll notice that the model achieves a higher accuracy on the validation than
    on the training set; this is a clear sign of underfitting.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，模型在验证集上的准确度高于训练集；这是过拟合的明显迹象。
- en: Dropout
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout
- en: 'As the title of the 2014 paper by Srivastava et al gives away, *Dropout is
    A Simple Way to Prevent Neural Networks from Overfitting*. It achieves this by
    randomly removing nodes from the neural network:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Srivastava等人2014年论文标题所揭示的，*Dropout是一种简单的方法来防止神经网络过拟合*。它通过随机去除神经网络中的节点来实现这一点：
- en: '![Dropout](img/B10354_03_15.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B10354_03_15.jpg)'
- en: 'Schematic of the dropout method. From Srivastava et al, "Dropout: A Simple
    Way to Prevent Neural Networks from Overfitting," 2014'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 'Dropout方法的示意图。来自Srivastava等人，"Dropout: A Simple Way to Prevent Neural Networks
    from Overfitting"，2014年'
- en: With dropout, each node has a small probability of having its activation set
    to zero. This means that the learning algorithm can no longer rely heavily on
    single nodes, much like in L2 and L1 regularization. Dropout therefore also has
    a regularizing effect.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 使用dropout时，每个节点都有小概率将其激活值设置为零。这意味着学习算法不再可以过度依赖单一节点，就像L2和L1正则化一样。因此，dropout也具有正则化作用。
- en: 'In Keras, dropout is a new type of layer. It''s put after the activations you
    want to apply dropout to. It passes on activations, but sometimes it sets them
    to zero, achieving the same effect as a dropout in the cells directly. We can
    see this in the following code:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，dropout 是一种新的层。它被放置在你希望应用 dropout 的激活函数后面。它会传递激活值，但有时会将其设置为零，从而实现与直接在单元中使用
    dropout 相同的效果。我们可以通过以下代码看到这一点：
- en: '[PRE34]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: A dropout value of 0.5 is considered a good choice if overfitting is a serious
    problem, while values that are over 0.5 are not very helpful, as the network would
    have too few values to work with. In this case, we chose a dropout value of 0.2,
    meaning that each cell has a 20% chance of being set to zero.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果过拟合是一个严重问题，dropout 值为 0.5 被认为是一个不错的选择，而大于 0.5 的值则不太有用，因为网络会处理太少的值。在这种情况下，我们选择了
    0.2 的 dropout 值，这意味着每个单元有 20% 的概率被设置为零。
- en: 'Note that dropout is used after pooling:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，dropout 是在池化之后使用的：
- en: '[PRE35]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The low dropout value creates nice results for us, but again, the network does
    better on the validation set rather than the training set, a clear sign of underfitting
    taking place. Note that dropout is only applied at training time. When the model
    is used for predictions, dropout doesn't do anything.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的 dropout 值为我们创造了良好的结果，但同样，网络在验证集上的表现优于训练集，这是出现欠拟合的明显标志。请注意，dropout 只在训练时应用。当模型用于预测时，dropout
    不会起作用。
- en: Batchnorm
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Batchnorm
- en: '**Batchnorm**, short for **batch** **normalization**, is a technique for "normalizing"
    input data to a layer batch-wise. Each batchnorm computes the mean and standard
    deviation of the data and applies a transformation so that the mean is zero and
    the standard deviation is one.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**Batchnorm**，即**批量归一化**，是一种对输入数据进行“归一化”的技术，按批次进行处理。每个 batchnorm 计算数据的均值和标准差，并应用变换，使得均值为零，标准差为一。'
- en: This makes training easier because the loss surface becomes more "round." Different means
    and standard deviations along different input dimensions would mean that the network
    would have to learn a more complicated function.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得训练变得更加容易，因为损失曲面变得更加“圆滑”。不同输入维度的均值和标准差意味着网络必须学习一个更复杂的函数。
- en: 'In Keras, batchnorm is a new layer as well, as you can see in the following
    code:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，batchnorm 也是一个新的层，如以下代码所示：
- en: '[PRE37]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Batchnorm often accelerates training by making it easier. You can see how the
    accuracy rate jumps up in the first epoch here:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Batchnorm 通常通过简化训练过程来加速训练。你可以看到在第一轮训练中准确率的跃升：
- en: '![Batchnorm](img/B10354_03_17.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![Batchnorm](img/B10354_03_17.jpg)'
- en: Training and validation accuracy of our MNIST classifier with batchnorm
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 batchnorm 的 MNIST 分类器的训练和验证准确率
- en: Batchnorm also has a mildly regularizing effect. Extreme values are often overfitted
    to, and batchnorm reduces extreme values, similar to activity regularization.
    All this makes batchnorm an extremely popular tool in computer vision.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Batchnorm 也有轻微的正则化效果。极端值通常会被过拟合，而 batchnorm 减少了这些极端值，类似于激活正则化。这一切使得 batchnorm
    在计算机视觉中成为一个非常流行的工具。
- en: Working with big image datasets
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理大规模图像数据集
- en: Images tend to be big files. In fact, it's likely that you will not be able
    to fit your entire image dataset into your machine's RAM.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图像通常是大文件。事实上，可能无法将整个图像数据集加载到机器的内存中。
- en: Therefore, we need to load the images from disk "just in time" rather than loading
    them all in advance. In this section, we will be setting up an image data generator
    that loads images on the fly.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要“及时”从磁盘加载图像，而不是提前加载所有图像。在这一部分，我们将设置一个图像数据生成器，按需加载图像。
- en: We'll be using a dataset of plant seedlings in this case. This was provided
    by Thomas Giselsson and others, 2017, via their publication, *A Public Image Database
    for Benchmark of Plant Seedling Classification Algorithms*.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用植物幼苗的数据集。该数据集由 Thomas Giselsson 等人于 2017 年通过他们的出版物《A Public Image
    Database for Benchmark of Plant Seedling Classification Algorithms》提供。
- en: 'This dataset is available from the following link: [https://arxiv.org/abs/1711.05458](https://arxiv.org/abs/1711.05458).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集可以通过以下链接访问：[https://arxiv.org/abs/1711.05458](https://arxiv.org/abs/1711.05458)。
- en: You may be wondering why we're looking at plants; after all, plant classifications
    are not a common problem that is faced in the finance sector. The simple answer
    is that this dataset lends itself to demonstrating many common computer vision
    techniques and is available under an open domain license; it's therefore a great
    training dataset for us to use.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们要研究植物呢？毕竟，植物分类在金融领域并不是一个常见的问题。简单的回答是，这个数据集适合展示许多常见的计算机视觉技术，并且它是开放域许可的，因此是一个非常适合我们使用的训练数据集。
- en: 'Readers who wish to test their knowledge on a more relevant dataset should
    take a look at the *State Farm Distracted Driver* dataset as well as the *Planet:
    Understanding the Amazon from Space* dataset.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '想要在更相关的数据集上测试自己知识的读者，可以查看*State Farm Distracted Driver*数据集和*Planet: Understanding
    the Amazon from Space*数据集。'
- en: Note
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The code and data for this section and the section on stacking pretrained models
    can be found and run here: [https://www.kaggle.com/jannesklaas/stacking-vgg](https://www.kaggle.com/jannesklaas/stacking-vgg).'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 本节以及关于堆叠预训练模型的章节中的代码和数据可以在此处找到并运行：[https://www.kaggle.com/jannesklaas/stacking-vgg](https://www.kaggle.com/jannesklaas/stacking-vgg)。
- en: 'Keras comes with an image data generator that can load files from disk out
    of the box. To do this, you simply need to run:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Keras自带了一个图像数据生成器，可以直接从磁盘加载文件。为此，您只需运行：
- en: '[PRE39]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: To obtain a generator reading from the files, we first have to specify the generator.
    In Keras, `ImageDataGenerator` offers a range of image augmentation tools, but
    in our example, we will only be making use of the rescaling function.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从文件中获取生成器，我们首先需要指定生成器。在Keras中，`ImageDataGenerator`提供了一系列图像增强工具，但在我们的示例中，我们只会使用重新缩放功能。
- en: 'Rescaling multiplies all values in an image with a constant. For most common
    image formats, the color values range from 0 to 255, so we want to rescale by
    1/255\. We can achieve this by running the following:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 重新缩放是将图像中的所有值与常数相乘。对于大多数常见的图像格式，颜色值的范围是从0到255，所以我们希望将其缩放为1/255。我们可以通过运行以下代码来实现：
- en: '[PRE40]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This, however, is not yet the generator that loads the images for us. The `ImageDataGenerator`
    class offers a range of generators that can be created by calling functions on
    it.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这还不是一个加载图像的生成器。`ImageDataGenerator`类提供了一系列可以通过调用其函数来创建的生成器。
- en: To obtain a generator loading file, we have to call `flow_from_directory`.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取一个加载文件的生成器，我们需要调用`flow_from_directory`。
- en: 'We then have to specify the directory Keras should use, the batch size we would
    like, in this case `32`, as well as the target size the images should be resized
    to, in this case 150x150 pixels. To do this, we can simply run the following code:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要指定Keras应使用的目录、我们希望的批次大小，在本例中为`32`，以及图像应调整为的目标大小，在本例中为150x150像素。为此，我们可以简单地运行以下代码：
- en: '[PRE41]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'How did Keras find the images and how does it know which classes the images
    belong to? The Keras generator expects the following folder structure:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是如何找到图像的，如何知道这些图像属于哪个类别的呢？Keras的生成器需要以下文件夹结构：
- en: 'Root:'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根目录：
- en: Class 0
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别 0
- en: img
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: img
- en: img
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: img
- en: …
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: …
- en: Class 1
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别 1
- en: img
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: img
- en: img
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: img
- en: …
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: …
- en: Class 1
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别 1
- en: img
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: img
- en: Our dataset is already set up that way, and it's usually not hard to sort images
    to match the generator's expectations.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集已经按这种方式设置好了，通常将图像分类以匹配生成器的预期并不困难。
- en: Working with pretrained models
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练模型
- en: Training large computer vision models is not only hard, but computationally
    expensive. Therefore, it's common to use models that were originally trained for another
    purpose and fine-tune them for a new purpose. This is an example of transfer learning.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型计算机视觉模型不仅困难，而且计算开销巨大。因此，通常使用最初为其他目的训练的模型，并对其进行微调以适应新任务。这就是迁移学习的一个例子。
- en: Transfer learning aims to transfer the learning from one task to another task.
    As humans, we are very good at transferring what we have learned. When you see
    a dog that you have not seen before, you don't need to relearn everything about
    dogs for this particular dog; instead, you just transfer new learning to what
    you already knew about dogs. It's not economical to retrain a big network every
    time, as you'll often find that there are parts of the model that we can reuse.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的目标是将一个任务的学习迁移到另一个任务中。作为人类，我们非常擅长转移我们学到的知识。当你看到一只以前没见过的狗时，你不需要为这只特定的狗重新学习关于狗的所有知识；相反，你只是将新学到的知识转移到你已经知道的关于狗的知识上。每次重新训练一个大网络并不经济，因为你会发现模型中有一些部分是可以重用的。
- en: In this section, we will fine-tune VGG-16, originally trained on the ImageNet
    dataset. The ImageNet competition is an annual computer vision competition, and
    the ImageNet dataset consists of millions of images of real-world objects, from
    dogs to planes.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对VGG-16进行微调，该模型最初是在ImageNet数据集上训练的。ImageNet竞赛是一个年度计算机视觉竞赛，ImageNet数据集包含了数百万张现实世界物体的图像，从狗到飞机。
- en: In the ImageNet competition, researchers compete to build the most accurate
    models. In fact, ImageNet has driven much of the progress in computer vision over
    the recent years, and the models built for ImageNet competitions are a popular
    basis to fine-tune models from.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在ImageNet竞赛中，研究人员竞争构建最精确的模型。事实上，ImageNet推动了计算机视觉领域近年来的许多进展，而为ImageNet竞赛构建的模型也是微调其他模型的常见基础。
- en: VGG-16 is a model architecture developed by the visual geometry group at Oxford
    University. The model consists of a convolutional part and a classification part.
    We will only be using the convolutional part. In addition, we will be adding our
    own classification part that can classify plants.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: VGG-16是由牛津大学视觉几何组开发的模型架构。该模型由卷积部分和分类部分组成。我们只会使用卷积部分。此外，我们将添加我们自己的分类部分，用于植物分类。
- en: 'VGG-16 can be downloaded via Keras by using the following code:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下代码在Keras中下载VGG-16：
- en: '[PRE42]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'When downloading the data, we want to let Keras know that we don''t want to
    include the top part (the classification part); we also want to let Keras know
    the desired input shape. If we do not specify the input shape, the model will
    accept any image size, and it will not be possible to add `Dense` layers on top:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 下载数据时，我们希望让Keras知道我们不希望包含顶部部分（分类部分）；我们还希望告诉Keras期望的输入形状。如果不指定输入形状，模型将接受任何图像大小，并且无法在其上添加`Dense`层：
- en: '[PRE44]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: As you can see, the VGG model is very large, with over 14.7 million trainable
    parameters. It also consists of both `Conv2D` and `MaxPooling2D` layers, both
    of which we've already learned about when working on the MNIST dataset.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，VGG模型非常庞大，具有超过1470万个可训练参数。它还由`Conv2D`和`MaxPooling2D`层组成，这两种层我们在处理MNIST数据集时已经学习过。
- en: 'From this point, there are two different ways we can proceed:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 从此点开始，我们有两种不同的方式可以继续：
- en: Add layers and build a new model.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加层并构建一个新模型。
- en: Preprocess all the images through the pertained model and then train a new model.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过预训练模型预处理所有图像，然后训练一个新的模型。
- en: Modifying VGG-16
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改VGG-16
- en: In this section, we will be adding layers on top of the VGG-16 model, and then
    from there, we will train the new, big model.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在VGG-16模型的基础上添加层，然后从那里开始训练新的大模型。
- en: 'We do not want to retrain all those convolutional layers that have been trained
    already, however. So, we must first "freeze" all the layers in VGG-16, which we
    can do by running the following:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们并不想重新训练那些已经训练好的卷积层。因此，我们必须首先“冻结”VGG-16中的所有层，我们可以通过运行以下代码来实现：
- en: '[PRE46]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Keras downloads VGG as a functional API model. We will learn more about the
    functional API in [Chapter 6](ch06.xhtml "Chapter 6. Using Generative Models"),
    *Using Generative Models*, but for now, we just want to use the Sequential API,
    which allows us to stack layers through `model.add()`. We can convert a model
    with the functional API with the following code:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Keras将VGG下载为一个函数式API模型。我们将在[第6章](ch06.xhtml "第6章：使用生成模型")《使用生成模型》中详细学习函数式API，但目前我们只想使用顺序API，它允许我们通过`model.add()`堆叠层。我们可以使用以下代码将一个函数式API模型转换为顺序API模型：
- en: '[PRE47]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: As a result of running the code, we have now created a new model called `finetune`
    that works just like a normal Sequential model. We need to remember that converting
    models with the Sequential API only works if the model can actually be expressed
    in the Sequential API. Some more complex models cannot be converted.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码的结果是，我们现在已经创建了一个新的模型，名为`finetune`，它的功能与普通的顺序模型相同。我们需要记住，使用顺序API转换模型仅在模型能够实际表达为顺序API时才有效。一些更复杂的模型无法转换。
- en: 'As a result of everything we''ve just done, adding layers to our model is now
    simple:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们刚才所做的一切，向我们的模型中添加层现在变得很简单：
- en: '[PRE48]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The newly added layers are by default trainable, while the reused model socket
    is not. We can train this stacked model just as we would train any other model,
    on the data generator we defined in the previous section. This can be executed
    by running the following code:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 新添加的层默认是可训练的，而重用的模型插槽则不是。我们可以像训练任何其他模型一样，在我们之前定义的数据生成器上训练这个堆叠模型。可以通过运行以下代码来执行：
- en: '[PRE49]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: After running this, the model manages to achieve a rate of about 75% validation
    accuracy.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后，模型的验证准确率大约能达到75%。
- en: Random image augmentation
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机图像增强
- en: A general problem in machine learning is that no matter how much data we have,
    having more data will always be better, as it would increase the quality of our
    output while also preventing overfitting and allowing our model to deal with a
    larger variety of inputs. It's therefore common to apply random augmentation to
    images, for example, a rotation or a random crop.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的一个普遍问题是，不管我们有多少数据，更多的数据总是更好的，因为它可以提高输出质量，同时防止过拟合，并让我们的模型处理更广泛的输入。因此，通常会对图像应用随机增强，例如旋转或随机裁剪。
- en: The idea is to get a large number of different images out of one image, therefore
    reducing the chance that the model will overfit. For most image augmentation purposes,
    we can just use Keras' `ImageDataGenerator`.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这个思路是通过从一张图像中生成大量不同的图像，从而减少模型过拟合的机会。对于大多数图像增强目的，我们可以直接使用Keras的`ImageDataGenerator`。
- en: More advanced augmentations can be done with the OpenCV library. However, focusing
    on this is outside the scope of this chapter.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 更高级的增强可以通过OpenCV库来实现。然而，专注于这一点超出了本章的范围。
- en: Augmentation with ImageDataGenerator
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用ImageDataGenerator进行增强
- en: When using an augmenting data generator, we only usually use it for training.
    The validation generator should not use the augmentation features because when
    we validate our model, we want to estimate how well it is doing on unseen, actual
    data, and not augmented data.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用增强数据生成器时，我们通常只在训练中使用它。验证生成器不应使用增强特性，因为当我们验证模型时，我们希望评估它在未见过的真实数据上的表现，而不是增强数据。
- en: 'This is different from rule-based augmentation, where we try to create images
    that are easier to classify. For this reason, we need to create two `ImageDataGenerator`
    instances, one for training and one for validation. This can be done by running
    the following code:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这与基于规则的增强不同，在基于规则的增强中，我们试图创建更容易分类的图像。因此，我们需要创建两个`ImageDataGenerator`实例，一个用于训练，另一个用于验证。这可以通过运行以下代码实现：
- en: '[PRE50]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This training data generator makes use of a few built-in augmentation techniques.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练数据生成器利用了一些内建的增强技术。
- en: Note
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: There are more commands available in Keras. For a full list, you
    should refer to the Keras documentation at [https://keras.io/](https://keras.io/).'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：Keras中还有更多命令可用。完整的命令列表请参考Keras文档：[https://keras.io/](https://keras.io/)。'
- en: 'In the following list, we''ve highlighted several commonly used commands:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的列表中，我们已突出显示几个常用的命令：
- en: '`rescale` scales the values in the image. We used it before and will also use
    it for validation.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale`对图像中的值进行缩放。我们之前使用过它，也将在验证中使用它。'
- en: '`rotation_range` is a range (0 to 180 degrees) in which to randomly rotate
    the image.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rotation_range`是一个范围（0到180度），用于随机旋转图像。'
- en: '`width_shift_range` and `height_shift_range` are ranges (relative to the image
    size, so here 20%) in which to randomly stretch images horizontally or vertically.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width_shift_range`和`height_shift_range`是范围（相对于图像大小，这里为20%），用于随机地水平或垂直拉伸图像。'
- en: '`shear_range` is a range (again, relative to the image) in which to randomly
    apply shear.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shear_range`是一个范围（同样，相对于图像）用于随机应用剪切。'
- en: '`zoom_range` is the range in which to randomly zoom into a picture.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zoom_range`是随机缩放图像的范围。'
- en: '`horizontal_flip` specifies whether to randomly flip the image.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`horizontal_flip`指定是否随机翻转图像。'
- en: '`fill_mode` specifies how to fill empty spaces created by, for example, rotation.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fill_mode`指定如何填充由旋转等操作创建的空白区域。'
- en: We can check out what the generator does by running one image through it multiple
    times.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将一张图片多次通过生成器来查看它的作用。
- en: 'First, we need to import the Keras image tools and specify an image path (this one
    was chosen at random). This can be done by running the following:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入Keras图像工具并指定图像路径（这个是随机选择的）。这可以通过运行以下代码实现：
- en: '[PRE51]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We then need to load the image and convert it to a NumPy array, which is achieved
    with the following code:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要加载图像并将其转换为NumPy数组，这可以通过以下代码实现：
- en: '[PRE52]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'As before, we have to add a batch size dimension to the image:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们需要向图像添加一个批处理大小维度：
- en: '[PRE53]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We then use the `ImageDataGenerator` instance we just created, but instead
    of using `flow_from_directory`, we''ll use `flow`, which allows us to pass the
    data directly into the generator. We then pass that one image we want to use,
    which we can do by running this:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用刚才创建的`ImageDataGenerator`实例，但不是使用`flow_from_directory`，而是使用`flow`，它允许我们直接将数据传递给生成器。然后我们传递我们想要使用的那一张图像，可以通过运行以下代码来实现：
- en: '[PRE54]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'In a loop, we then call `next` on our generator four times:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个循环中，我们接着对生成器调用`next`四次：
- en: '[PRE55]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This will produce the following output:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Augmentation with ImageDataGenerator](img/B10354_03_18.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![使用ImageDataGenerator进行数据增强](img/B10354_03_18.jpg)'
- en: A few samples of the randomly modified image
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 一些随机修改过的图像样本
- en: The modularity tradeoff
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块化的权衡
- en: This chapter has shown that it is possible, and often useful, to aid a machine
    learning model with some rule-based system. You might also have noticed that the
    images in the dataset were all cropped to show only one plant.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示了通过一些基于规则的系统来辅助机器学习模型是可能的，且往往是有用的。你可能还注意到数据集中图像都被裁剪，只显示了一种植物。
- en: While we could have built a model to locate and classify the plants for us,
    in addition to classifying it, we could have also built a system that would output
    the treatment a plant should directly receive. This begs the question of how modular
    we should make our systems.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们本可以构建一个模型来定位并分类植物，但除了分类之外，我们也可以构建一个系统，直接输出植物应接受的治疗。这就引出了一个问题：我们应该让系统的模块化程度达到什么水平？
- en: 'End-to-end deep learning was all the rage for several years. If given a huge
    amount of data, a deep learning model can learn what would otherwise have taken
    a system with many components much longer to learn. However, end-to-end deep learning
    does have several drawbacks:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端深度学习在几年前非常流行。如果有大量数据，深度学习模型可以学习到原本需要多个组件的系统才能学会的内容。然而，端到端深度学习也有几个缺点：
- en: End-to-end deep learning needs huge amounts of data. Because models have so
    many parameters, a large amount of data is needed in order to avoid overfitting.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端到端深度学习需要大量的数据。因为模型有很多参数，所以需要大量数据来避免过拟合。
- en: End-to-end deep learning is hard to debug. If you replace your entire system
    with one black box model, you have little hope of finding out why certain things
    happened.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端到端深度学习很难调试。如果你用一个黑盒模型替换整个系统，你几乎没有希望找出为什么某些事情发生了。
- en: Some things are hard to learn but easy to write down as a code, especially sanity-check
    rules.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些东西很难学习，但很容易写成代码，特别是理智检查规则。
- en: 'Recently, researchers have begun to make their models more modular. A great
    example is Ha and Schmidthuber''s *World Models*, which can be read here: [https://worldmodels.github.io/](https://worldmodels.github.io/).
    In this, they''ve encoded visual information, made predictions about the future,
    and chosen actions with three different models.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究人员开始让他们的模型更加模块化。一个很好的例子是Ha和Schmidthuber的*世界模型*，你可以在这里阅读：[https://worldmodels.github.io/](https://worldmodels.github.io/)。在这个模型中，他们编码了视觉信息，做出了关于未来的预测，并通过三个不同的模型选择行动。
- en: 'On the practical side, we can take a look at Airbnb, who combine structural
    modeling with machine learning for their pricing engine. You can read more about
    it here: [https://medium.com/airbnb-engineering/learning-market-dynamics-for-optimal-pricing-97cffbcc53e3](https://medium.com/airbnb-engineering/learning-market-dynamics-for-optimal-pricing-97cffbcc53e3).
    Modelers knew that bookings roughly follow a Poisson Distribution and that there
    are also seasonal effects. So, Airbnb built a model to predict the parameters
    of the distribution and seasonality directly, rather than letting the model predict
    bookings directly.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际角度来看，我们可以看看Airbnb，他们将结构化建模与机器学习结合，用于其定价引擎。你可以在这里阅读更多内容：[https://medium.com/airbnb-engineering/learning-market-dynamics-for-optimal-pricing-97cffbcc53e3](https://medium.com/airbnb-engineering/learning-market-dynamics-for-optimal-pricing-97cffbcc53e3)。建模人员知道预订大致遵循泊松分布，并且也有季节性效应。因此，Airbnb构建了一个模型，直接预测分布的参数和季节性，而不是让模型直接预测预订量。
- en: If you have a small amount of data, then your algorithm's performance needs
    to come from human insight. If some subtasks can be easily expressed in code,
    then it's usually better to express them in code. If you need explainability and
    want to see why certain choices were made, a modular setup with clearly interpretable
    intermediate outputs is a good choice. However, if a task is hard and you don't
    know exactly what subtasks it entails, and you have lots of data, then it's often
    better to use an end-to-end approach.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只有少量数据，那么你的算法性能需要依赖于人类的洞察力。如果某些子任务可以轻松地用代码表达，那么通常最好通过代码来表达它们。如果你需要可解释性，并且希望理解为何做出某些选择，模块化设置与清晰可解释的中间输出是一个不错的选择。然而，如果任务很难，且你不清楚它包含哪些子任务，并且有大量数据，那么通常最好使用端到端方法。
- en: It's very rare to use a *pure* end-to-end approach. Images, for example, are
    always preprocessed from the camera chip, you never really work with raw data.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*纯*端到端方法是非常罕见的。例如，图像总是经过相机芯片的预处理，你从不直接处理原始数据。
- en: Being smart about dividing a task can boost performance and reduce risk.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 聪明地划分任务可以提高性能并降低风险。
- en: Computer vision beyond classification
  id: totrans-379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉超越分类
- en: As we have seen, there are many techniques that we can use to make our image
    classifier work better. These are techniques that you'll find used throughout
    this book, and not only for computer vision applications.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，有许多技术可以帮助我们的图像分类器提高性能。这些技术贯穿本书的多个章节，不仅限于计算机视觉应用。
- en: In this final section of the chapter, we will discuss some approaches that go
    beyond classifying images. These tasks often require more creative use of neural
    networks than what we've discussed throughout this chapter.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一节中，我们将讨论一些超越图像分类的方法。这些任务通常需要比本章所讨论的内容更具创意的神经网络使用方法。
- en: To get the most out of this section, you don't need to worry too much about
    the details of the techniques presented, but instead look at how researchers were
    creative about using neural networks. We're taking this approach because you will
    often find that the tasks you are looking to solve require similar creativity.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分理解本节内容，你不需要过多担心所呈现技术的细节，而是要关注研究人员在使用神经网络时的创意方法。我们采取这种方法是因为你会经常发现，解决的问题往往也需要类似的创造性。
- en: Facial recognition
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面部识别
- en: Facial recognition has many applications for retail institutions. For instance,
    if you're in the front office, you might want to automatically recognize your
    customer at an ATM, or alternatively, you might want to offer face-based security
    features, such as the iPhone offers. In the back office, however, you need to
    comply with KYC regulations, which require you to identify which customer you
    are working with.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 面部识别在零售行业有许多应用。例如，如果你在前台，可能希望在ATM上自动识别客户，或者你可能想提供基于面部的安全功能，如iPhone提供的那样。然而，在后台，你需要遵守KYC（了解你的客户）法规，要求你识别正在合作的客户。
- en: On the surface, facial recognition looks like a classification task. You give
    an image of a face to the machine, and it will predict which person it is. The
    trouble is that you might have millions of customers, but only one or two pictures
    per customer.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 表面上，面部识别看起来像是一个分类任务。你将一张面部图像输入机器，它会预测这张脸属于哪个人。问题是，你可能有数百万个客户，但每个客户只有一两张照片。
- en: On top of that, you'll likely be continuously getting new customers. You can't
    change your model every time you get a new customer, and a simple classification
    approach will fail if it has to choose between millions of classes with only one
    example for each class.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可能会不断获得新客户。每次获得新客户时，你无法改变模型，如果它必须在数百万个类别中选择，而每个类别只有一个样本，那么简单的分类方法将失败。
- en: 'The creative insight here is that instead of classifying the customer''s face,
    you can see whether two images show the same face. You can see a visual representation
    of this idea in the following diagram:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的创意思路是，与其对客户的面部进行分类，不如判断两张图像是否展示的是同一张面孔。你可以在以下示意图中看到这一思路的可视化表示：
- en: '![Facial recognition](img/B10354_03_19.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![面部识别](img/B10354_03_19.jpg)'
- en: Schematic of a Siamese network
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 一种孪生网络示意图
- en: To this end, you'll have to run the two images through first. A Siamese network
    is a class of neural network architecture that contains two or more identical
    subnetworks, both of which are identical and contain the same weights. In Keras,
    you can achieve such a setup by defining the layers first and then using them
    in both networks. The two networks then feed into a single classification layer,
    which determines whether the two images show the same face.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，您需要首先将两张图像通过网络。Siamese网络是一类包含两个或更多相同子网络的神经网络架构，这些子网络完全相同并且包含相同的权重。在Keras中，您可以通过先定义层，然后在两个网络中使用它们来实现这种设置。然后，这两个网络的输出会输入到一个单一的分类层，来判断这两张图像是否显示相同的面孔。
- en: To avoid running all of the customer images in our database through the entire
    Siamese network every time we want to recognize a face, it's common to save the
    final output of the Siamese network. The final output of the Siamese network for
    an image is called the face embedding. When we want to recognize a customer, we compare
    the embedding of the image of the customer's face with the embeddings stored in
    our database. We can do this with a single classification layer.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免每次想要识别人脸时，都将我们数据库中的所有客户图像通过整个Siamese网络，一般会保存Siamese网络的最终输出。Siamese网络对图像的最终输出被称为人脸嵌入。当我们想要识别一个客户时，我们将客户面部图像的嵌入与我们数据库中存储的嵌入进行比较。我们可以通过单一分类层来实现这一点。
- en: Storing facial embedding is very beneficial as it will save us a significant
    amount of computational cost, in addition to allowing for the clustering of faces.
    Faces will cluster together according to traits such as sex, age, and race. By
    only comparing an image to the images in the same cluster, we can save even more
    computational power and, as a result, get even faster recognition.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 存储人脸嵌入非常有益，因为它能节省大量计算成本，并且允许进行人脸聚类。人脸会根据性别、年龄、种族等特征聚集在一起。通过仅将图像与同一聚类中的图像进行比较，我们可以进一步节省计算资源，从而实现更快的识别。
- en: 'There are two ways to train Siamese networks. We can train them together with
    the classifier by creating pairs of matching and non-matching images and then
    using binary cross-entropy classification loss to train the entire model. However,
    another, and in many respects better, option is to train the model to generate
    face embeddings directly. This approach is described in Schroff, Kalenichenko,
    and Philbin''s 2015 paper, *FaceNet: A Unified Embedding for Face Recognition
    and Clustering*, which you can read here: [https://arxiv.org/abs/1503.03832](https://arxiv.org/abs/1503.03832).'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '训练Siamese网络有两种方法。我们可以通过创建匹配和不匹配图像的对，并使用二元交叉熵分类损失来训练整个模型，从而将它们与分类器一起训练。然而，另一种方法，通常被认为更好的方法，是直接训练模型生成面部嵌入。这种方法在Schroff、Kalenichenko和Philbin的2015年论文《FaceNet:
    A Unified Embedding for Face Recognition and Clustering》中有详细描述，您可以在这里阅读：[https://arxiv.org/abs/1503.03832](https://arxiv.org/abs/1503.03832)。'
- en: 'The idea is to create triplets of images: one anchor image, one positive image
    showing the same face as the anchor image, and one negative image showing a different
    face than the anchor image. A triplet loss is used to make the distance between
    the anchor''s embedding and the positive''s embedding smaller, and the distance
    between the anchor and the negative larger.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是创建图像三元组：一个锚定图像，一个显示与锚定图像相同面孔的正样本图像，以及一个显示与锚定图像不同面孔的负样本图像。使用三元组损失来使锚定图像的嵌入与正样本嵌入之间的距离更小，而锚定图像与负样本之间的距离更大。
- en: 'The loss function looks like this:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数看起来是这样的：
- en: '![Facial recognition](img/B10354_03_019.jpg)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![人脸识别](img/B10354_03_019.jpg)'
- en: Here ![Facial recognition](img/B10354_03_020.jpg) is an anchor image, and ![Facial
    recognition](img/B10354_03_021.jpg) is the output of the Siamese network, the
    anchor image's embedding. The triplet loss is the Euclidean distance between the
    anchor and the positive minus the Euclidean distance between the anchor and the
    negative. A small constant, ![Facial recognition](img/B10354_03_022.jpg), is a
    margin enforced between positive and negative pairs. To reach zero loss, the difference
    between distances needs to be ![Facial recognition](img/B10354_03_023.jpg).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这里![人脸识别](img/B10354_03_020.jpg)是一个锚定图像，![人脸识别](img/B10354_03_021.jpg)是Siamese网络的输出，锚定图像的嵌入。三元组损失是锚定图像与正样本之间的欧几里得距离减去锚定图像与负样本之间的欧几里得距离。一个小常数，![人脸识别](img/B10354_03_022.jpg)，是在正负样本对之间施加的边距。为了达到零损失，距离之间的差异需要是![人脸识别](img/B10354_03_023.jpg)。
- en: You should be able to understand that you can use a neural network to predict
    whether two items are semantically the same in order to get around large classification
    problems. You can train the Siamese model through some binary classification tasks
    but also by treating the outputs as embeddings and using a triplet loss. This
    insight extends to more than faces. If you wanted to compare time series to classify
    events, then you could use the exact same approach.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够理解，你可以使用神经网络来预测两个项目是否在语义上相同，从而解决大规模分类问题。你可以通过一些二分类任务来训练Siamese模型，也可以通过将输出视为嵌入并使用三元组损失来训练。这一见解不仅适用于人脸。如果你想比较时间序列以进行事件分类，那么你可以使用完全相同的方法。
- en: Bounding box prediction
  id: totrans-399
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 边界框预测
- en: 'The likelihood is that at some point, you''ll be interested in locating objects
    within images. For instance, say you are an insurance company that needs to inspect
    the roofs it insures. Getting people to climb on roofs to check them is expensive,
    so an alternative is to use satellite imagery. Having acquired the images, you
    now need to find the roofs in them, as we can see in the following screenshot.
    You can then crop out the roofs and send the roof images to your experts, who
    will check them:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有某个时刻，你会对在图像中定位物体感兴趣。例如，假设你是一家保险公司，需要检查所承保的房屋屋顶。让人爬上屋顶进行检查非常昂贵，因此一种替代方案是使用卫星图像。在获得这些图像后，你现在需要找到其中的屋顶，就像我们在以下屏幕截图中看到的那样。然后，你可以裁剪出屋顶图像并将其发送给专家，让他们进行检查：
- en: '![Bounding box prediction](img/B10354_03_20.jpg)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![边界框预测](img/B10354_03_20.jpg)'
- en: California homes with bounding boxes around their roofs
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 标出屋顶边界框的加州住宅
- en: What you need are bounding box predictions. A bounding box predictor outputs
    the coordinates of several bounding boxes together with predictions for what object
    is shown in the box.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要的是边界框预测。边界框预测器输出多个边界框的坐标，并预测每个框中显示的物体。
- en: There are two approaches to obtaining such bounding boxes.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 获取这种边界框有两种方法。
- en: A **Region-based Convolutional Neural Network** (**R-CNN**) reuses a classification
    model. It takes an image and slides the classification model over the image. The
    result is many classifications for different parts of the image. Using this feature
    map, a region proposal network performs a regression task to come up with bounding
    boxes and a classification network creates classifications for each bounding box.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于区域的卷积神经网络**（**R-CNN**）重用分类模型。它获取一张图像，并将分类模型滑动到图像上。结果是对图像的不同部分进行多次分类。利用这个特征图，区域提议网络执行回归任务，生成边界框，分类网络则为每个边界框创建分类。'
- en: 'The approach has been refined, culminating in Ren and others'' 2016 paper,
    *Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks*,
    which is available at [https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497),
    but the basic concept of sliding a classifier over an image has remained the same.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '这种方法已经得到了改进，并在Ren等人2016年的论文《*Faster R-CNN: Towards Real-Time Object Detection
    with Region Proposal Networks*》中达到了顶峰，论文可以在[https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)找到，但滑动分类器在图像上这一基本概念依然保持不变。'
- en: '**You Only Look Once** (**YOLO**), on the other hand, uses a single model consisting
    of only convolutional layers. It divides an image into a grid and predicts an
    object class for each grid cell. It then predicts several possible bounding boxes
    containing objects for each grid cell.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '**一回归看全**（**YOLO**）则使用一个仅包含卷积层的单一模型。它将图像划分为网格，并为每个网格单元预测一个物体类别。然后，它为每个网格单元预测几个可能包含物体的边界框。'
- en: For each bounding box, it regresses coordinates and both width and height values,
    as well as a confidence score that this bounding box actually contains an object.
    It then eliminates all bounding boxes with a too low confidence score or with
    a too large overlap with another, a more confident bounding box.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个边界框，它回归坐标以及宽度和高度值，并给出一个置信度评分，表示该边界框确实包含一个物体。然后，它会去除那些置信度太低或与其他置信度较高的边界框重叠过多的边界框。
- en: Note
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For a more detailed description, read Redmon and Farhadi''s 2016 paper, *YOLO9000:
    Better, Faster, Stronger*, available at [https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242).
    Further reading includes the 2018 paper, *YOLOv3: An Incremental Improvement*.
    This is available at [https://arxiv.org/abs/1804.027](https://arxiv.org/abs/1804.027).'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '想要更详细的描述，请阅读 Redmon 和 Farhadi 在 2016 年的论文，*YOLO9000: Better, Faster, Stronger*，可在
    [https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242) 阅读。进一步阅读可以参考
    2018 年的论文，*YOLOv3: An Incremental Improvement*，该论文可以在 [https://arxiv.org/abs/1804.027](https://arxiv.org/abs/1804.027)
    阅读。'
- en: Both are well-written, tongue-in-cheek papers, that explain the YOLO concept
    in more detail.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 这两篇论文都写得很好，带有些许讽刺幽默，能够更详细地解释 YOLO 的概念。
- en: The main advantage of YOLO over an R-CNN is that it's much faster. Not having
    to slide a large classification model is much more efficient. However, an R-CNN's
    main advantage is that it is somewhat more accurate than a YOLO model. If your
    task requires real-time analysis, you should use YOLO; however, if you do not
    need real-time speed but just want the best accuracy, then using an R-CNN is the
    way to go.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 相较于 R-CNN 的主要优势是速度更快。无需滑动一个庞大的分类模型，效率要高得多。然而，R-CNN 的主要优势是它在准确性上比 YOLO 模型略强。如果你的任务需要实时分析，应该使用
    YOLO；但是如果你不需要实时速度，而只是追求最佳的准确性，那么使用 R-CNN 更合适。
- en: Bounding box detection is often used as one of many processing steps. In the
    insurance case, the bounding box detector would crop out all roofs. The roof images
    can then be judged by a human expert, or by a separate deep learning model that
    classifies damaged roofs. Of course, you could train an object locator to distinguish
    between damaged and intact roofs directly, but in practice, this is usually not
    a good idea.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框检测通常作为多个处理步骤之一来使用。在保险案例中，边界框检测器会裁剪出所有的屋顶部分。然后，这些屋顶图像可以由人类专家进行判断，或者由一个单独的深度学习模型来分类损坏的屋顶。当然，你也可以训练一个物体定位器来直接区分损坏和完好的屋顶，但实际上，这通常不是一个好的主意。
- en: If you're interested in reading more about this, [Chapter 4](ch04.xhtml "Chapter 4. Understanding
    Time Series"), *Understanding Time Series*, has a great discussion on modularity.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣进一步阅读相关内容，*时间序列理解*（[第4章](ch04.xhtml "Chapter 4. Understanding Time Series)")
    中有一篇关于模块化的精彩讨论。
- en: Exercises
  id: totrans-415
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Fashion MNIST is a drop-in replacement for MNIST, but instead of handwritten
    digits, it is about classifying clothes. Try out the techniques we have used in
    this chapter on Fashion MNIST. How do they work together? What gives good results?
    You can find the dataset on Kaggle at [https://www.kaggle.com/zalando-research/fashionmnist](https://www.kaggle.com/zalando-research/fashionmnist).
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: Fashion MNIST 是 MNIST 的替代版本，但它分类的是衣物而非手写数字。尝试在 Fashion MNIST 上应用我们在本章中使用的技术。它们是如何协同工作的？哪些方法能得到良好的结果？你可以在
    Kaggle 上找到该数据集，网址为 [https://www.kaggle.com/zalando-research/fashionmnist](https://www.kaggle.com/zalando-research/fashionmnist)。
- en: 'Take on the whale recognition challenge and read the top kernels and discussion
    posts. The link can be found here: [https://www.kaggle.com/c/whale-categorization-playground](https://www.kaggle.com/c/whale-categorization-playground).
    The task of recognizing whales by their fluke is similar to recognizing humans
    by their face. There are good kernels showing off bounding boxes as well as Siamese
    networks. We have not covered all the technical tools needed to solve the task
    yet, so do not worry about the code in detail but instead focus on the concepts
    shown.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 参与鲸鱼识别挑战，阅读顶尖的 kernel 和讨论帖子。链接在这里：[https://www.kaggle.com/c/whale-categorization-playground](https://www.kaggle.com/c/whale-categorization-playground)。通过尾鳍识别鲸鱼的任务与通过面部识别人类的任务相似。有一些不错的
    kernel 展示了边界框和孪生网络。我们还没有涵盖解决该任务所需的所有技术工具，因此不必过于担心代码的细节，应该专注于展示的概念。
- en: Summary
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have seen the building blocks of computer vision models.
    We've learned about convolutional layers, and both the ReLU activation and regularization
    methods. You have also seen a number of ways to use neural networks creatively,
    such as with Siamese networks and bounding box predictors.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经了解了计算机视觉模型的基本构建模块。我们学习了卷积层，以及ReLU激活函数和正则化方法。你还看到了几种创造性地使用神经网络的方法，如孪生网络和边界框预测器。
- en: You have also successfully implemented and tested all these approaches on a simple
    benchmark task, the MNIST dataset. We scaled up our training and used a pretrained
    VGG model to classify thousands of plant images, before then using a Keras generator
    to load images from disk on the fly and customizing the VGG model to fit our new
    task.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 你还成功地在一个简单的基准任务——MNIST数据集上实现并测试了所有这些方法。我们扩大了训练规模，使用了预训练的VGG模型来对数千张植物图像进行分类，然后使用Keras生成器从磁盘动态加载图像，并定制VGG模型以适应我们的新任务。
- en: We also learned about the importance of image augmentation and the modularity
    tradeoff in building computer vision models. Many of these building blocks, such
    as convolutions, batchnorm, and dropout, are used in other areas beyond computer
    vision. They are fundamental tools that you will see outside of computer vision
    applications as well. By learning about them here, you have set yourself up to discover
    a wide range of possibilities in, for example, time series or generative models.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了图像增强和构建计算机视觉模型时模块化权衡的重要性。许多这些构建模块，如卷积、批归一化和丢弃法，除了在计算机视觉中使用外，也广泛应用于其他领域。它们是基本工具，你将会在计算机视觉应用之外的其他地方看到它们。通过在这里学习它们，你为自己发现其他领域的广泛可能性奠定了基础，例如时间序列或生成模型。
- en: Computer vision has many applications in the financial industry, especially
    in back-office functions as well as alternative alpha generation. It is one application
    of modern machine learning that can translate into real value for many corporations
    today. An increasing number of firms incorporate image-based data sources in their decision
    making; you are now prepared to tackle such problems head-on.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉在金融行业有许多应用，特别是在后台职能和替代阿尔法生成方面。它是现代机器学习的一种应用，能够为许多公司带来实际价值。越来越多的公司在决策中纳入基于图像的数据来源；现在，你已经准备好迎接这些问题的挑战。
- en: Over the course of this chapter, we've seen that an entire pipeline is involved
    in a successful computer vision project, and working on the pipeline often has
    a similar or greater benefit as compared to working on the model.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的过程中，我们看到成功的计算机视觉项目涉及到一个完整的管道，并且在管道上工作的收益往往与在模型上工作的收益相似或更大。
- en: 'In the next chapter, we will look at the most iconic and common form of financial
    data: time series. We will tackle the task of forecasting web traffic using more
    traditional statistical methods, such as **ARIMA** (short for **AutoRegressive
    Integrated Moving Average**), as well as modern neural network-based approaches.
    You will also learn about feature engineering with autocorrelation and Fourier
    transformations. Finally, you will learn how to compare and contrast different
    forecasting methods and build a high-quality forecasting system.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨最具代表性和常见的金融数据形式：时间序列。我们将使用更传统的统计方法，如**ARIMA**（即**自回归积分滑动平均**），以及基于现代神经网络的方法来解决预测网页流量的任务。你还将学习如何通过自相关和傅里叶变换进行特征工程。最后，你将学习如何比较和对比不同的预测方法，并构建一个高质量的预测系统。
