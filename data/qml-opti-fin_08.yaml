- en: '9'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '9'
- en: Quantum Circuit Born Machine
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 量子电路博恩机器
- en: The arrival of the new computational paradigm of quantum computing and the progress
    achieved in developing quantum computing hardware prompted intensive research
    in exploring the capabilities of quantum machine learning models and, more specifically,
    quantum generative models that can be viewed as quantum counterparts of the classical
    RBMs introduced in Chapter [5](Chapter_5.xhtml#x1-960005). Classical generative
    models form one of the most important classes of unsupervised machine learning
    techniques with numerous applications in finance, such as the generation of synthetic
    market data  [[48](Biblography.xhtml#XBuehler2020), [173](Biblography.xhtml#XKS2020)],
    the development of systematic trading strategies  [[176](Biblography.xhtml#Xkoshiyama2021generative)]
    or data anonymisation   [[174](Biblography.xhtml#XKSH2020)], to name just a few.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 量子计算这一新计算范式的到来以及量子计算硬件的进展，促使了对量子机器学习模型，特别是量子生成模型的深入研究，这些模型可以看作是经典RBM的量子对应物，后者在第[5](Chapter_5.xhtml#x1-960005)章中有介绍。经典生成模型是无监督机器学习技术中最重要的类别之一，广泛应用于金融领域，如合成市场数据的生成[[48](Biblography.xhtml#XBuehler2020), [173](Biblography.xhtml#XKS2020)]、系统性交易策略的开发[[176](Biblography.xhtml#Xkoshiyama2021generative)]，或数据匿名化[[174](Biblography.xhtml#XKSH2020)]，仅举几例。
- en: Quantum generative models have all the necessary qualities needed to establish
    quantum advantage on NISQ devices. Probably the most well known example of such
    models is the Quantum Circuit Born Machine (QCBM), which consists of several layers
    of adjustable and fixed gates followed by measurement operators. The input is
    a quantum state where all qubits are initialised as |0⟩ in the computational basis.
    The output is a bitstring, which is a sample from the probability distribution
    encoded in the final state constructed by the application of adjustable and fixed
    gates to the initial state.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 量子生成模型具备在NISQ设备上建立量子优势所需的所有必要特性。最著名的此类模型之一是量子电路博恩机器（QCBM），它由若干层可调和固定门组成，后面跟随测量操作符。输入是一个量子态，其中所有量子比特在计算基底中初始化为|0⟩。输出是一个比特串，这是通过对初始状态应用可调和固定门构建的最终态中编码的概率分布中提取的样本。
- en: 'The expectation of experimental proof of the quantum advantage is motivated
    by the following observations: First, QCBMs have strictly larger expressive power
    than classical RBMs when only a polynomial number of parameters is allowed (the
    number of qubits in QCBM or the number of visible activation units in RBM)  [[88](Biblography.xhtml#XDu2018)].
    Second, generating an independent sample from the learned distribution can be
    done in a single run of the quantum circuit in the case of QCBM – this compares
    favorably with up to 10³-10⁴ forward and backward passes through the network in
    the case of RBM, which are needed to achieve the state of thermal equilibrium  [[173](Biblography.xhtml#XKS2020)].
    This points towards material quantum speedup. Third, quantum generative models
    can be used to load data into a quantum state, thus facilitating realisations
    of many promising quantum algorithms  [[314](Biblography.xhtml#XZoufal2019)].'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 实验验证量子优势的期望来自以下观察：首先，当只允许多项式数量的参数时（QCBM中的量子比特数量或RBM中可见激活单元的数量），QCBM的表达能力明显大于经典RBM[[88](Biblography.xhtml#XDu2018)]。其次，在QCBM的情况下，可以通过量子电路的一次运行生成从已学习分布中独立的样本——相比之下，RBM需要通过网络进行最多10³到10⁴次的前向和反向传播才能达到热平衡状态[[173](Biblography.xhtml#XKS2020)]。这表明了量子加速的可能性。第三，量子生成模型可以用于将数据加载到量子态中，从而促进多种有前景的量子算法的实现[[314](Biblography.xhtml#XZoufal2019)]。
- en: 9.1 Constructing QCBM
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 构建QCBM
- en: As we have seen in Chapter [8](Chapter_8.xhtml#x1-1620008), the art of building
    a QML model that can be run on a NISQ computer consists of finding an optimal
    PQC architecture that can be embedded into the chosen QPU graph. In this section,
    we show how it can be done for the QCBM compatible with IBM’s Melbourne and Rochester
    systems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第[8](Chapter_8.xhtml#x1-1620008)章中所见，构建可以在NISQ计算机上运行的QML模型的艺术在于寻找一个最优的PQC架构，以便将其嵌入到所选的QPU图中。在本节中，我们将展示如何为与IBM的墨尔本和罗切斯特系统兼容的QCBM构建此架构。
- en: 9.1.1 QCBM architecture
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.1 QCBM架构
- en: QCBM is a parameterised quantum circuit where a layer of adjustable one-qubit
    gates is followed by a layer of fixed two-qubit gates. Such a pattern can be repeated
    any number of times, building a progressively deeper circuit. The input is a quantum
    state where all qubits are initialised as |0⟩ in the computational basis. The
    final layer consists of measurement operators producing a bitstring sample from
    the learned distribution. Therefore, to specify the QCBM architecture means to
    specify the number of layers, the type of adjustable gates, and the type of fixed
    gates for each layer. Since the theory of PQC is still being developed  [[29](Biblography.xhtml#XBenedetti2019)],
    we can rely on similarities and analogies between PQCs and classical neural networks
    to come up with some initial guesses about the possible QCBM architecture.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: QCBM是一个参数化量子电路，其中一层可调的单量子比特门后面跟着一层固定的双量子比特门。这样的模式可以重复多次，构建一个逐渐加深的电路。输入是一个量子态，所有量子比特都初始化为|0⟩，以计算基表示。最终层由测量算符组成，从学习到的分布中生成一个比特串样本。因此，指定QCBM架构意味着指定层数、每层的可调门类型以及每层的固定门类型。由于PQC的理论仍在发展中[[29](Biblography.xhtml#XBenedetti2019)]，我们可以依靠PQC和经典神经网络之间的相似性和类比，提出一些关于可能QCBM架构的初步猜测。
- en: '![Figure 9.1: QCBM(12, 7). ](img/file808.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1：QCBM(12, 7)。](img/file808.jpg)'
- en: 'Figure 9.1: QCBM(12, 7).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：QCBM(12, 7)。
- en: 'Figure [9.1](#9.1) displays a 12-qubit QCBM with two layers of controlled rotation
    gates R = R[G](*ϕ*) for G ∈{X*,*Y*,*Z} and *ϕ* ∈ [−*π,π*], where G and *ϕ* are
    fixed, and three layers of one-qubit gates R[X](*𝜃*) and R[Z](*𝜃*) with a total
    of seven adjustable gates per quantum register. The circuit is wide enough and
    deep enough to learn a complex distribution of a continuous random variable while
    remaining implementable on existing NISQ devices: the 12-digit binary representation
    of a continuous random variable provides sufficient precision and seven adjustable
    parameters (rotation angles) per qubit provide sufficient flexibility. At the
    same time, the circuit is not too deep to be compromised by the gate fidelity
    achievable in existing quantum hardware  [[46](Biblography.xhtml#XBruzewicz2019), [164](Biblography.xhtml#XKjaergaard2019)].'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9.1](#9.1)显示了一个12量子比特的QCBM，具有两层受控旋转门R = R[G](*ϕ*)，其中G ∈{X*,*Y*,*Z}且*ϕ* ∈ [−*π,π*]，其中G和*ϕ*是固定的，以及三层单量子比特门R[X](*𝜃*)和R[Z](*𝜃*)，每个量子寄存器总共有七个可调门。该电路足够宽且足够深，可以学习一个连续随机变量的复杂分布，同时仍能在现有的NISQ设备上实现：一个连续随机变量的12位二进制表示提供足够的精度，而每个量子比特的七个可调参数（旋转角度）提供足够的灵活性。同时，电路的深度又不会过大，以至于受到现有量子硬件可实现的门保真度的影响[[46](Biblography.xhtml#XBruzewicz2019),
    [164](Biblography.xhtml#XKjaergaard2019)]。
- en: 9.1.2 QCBM embedding
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.2 QCBM嵌入
- en: The chosen QCBM architecture is compatible with the limited connectivity observed
    in the current generation of quantum processors. For example, the proposed circuit
    requires sequential qubit connectivity where qubit *n* is directly connected with
    qubits *n*− 1 and *n* + 1 but does not have to be directly connected with other
    qubits. This architecture can for example be supported by IBM’s Melbourne system  [[208](Biblography.xhtml#XMelbourne2019)]
    in Figure [9.2](#9.2), where the 12 shaded qubits correspond to the 12 quantum
    registers in Figure [9.1](#9.1). The thick lines represent connections used in
    the QCBM ansatz while the thin lines represent all other available qubit connections.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的QCBM架构与当前一代量子处理器中观察到的有限连接性兼容。例如，所提议的电路需要顺序的量子比特连接，其中量子比特*n*直接连接量子比特*n*− 1和*n*
    + 1，但不需要直接连接其他量子比特。该架构例如可以由IBM的墨尔本系统[[208](Biblography.xhtml#XMelbourne2019)]支持，如图[9.2](#9.2)所示，其中12个阴影部分的量子比特对应图[9.1](#9.1)中的12个量子寄存器。粗线表示QCBM
    Ansatz中使用的连接，而细线表示所有其他可用的量子比特连接。
- en: '![Figure 9.2: IBM’s Melbourne system. ](img/file809.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2：IBM的墨尔本系统。](img/file809.jpg)'
- en: 'Figure 9.2: IBM’s Melbourne system.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：IBM的墨尔本系统。
- en: The 53-qubit Rochester device  [[208](Biblography.xhtml#XMelbourne2019)] in
    Figure [9.3](#9.3) can also be used to implement this QCBM architecture. Here,
    we have several choices for embedding the QCBM circuit (12 linearly connected
    qubits forming a closed loop); shaded qubits show one such possibility.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9.3](#9.3)中的53量子比特罗切斯特设备[[208](Biblography.xhtml#XMelbourne2019)]也可以用来实现这个QCBM架构。在这里，我们有几种选择来嵌入QCBM电路（12个线性连接的量子比特形成一个闭环）；阴影量子比特显示了其中一种可能性。
- en: '![Figure 9.3: IBM’s Rochester system. ](img/file810.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3：IBM的罗切斯特系统。](img/file810.jpg)'
- en: 'Figure 9.3: IBM’s Rochester system.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：IBM的罗切斯特系统。
- en: IBM systems, such as Melbourne and Rochester, are based on superconducting qubits.
    The choice of the underlying technology means that there is a set of native gates
    – the quantum gates derived directly from the types of interactions that occur
    in the given technical realisation of the quantum chip.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: IBM系统，如墨尔本和罗切斯特，基于超导量子比特。基础技术的选择意味着存在一组本地门——这些量子门直接来自于在给定量子芯片技术实现中发生的相互作用类型。
- en: In the case of IBM devices, the cross resonance gate generates the ZX interaction
    that leads to a CNOT gate. When it comes to single-qubit gates, we note that R[Z]
    is a diagonal gate given by ([6.3.3](Chapter_6.xhtml#x1-130014r3)) and can be
    implemented virtually in hardware via frame change (at zero error and duration)  [[239](Biblography.xhtml#XQiskitRZGate)].
    Therefore, it is sufficient to have just an X drive to rotate the qubit on the
    Bloch sphere (one can move a qubit between two arbitrary points on the Bloch sphere
    with the help of just two gates, R[X] and R[Z]).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于IBM设备，交叉共振门产生ZX相互作用，从而导致CNOT门。对于一比特门，我们注意到R[Z]是一个对角门，由([6.3.3](Chapter_6.xhtml#x1-130014r3))给出，并且可以通过帧变换在硬件中虚拟实现（在零误差和持续时间下）[[239](Biblography.xhtml#XQiskitRZGate)]。因此，仅需一个X驱动就足以旋转Bloch球上的量子比特（借助两个门R[X]和R[Z]，就可以在Bloch球上将量子比特从一个任意点移动到另一个任意点）。
- en: This means that we can introduce the concept of a *hardware-efficient* architecture
    not only in terms of connectivity but also in terms of the choice of one-qubit
    and two-qubit gates. Taking into account the CNOT and CPHASE gate decomposition
    shown in Figures [6.19](Chapter_6.xhtml#6.19) and [6.20](Chapter_6.xhtml#6.20),
    the hardware-efficient QCBM architecture for the Melbourne and Rochester systems
    would consist of a combination of R[X] and R[Z] adjustable single-qubit gates
    and CNOT and CPHASE fixed two-qubit gates  [[153](Biblography.xhtml#XKandala2017), [30](Biblography.xhtml#XBenedetti2021)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们不仅可以从连接性角度，还可以从一比特和两比特门的选择角度引入*硬件高效*架构的概念。考虑到图[6.19](Chapter_6.xhtml#6.19)和[6.20](Chapter_6.xhtml#6.20)中展示的CNOT和CPHASE门分解，墨尔本和罗切斯特系统的硬件高效QCBM架构将由可调节的R[X]和R[Z]单比特门以及固定的CNOT和CPHASE两比特门组合而成[[153](Biblography.xhtml#XKandala2017),
    [30](Biblography.xhtml#XBenedetti2021)]。
- en: QCBM is a PQC trained as a generative ML model. QCBM operating on *N* quantum
    registers transforms the initial quantum state ![|0 ⟩](img/file811.jpg)^(⊗N) into
    the quantum state encoding the learned probability distribution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: QCBM是作为生成式机器学习模型训练的PQC。QCBM作用于*N*量子寄存器，将初始量子态![|0 ⟩](img/file811.jpg)^(⊗N)转化为编码学习到的概率分布的量子态。
- en: 9.2 Differentiable Learning of QCBM
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 QCBM的可微学习
- en: The output of a QCBM circuit is a bitstring that represents a sample from the
    probability distribution encoded in the quantum state. The circuit itself is,
    essentially, a mechanism of transforming an initial state |0⟩^(⊗n) into a final
    state from which a sample is generated by means of measuring the qubits in the
    computational basis.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: QCBM电路的输出是一个比特串，表示从量子态中编码的概率分布中抽取的样本。电路本身本质上是一个机制，将初始态|0⟩^(⊗n)转化为最终态，然后通过测量量子比特（在计算基中）生成样本。
- en: Different configurations of one-qubit and multi-qubit gates encode different
    probability distributions – the training of QCBM consists of finding an optimal
    circuit configuration (ansatz) and an optimal set of adjustable parameters that
    minimise the distance between the probability distribution encoded in the final
    quantum state (before measurement, or "before sampling") and the probability distribution
    of the training dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一比特和多比特门的不同配置编码了不同的概率分布——QCBM的训练包括寻找一个最佳的电路配置（ansatz）和一组最佳的可调参数，以最小化最终量子态（测量之前，或称“采样之前”）中编码的概率分布与训练数据集的概率分布之间的距离。
- en: Following the structure we adopted in Chapter [8](Chapter_8.xhtml#x1-1620008),
    we start with the differentiable learning approach, before moving to the non-differentiable
    learning method based on a different kind of evolutionary search heuristic – Genetic
    Algorithm.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 按照我们在第[8](Chapter_8.xhtml#x1-1620008)章中采用的结构，我们首先介绍可微学习方法，然后再介绍基于另一种进化搜索启发式方法的非可微学习方法——遗传算法。
- en: 9.2.1 Sample encoding
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.1 样本编码
- en: 'In the most general case, a training dataset consists of samples containing
    continuous, integer and categorical features. However, QCBM operates on binary
    variables. Therefore, we need to design a method to convert continuous features
    into binary ones and a method for converting generated binary QCBM output (sampling)
    into continuous variables. The integer and binary features can be treated as special
    cases of continuous features and categorical features can be first converted into
    binary features through one-hot encoding. Such a method can be realised as a two-step
    routine (Algorithm [6](#x1-190006r6)):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在最一般的情况下，训练数据集由包含连续、整数和类别特征的样本组成。然而，QCBM 仅处理二进制变量。因此，我们需要设计一种方法将连续特征转换为二进制特征，并设计一种方法将生成的二进制
    QCBM 输出（采样）转换为连续变量。整数和二进制特征可以视为连续特征的特例，类别特征可以通过一热编码转换为二进制特征。此方法可以实现为两步过程（算法 [6](#x1-190006r6)）：
- en: Conversion of a continuous variable into the corresponding integer variable.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将连续变量转换为相应的整数变量。
- en: Conversion of the integer variable into the corresponding binary variable.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将整数变量转换为相应的二进制变量。
- en: 'Given the generated binary output, the same routine can be used in reverse
    mode to produce continuous samples (Algorithm  [7](#x1-190009r7)):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 给定生成的二进制输出，可以使用相同的过程以反向模式生成连续样本（算法 [7](#x1-190009r7)）：
- en: Conversion of the generated binary QCBM output into integer samples.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的二进制 QCBM 输出转换为整数样本。
- en: Conversion of integer samples into the corresponding continuous samples.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将整数样本转换为相应的连续样本。
- en: '![--------------------------------------------------------------------- Algorithm
    6: Continuous to integer to binary transformation (training phase) ---------------------------------------------------------------------
    Result: Conversion of continuous variables into M -digit binary features. ( (n)
    ) Input: Xreal(l) l=1,...,Nsamples;n=1,...,Nvariables – continuous data sample.
    for n = 1,...,Nvariables do ( ) | X (mni)n ← minl=1,...,Nsamples X (rnea)l(l)
    − 𝜀(nm)in, for 𝜀(nm)in ≥ 0 | (n) ( (n) ) (n) (n) | X max ← maxl=1,...,Nsamples
    X real(l) + 𝜀max, for 𝜀max ≥ 0 | | for l = 1,...,Nsamples do ( ) | | (n) ( M )
    X (nre)al(l)− X (mni)n | | X integer(l) ← int 2 − 1 ---(n)-----(n)- | | X max
    − X min | | (n) ( (n) ) | | Xbinary(l) ← bin X integer(l) | | end end Each data
    sample is represented by an M -digit binary number with every digit becoming a
    separate feature. The total number of features is M × Nvariables. ---------------------------------------------------------------------
    ](img/file812.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- 算法
    6：连续到整数到二进制转换（训练阶段）---------------------------------------------------------------------
    结果：将连续变量转换为 M 位二进制特征。（（n））输入：Xreal(l) l=1,...,Nsamples；n=1,...,Nvariables - 连续数据样本。对于
    n = 1,...,Nvariables 做：（） | X (mni)n ← minl=1,...,Nsamples X (rnea)l(l) − 𝜖(nm)in,
    如果 𝜖(nm)in ≥ 0 | （n） （（n）） （n） （n） | X max ← maxl=1,...,Nsamples X real(l) + 𝜖max,
    如果 𝜖max ≥ 0 | | 对于 l = 1,...,Nsamples 做（ ） | | （n） （ M ） X (nre)al(l)− X (mni)n
    | | X integer(l) ← int 2 − 1 ---(n)-----(n)- | | X max − X min | | （n） （（n）） |
    | Xbinary(l) ← bin X integer(l) | | 结束 结束 每个数据样本由一个 M 位二进制数表示，每一位成为一个独立的特征。特征总数为
    M × Nvariables。---------------------------------------------------------------------
    ](img/file812.jpg)'
- en: '![--------------------------------------------------------------------- Algorithm
    7: Binary to integer to continuous transformation (sampling phase) ---------------------------------------------------------------------
    Result: Conversion of the generated M -digit binary sample into continuous sample.
    ( ^ (n)) Input: X[m] m=0,...,M −1;n=1,...,Nvariables – generated M -digit binary
    sample. for n = 1,...,Nvariables do | (n) M∑−1 (n) | X^integer := 2m X^[M −1−m
    ] | m=0 | (n) (n) 1 (n) ( (n)) | ^X real ← X min +-M----X^integer Xm(na)x − X
    min | 2 − 1 end ---------------------------------------------------------------------
    ](img/file813.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- 算法
    7：二进制到整数到连续转换（采样阶段）---------------------------------------------------------------------
    结果：将生成的 M 位二进制样本转换为连续样本。（ ^（n））输入：X[m] m=0,...,M −1；n=1,...,Nvariables - 生成的 M
    位二进制样本。对于 n = 1,...,Nvariables 做 | （n） M∑−1 （n） | X^integer := 2m X^[M −1−m ]
    | m=0 | （n） （n） 1 （n） （（n）） | ^X real ← X min +-M----X^integer Xm(na)x − X min
    | 2 − 1 结束 ---------------------------------------------------------------------
    ](img/file813.jpg)'
- en: 'Algorithms [6](#x1-190006r6) and [7](#x1-190009r7) describe the transformations
    of continuous variables into *M*-digit binary variables and then back into continuous
    variables  [[173](Biblography.xhtml#XKS2020)]. It is important to note the role
    of the parameters *𝜀*[min] and *𝜀*[max]. They are non-negative and expand the
    interval on which the variables are defined. In the case where *𝜀*[min] = *𝜀*[max]
    = 0, this interval is determined by the minimum and maximum values of the variable
    as observed in the training dataset. By allowing *𝜀*[min] and *𝜀*[max] to take
    positive values, we expand the interval of possible values the variable can take.
    This allows the model to generate a wider range of possible scenarios: with some
    (small) probability the generated values can fall outside the interval given by
    the samples from the training dataset.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 [6](#x1-190006r6) 和 [7](#x1-190009r7) 描述了将连续变量转换为 *M* 位二进制变量，然后再转换回连续变量的过程[[173](Biblography.xhtml#XKS2020)]。需要特别注意参数
    *𝜖*[min] 和 *𝜖*[max] 的作用。它们是非负的，并且扩展了变量定义的区间。在 *𝜖*[min] = *𝜖*[max] = 0 的情况下，这个区间由训练数据集中的变量最小值和最大值决定。通过允许
    *𝜖*[min] 和 *𝜖*[max] 取正值，我们可以扩展变量可以取值的区间。这使得模型能够生成更广泛的可能场景：在某些（小）概率下，生成的值可能会落在训练数据集样本所给出的区间之外。
- en: The precision of the binary representation is feature specific. More important
    features can have more granular representation. The right choice of precision
    is important for NISQ devices that operate with a limited number of quantum registers.
    For example, the QCBM ansatz shown in Figure [9.1](#9.1) can be used to encode
    two continuous variables with 6-digit binary precision each. Alternatively, the
    more important variable can be encoded with, e.g., 8-digit binary precision and
    the less important one with only 4-digit binary precision.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制表示的精度是特征特定的。更重要的特征可以拥有更精细的表示。精度的正确选择对于具有有限量子寄存器的 NISQ 设备非常重要。例如，图 [9.1](#9.1)
    中所示的 QCBM 假设可以用来编码两个连续变量，每个变量使用 6 位二进制精度。或者，更重要的变量可以使用例如 8 位二进制精度进行编码，而较不重要的变量则只使用
    4 位二进制精度。
- en: 'Figure [9.2](#9.4) illustrates how the readout from 12 quantum registers can
    be translated into a sample consisting of two continuous variables: the value
    of the first one is encoded as a 7-digit binary number and the value of the second
    one is encoded as a 5-digit binary number. In this example, we assume that both
    variables take values in the interval [−1*,*1].'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [9.2](#9.4) 演示了如何将来自 12 个量子寄存器的读出结果转化为由两个连续变量组成的样本：第一个变量的值被编码为一个 7 位二进制数，第二个变量的值被编码为一个
    5 位二进制数。在这个例子中，我们假设两个变量都取值于区间 [−1*,*1]。
- en: '![Figure 9.4: Sample QCBM readout and data transformation for two continuous
    variables taking values in the interval [−1,1] and where we set 𝜀min = 𝜀max =
    0\. ](img/file814.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4：示例 QCBM 读出和数据变换，针对取值在区间 [−1,1] 的两个连续变量，并且我们设置了 𝜖[min] = 𝜖[max] = 0\.](img/file814.jpg)'
- en: 'Figure 9.4: Sample QCBM readout and data transformation for two continuous
    variables taking values in the interval [−1,1] and where we set 𝜀[min] = 𝜀[max]
    = 0\.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：示例 QCBM 读出和数据变换，针对取值在区间 [−1,1] 的两个连续变量，并且我们设置了 𝜖[min] = 𝜖[max] = 0\。
- en: 9.2.2 Choosing the right cost function
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.2 选择合适的成本函数
- en: 'The differentiable learning of QCBM follows the same principles as that of
    training the quantum neural networks outlined in Chapter [8](Chapter_8.xhtml#x1-1620008):
    minimisation of the cost function with the gradient descent method. The main difference
    is the form of the cost function. In the case of a QNN-based classifier, the cost
    function represents the classification error while the cost function for QCBM
    represents the distance between two probability distributions: the distribution
    of samples in the training dataset and the distribution of samples in the generated
    dataset.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: QCBM 的可微学习遵循与训练量子神经网络相同的原则，详见第 [8](Chapter_8.xhtml#x1-1620008) 章：通过梯度下降法最小化成本函数。主要的区别在于成本函数的形式。在基于
    QNN 的分类器中，成本函数表示分类错误，而 QCBM 的成本函数表示两个概率分布之间的距离：训练数据集中的样本分布和生成数据集中的样本分布。
- en: Let 𝜃 denote the set of adjustable QCBM parameters, *p*[𝜃](⋅) the QCBM distribution,
    and *π*(⋅) the data distribution. Then we can define the cost function *L*(𝜃)
    as
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 令 𝜃 表示可调节的 QCBM 参数集合，*p*[𝜃](⋅) 为 QCBM 分布，*π*(⋅) 为数据分布。那么我们可以将成本函数 *L*(𝜃) 定义为
- en: '| ![ ∑ L (𝜃 ) := &#124;p𝜃(x )− π(x)&#124;, x ](img/file815.jpg) |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑ L (𝜃 ) := &#124;p𝜃(x )− π(x)&#124;, x ](img/file815.jpg) |  |'
- en: 'where the sum goes over all samples x in the dataset. This cost function is
    a strong metric but may not be the easiest to deal with  [[73](Biblography.xhtml#XCoyle2019)].
    An efficient alternative choice of the cost function is the *maximum mean* *discrepancy*  [[189](Biblography.xhtml#XLiuWang2018)]:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中求和遍历数据集中的所有样本x。这个成本函数是一个强有力的度量，但可能不是最容易处理的[[73](Biblography.xhtml#XCoyle2019)]。一个有效的替代选择是*最大均值*
    *偏差* [[189](Biblography.xhtml#XLiuWang2018)]：
- en: '| ![L(𝜃) := 𝔼 [K (x,y)]− 2 𝔼 [K (x,y)]+ 𝔼 [K (x,y)], x∼p𝜃,y∼p𝜃 x∼p𝜃,y∼π x∼π,y∼π
    ](img/file816.jpg) |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ![L(𝜃) := 𝔼 [K (x,y)]− 2 𝔼 [K (x,y)]+ 𝔼 [K (x,y)], x∼p𝜃,y∼p𝜃 x∼p𝜃,y∼π x∼π,y∼π
    ](img/file816.jpg) |  |'
- en: 'where *K*(⋅*,*⋅) is a *kernel function*, i.e., a measure of similarity between
    points in the sample space. A popular choice of kernel function is the Gaussian
    mixture:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*K*(⋅*,*⋅)是*核函数*，即样本空间中点之间相似度的度量。一个流行的核函数选择是高斯混合模型：
- en: '| ![ ∑ c ( 2) K (x,y) = 1- exp − ∥x−-y2∥-- , c i=1 2σi ](img/file817.jpg) |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑ c ( 2) K (x,y) = 1- exp − ∥x−-y2∥-- , c i=1 2σi ](img/file817.jpg) |  |'
- en: for some *c* ∈ℕ and where (*σ*[i])[i=1,…,c] are the bandwidth parameters of
    each Gaussian kernel and ∥⋅∥ is the *L*[2] norm.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些*c* ∈ℕ，并且(*σ*[i])[i=1,…,c]是每个高斯核的带宽参数，∥⋅∥是*L*[2]范数。
- en: 'We can also explore the possibility of using *quantum kernels*. Quantum kernels
    can provide an advantage over classical methods for kernels that are difficult
    to compute on a classical device. For example, we can consider a non-variational
    quantum kernel method  [[232](Biblography.xhtml#XPeters2021)], which uses a quantum
    circuit U(x) to map real data into a quantum state |*ϕ*⟩ via a *feature map*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以探索使用*量子核*的可能性。量子核在计算上难以处理的核函数上，相比经典方法可以提供一定的优势。例如，我们可以考虑一种非变分量子核方法[[232](Biblography.xhtml#XPeters2021)]，该方法利用量子电路U(x)通过*特征映射*将真实数据映射到量子态|*ϕ*⟩：
- en: '| ![&#124;ϕ(x)⟩ = U(x) &#124;0⟩⊗n . ](img/file818.jpg) |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| ![&#124;ϕ(x)⟩ = U(x) &#124;0⟩⊗n . ](img/file818.jpg) |  |'
- en: The kernel function is then defined as the squared inner product
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数被定义为平方内积
- en: '| ![K (x,y ) = &#124;⟨ϕ(x)&#124;ϕ (y )⟩ &#124;2\. ](img/file819.jpg) |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| ![K (x,y ) = &#124;⟨ϕ(x)&#124;ϕ (y )⟩ &#124;2\. ](img/file819.jpg) |  |'
- en: This quantum kernel is evaluated on a quantum computer and is hard to compute
    on a classical one  [[129](Biblography.xhtml#XHavlicek2019)]. We investigate the
    question of expressive power of various models in Chapter [12](Chapter_12.xhtml#x1-22500012)
    and provide a detailed analysis of the quantum kernel approach in Chapter [13](Chapter_13.xhtml#x1-23600013).
    Taking into account the mapping ([9.2.2](#x1-1910002)) and denoting ![|0⟩](img/file820.jpg)
    = ![|0⟩](img/file821.jpg)^(⊗n), the kernel becomes
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个量子核在量子计算机上进行评估，而在经典计算机上则难以计算[[129](Biblography.xhtml#XHavlicek2019)]。我们将在第[12](Chapter_12.xhtml#x1-22500012)章探讨各种模型的表达能力问题，并在第[13](Chapter_13.xhtml#x1-23600013)章对量子核方法进行详细分析。考虑到映射（[9.2.2](#x1-1910002)）并且表示![|0⟩](img/file820.jpg)
    = ![|0⟩](img/file821.jpg)^(⊗n)，核函数变为
- en: '| ![ † 2 K (x,y) = &#124;⟨0&#124;U (x )U(y) &#124;0 ⟩&#124;, ](img/file822.jpg)
    |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ![ † 2 K (x,y) = &#124;⟨0&#124;U (x )U(y) &#124;0 ⟩&#124;, ](img/file822.jpg)
    |  |'
- en: which is the probability of measuring the all-zero outcome. It can be calculated
    by measuring, in the computational basis, the state which results from running
    the circuit given by U(y), followed by that of U^†(x).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是测量全零结果的概率。它可以通过在计算基底上测量运行电路U(y)所得到的状态，再测量U^†(x)的结果来计算。
- en: 9.3 Non-Differentiable Learning of QCBM
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3 非可微分的QCBM学习
- en: The hardware-efficient ansatz we proposed for the QCBM architecture, while simple
    and intuitive, may be vulnerable to barren plateaus, or regions of exponentially
    vanishing gradient magnitudes that make training untenable  [[54](Biblography.xhtml#XCerezo2021), [139](Biblography.xhtml#XHolmes2021), [299](Biblography.xhtml#XWang2020)].
    This provides a strong motivation for exploring a non-differentiable learning
    alternative such as Genetic Algorithm.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为QCBM架构提出的硬件高效假设虽然简单直观，但可能容易受到贫瘠平台（即梯度消失的指数区域）的影响，这使得训练变得不可行[[54](Biblography.xhtml#XCerezo2021),
    [139](Biblography.xhtml#XHolmes2021), [299](Biblography.xhtml#XWang2020)]。这为探索非可微分学习的替代方法提供了强有力的动机，比如遗传算法。
- en: 9.3.1 The principles of Genetic Algorithm
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.1 遗传算法原理
- en: GA is a powerful evolutionary search heuristic  [[214](Biblography.xhtml#XMitchell1998)]
    that was introduced in Chapter [3](Chapter_3.xhtml#x1-630003). It performs a multi-directional
    search by maintaining a population of proposed solutions (chromosomes) for a given
    problem. Each solution is represented in a fixed alphabet with an established
    meaning (genes). The population undergoes a simulated evolution with relatively
    good solutions producing offspring, which subsequently replace the worse ones,
    and the quality of a solution is estimated with some objective function (environment).
    GAs have found applications is such diverse fields as quantitative finance (for
    portfolio optimisation problems  [[172](Biblography.xhtml#XKondratyev2017)]) and
    experiments with adiabatic quantum computing (as a classical benchmark  [[296](Biblography.xhtml#XVenturelli2019)]).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传算法（GA）是一种强大的进化搜索启发式方法[[214](Biblography.xhtml#XMitchell1998)]，它在第[3](Chapter_3.xhtml#x1-630003)章中进行了介绍。GA通过维护一个给定问题的候选解（染色体）种群，进行多方向搜索。每个解在一个固定字母表中表示，并且具有既定的含义（基因）。种群经历模拟进化，较好的解决方案产生后代，继而替代较差的解，并且解决方案的质量通过某些目标函数（环境）进行估计。GA已被应用于金融定量分析（如投资组合优化问题[[172](Biblography.xhtml#XKondratyev2017)]）以及绝热量子计算实验（作为经典基准[[296](Biblography.xhtml#XVenturelli2019)]）。
- en: 'The simulation cycle is performed in three basic steps. During the selection
    step, a new population is formed by stochastic sampling (with replacement). Then,
    some of the members of the newly selected populations recombine. Finally, all
    new individuals are re-evaluated. The mating process (recombination) is based
    on the application of two operators: mutation and crossover. Mutation introduces
    random variability into the population, and crossover exchanges random pieces
    of two chromosomes in the hope of propagating partial solutions.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟过程分为三个基本步骤。在选择步骤中，通过随机抽样（有放回）形成新的种群。然后，新选中的种群成员会进行重组。最后，所有新的个体会重新评估。配对过程（重组）基于两种操作符：突变和交叉。突变向种群引入随机变异，而交叉则交换两个染色体的随机片段，旨在传播部分解决方案。
- en: The training of the QCBM specified in Figure [9.1](#9.1) consists of finding
    an optimal configuration of the rotation angles (*𝜃*[i]^j)[i=1,…,12; j=1,…,7]
    that would minimise a chosen cost function given a particular choice of the fixed
    2-qubit gates. Since we only deal with 84 adjustable parameters (rather than tens
    of thousands), we do not need to implement the crossover mechanism and can rely
    on parameter mutations to achieve GA convergence to the minimum of the cost function.
    This significantly simplifies the algorithm.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9.1](#9.1)所示的QCBM训练过程包括找到旋转角度的最佳配置（*𝜃*[i]^j)[i=1,…,12; j=1,…,7]，以最小化给定特定2量子比特门的成本函数。由于我们仅处理84个可调参数（而不是数万个），我们不需要实现交叉机制，可以依赖参数突变来实现遗传算法收敛到成本函数的最小值。这大大简化了算法。
- en: 9.3.2 Training QCBM with a Genetic Algorithm
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.2 使用遗传算法训练QCBM
- en: Algorithm [8](#x1-194004r8) outlines the proposed approach. However, before
    we provide a formal description of the algorithm, we have to specify the main
    individual components.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 算法[8](#x1-194004r8)概述了提出的方法。然而，在我们提供算法的正式描述之前，我们需要明确算法的主要组成部分。
- en: '**Solution.** The solution is a 12 × 7 matrix of rotation angles:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解决方案。** 解决方案是一个12 × 7的旋转角度矩阵：'
- en: '| ![ ⌊ 1 7⌋ &#124; 𝜃1 ... 𝜃1&#124; 𝜃 = &#124; ... ... ...&#124; . ⌈ ⌉ 𝜃112
    ... 𝜃712 ](img/file823.jpg) |  |'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![ ⌊ 1 7⌋ &#124; 𝜃1 ... 𝜃1&#124; 𝜃 = &#124; ... ... ...&#124; . ⌈ ⌉ 𝜃112
    ... 𝜃712 ](img/file823.jpg) |  |'
- en: In GA language, the matrix 𝜃 plays the role of a chromosome and its components
    *𝜃*[i]^j play the roles of individual genes.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在遗传算法语言中，矩阵𝜃充当染色体的角色，而其组成部分*𝜃*[i]^j充当单个基因的角色。
- en: '**Mutation.** The genes can mutate from generation to generation. The mutation
    rate can be either constant or time dependent. For example, the mutation rate
    can start at some large value and then decrease exponentially such that it halves
    after each *κ* generations. In Algorithm [8](#x1-194004r8), we adopt the following
    mutation dynamics:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**突变。** 基因可以在代际间发生突变。突变率可以是常数值，也可以随时间变化。例如，突变率可以从一个较大的值开始，然后以指数方式减小，使得每经过*κ*代，突变率减半。在算法[8](#x1-194004r8)中，我们采用以下突变动态：'
- en: A rotation angle (gene) can mutate to any of the allowed discrete values with
    equal probability.
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旋转角度（基因）可以以相等的概率突变为任何允许的离散值。
- en: Mutation is controlled by a single global parameter *α* ∈ (0*,*1], which can
    be either constant or exponentially decreasing at some fixed rate *β* ≥ 0.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变异是由一个单一的全局参数 *α* ∈ (0*,*1] 控制的，该参数可以是常数，或者以某个固定的速率 *β* ≥ 0 逐渐减小。
- en: Mutations happen independently for each column in 𝜃.
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一列的变异是独立发生的。
- en: For each column in 𝜃, at each generation, a single rotation angle mutation happens
    with probability *α*. All rotation angles are equally likely to mutate. After
    that, one more mutation can happen with probability *α∕*2\. Again, all rotation
    angles are equally likely to mutate. This ensures that we can have scenarios where
    two rotation angles within the same column can mutate simultaneously.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 𝜃 中的每一列，在每一代中，以 *α* 的概率发生一次单一的旋转角度变异。所有旋转角度发生变异的概率相同。之后，以 *α∕*2 的概率发生另一次变异。再次地，所有旋转角度发生变异的概率相同。这确保了我们可以有这样的情况，即同一列中的两个旋转角度可以同时发生变异。
- en: '**Search Space.** The rotation angles *𝜃*[i]^j are defined in [−*π,π*], which
    we split into 2^m equal subintervals, so that the possible values for *𝜃*[i]^j
    are (−*π* + *nπ∕*2^(m−1))[n=0,…,2^m−1]. A rotation angle can mutate into any of
    these values. The search space can quickly become enormous even for the relatively
    small values of *m*. For example, for *m* = 7 we have 128 possible values for
    each rotation angle making the total number of possible configurations ∼ 10^(177).
    The GA can only explore a tiny fraction of the search space. But due to the GA’s
    ability to propagate best solutions and to avoid being trapped in local minima,
    the algorithm can achieve reasonably fast convergence to the solution in the vicinity
    of the global minimum. For a detailed analysis of the rate of convergence of genetic
    algorithms, we refer the interested reader to  [[130](Biblography.xhtml#XHe1999), [264](Biblography.xhtml#XSharapov2006)].'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索空间。** 旋转角度 *𝜃*[i]^j 定义在 [−*π,π*] 范围内，我们将其分割成 2^m 个相等的子区间，使得 *𝜃*[i]^j 的可能取值为
    (−*π* + *nπ∕*2^(m−1))[n=0,…,2^m−1]。一个旋转角度可以变异为这些值中的任何一个。即使对于相对较小的 *m* 值，搜索空间也可以迅速变得庞大。例如，对于
    *m* = 7，每个旋转角度有 128 个可能的取值，使得可能的配置总数大约为 10^(177)。遗传算法只能探索搜索空间的极小部分。但由于遗传算法能够传播最佳解并避免陷入局部最小值，算法能够在接近全局最小值的区域内迅速收敛。关于遗传算法收敛速率的详细分析，感兴趣的读者可以参考
    [[130](Biblography.xhtml#XHe1999), [264](Biblography.xhtml#XSharapov2006)]。'
- en: '**Cost Function.** A cost function is a measure of how far the distribution
    of generated samples is from the distribution of original samples provided by
    the training dataset. Let u := (*u*[1]*,…,u*[K]) be a sample from the training
    dataset and v(𝜃) := (*v*[1](𝜃)*,…,v*[K](𝜃)) a sample from the QCBM generated dataset
    that corresponds to a particular configuration of rotation angles 𝜃. Let us order
    these samples from the smallest to the largest with any suitable sort(⋅) function:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本函数。** 成本函数是衡量生成样本的分布与训练数据集中原始样本分布之间差异的度量。设 u := (*u*[1]*,…,u*[K]) 为来自训练数据集的样本，v(𝜃)
    := (*v*[1](𝜃)*,…,v*[K](𝜃)) 为来自 QCBM 生成数据集的样本，对应于某一特定的旋转角度配置 𝜃。我们将这些样本按从小到大的顺序排列，使用任何合适的
    sort(⋅) 函数：'
- en: '| ![-- -- u = sort(u), v(𝜃) = sort(v(𝜃 )). ](img/file824.jpg) |  |'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![-- -- u = sort(u), v(𝜃) = sort(v(𝜃 )). ](img/file824.jpg) |  |'
- en: The cost function *L*(⋅) can then be defined as
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成本函数 *L*(⋅) 可以定义为
- en: '| ![ K∑ -- -- 2 L (𝜃 ) := (uk − vk(𝜃 )) . k=1 ](img/file825.jpg) |  |'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![ K∑ -- -- 2 L (𝜃 ) := (uk − vk(𝜃 )) . k=1 ](img/file825.jpg) |  |'
- en: The sort(⋅) function in ([9.3.2](#x1-1940002)) can be, e.g., quicksort  [[137](Biblography.xhtml#XHoare1961)]
    or mergesort  [[166](Biblography.xhtml#XKnuth1998)], which belong to the class
    of divide-and-conquer algorithms. Alternatively, it can be, e.g., heapsort  [[303](Biblography.xhtml#XWilliams1964)]
    – a comparison-based sorting algorithm.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ([9.3.2](#x1-1940002)) 中的 sort(⋅) 函数可以是，例如，快速排序 [[137](Biblography.xhtml#XHoare1961)]
    或归并排序 [[166](Biblography.xhtml#XKnuth1998)]，它们属于分治算法类别。或者，它可以是例如堆排序 [[303](Biblography.xhtml#XWilliams1964)]
    —— 一种基于比较的排序算法。
- en: '![--------------------------------------------------------------------- -Algorithm---8:-Genetic
    Algorithm------------------------------------ Result: Optimal con figuration of
    the set of QCBM parameters 𝜃∗ minimising the cost function. Input: • u ∈ ℝK :
    vector of sample training dataset; • L: number of iterations (generations); •
    M : number of best solutions in the given generation, chosen for further mutation;
    • N : number of solutions in each generation (N = DM , D ∈ ℕ); • α, β: mutation
    parameters; • m: search space parameter. ( The poss)ible values of rotation angles
    are − π + -νπ-- . 2m− 1 ν=0,...,2m− 1 Initialise and evaluate the first generation
    of solutions: for n = 1,...,N do | Generate a configuration 𝜃 (0;n ) by randomly
    drawing each | rotation angle 𝜃j(0;n ) from the uniform distribution on the set
    | i | of possible values of rotation angles given by m. | | for k = 1,...,K do
    | | Run the quantum circuit with con figuration 𝜃(0;n) and | | generate new sample
    v (𝜃(0;n)). | k | end | Evaluate the cost function L(𝜃(0;n)). end Order solutions
    from best (minimum of cost function) to worst (maximum of cost function ). 𝜃∗
    ← con figuration corresponding to the minimum of cost function. ---------------------------------------------------------------------
    ](img/file826.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- -算法---8:-遗传算法------------------------------------
    结果：最优配置的QCBM参数集𝜃∗，最小化代价函数。 输入： • u ∈ ℝK：样本训练数据集的向量； • L：迭代次数（代数）； • M：当前代中选出的最佳解的数量，用于进一步变异；
    • N：每代中的解的数量（N = DM，D ∈ ℕ）； • α，β：变异参数； • m：搜索空间参数。（旋转角度的可能值为− π + -νπ-- . 2m−
    1 ν=0,...,2m− 1 初始化并评估第一代解：对于n = 1,...,N，执行以下操作 | 随机抽取每个旋转角度𝜃j(0;n)在旋转角度的可能值集合上，
    | 并生成配置𝜃(0;n)。 | | 对于k = 1,...,K，执行以下操作 | | 运行量子电路，使用配置𝜃(0;n)并 | | 生成新的样本v(𝜃(0;n))。
    | k | 结束 | 评估代价函数L(𝜃(0;n))。 结束 将解从最优（代价函数最小）到最差（代价函数最大）进行排序。 𝜃∗ ← 对应于代价函数最小值的配置。
    --------------------------------------------------------------------- ](img/file826.jpg)'
- en: '![--------------------------------------------------------------------- ---------------------------------------------------------------------
    Iterations: for l = 1,...,L do | −β | α ← αe | | Select M best solutions from
    generation l − 1 and generate new | | solutions (𝜃(l;n ))n=1,...,N by mutating
    the rotation angles using | the updated mutation rate α. Each of the M best solutions
    is | | used to produce D new solutions. | | for n = 1,...,N do | | for k = 1,...,K
    do | | | | | | Run the quantum circuit with 𝜃(l;n ) and generate new | | | sample
    vk(𝜃 (l;n)). | | end | | | | | Evaluate the cost function L(𝜃(l;n)). | end | |
    Order solutions from best (minimum of the cost function) to | | worst (maximum
    of the cost function). | | 𝜃∗(l) ← configuration corresponding to the minimum
    of the cost | | function (l-th generation). | | if L(𝜃∗(l)) < L(𝜃∗) then | 𝜃 ∗
    ← 𝜃∗(l) | end end ---------------------------------------------------------------------
    ](img/file827.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- ---------------------------------------------------------------------
    迭代：对于l = 1,...,L，执行以下操作 | −β | α ← αe | | 从上一代l − 1中选择M个最佳解并生成新的 | | 解（𝜃(l;n ))n=1,...,N，方法是通过使用更新后的变异率α来变换旋转角度。每一个M个最佳解都
    | | 用于生成D个新的解。 | | 对于n = 1,...,N，执行以下操作 | | 对于k = 1,...,K，执行以下操作 | | | | | | 运行量子电路，使用𝜃(l;n)并生成新的
    | | | 样本vk(𝜃 (l;n))。 | | 结束 | | | | | 评估代价函数L(𝜃(l;n))。 | 结束 | | 将解从最优（代价函数最小）到最差（代价函数最大）进行排序。
    | | 𝜃∗(l) ← 对应于代价函数最小值的配置 | | （第l代）。 | | 如果L(𝜃∗(l)) < L(𝜃∗)，则 | 𝜃∗ ← 𝜃∗(l) | 结束
    结束 --------------------------------------------------------------------- ](img/file827.jpg)'
- en: Having described the training algorithm, we now specify the classical benchmark
    before comparing the results obtained by the quantum and the classical generative
    models on the sample datasets.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述了训练算法之后，我们现在指定经典基准，然后再比较量子生成模型和经典生成模型在样本数据集上的结果。
- en: 9.4 Classical Benchmark
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4 经典基准
- en: 'There is a deep connection between QCBM and its classical counterpart – Restricted
    Boltzmann Machine  [[60](Biblography.xhtml#XCheng2017)]. RBM, introduced and discussed
    in Chapter [5](Chapter_5.xhtml#x1-960005) in the context of quantum annealing,
    is a generative model inspired by statistical physics, where the probability of
    a particular data sample, v, is given by the Boltzmann distribution:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: QCBM与其经典对应物——限制玻尔兹曼机（RBM）之间存在深刻的联系 [[60](Biblography.xhtml#XCheng2017)]。RBM在量子退火的背景下被介绍并讨论，详见第[5](Chapter_5.xhtml#x1-960005)章，是一种受到统计物理启发的生成模型，其中某个特定数据样本v的概率由玻尔兹曼分布给出：
- en: '| ![ 1 − E(v) ℙ(v) = Ze . ](img/file828.jpg) |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| ![ 1 − E(v) ℙ(v) = Ze . ](img/file828.jpg) |  |'
- en: 'Here, *E*(v) is the (positive) *energy* of the data sample (data samples with
    lower energy have higher probabilities) and *Z* is the partition function, namely
    the normalisation factor of the probability density:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*E*(v)是数据样本的（正）*能量*（具有较低能量的数据样本具有较高的概率），*Z*是配分函数，即概率密度的归一化因子：
- en: '| ![ ∑ Z = e−E(v). v ](img/file829.jpg) |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑ Z = e−E(v). v ](img/file829.jpg) |  |'
- en: 'Alternatively, we can use the inherent probabilistic nature of quantum mechanics
    that allows us to model the probability distribution using a quantum state ![|ψ⟩](img/file830.jpg):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以利用量子力学固有的概率性质，利用量子态来建模概率分布 ![|ψ⟩](img/file830.jpg)：
- en: '| ![ℙ(v) = ⟨ψ&#124;𝒫†v𝒫v &#124;ψ ⟩, ](img/file831.jpg) |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| ![ℙ(v) = ⟨ψ&#124;𝒫†v𝒫v &#124;ψ ⟩, ](img/file831.jpg) |  |'
- en: where 𝒫[v] is the measurement operator introduced in Section [1.2.3](Chapter_1.xhtml#x1-380003)
    and, since the quantum state ![|ψ ⟩](img/file832.jpg) is a unit vector, we have
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中𝒫[v]是第[1.2.3](Chapter_1.xhtml#x1-380003)节中介绍的测量算符，并且由于量子态 ![|ψ ⟩](img/file832.jpg)
    是单位向量，我们有
- en: '| ![⟨ψ &#124;ψ ⟩ = 1\. ](img/file833.jpg) |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ![⟨ψ &#124;ψ ⟩ = 1\. ](img/file833.jpg) |  |'
- en: We realise this approach in the Quantum Circuit Born Machine, where generative
    modelling of probability density is translated into learning a quantum state.
    The sole purpose of QCBM’s parameterised circuit is to create the quantum state
    ![|ψ ⟩](img/file834.jpg) that encodes the desired probability distribution starting
    from the initial state |0⟩^(⊗n), with sampling performed by applying the measurement
    operators.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在量子电路Born机器（Quantum Circuit Born Machine, QCBM）中实现了这种方法，其中生成的概率密度建模被转化为学习一个量子态。QCBM的参数化电路的唯一目的是生成量子态
    ![|ψ ⟩](img/file834.jpg)，该态编码了从初始状态|0⟩^(⊗n)开始的期望概率分布，采样是通过施加测量算符来执行的。
- en: 'Therefore, providing a classical benchmark for QCBM consists in finding a suitable
    RBM configuration that will allow us to compare two methods generating the probability
    distribution ℙ(v): one given by ([9.4](#x1-1950004)) for RBM and another one given
    by ([9.4](#x1-1950004)) for QCBM  [[170](Biblography.xhtml#XKondratyev2020)].'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，提供QCBM的经典基准的方法是找到一个合适的RBM配置，使我们能够比较生成概率分布ℙ(v)的两种方法：一种是由 ([9.4](#x1-1950004))
    产生的RBM方法，另一种是由 ([9.4](#x1-1950004)) 产生的QCBM方法  [[170](Biblography.xhtml#XKondratyev2020)]。
- en: Figure [9.5](#9.5) shows an RBM with 12 stochastic binary visible activation
    units and 7 stochastic binary hidden activation units, where (*a*[i])[i=1,…,12],
    (*b*[j])[j=1,…,7], and (*w*[ij])[i=1,…,12; j=1,…,7] denote, respectively, the
    biases for the visible and hidden layers and the network weights.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9.5](#9.5)展示了一个具有12个随机二进制可见激活单元和7个随机二进制隐藏激活单元的RBM，其中 (*a*[i])[i=1,…,12]，(*b*[j])[j=1,…,7]
    和 (*w*[ij])[i=1,…,12; j=1,…,7] 分别表示可见层和隐藏层的偏置以及网络权重。
- en: This network architecture makes RBM equivalent to QCBM as described in Section [9.1](#x1-1860001)
    in the sense that both generative models have the same number of adjustable parameters
    (the number of RBM weights is equal to the number of adjustable rotation angles
    in QCBM) and the number of visible activation units is equal to the number of
    quantum registers. The latter ensures that both generative models can learn the
    empirical distribution of a continuous random variable with the same precision
    (12-digit binary representation).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络架构使得RBM等价于QCBM，如第[9.1](#x1-1860001)节中所述，因为这两种生成模型具有相同数量的可调参数（RBM的权重数量等于QCBM中可调旋转角度的数量），并且可见激活单元的数量等于量子寄存器的数量。后者确保这两种生成模型能够以相同的精度（12位二进制表示）学习连续随机变量的经验分布。
- en: '![Figure 9.5: RBM(12, 7). ](img/file835.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5: RBM(12, 7). ](img/file835.jpg)'
- en: 'Figure 9.5: RBM(12, 7).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.5: RBM(12, 7).'
- en: QCBM performance should be compared against the performance of its classical
    counterpart, the Restricted Boltzmann Machine. Both models operate on the binary
    representation of the dataset and have a comparable number of adjustable parameters.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: QCBM的性能应与其经典对应物——限制玻尔兹曼机（RBM）的性能进行比较。两种模型都在数据集的二进制表示上操作，并且具有相似数量的可调参数。
- en: 9.5 QCBM as a Market Generator
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5 QCBM作为市场生成器
- en: The most obvious financial application of QCBM is as a market generator. An
    efficient generation of realistic market scenarios, for example sampling from
    the joint distribution of risk factors, is one of the most important and challenging
    problems in quantitative finance today. We thus need to investigate how well QCBM
    can execute this task, and compare it to classical benchmarks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: QCBM最明显的金融应用是作为市场生成器。例如，从风险因子的联合分布中抽样，进行高效的现实市场情境生成，是今天定量金融领域中最重要和最具挑战性的问题之一。因此，我们需要调查QCBM如何执行这一任务，并将其与经典基准进行比较。
- en: 9.5.1 Non-parametric modelling of market risk factors
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.5.1 市场风险因子的非参数建模
- en: Historically, the problem of producing reliable synthetic market scenarios was
    solved through sampling from some easy-to-calibrate parametric models, such as
    the multivariate Normal distribution of risk factor log-returns (equities) or
    a Gaussian copula combining the multivariate Normal dependence structure with
    heavy-tailed univariate marginal distributions of individual risk factors (credit).
    However, there are well-known issues with this approach that often outweigh the
    benefits provided by simplicity and transparency  [[217](Biblography.xhtml#XMorini2009)].
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，解决生成可靠的合成市场情境的问题通常是通过从一些易于校准的参数模型中进行抽样来完成的，例如风险因子对数收益（股票）的多元正态分布，或结合多元正态依赖结构和单个风险因子的重尾单变量边际分布（信用）的高斯
    copula。然而，这种方法存在一些众所周知的问题，这些问题往往超过了其在简洁性和透明性上的优势[[217](Biblography.xhtml#XMorini2009)]。
- en: 'A parametric model is often a poor approximation of reality. To be useful,
    it has to be relatively simple: one should be able to describe the key features
    of the risk factor distribution with a handful of parameters achieving the best
    possible fit to either the empirical distribution derived from historical data
    or from prices of traded instruments observed in the market at the time of model
    calibration. Making the parametric model too complex would lead to overfitting
    and poor generalisation.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 参数模型往往是现实的一个较差近似。为了有效，它必须相对简单：应该能够用少量参数描述风险因子分布的关键特征，从而最好地拟合从历史数据中推导出的经验分布或从市场中观察到的交易工具价格（在模型校准时）。将参数模型做得过于复杂会导致过拟合和较差的泛化能力。
- en: It is even more difficult to model a realistic dependence structure. A typical
    parametric approach used in most Monte Carlo risk engines starts with modelling
    the dynamics of various risk factors independently, and then imposes a dependence
    structure by correlating the corresponding stochastic drivers. These are, almost
    invariably, Brownian motions, and the linear correlations between them are supposed
    to be sufficient to construct the joint distribution of risk factors.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 建模一个现实的依赖结构更为困难。大多数蒙特卡洛风险引擎中常用的典型参数化方法是首先独立地建模各种风险因子的动态，然后通过相关相应的随机驱动因素来强加一个依赖结构。这些驱动因素几乎总是布朗运动，它们之间的线性相关性应该足以构建风险因子的联合分布。
- en: An alternative approach is to use non-parametric modelling, where the joint
    and marginal distributions of risk factors are learned directly from the available
    datasets. Classically, we can realise this approach with the help of a Restricted
    Boltzmann Machine – the classical benchmark of choice described in the previous
    section and successfully applied to a number of financial use cases  [[173](Biblography.xhtml#XKS2020), [174](Biblography.xhtml#XKSH2020)].
    Another possibility is to use the Generative Adversarial Network (GAN) framework,
    where the distribution learned from the dataset by a generative neural network
    is tested by a discriminative neural network trying to judge whether samples are
    coming from the true distribution (data) or from the reconstructed distribution
    (generated samples)  [[114](Biblography.xhtml#XGoodfellow2014)].
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代方法是使用非参数建模，其中风险因子的联合分布和边际分布是直接从可用数据集学习得出的。经典地，我们可以借助限制玻尔兹曼机（Restricted Boltzmann
    Machine）来实现这一方法——前一节中描述的经典基准，且已成功应用于多个金融案例[[173](Biblography.xhtml#XKS2020), [174](Biblography.xhtml#XKSH2020)]。另一种可能性是使用生成对抗网络（GAN）框架，其中通过生成神经网络从数据集中学习到的分布被判别神经网络测试，后者试图判断样本是来自真实分布（数据）还是来自重建分布（生成的样本）[[114](Biblography.xhtml#XGoodfellow2014)]。
- en: Chapter [12](Chapter_12.xhtml#x1-22500012) explores the question of the larger
    expressive power of QCBM in comparison with classical neural networks (RBM). However,
    the first step should be an experimental verification of their performance characteristics.
    With this in mind, we would like to test the ability of both QCBM and RBM to learn
    relatively complex probability distributions and then efficiently sample from
    them.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第 [12](Chapter_12.xhtml#x1-22500012)章探讨了QCBM在表达能力上与经典神经网络（RBM）相比的问题。然而，第一步应该是对它们的性能特征进行实验验证。考虑到这一点，我们希望测试QCBM和RBM在学习相对复杂的概率分布方面的能力，然后高效地从中采样。
- en: 9.5.2 Sampling from the learned probability distributions
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.5.2 从学习到的概率分布中采样
- en: 'We are going to test the performance of QCBM and RBM on two datasets:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将测试 QCBM 和 RBM 在两个数据集上的表现：
- en: '**Dataset A.** A heavy-tailed distribution of daily S&P 500 index returns observed
    between 5 January 2009 and 22 February 2011 (UCI Machine Learning Repository  [[10](Biblography.xhtml#XAkbilgic2013), [9](Biblography.xhtml#XUCI_SP)]).
    The dataset consists of 536 samples.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集 A**。2009年1月5日至2011年2月22日之间观察到的S&P 500指数回报的重尾分布（UCI机器学习库 [[10](Biblography.xhtml#XAkbilgic2013)， [9](Biblography.xhtml#XUCI_SP)]）。数据集包含536个样本。'
- en: '**Dataset B.** A specially constructed distribution of a continuous random
    variable with a highly spiky probability density function (pdf) modelled as a
    mixture of Normal distributions. The dataset consists of 5,000 generated samples
    from a mixture of four Normal distributions with the following means, standard
    deviations, and weights:'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集 B**。一个特别构造的连续随机变量分布，具有高度尖锐的概率密度函数（pdf），该分布被建模为多个正态分布的混合。数据集由5,000个样本组成，这些样本来自四个正态分布的混合，具有以下均值、标准差和权重：'
- en: '| Mean | Standard deviation | Weight |'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 均值 | 标准差 | 权重 |'
- en: '| −3 | 0.3 | 0.1 |'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| −3 | 0.3 | 0.1 |'
- en: '| −1 | 0.3 | 0.2 |'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| −1 | 0.3 | 0.2 |'
- en: '| 1 | 0.3 | 0.3 |'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1 | 0.3 | 0.3 |'
- en: '| 3 | 0.3 | 0.4 |'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 3 | 0.3 | 0.4 |'
- en: 'Table 9.1: Parameters of the mixture of standard Normal distributions.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 9.1：标准正态分布混合的参数。
- en: 'In both cases, we convert the continuous samples into the corresponding 12-digit
    binary representation as per Algorithm [6](#x1-190006r6). Once the networks are
    trained (QCBM with Algorithm [8](#x1-194004r8) and RBM with Algorithm [2](Chapter_5.xhtml#x1-106003r2)),
    we generate new samples: 536 new samples for Dataset A and 5,000 new samples for
    Dataset B. This allows us to visualise the quality of the generated samples (once
    they are converted into the corresponding continuous representation as per Algorithm
    [7](#x1-190009r7)) by producing the empirical pdf and the QQ-plots as shown in
    Figures [9.6](#9.6) and [9.7](#9.7), which display sample simulation results for
    the fully trained models. We can see that both QCBM(12, 7) and RBM(12, 7) can
    successfully learn complex empirical distributions (heavy-tailed in the case of
    Dataset A and light-tailed with spiky pdf in the case of Dataset B). We have chosen CX
    for the fixed gates in QCBM and used the Qiskit quantum simulator to simulate
    the quantum parts of the training and sampling algorithms.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们将连续样本转换为相应的12位二进制表示，按照算法 [6](#x1-190006r6)。一旦网络训练完成（QCBM 使用算法 [8](#x1-194004r8)，RBM
    使用算法 [2](Chapter_5.xhtml#x1-106003r2)），我们生成新样本：数据集 A 生成 536 个新样本，数据集 B 生成 5,000
    个新样本。这使我们能够通过生成经验pdf和QQ图来可视化生成样本的质量，正如图 [9.6](#9.6) 和 [9.7](#9.7) 所示，它们展示了完全训练模型的样本模拟结果。我们可以看到，QCBM(12,
    7) 和 RBM(12, 7) 都能成功学习复杂的经验分布（数据集 A 为重尾分布，数据集 B 为轻尾且具有尖锐pdf的分布）。我们选择了CX门作为QCBM中的固定门，并使用Qiskit量子模拟器来模拟训练和采样算法中的量子部分。
- en: 'The following sets of hyperparameters were used to train the models:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 以下超参数集用于训练模型：
- en: '**Genetic Algorithm for training QCBM (Algorithm **[**8**](#x1-194004r8)**)**
    *N* = 1000, *M* = 25, *m* = 7, *α* = 1*.*0, *β* = 0*.*013863, *κ* = 50, *L* =
    200\. The value of *β* ensures that mutation rate halves after each *κ* generations.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用于训练 QCBM 的遗传算法（算法 **[**8**](#x1-194004r8)**）** *N* = 1000, *M* = 25, *m*
    = 7, *α* = 1*.*0, *β* = 0*.*013863, *κ* = 50, *L* = 200。*β* 的值确保每隔 *κ* 代变异率减半。'
- en: '**Contrastive Divergence algorithm for training RBM** (`sklearn.neural_network.BernoulliRBM`)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用于训练 RBM 的对比散度算法** (`sklearn.neural_network.BernoulliRBM`)'
- en: n_components = 7 – number of hidden activation units for RBM(12, 7) learning_rate
    = 0.0005 – learning rate *η* in Algorithm [2](Chapter_5.xhtml#x1-106003r2) batch_size
    = 10 – size of the training minibatches *S* in Algorithm [2](Chapter_5.xhtml#x1-106003r2)
    n_iter = 40000 – number of iterations
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'n_components = 7 – RBM(12, 7) 的隐藏激活单元数  '
- en: Although a visual inspection of the pdf and QQ-plots in Figures [9.6](#9.6)
    and [9.7](#9.7) suggests that both QCBM and RBM are doing a good job in generating
    high-quality samples from the learned empirical distributions encoded in model
    parameters, we would like to have a more objective measure of the model performance.
    This is especially important since we deal with generative models and very little
    can be concluded from a single model run.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从图[9.6](#9.6)和图[9.7](#9.7)中的概率密度函数（pdf）和 QQ 图来看，QCBM 和 RBM 都在从学习到的经验分布中生成高质量样本方面表现良好，但我们希望能有一个更客观的模型性能衡量标准。特别是在我们处理生成模型时，单次模型运行得出的结论非常有限。
- en: Running the quantum circuit multiple times for a particular configuration of
    model parameters (e.g., an optimal set of rotation angles found with the help
    of GA) results in the distribution of objective function values. This gives us
    an idea of what metrics can be used to measure the performance of QCBM and RBM  [[170](Biblography.xhtml#XKondratyev2020)].
    The cost function ([9.3.2](#x1-1940002)) we used for training QCBM can be calculated
    on the samples generated by RBM. In other words, we can compare the performance
    of QCBM and RBM by comparing the distributions of the cost function values calculated
    for the samples generated by these models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 针对某一特定模型参数配置（例如，通过遗传算法（GA）找到的最优旋转角度集）多次运行量子电路，会产生目标函数值的分布。这使我们能够了解可以用来衡量 QCBM
    和 RBM 性能的度量标准[[170](Biblography.xhtml#XKondratyev2020)]。我们在训练 QCBM 时使用的成本函数（[9.3.2](#x1-1940002)）可以通过
    RBM 生成的样本计算得到。换句话说，我们可以通过比较这两种模型生成的样本计算出的成本函数值的分布，来比较 QCBM 和 RBM 的性能。
- en: Table [9.2](#x1-198004r2) shows the means and standard deviations of the cost
    functions calculated for 100 runs of QCBM(12, 7) and RBM(12, 7). Each run generated
    5,000 samples from the learned empirical distribution (the models were trained
    on Dataset B, which consists of 5,000 samples from the mixture of four Normal
    distributions).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表[9.2](#x1-198004r2)展示了针对 QCBM(12, 7) 和 RBM(12, 7) 进行 100 次训练的成本函数的均值和标准差。每次训练都从学习到的经验分布中生成
    5000 个样本（模型是在数据集 B 上训练的，数据集 B 包含 5000 个来自四个正态分布混合的样本）。
- en: '| Model | Mean | Standard deviation |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均值 | 标准差 |'
- en: '| QCBM(12, 7) | 30.5 | 23.6 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| QCBM(12, 7) | 30.5 | 23.6 |'
- en: '| RBM(12, 7) | 39.6 | 30.8 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| RBM(12, 7) | 39.6 | 30.8 |'
- en: 'Table 9.2: Cost function statistics for the models trained on Dataset B.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.2：在数据集 B 上训练的模型的成本函数统计数据。
- en: It is clear from Table [9.2](#x1-198004r2) that QCBM(12, 7) with a weakly optimised
    set of hyperparameters performs better than RBM(12, 7) trained with equally weakly
    optimised hyperparameters (a small learning rate combined with a large number
    of iterations and the small size of the minibatches  [[134](Biblography.xhtml#XHinton2010)]).
    Although this cannot be seen as proper evidence of quantum advantage, this nevertheless
    opens the gate to promising further research.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从表[9.2](#x1-198004r2)可以明显看出，QCBM(12, 7) 在使用弱优化的超参数设置时，比使用同样弱优化的超参数（较小的学习率与较大的迭代次数以及小批量的大小[[134](Biblography.xhtml#XHinton2010)]）训练的
    RBM(12, 7) 表现更好。虽然这不能视为量子优势的确凿证据，但它无疑为有前景的进一步研究开辟了大门。
- en: '![Figure 9.6: Mixture of Normal distributions. ](img/file836.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6：正态分布混合模型。](img/file836.png)'
- en: 'Figure 9.6: Mixture of Normal distributions.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：正态分布混合模型。
- en: '![Figure 9.7: Distribution of S&P 500 index returns. ](img/file837.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7：标准普尔 500 指数回报分布。](img/file837.png)'
- en: 'Figure 9.7: Distribution of S&P 500 index returns.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：标准普尔 500 指数回报分布。
- en: Let us now turn our attention to Dataset A. The dataset consists of just 536
    samples and, as we can see in Figure [9.7](#9.7), the empirical pdf displays pronounced
    heavy tails which are also clearly seen in the QQ-plot against the Normal distribution.
    The relatively small number of samples means that we have to deal with a substantial
    amount of noise. Therefore, we need to use a robust statistical test to compare
    QCBM and RBM. Since we are working with a univariate distribution, we can estimate
    the quality of generated samples with the Kolmogorov-Smirnov test  [[233](Biblography.xhtml#XPfaffenberger1987)].
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们关注数据集A。该数据集仅包含536个样本，正如我们在图[9.7](#9.7)中看到的，经验概率密度函数显示出明显的重尾现象，这在与正态分布的QQ图中也清晰可见。样本数量相对较少意味着我们必须处理大量噪声。因此，我们需要使用稳健的统计检验来比较QCBM和RBM。由于我们处理的是单变量分布，因此我们可以使用Kolmogorov-Smirnov检验[[233](Biblography.xhtml#XPfaffenberger1987)]来估算生成样本的质量。
- en: Table [9.3](#x1-198011r3) provides the p-values and Kolmogorov-Smirnov statistics
    for the RBM and the QCBM generated samples as well as a Normal distribution fitted
    to the original dataset (by matching the first two moments). The p-value represents
    the probability of obtaining test results supporting the null hypothesis of the
    two datasets coming from the same distribution. In the context of our numerical
    experiments, the larger the p-value the more likely the generated samples were
    drawn from the correct distribution.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[9.3](#x1-198011r3)提供了RBM和QCBM生成的样本的p值和Kolmogorov-Smirnov统计量，以及通过匹配前两个矩得到的拟合到原始数据集的正态分布。p值表示获得支持零假设的测试结果的概率，即两个数据集来自相同分布的概率。在我们的数值实验中，p值越大，生成的样本越可能来自正确的分布。
- en: '| Distribution | p-value | K-S statistic |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 分布 | p值 | K-S统计量 |'
- en: '| Normal | 0.004 ± 0.009 | 0.121 ± 0.017 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 正态分布 | 0.004 ± 0.009 | 0.121 ± 0.017 |'
- en: '| RBM generated samples | 0.46 ± 0.23 | 0.055 ± 0.011 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| RBM生成样本 | 0.46 ± 0.23 | 0.055 ± 0.011 |'
- en: '| QCBM generated samples | 0.46 ± 0.11 | 0.053 ± 0.005 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| QCBM生成样本 | 0.46 ± 0.11 | 0.053 ± 0.005 |'
- en: 'Table 9.3: p-value and K-S statistic for Normal, RBM and QCBM generated samples
    in the format: mean ± standard deviation. Number of Normal, RBM and QCBM generated
    datasets: 20\. Number of samples in each generated dataset: 536 (equal to the
    number of samples in the original dataset).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表格9.3：正态分布、RBM和QCBM生成样本的p值和K-S统计量，格式为：均值 ± 标准差。正态分布、RBM和QCBM生成的数据集数量：20。每个生成的数据集的样本数：536（与原始数据集中的样本数相等）。
- en: The K-S statistic takes the largest absolute difference between the two distribution
    functions across all values of the random variable. The larger the K-S statistic
    the less likely the generated samples were drawn from the correct distribution.
    The K-S statistic can be compared with the critical values calculated for the
    given confidence level and number of samples. For example, the critical value
    corresponding to the 95th percentile confidence level and 536 samples in both
    datasets is 0*.*0587\. If the K-S statistic is larger then, with 95% certainty,
    we can reject the null hypothesis that 536 generated samples were drawn from the
    right distribution.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: K-S统计量取所有随机变量值下两个分布函数之间的最大绝对差。K-S统计量越大，生成样本来自正确分布的可能性就越小。K-S统计量可以与给定置信水平和样本数量计算得到的临界值进行比较。例如，对应于95%置信水平和536个样本的临界值是0*.*0587。如果K-S统计量较大，那么我们可以以95%的置信度拒绝“536个生成样本来自正确分布”的零假设。
- en: 'The first observation is that we can definitely reject the null hypothesis
    that the daily S&P 500 index returns are Normally distributed. The corresponding
    p-value is much smaller than 1, and the K-S statistic is twice the critical value.
    More importantly, QCBM performs at par with RBM in terms of both the p-value and
    the K-S statistic: we therefore cannot reject the null hypothesis that QCBM and
    RBM generated samples were drawn from the same distribution as the original dataset.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个观察结果是，我们可以明确拒绝“每日S&P 500指数回报服从正态分布”的零假设。相应的p值远小于1，K-S统计量是临界值的两倍。更重要的是，QCBM在p值和K-S统计量方面的表现与RBM相当：因此我们无法拒绝QCBM和RBM生成的样本来自与原始数据集相同分布的零假设。
- en: 9.5.3 Training algorithm convergence and hyperparameter optimisation
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.5.3 训练算法收敛性和超参数优化
- en: Next, we would like to explore the GA behaviour for various model configurations.
    In particular, it is interesting to investigate the algorithm convergence for
    different types of fixed gates, not just CX, and for different choices of the
    mutation rate. The charts in Figure [9.8](#9.8) confirm our intuition about CX
    being the best choice of fixed gate given the configuration of one-qubit gates
    (Figure [9.1](#9.1)) and the exponentially decreasing mutation rate performing
    better than the constant mutation rates. Here, we continue working with Dataset B.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们希望探讨 GA 在不同模型配置下的表现。特别是，调查不同类型固定门（不仅仅是 CX）和不同变异率选择下的算法收敛性是非常有趣的。图 [9.8](#9.8)
    中的图表确认了我们的直觉：给定单量子比特门的配置（图 [9.1](#9.1)），CX 是最佳的固定门选择，而指数衰减的变异率优于常数变异率。这里，我们继续使用数据集
    B。
- en: 'As we can see in Figure [9.1](#9.1), the fixed gates are flanked by one-qubit
    gates performing rotations around the *z*-axis. Therefore, adding another rotation
    around the *z*-axis by *ϕ* = *π* (Z = R[Z](*π*)) may not offer the same flexibility
    as rotation around the *x*-axis by *ϕ* = *π* (X = R[X](*π*)). Controlled rotations
    around the *z*-axis by an angle *ϕ < π* are likely to perform even worse. This
    is exactly what we see in Figure [9.8](#9.8) (left chart) for three different
    types of fixed gates: CX, CZ, and CR[Z](*π∕*4).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在图 [9.1](#9.1) 中所见，固定门两侧是执行绕 *z* 轴旋转的单量子比特门。因此，通过 *ϕ* = *π* 进行的绕 *z* 轴旋转（Z
    = R[Z](*π*)）可能无法提供与绕 *x* 轴旋转（X = R[X](*π*)）相同的灵活性。绕 *z* 轴的控制旋转角度 *ϕ < π* 可能表现得更差。这正是我们在图
    [9.8](#9.8)（左图）中对三种不同类型的固定门（CX、CZ 和 CR[Z](*π∕*4)）所看到的情况。
- en: Our intuition about the optimal choice of mutation rate suggests that it should
    be productive to start the algorithm with a really large mutation rate in order
    to explore the search space as broadly as possible (the "exploration" phase).
    Then, as the algorithm finds progressively better solutions, it should be useful
    to reduce the mutation rate in order to perform a more detailed search in the
    vicinity of the best solutions found so far (the "exploitation" phase). As the
    algorithm converges, we may want to perform more and more refined searches by
    only mutating one or two parameters. Figure [9.8](#9.8) (right chart) shows that
    this is indeed the case. Here, the maximum value of the mutation rate is *α* =
    1*.*0 and the minimum value is *α* = 0*.*0625 – the value reached after *L* =
    200 algorithm iterations when the algorithm is run with the initial value of mutation
    rate *α* = 1*.*0 and exponential decay factor *β* = 0*.*013863.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对变异率最佳选择的直觉表明，开始时使用一个非常大的变异率应该是有益的，以便尽可能广泛地探索搜索空间（“探索”阶段）。然后，随着算法逐步找到更好的解决方案，减少变异率应该会更有利，以便在找到的最佳解决方案附近进行更详细的搜索（“开发”阶段）。随着算法的收敛，我们可能希望通过仅变异一个或两个参数来进行越来越精细的搜索。图
    [9.8](#9.8)（右图）显示了这一点。这里，变异率的最大值为 *α* = 1*.*0，最小值为 *α* = 0*.*0625——这是在变异率初始值 *α*
    = 1*.*0 和指数衰减因子 *β* = 0*.*013863 的条件下，算法经过 *L* = 200 次迭代后达到的值。
- en: '![Figure 9.8: Left: GA convergence as a function of fixed gate type. Right:
    GA convergence as a function of mutation rate for CX fixed gates. Dots indicate
    mean values and error bars indicate the 10th and the 90th percentiles. GA parameters:
    N = 1000, M = 25, m = 7, 20 GA runs. ](img/file838.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8：左图：GA 收敛性与固定门类型的关系。右图：GA 收敛性与 CX 固定门的变异率关系。圆点表示均值，误差条表示第 10 和第 90 百分位数。GA
    参数：N = 1000，M = 25，m = 7，进行 20 次 GA 运行。](img/file838.jpg)'
- en: 'Figure 9.8: Left: GA convergence as a function of fixed gate type. Right: GA
    convergence as a function of mutation rate for CX fixed gates. Dots indicate mean
    values and error bars indicate the 10th and the 90th percentiles. GA parameters:
    N = 1000, M = 25, m = 7, 20 GA runs.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：左图：GA 收敛性与固定门类型的关系。右图：GA 收敛性与 CX 固定门的变异率关系。圆点表示均值，误差条表示第 10 和第 90 百分位数。GA
    参数：N = 1000，M = 25，m = 7，进行 20 次 GA 运行。
- en: Finally, we need to investigate the convergence of the algorithm as a function
    of the rotation angle discretisation scheme. In principle, an arbitrary rotation
    poses a problem as it must be approximated by a sequence of discrete gates because
    only discrete sets of gates can be implemented fault-tolerantly  [[180](Biblography.xhtml#XKudrow2013)].
    Since a GA operates on a discrete set of rotation angles, we face a trade-off
    between higher accuracy achieved through a finer discretisation scheme and implementation
    efficiency in the case of a less granular set of rotation angles. Additionally,
    all rotation gates can be executed with finite precision and the discretisation
    scheme should take this into account. Hence, in order to facilitate the efficient
    implementation of the rotation gates R[X](*𝜃*) and R[Z](*𝜃*), the GA operates
    on the rotation angles *𝜃* that take discrete values (−*π* + *νπ∕*2^(m−1))[ν=0,…,2^m−1],
    thus splitting the [−*π,π*] interval into 2^m equal subintervals.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要研究算法的收敛性，作为旋转角度离散化方案的函数。从原则上讲，任意旋转都会带来问题，因为它必须通过一系列离散门进行逼近，因为只有离散的门集合才能够容错实现
    [[180](Biblography.xhtml#XKudrow2013)]。由于遗传算法（GA）在离散的旋转角度集合上运行，我们面临一个折衷问题，即通过更精细的离散化方案实现更高的精度，还是在较不精细的旋转角度集合下提高实现效率。此外，所有旋转门都可以在有限精度下执行，离散化方案应该考虑到这一点。因此，为了促进旋转门
    R[X](*𝜃*) 和 R[Z](*𝜃*) 的高效实现，GA 在取离散值的旋转角度 *𝜃* 上运行，这些值为 (−*π* + *νπ∕*2^(m−1))[ν=0,…,2^m−1]，从而将
    [−*π,π*] 区间分成 2^m 个相等的子区间。
- en: 'Therefore, we must answer the question of GA convergence for various values
    of *m*. Figure [9.9](#9.9) shows the minimum values of the objective function ([9.3.2](#x1-1940002))
    as a function of the number of algorithm iterations for three different values
    of *m*:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们必须回答关于 GA 收敛性的问题，针对不同的 *m* 值。图 [9.9](#9.9) 显示了目标函数的最小值（[9.3.2](#x1-1940002)）作为算法迭代次数的函数，对于三种不同的
    *m* 值：
- en: '*m* = 3, rotation angle step Δ*𝜃* = *π∕*4;'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m* = 3，旋转角度步长 Δ*𝜃* = *π∕*4；'
- en: '*m* = 5, rotation angle step Δ*𝜃* = *π∕*16;'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m* = 5，旋转角度步长 Δ*𝜃* = *π∕*16；'
- en: '*m* = 7, rotation angle step Δ*𝜃* = *π∕*64.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m* = 7，旋转角度步长 Δ*𝜃* = *π∕*64。'
- en: We can see that the GA performance improves only marginally for *m >* 5\. This
    is good news suggesting that it may be sufficient to operate with rotation angle
    step Δ*𝜃* = *π∕*16 to achieve the desired precision in learning the target distribution.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当 *m >* 5 时，GA 的性能仅有轻微提升。这是个好消息，表明使用旋转角度步长 Δ*𝜃* = *π∕*16 就足以在学习目标分布时达到期望的精度。
- en: '![Figure 9.9: GA convergence as a function of the rotation angle discretisation
    scheme for CX fixed gates and exponentially decreasing mutation rate. Dots indicate
    mean values and error bars indicate the 10th and the 90th percentiles. GA parameters:
    N = 1000, M = 25, α = 1.0, β = 0.013863, 20 GA runs. ](img/file839.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9：GA 收敛性作为旋转角度离散化方案的函数，针对固定的 CX 门和指数下降的突变率。点表示平均值，误差条表示 10th 和 90th 百分位数。GA
    参数：N = 1000，M = 25，α = 1.0，β = 0.013863，进行 20 次 GA 运行。](img/file839.jpg)'
- en: 'Figure 9.9: GA convergence as a function of the rotation angle discretisation
    scheme for CX fixed gates and exponentially decreasing mutation rate. Dots indicate
    mean values and error bars indicate the 10th and the 90th percentiles. GA parameters:
    N = 1000, M = 25, α = 1.0, β = 0.013863, 20 GA runs.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：GA 收敛性作为旋转角度离散化方案的函数，针对固定的 CX 门和指数下降的突变率。点表示平均值，误差条表示 10th 和 90th 百分位数。GA
    参数：N = 1000，M = 25，α = 1.0，β = 0.013863，进行 20 次 GA 运行。
- en: The non-differentiable learning of QCBM with Genetic Algorithm is a viable approach
    to the training of PQCs. QCBM trained with GA performs at least as well as an
    equivalent classical neural network (RBM). The performance of QCBM and its classical
    counterpart were tested on two different datasets (heavy-tail distributed samples
    derived from the financial time series and light-tail distributed samples drawn
    from the specially constructed distribution with spiky pdf) and in both cases
    QCBM demonstrated its ability to learn the empirical distribution and generate
    new synthetic samples that have the same statistical properties as the original
    ones, as can be seen in the pdf and QQ-plots.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用遗传算法（GA）进行 QCBM 的非可微学习是一种可行的 PQC 训练方法。用 GA 训练的 QCBM 至少能够与等效的经典神经网络（RBM）表现相当。QCBM
    和其经典对比模型在两个不同的数据集上进行了测试（分别来源于金融时间序列的重尾分布样本和从特别构造的分布中提取的轻尾分布样本，并且其概率密度函数为尖峰型），在这两种情况下，QCBM
    都展示了它学习经验分布并生成新的合成样本的能力，这些新样本具有与原始样本相同的统计特性，如概率密度函数和 QQ 图所示。
- en: Analysing the GA convergence for different sets of hyperparameters, we observe
    that the best results were achieved with CX fixed gates and an exponentially decreasing
    mutation rate (starting from the maximum value of the mutation rate and setting
    the decay rate at a reasonably small value). More importantly, we see that more
    granular rotation angle discretisation schemes provide progressively less incremental
    value beyond some point. This means that for many practical purposes it is sufficient
    to implement rotations with the step Δ*𝜃* = *π∕*16 in order to encode target distribution
    with the desired accuracy for deep enough QCBM architectures (at least two layers
    of fixed 2-qubit gates). Since qubit rotations on NISQ devices can be implemented
    with finite precision, this ensures that QCBMs can be used productively for many
    real-world use cases.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 分析不同超参数集的遗传算法收敛性时，我们观察到最佳结果是在CX固定门和指数衰减的变异率下实现的（从变异率的最大值开始，并将衰减率设置为一个合理的小值）。更重要的是，我们发现，更细粒度的旋转角度离散化方案在某个临界点之后提供的增量价值逐渐减小。这意味着，对于许多实际应用而言，实现Δ*𝜃*
    = *π∕*16的旋转即可，在足够深的QCBM架构中（至少两层固定2量子比特门）就能以所需的准确度编码目标分布。由于在NISQ设备上量子比特旋转可以实现有限精度，这确保了QCBM可以在许多实际应用中有效使用。
- en: QCBM is a viable choice for building market generators. It performs at least
    as well as its classical counterpart, RBM, and demonstrates potential for achieving
    quantum advantage on near-term quantum processors.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: QCBM是构建市场生成器的可行选择。它的表现至少与其经典对手RBM相当，并展示了在近端量子处理器上实现量子优势的潜力。
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to construct and train a generative QML model
    – Quantum Circuit Born Machine. We started with the general concept of a PQC as
    a generative model, where the readout operation produces a sample from the probability
    distribution encoded in the PQC parameters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何构建和训练一个生成型QML模型——量子电路出生机。我们从PQC作为生成模型的总体概念开始，在该模型中，读取操作从PQC参数中编码的概率分布中生成一个样本。
- en: Next, we introduced the concept of a hardware-efficient PQC ansatz. Additionally,
    to build a model that is compatible with QPU connectivity and can easily be embedded
    into a QPU graph, we tried to use adjustable (one-qubit) and fixed (two-qubit)
    gates from the set of the native quantum gates for the given system.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍了硬件高效PQC假设的概念。此外，为了构建一个与QPU连接性兼容并且能轻松嵌入QPU图的模型，我们尝试使用给定系统的原生量子门集中的可调（单量子比特）和固定（双量子比特）门。
- en: Then, we studied differentiable and non-differentiable learning algorithms and
    experimented with the QCBM trained using Genetic Algorithm. Comparison with the
    classical benchmark (RBM) demonstrated a realistic possibility of quantum advantage
    for generative QML models.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们研究了可微分和不可微分的学习算法，并进行了使用遗传算法训练的QCBM实验。与经典基准（RBM）的比较展示了量子生成QML模型实现量子优势的现实可能性。
- en: Finally, we explored the question of training algorithm convergence for various
    sets of model parameters.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨了不同模型参数集的训练算法收敛性问题。
- en: In the next chapter, we will study another important and exceptionally promising
    QML model – Variational Quantum Eigensolver.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究另一种重要且极具潜力的QML模型——变分量子特征求解器。
- en: Join our book’s Discord space
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/quantum](https://packt.link/quantum)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，与志同道合的人交流，并与超过2000名成员一起学习，网址：[https://packt.link/quantum](https://packt.link/quantum)
- en: '![PIC](img/file1.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
