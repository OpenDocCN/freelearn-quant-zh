- en: '9'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '9'
- en: Quantum Circuit Born Machine
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­ç”µè·¯åšæ©æœºå™¨
- en: The arrival of the new computational paradigm of quantum computing and the progress
    achieved in developing quantum computing hardware prompted intensive research
    in exploring the capabilities of quantum machine learning models and, more specifically,
    quantum generative models that can be viewed as quantum counterparts of the classical
    RBMs introduced in ChapterÂ [5](Chapter_5.xhtml#x1-960005). Classical generative
    models form one of the most important classes of unsupervised machine learning
    techniques with numerous applications in finance, such as the generation of synthetic
    market dataÂ Â [[48](Biblography.xhtml#XBuehler2020),Â [173](Biblography.xhtml#XKS2020)],
    the development of systematic trading strategiesÂ Â [[176](Biblography.xhtml#Xkoshiyama2021generative)]
    or data anonymisationÂ  Â [[174](Biblography.xhtml#XKSH2020)], to name just a few.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­è®¡ç®—è¿™ä¸€æ–°è®¡ç®—èŒƒå¼çš„åˆ°æ¥ä»¥åŠé‡å­è®¡ç®—ç¡¬ä»¶çš„è¿›å±•ï¼Œä¿ƒä½¿äº†å¯¹é‡å­æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯é‡å­ç”Ÿæˆæ¨¡å‹çš„æ·±å…¥ç ”ç©¶ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥çœ‹ä½œæ˜¯ç»å…¸RBMçš„é‡å­å¯¹åº”ç‰©ï¼Œåè€…åœ¨ç¬¬[5](Chapter_5.xhtml#x1-960005)ç« ä¸­æœ‰ä»‹ç»ã€‚ç»å…¸ç”Ÿæˆæ¨¡å‹æ˜¯æ— ç›‘ç£æœºå™¨å­¦ä¹ æŠ€æœ¯ä¸­æœ€é‡è¦çš„ç±»åˆ«ä¹‹ä¸€ï¼Œå¹¿æ³›åº”ç”¨äºé‡‘èé¢†åŸŸï¼Œå¦‚åˆæˆå¸‚åœºæ•°æ®çš„ç”Ÿæˆ[[48](Biblography.xhtml#XBuehler2020),Â [173](Biblography.xhtml#XKS2020)]ã€ç³»ç»Ÿæ€§äº¤æ˜“ç­–ç•¥çš„å¼€å‘[[176](Biblography.xhtml#Xkoshiyama2021generative)]ï¼Œæˆ–æ•°æ®åŒ¿ååŒ–[[174](Biblography.xhtml#XKSH2020)]ï¼Œä»…ä¸¾å‡ ä¾‹ã€‚
- en: Quantum generative models have all the necessary qualities needed to establish
    quantum advantage on NISQ devices. Probably the most well known example of such
    models is the Quantum Circuit Born Machine (QCBM), which consists of several layers
    of adjustable and fixed gates followed by measurement operators. The input is
    a quantum state where all qubits are initialised as |0âŸ© in the computational basis.
    The output is a bitstring, which is a sample from the probability distribution
    encoded in the final state constructed by the application of adjustable and fixed
    gates to the initial state.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­ç”Ÿæˆæ¨¡å‹å…·å¤‡åœ¨NISQè®¾å¤‡ä¸Šå»ºç«‹é‡å­ä¼˜åŠ¿æ‰€éœ€çš„æ‰€æœ‰å¿…è¦ç‰¹æ€§ã€‚æœ€è‘—åçš„æ­¤ç±»æ¨¡å‹ä¹‹ä¸€æ˜¯é‡å­ç”µè·¯åšæ©æœºå™¨ï¼ˆQCBMï¼‰ï¼Œå®ƒç”±è‹¥å¹²å±‚å¯è°ƒå’Œå›ºå®šé—¨ç»„æˆï¼Œåé¢è·Ÿéšæµ‹é‡æ“ä½œç¬¦ã€‚è¾“å…¥æ˜¯ä¸€ä¸ªé‡å­æ€ï¼Œå…¶ä¸­æ‰€æœ‰é‡å­æ¯”ç‰¹åœ¨è®¡ç®—åŸºåº•ä¸­åˆå§‹åŒ–ä¸º|0âŸ©ã€‚è¾“å‡ºæ˜¯ä¸€ä¸ªæ¯”ç‰¹ä¸²ï¼Œè¿™æ˜¯é€šè¿‡å¯¹åˆå§‹çŠ¶æ€åº”ç”¨å¯è°ƒå’Œå›ºå®šé—¨æ„å»ºçš„æœ€ç»ˆæ€ä¸­ç¼–ç çš„æ¦‚ç‡åˆ†å¸ƒä¸­æå–çš„æ ·æœ¬ã€‚
- en: 'The expectation of experimental proof of the quantum advantage is motivated
    by the following observations: First, QCBMs have strictly larger expressive power
    than classical RBMs when only a polynomial number of parameters is allowed (the
    number of qubits in QCBM or the number of visible activation units in RBM)Â Â [[88](Biblography.xhtml#XDu2018)].
    Second, generating an independent sample from the learned distribution can be
    done in a single run of the quantum circuit in the case of QCBM â€“ this compares
    favorably with up to 10Â³-10â´ forward and backward passes through the network in
    the case of RBM, which are needed to achieve the state of thermal equilibriumÂ Â [[173](Biblography.xhtml#XKS2020)].
    This points towards material quantum speedup. Third, quantum generative models
    can be used to load data into a quantum state, thus facilitating realisations
    of many promising quantum algorithmsÂ Â [[314](Biblography.xhtml#XZoufal2019)].'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å®éªŒéªŒè¯é‡å­ä¼˜åŠ¿çš„æœŸæœ›æ¥è‡ªä»¥ä¸‹è§‚å¯Ÿï¼šé¦–å…ˆï¼Œå½“åªå…è®¸å¤šé¡¹å¼æ•°é‡çš„å‚æ•°æ—¶ï¼ˆQCBMä¸­çš„é‡å­æ¯”ç‰¹æ•°é‡æˆ–RBMä¸­å¯è§æ¿€æ´»å•å…ƒçš„æ•°é‡ï¼‰ï¼ŒQCBMçš„è¡¨è¾¾èƒ½åŠ›æ˜æ˜¾å¤§äºç»å…¸RBM[[88](Biblography.xhtml#XDu2018)]ã€‚å…¶æ¬¡ï¼Œåœ¨QCBMçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥é€šè¿‡é‡å­ç”µè·¯çš„ä¸€æ¬¡è¿è¡Œç”Ÿæˆä»å·²å­¦ä¹ åˆ†å¸ƒä¸­ç‹¬ç«‹çš„æ ·æœ¬â€”â€”ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRBMéœ€è¦é€šè¿‡ç½‘ç»œè¿›è¡Œæœ€å¤š10Â³åˆ°10â´æ¬¡çš„å‰å‘å’Œåå‘ä¼ æ’­æ‰èƒ½è¾¾åˆ°çƒ­å¹³è¡¡çŠ¶æ€[[173](Biblography.xhtml#XKS2020)]ã€‚è¿™è¡¨æ˜äº†é‡å­åŠ é€Ÿçš„å¯èƒ½æ€§ã€‚ç¬¬ä¸‰ï¼Œé‡å­ç”Ÿæˆæ¨¡å‹å¯ä»¥ç”¨äºå°†æ•°æ®åŠ è½½åˆ°é‡å­æ€ä¸­ï¼Œä»è€Œä¿ƒè¿›å¤šç§æœ‰å‰æ™¯çš„é‡å­ç®—æ³•çš„å®ç°[[314](Biblography.xhtml#XZoufal2019)]ã€‚
- en: 9.1 Constructing QCBM
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 æ„å»ºQCBM
- en: As we have seen in ChapterÂ [8](Chapter_8.xhtml#x1-1620008), the art of building
    a QML model that can be run on a NISQ computer consists of finding an optimal
    PQC architecture that can be embedded into the chosen QPU graph. In this section,
    we show how it can be done for the QCBM compatible with IBMâ€™s Melbourne and Rochester
    systems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬[8](Chapter_8.xhtml#x1-1620008)ç« ä¸­æ‰€è§ï¼Œæ„å»ºå¯ä»¥åœ¨NISQè®¡ç®—æœºä¸Šè¿è¡Œçš„QMLæ¨¡å‹çš„è‰ºæœ¯åœ¨äºå¯»æ‰¾ä¸€ä¸ªæœ€ä¼˜çš„PQCæ¶æ„ï¼Œä»¥ä¾¿å°†å…¶åµŒå…¥åˆ°æ‰€é€‰çš„QPUå›¾ä¸­ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä¸ºä¸IBMçš„å¢¨å°”æœ¬å’Œç½—åˆ‡æ–¯ç‰¹ç³»ç»Ÿå…¼å®¹çš„QCBMæ„å»ºæ­¤æ¶æ„ã€‚
- en: 9.1.1 QCBM architecture
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.1 QCBMæ¶æ„
- en: QCBM is a parameterised quantum circuit where a layer of adjustable one-qubit
    gates is followed by a layer of fixed two-qubit gates. Such a pattern can be repeated
    any number of times, building a progressively deeper circuit. The input is a quantum
    state where all qubits are initialised as |0âŸ© in the computational basis. The
    final layer consists of measurement operators producing a bitstring sample from
    the learned distribution. Therefore, to specify the QCBM architecture means to
    specify the number of layers, the type of adjustable gates, and the type of fixed
    gates for each layer. Since the theory of PQC is still being developedÂ Â [[29](Biblography.xhtml#XBenedetti2019)],
    we can rely on similarities and analogies between PQCs and classical neural networks
    to come up with some initial guesses about the possible QCBM architecture.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: QCBMæ˜¯ä¸€ä¸ªå‚æ•°åŒ–é‡å­ç”µè·¯ï¼Œå…¶ä¸­ä¸€å±‚å¯è°ƒçš„å•é‡å­æ¯”ç‰¹é—¨åé¢è·Ÿç€ä¸€å±‚å›ºå®šçš„åŒé‡å­æ¯”ç‰¹é—¨ã€‚è¿™æ ·çš„æ¨¡å¼å¯ä»¥é‡å¤å¤šæ¬¡ï¼Œæ„å»ºä¸€ä¸ªé€æ¸åŠ æ·±çš„ç”µè·¯ã€‚è¾“å…¥æ˜¯ä¸€ä¸ªé‡å­æ€ï¼Œæ‰€æœ‰é‡å­æ¯”ç‰¹éƒ½åˆå§‹åŒ–ä¸º|0âŸ©ï¼Œä»¥è®¡ç®—åŸºè¡¨ç¤ºã€‚æœ€ç»ˆå±‚ç”±æµ‹é‡ç®—ç¬¦ç»„æˆï¼Œä»å­¦ä¹ åˆ°çš„åˆ†å¸ƒä¸­ç”Ÿæˆä¸€ä¸ªæ¯”ç‰¹ä¸²æ ·æœ¬ã€‚å› æ­¤ï¼ŒæŒ‡å®šQCBMæ¶æ„æ„å‘³ç€æŒ‡å®šå±‚æ•°ã€æ¯å±‚çš„å¯è°ƒé—¨ç±»å‹ä»¥åŠæ¯å±‚çš„å›ºå®šé—¨ç±»å‹ã€‚ç”±äºPQCçš„ç†è®ºä»åœ¨å‘å±•ä¸­[[29](Biblography.xhtml#XBenedetti2019)]ï¼Œæˆ‘ä»¬å¯ä»¥ä¾é PQCå’Œç»å…¸ç¥ç»ç½‘ç»œä¹‹é—´çš„ç›¸ä¼¼æ€§å’Œç±»æ¯”ï¼Œæå‡ºä¸€äº›å…³äºå¯èƒ½QCBMæ¶æ„çš„åˆæ­¥çŒœæµ‹ã€‚
- en: '![FigureÂ 9.1: QCBM(12, 7). ](img/file808.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾9.1ï¼šQCBM(12, 7)ã€‚](img/file808.jpg)'
- en: 'FigureÂ 9.1: QCBM(12, 7).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.1ï¼šQCBM(12, 7)ã€‚
- en: 'FigureÂ [9.1](#9.1) displays a 12-qubit QCBM with two layers of controlled rotation
    gates R = R[G](*Ï•*) for G âˆˆ{X*,*Y*,*Z} and *Ï•* âˆˆ [âˆ’*Ï€,Ï€*], whereÂ G andÂ *Ï•* are
    fixed, and three layers of one-qubit gates R[X](*ğœƒ*) and R[Z](*ğœƒ*) with a total
    of seven adjustable gates per quantum register. The circuit is wide enough and
    deep enough to learn a complex distribution of a continuous random variable while
    remaining implementable on existing NISQ devices: the 12-digit binary representation
    of a continuous random variable provides sufficient precision and seven adjustable
    parameters (rotation angles) per qubit provide sufficient flexibility. At the
    same time, the circuit is not too deep to be compromised by the gate fidelity
    achievable in existing quantum hardwareÂ Â [[46](Biblography.xhtml#XBruzewicz2019),Â [164](Biblography.xhtml#XKjaergaard2019)].'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[9.1](#9.1)æ˜¾ç¤ºäº†ä¸€ä¸ª12é‡å­æ¯”ç‰¹çš„QCBMï¼Œå…·æœ‰ä¸¤å±‚å—æ§æ—‹è½¬é—¨R = R[G](*Ï•*)ï¼Œå…¶ä¸­G âˆˆ{X*,*Y*,*Z}ä¸”*Ï•* âˆˆ [âˆ’*Ï€,Ï€*]ï¼Œå…¶ä¸­Gå’Œ*Ï•*æ˜¯å›ºå®šçš„ï¼Œä»¥åŠä¸‰å±‚å•é‡å­æ¯”ç‰¹é—¨R[X](*ğœƒ*)å’ŒR[Z](*ğœƒ*)ï¼Œæ¯ä¸ªé‡å­å¯„å­˜å™¨æ€»å…±æœ‰ä¸ƒä¸ªå¯è°ƒé—¨ã€‚è¯¥ç”µè·¯è¶³å¤Ÿå®½ä¸”è¶³å¤Ÿæ·±ï¼Œå¯ä»¥å­¦ä¹ ä¸€ä¸ªè¿ç»­éšæœºå˜é‡çš„å¤æ‚åˆ†å¸ƒï¼ŒåŒæ—¶ä»èƒ½åœ¨ç°æœ‰çš„NISQè®¾å¤‡ä¸Šå®ç°ï¼šä¸€ä¸ªè¿ç»­éšæœºå˜é‡çš„12ä½äºŒè¿›åˆ¶è¡¨ç¤ºæä¾›è¶³å¤Ÿçš„ç²¾åº¦ï¼Œè€Œæ¯ä¸ªé‡å­æ¯”ç‰¹çš„ä¸ƒä¸ªå¯è°ƒå‚æ•°ï¼ˆæ—‹è½¬è§’åº¦ï¼‰æä¾›è¶³å¤Ÿçš„çµæ´»æ€§ã€‚åŒæ—¶ï¼Œç”µè·¯çš„æ·±åº¦åˆä¸ä¼šè¿‡å¤§ï¼Œä»¥è‡³äºå—åˆ°ç°æœ‰é‡å­ç¡¬ä»¶å¯å®ç°çš„é—¨ä¿çœŸåº¦çš„å½±å“[[46](Biblography.xhtml#XBruzewicz2019),
    [164](Biblography.xhtml#XKjaergaard2019)]ã€‚
- en: 9.1.2 QCBM embedding
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.2 QCBMåµŒå…¥
- en: The chosen QCBM architecture is compatible with the limited connectivity observed
    in the current generation of quantum processors. For example, the proposed circuit
    requires sequential qubit connectivity where qubit *n* is directly connected with
    qubits *n*âˆ’ 1 and *n* + 1 but does not have to be directly connected with other
    qubits. This architecture can for example be supported by IBMâ€™s Melbourne systemÂ Â [[208](Biblography.xhtml#XMelbourne2019)]
    in FigureÂ [9.2](#9.2), where theÂ 12 shaded qubits correspond to theÂ 12 quantum
    registers in FigureÂ [9.1](#9.1). The thick lines represent connections used in
    the QCBM ansatz while the thin lines represent all other available qubit connections.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©çš„QCBMæ¶æ„ä¸å½“å‰ä¸€ä»£é‡å­å¤„ç†å™¨ä¸­è§‚å¯Ÿåˆ°çš„æœ‰é™è¿æ¥æ€§å…¼å®¹ã€‚ä¾‹å¦‚ï¼Œæ‰€æè®®çš„ç”µè·¯éœ€è¦é¡ºåºçš„é‡å­æ¯”ç‰¹è¿æ¥ï¼Œå…¶ä¸­é‡å­æ¯”ç‰¹*n*ç›´æ¥è¿æ¥é‡å­æ¯”ç‰¹*n*âˆ’ 1å’Œ*n*
    + 1ï¼Œä½†ä¸éœ€è¦ç›´æ¥è¿æ¥å…¶ä»–é‡å­æ¯”ç‰¹ã€‚è¯¥æ¶æ„ä¾‹å¦‚å¯ä»¥ç”±IBMçš„å¢¨å°”æœ¬ç³»ç»Ÿ[[208](Biblography.xhtml#XMelbourne2019)]æ”¯æŒï¼Œå¦‚å›¾[9.2](#9.2)æ‰€ç¤ºï¼Œå…¶ä¸­12ä¸ªé˜´å½±éƒ¨åˆ†çš„é‡å­æ¯”ç‰¹å¯¹åº”å›¾[9.1](#9.1)ä¸­çš„12ä¸ªé‡å­å¯„å­˜å™¨ã€‚ç²—çº¿è¡¨ç¤ºQCBM
    Ansatzä¸­ä½¿ç”¨çš„è¿æ¥ï¼Œè€Œç»†çº¿è¡¨ç¤ºæ‰€æœ‰å…¶ä»–å¯ç”¨çš„é‡å­æ¯”ç‰¹è¿æ¥ã€‚
- en: '![FigureÂ 9.2: IBMâ€™s Melbourne system. ](img/file809.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾9.2ï¼šIBMçš„å¢¨å°”æœ¬ç³»ç»Ÿã€‚](img/file809.jpg)'
- en: 'FigureÂ 9.2: IBMâ€™s Melbourne system.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.2ï¼šIBMçš„å¢¨å°”æœ¬ç³»ç»Ÿã€‚
- en: The 53-qubit Rochester deviceÂ Â [[208](Biblography.xhtml#XMelbourne2019)] in
    FigureÂ [9.3](#9.3) can also be used to implement this QCBM architecture. Here,
    we have several choices for embedding the QCBM circuit (12 linearly connected
    qubits forming a closed loop); shaded qubits show one such possibility.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[9.3](#9.3)ä¸­çš„53é‡å­æ¯”ç‰¹ç½—åˆ‡æ–¯ç‰¹è®¾å¤‡[[208](Biblography.xhtml#XMelbourne2019)]ä¹Ÿå¯ä»¥ç”¨æ¥å®ç°è¿™ä¸ªQCBMæ¶æ„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰å‡ ç§é€‰æ‹©æ¥åµŒå…¥QCBMç”µè·¯ï¼ˆ12ä¸ªçº¿æ€§è¿æ¥çš„é‡å­æ¯”ç‰¹å½¢æˆä¸€ä¸ªé—­ç¯ï¼‰ï¼›é˜´å½±é‡å­æ¯”ç‰¹æ˜¾ç¤ºäº†å…¶ä¸­ä¸€ç§å¯èƒ½æ€§ã€‚
- en: '![FigureÂ 9.3: IBMâ€™s Rochester system. ](img/file810.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾9.3ï¼šIBMçš„ç½—åˆ‡æ–¯ç‰¹ç³»ç»Ÿã€‚](img/file810.jpg)'
- en: 'FigureÂ 9.3: IBMâ€™s Rochester system.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.3ï¼šIBMçš„ç½—åˆ‡æ–¯ç‰¹ç³»ç»Ÿã€‚
- en: IBM systems, such as Melbourne and Rochester, are based on superconducting qubits.
    The choice of the underlying technology means that there is a set of native gates
    â€“ the quantum gates derived directly from the types of interactions that occur
    in the given technical realisation of the quantum chip.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: IBMç³»ç»Ÿï¼Œå¦‚å¢¨å°”æœ¬å’Œç½—åˆ‡æ–¯ç‰¹ï¼ŒåŸºäºè¶…å¯¼é‡å­æ¯”ç‰¹ã€‚åŸºç¡€æŠ€æœ¯çš„é€‰æ‹©æ„å‘³ç€å­˜åœ¨ä¸€ç»„æœ¬åœ°é—¨â€”â€”è¿™äº›é‡å­é—¨ç›´æ¥æ¥è‡ªäºåœ¨ç»™å®šé‡å­èŠ¯ç‰‡æŠ€æœ¯å®ç°ä¸­å‘ç”Ÿçš„ç›¸äº’ä½œç”¨ç±»å‹ã€‚
- en: In the case of IBM devices, the cross resonance gate generates the ZX interaction
    that leads to a CNOT gate. When it comes to single-qubit gates, we note that R[Z]
    is a diagonal gate given byÂ ([6.3.3](Chapter_6.xhtml#x1-130014r3)) and can be
    implemented virtually in hardware via frame change (at zero error and duration)Â Â [[239](Biblography.xhtml#XQiskitRZGate)].
    Therefore, it is sufficient to have just an X drive to rotate the qubit on the
    Bloch sphere (one can move a qubit between two arbitrary points on the Bloch sphere
    with the help of just two gates, R[X] and R[Z]).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºIBMè®¾å¤‡ï¼Œäº¤å‰å…±æŒ¯é—¨äº§ç”ŸZXç›¸äº’ä½œç”¨ï¼Œä»è€Œå¯¼è‡´CNOTé—¨ã€‚å¯¹äºä¸€æ¯”ç‰¹é—¨ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°R[Z]æ˜¯ä¸€ä¸ªå¯¹è§’é—¨ï¼Œç”±([6.3.3](Chapter_6.xhtml#x1-130014r3))ç»™å‡ºï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡å¸§å˜æ¢åœ¨ç¡¬ä»¶ä¸­è™šæ‹Ÿå®ç°ï¼ˆåœ¨é›¶è¯¯å·®å’ŒæŒç»­æ—¶é—´ä¸‹ï¼‰[[239](Biblography.xhtml#XQiskitRZGate)]ã€‚å› æ­¤ï¼Œä»…éœ€ä¸€ä¸ªXé©±åŠ¨å°±è¶³ä»¥æ—‹è½¬Blochçƒä¸Šçš„é‡å­æ¯”ç‰¹ï¼ˆå€ŸåŠ©ä¸¤ä¸ªé—¨R[X]å’ŒR[Z]ï¼Œå°±å¯ä»¥åœ¨Blochçƒä¸Šå°†é‡å­æ¯”ç‰¹ä»ä¸€ä¸ªä»»æ„ç‚¹ç§»åŠ¨åˆ°å¦ä¸€ä¸ªä»»æ„ç‚¹ï¼‰ã€‚
- en: This means that we can introduce the concept of a *hardware-efficient* architecture
    not only in terms of connectivity but also in terms of the choice of one-qubit
    and two-qubit gates. Taking into account the CNOT and CPHASE gate decomposition
    shown in FiguresÂ [6.19](Chapter_6.xhtml#6.19) andÂ [6.20](Chapter_6.xhtml#6.20),
    the hardware-efficient QCBM architecture for the Melbourne and Rochester systems
    would consist of a combination of R[X] and R[Z] adjustable single-qubit gates
    and CNOT and CPHASE fixed two-qubit gatesÂ Â [[153](Biblography.xhtml#XKandala2017),Â [30](Biblography.xhtml#XBenedetti2021)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æˆ‘ä»¬ä¸ä»…å¯ä»¥ä»è¿æ¥æ€§è§’åº¦ï¼Œè¿˜å¯ä»¥ä»ä¸€æ¯”ç‰¹å’Œä¸¤æ¯”ç‰¹é—¨çš„é€‰æ‹©è§’åº¦å¼•å…¥*ç¡¬ä»¶é«˜æ•ˆ*æ¶æ„çš„æ¦‚å¿µã€‚è€ƒè™‘åˆ°å›¾[6.19](Chapter_6.xhtml#6.19)å’Œ[6.20](Chapter_6.xhtml#6.20)ä¸­å±•ç¤ºçš„CNOTå’ŒCPHASEé—¨åˆ†è§£ï¼Œå¢¨å°”æœ¬å’Œç½—åˆ‡æ–¯ç‰¹ç³»ç»Ÿçš„ç¡¬ä»¶é«˜æ•ˆQCBMæ¶æ„å°†ç”±å¯è°ƒèŠ‚çš„R[X]å’ŒR[Z]å•æ¯”ç‰¹é—¨ä»¥åŠå›ºå®šçš„CNOTå’ŒCPHASEä¸¤æ¯”ç‰¹é—¨ç»„åˆè€Œæˆ[[153](Biblography.xhtml#XKandala2017),
    [30](Biblography.xhtml#XBenedetti2021)]ã€‚
- en: QCBM is a PQC trained as a generative ML model. QCBM operating onÂ *N* quantum
    registers transforms the initial quantum state ![|0 âŸ©](img/file811.jpg)^(âŠ—N) into
    the quantum state encoding the learned probability distribution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: QCBMæ˜¯ä½œä¸ºç”Ÿæˆå¼æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„PQCã€‚QCBMä½œç”¨äº*N*é‡å­å¯„å­˜å™¨ï¼Œå°†åˆå§‹é‡å­æ€![|0 âŸ©](img/file811.jpg)^(âŠ—N)è½¬åŒ–ä¸ºç¼–ç å­¦ä¹ åˆ°çš„æ¦‚ç‡åˆ†å¸ƒçš„é‡å­æ€ã€‚
- en: 9.2 Differentiable Learning of QCBM
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 QCBMçš„å¯å¾®å­¦ä¹ 
- en: The output of a QCBM circuit is a bitstring that represents a sample from the
    probability distribution encoded in the quantum state. The circuit itself is,
    essentially, a mechanism of transforming an initial state |0âŸ©^(âŠ—n) into a final
    state from which a sample is generated by means of measuring the qubits in the
    computational basis.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: QCBMç”µè·¯çš„è¾“å‡ºæ˜¯ä¸€ä¸ªæ¯”ç‰¹ä¸²ï¼Œè¡¨ç¤ºä»é‡å­æ€ä¸­ç¼–ç çš„æ¦‚ç‡åˆ†å¸ƒä¸­æŠ½å–çš„æ ·æœ¬ã€‚ç”µè·¯æœ¬èº«æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæœºåˆ¶ï¼Œå°†åˆå§‹æ€|0âŸ©^(âŠ—n)è½¬åŒ–ä¸ºæœ€ç»ˆæ€ï¼Œç„¶åé€šè¿‡æµ‹é‡é‡å­æ¯”ç‰¹ï¼ˆåœ¨è®¡ç®—åŸºä¸­ï¼‰ç”Ÿæˆæ ·æœ¬ã€‚
- en: Different configurations of one-qubit and multi-qubit gates encode different
    probability distributions â€“ the training of QCBM consists of finding an optimal
    circuit configuration (ansatz) and an optimal set of adjustable parameters that
    minimise the distance between the probability distribution encoded in the final
    quantum state (before measurement, or "before sampling") and the probability distribution
    of the training dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ¯”ç‰¹å’Œå¤šæ¯”ç‰¹é—¨çš„ä¸åŒé…ç½®ç¼–ç äº†ä¸åŒçš„æ¦‚ç‡åˆ†å¸ƒâ€”â€”QCBMçš„è®­ç»ƒåŒ…æ‹¬å¯»æ‰¾ä¸€ä¸ªæœ€ä½³çš„ç”µè·¯é…ç½®ï¼ˆansatzï¼‰å’Œä¸€ç»„æœ€ä½³çš„å¯è°ƒå‚æ•°ï¼Œä»¥æœ€å°åŒ–æœ€ç»ˆé‡å­æ€ï¼ˆæµ‹é‡ä¹‹å‰ï¼Œæˆ–ç§°â€œé‡‡æ ·ä¹‹å‰â€ï¼‰ä¸­ç¼–ç çš„æ¦‚ç‡åˆ†å¸ƒä¸è®­ç»ƒæ•°æ®é›†çš„æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚
- en: Following the structure we adopted in ChapterÂ [8](Chapter_8.xhtml#x1-1620008),
    we start with the differentiable learning approach, before moving to the non-differentiable
    learning method based on a different kind of evolutionary search heuristic â€“ Genetic
    Algorithm.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰ç…§æˆ‘ä»¬åœ¨ç¬¬[8](Chapter_8.xhtml#x1-1620008)ç« ä¸­é‡‡ç”¨çš„ç»“æ„ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»å¯å¾®å­¦ä¹ æ–¹æ³•ï¼Œç„¶åå†ä»‹ç»åŸºäºå¦ä¸€ç§è¿›åŒ–æœç´¢å¯å‘å¼æ–¹æ³•çš„éå¯å¾®å­¦ä¹ æ–¹æ³•â€”â€”é—ä¼ ç®—æ³•ã€‚
- en: 9.2.1 Sample encoding
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.1 æ ·æœ¬ç¼–ç 
- en: 'In the most general case, a training dataset consists of samples containing
    continuous, integer and categorical features. However, QCBM operates on binary
    variables. Therefore, we need to design a method to convert continuous features
    into binary ones and a method for converting generated binary QCBM output (sampling)
    into continuous variables. The integer and binary features can be treated as special
    cases of continuous features and categorical features can be first converted into
    binary features through one-hot encoding. Such a method can be realised as a two-step
    routine (AlgorithmÂ [6](#x1-190006r6)):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€ä¸€èˆ¬çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒæ•°æ®é›†ç”±åŒ…å«è¿ç»­ã€æ•´æ•°å’Œç±»åˆ«ç‰¹å¾çš„æ ·æœ¬ç»„æˆã€‚ç„¶è€Œï¼ŒQCBM ä»…å¤„ç†äºŒè¿›åˆ¶å˜é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®¾è®¡ä¸€ç§æ–¹æ³•å°†è¿ç»­ç‰¹å¾è½¬æ¢ä¸ºäºŒè¿›åˆ¶ç‰¹å¾ï¼Œå¹¶è®¾è®¡ä¸€ç§æ–¹æ³•å°†ç”Ÿæˆçš„äºŒè¿›åˆ¶
    QCBM è¾“å‡ºï¼ˆé‡‡æ ·ï¼‰è½¬æ¢ä¸ºè¿ç»­å˜é‡ã€‚æ•´æ•°å’ŒäºŒè¿›åˆ¶ç‰¹å¾å¯ä»¥è§†ä¸ºè¿ç»­ç‰¹å¾çš„ç‰¹ä¾‹ï¼Œç±»åˆ«ç‰¹å¾å¯ä»¥é€šè¿‡ä¸€çƒ­ç¼–ç è½¬æ¢ä¸ºäºŒè¿›åˆ¶ç‰¹å¾ã€‚æ­¤æ–¹æ³•å¯ä»¥å®ç°ä¸ºä¸¤æ­¥è¿‡ç¨‹ï¼ˆç®—æ³• [6](#x1-190006r6)ï¼‰ï¼š
- en: Conversion of a continuous variable into the corresponding integer variable.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è¿ç»­å˜é‡è½¬æ¢ä¸ºç›¸åº”çš„æ•´æ•°å˜é‡ã€‚
- en: Conversion of the integer variable into the corresponding binary variable.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ•´æ•°å˜é‡è½¬æ¢ä¸ºç›¸åº”çš„äºŒè¿›åˆ¶å˜é‡ã€‚
- en: 'Given the generated binary output, the same routine can be used in reverse
    mode to produce continuous samples (AlgorithmÂ Â [7](#x1-190009r7)):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šç”Ÿæˆçš„äºŒè¿›åˆ¶è¾“å‡ºï¼Œå¯ä»¥ä½¿ç”¨ç›¸åŒçš„è¿‡ç¨‹ä»¥åå‘æ¨¡å¼ç”Ÿæˆè¿ç»­æ ·æœ¬ï¼ˆç®—æ³• [7](#x1-190009r7)ï¼‰ï¼š
- en: Conversion of the generated binary QCBM output into integer samples.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†ç”Ÿæˆçš„äºŒè¿›åˆ¶ QCBM è¾“å‡ºè½¬æ¢ä¸ºæ•´æ•°æ ·æœ¬ã€‚
- en: Conversion of integer samples into the corresponding continuous samples.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ•´æ•°æ ·æœ¬è½¬æ¢ä¸ºç›¸åº”çš„è¿ç»­æ ·æœ¬ã€‚
- en: '![--------------------------------------------------------------------- Algorithm
    6: Continuous to integer to binary transformation (training phase) ---------------------------------------------------------------------
    Result: Conversion of continuous variables into M -digit binary features. ( (n)
    ) Input: Xreal(l) l=1,...,Nsamples;n=1,...,Nvariables â€“ continuous data sample.
    for n = 1,...,Nvariables do ( ) | X (mni)n â† minl=1,...,Nsamples X (rnea)l(l)
    âˆ’ ğœ€(nm)in, for ğœ€(nm)in â‰¥ 0 | (n) ( (n) ) (n) (n) | X max â† maxl=1,...,Nsamples
    X real(l) + ğœ€max, for ğœ€max â‰¥ 0 | | for l = 1,...,Nsamples do ( ) | | (n) ( M )
    X (nre)al(l)âˆ’ X (mni)n | | X integer(l) â† int 2 âˆ’ 1 ---(n)-----(n)- | | X max
    âˆ’ X min | | (n) ( (n) ) | | Xbinary(l) â† bin X integer(l) | | end end Each data
    sample is represented by an M -digit binary number with every digit becoming a
    separate feature. The total number of features is M Ã— Nvariables. ---------------------------------------------------------------------
    ](img/file812.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- ç®—æ³•
    6ï¼šè¿ç»­åˆ°æ•´æ•°åˆ°äºŒè¿›åˆ¶è½¬æ¢ï¼ˆè®­ç»ƒé˜¶æ®µï¼‰---------------------------------------------------------------------
    ç»“æœï¼šå°†è¿ç»­å˜é‡è½¬æ¢ä¸º M ä½äºŒè¿›åˆ¶ç‰¹å¾ã€‚ï¼ˆï¼ˆnï¼‰ï¼‰è¾“å…¥ï¼šXreal(l) l=1,...,Nsamplesï¼›n=1,...,Nvariables - è¿ç»­æ•°æ®æ ·æœ¬ã€‚å¯¹äº
    n = 1,...,Nvariables åšï¼šï¼ˆï¼‰ | X (mni)n â† minl=1,...,Nsamples X (rnea)l(l) âˆ’ ğœ–(nm)in,
    å¦‚æœ ğœ–(nm)in â‰¥ 0 | ï¼ˆnï¼‰ ï¼ˆï¼ˆnï¼‰ï¼‰ ï¼ˆnï¼‰ ï¼ˆnï¼‰ | X max â† maxl=1,...,Nsamples X real(l) + ğœ–max,
    å¦‚æœ ğœ–max â‰¥ 0 | | å¯¹äº l = 1,...,Nsamples åšï¼ˆ ï¼‰ | | ï¼ˆnï¼‰ ï¼ˆ M ï¼‰ X (nre)al(l)âˆ’ X (mni)n
    | | X integer(l) â† int 2 âˆ’ 1 ---(n)-----(n)- | | X max âˆ’ X min | | ï¼ˆnï¼‰ ï¼ˆï¼ˆnï¼‰ï¼‰ |
    | Xbinary(l) â† bin X integer(l) | | ç»“æŸ ç»“æŸ æ¯ä¸ªæ•°æ®æ ·æœ¬ç”±ä¸€ä¸ª M ä½äºŒè¿›åˆ¶æ•°è¡¨ç¤ºï¼Œæ¯ä¸€ä½æˆä¸ºä¸€ä¸ªç‹¬ç«‹çš„ç‰¹å¾ã€‚ç‰¹å¾æ€»æ•°ä¸º
    M Ã— Nvariablesã€‚---------------------------------------------------------------------
    ](img/file812.jpg)'
- en: '![--------------------------------------------------------------------- Algorithm
    7: Binary to integer to continuous transformation (sampling phase) ---------------------------------------------------------------------
    Result: Conversion of the generated M -digit binary sample into continuous sample.
    ( ^ (n)) Input: X[m] m=0,...,M âˆ’1;n=1,...,Nvariables â€“ generated M -digit binary
    sample. for n = 1,...,Nvariables do | (n) Mâˆ‘âˆ’1 (n) | X^integer := 2m X^[M âˆ’1âˆ’m
    ] | m=0 | (n) (n) 1 (n) ( (n)) | ^X real â† X min +-M----X^integer Xm(na)x âˆ’ X
    min | 2 âˆ’ 1 end ---------------------------------------------------------------------
    ](img/file813.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- ç®—æ³•
    7ï¼šäºŒè¿›åˆ¶åˆ°æ•´æ•°åˆ°è¿ç»­è½¬æ¢ï¼ˆé‡‡æ ·é˜¶æ®µï¼‰---------------------------------------------------------------------
    ç»“æœï¼šå°†ç”Ÿæˆçš„ M ä½äºŒè¿›åˆ¶æ ·æœ¬è½¬æ¢ä¸ºè¿ç»­æ ·æœ¬ã€‚ï¼ˆ ^ï¼ˆnï¼‰ï¼‰è¾“å…¥ï¼šX[m] m=0,...,M âˆ’1ï¼›n=1,...,Nvariables - ç”Ÿæˆçš„ M
    ä½äºŒè¿›åˆ¶æ ·æœ¬ã€‚å¯¹äº n = 1,...,Nvariables åš | ï¼ˆnï¼‰ Mâˆ‘âˆ’1 ï¼ˆnï¼‰ | X^integer := 2m X^[M âˆ’1âˆ’m ]
    | m=0 | ï¼ˆnï¼‰ ï¼ˆnï¼‰ 1 ï¼ˆnï¼‰ ï¼ˆï¼ˆnï¼‰ï¼‰ | ^X real â† X min +-M----X^integer Xm(na)x âˆ’ X min
    | 2 âˆ’ 1 ç»“æŸ ---------------------------------------------------------------------
    ](img/file813.jpg)'
- en: 'AlgorithmsÂ [6](#x1-190006r6) andÂ [7](#x1-190009r7) describe the transformations
    of continuous variables into *M*-digit binary variables and then back into continuous
    variablesÂ Â [[173](Biblography.xhtml#XKS2020)]. It is important to note the role
    of the parametersÂ *ğœ€*[min] andÂ *ğœ€*[max]. They are non-negative and expand the
    interval on which the variables are defined. In the case where *ğœ€*[min] = *ğœ€*[max]
    = 0, this interval is determined by the minimum and maximum values of the variable
    as observed in the training dataset. By allowing *ğœ€*[min] and *ğœ€*[max] to take
    positive values, we expand the interval of possible values the variable can take.
    This allows the model to generate a wider range of possible scenarios: with some
    (small) probability the generated values can fall outside the interval given by
    the samples from the training dataset.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³•Â [6](#x1-190006r6) å’ŒÂ [7](#x1-190009r7) æè¿°äº†å°†è¿ç»­å˜é‡è½¬æ¢ä¸º *M* ä½äºŒè¿›åˆ¶å˜é‡ï¼Œç„¶åå†è½¬æ¢å›è¿ç»­å˜é‡çš„è¿‡ç¨‹[[173](Biblography.xhtml#XKS2020)]ã€‚éœ€è¦ç‰¹åˆ«æ³¨æ„å‚æ•°
    *ğœ–*[min] å’Œ *ğœ–*[max] çš„ä½œç”¨ã€‚å®ƒä»¬æ˜¯éè´Ÿçš„ï¼Œå¹¶ä¸”æ‰©å±•äº†å˜é‡å®šä¹‰çš„åŒºé—´ã€‚åœ¨ *ğœ–*[min] = *ğœ–*[max] = 0 çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªåŒºé—´ç”±è®­ç»ƒæ•°æ®é›†ä¸­çš„å˜é‡æœ€å°å€¼å’Œæœ€å¤§å€¼å†³å®šã€‚é€šè¿‡å…è®¸
    *ğœ–*[min] å’Œ *ğœ–*[max] å–æ­£å€¼ï¼Œæˆ‘ä»¬å¯ä»¥æ‰©å±•å˜é‡å¯ä»¥å–å€¼çš„åŒºé—´ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´å¹¿æ³›çš„å¯èƒ½åœºæ™¯ï¼šåœ¨æŸäº›ï¼ˆå°ï¼‰æ¦‚ç‡ä¸‹ï¼Œç”Ÿæˆçš„å€¼å¯èƒ½ä¼šè½åœ¨è®­ç»ƒæ•°æ®é›†æ ·æœ¬æ‰€ç»™å‡ºçš„åŒºé—´ä¹‹å¤–ã€‚
- en: The precision of the binary representation is feature specific. More important
    features can have more granular representation. The right choice of precision
    is important for NISQ devices that operate with a limited number of quantum registers.
    For example, the QCBM ansatz shown in FigureÂ [9.1](#9.1) can be used to encode
    two continuous variables with 6-digit binary precision each. Alternatively, the
    more important variable can be encoded with, e.g., 8-digit binary precision and
    the less important one with only 4-digit binary precision.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: äºŒè¿›åˆ¶è¡¨ç¤ºçš„ç²¾åº¦æ˜¯ç‰¹å¾ç‰¹å®šçš„ã€‚æ›´é‡è¦çš„ç‰¹å¾å¯ä»¥æ‹¥æœ‰æ›´ç²¾ç»†çš„è¡¨ç¤ºã€‚ç²¾åº¦çš„æ­£ç¡®é€‰æ‹©å¯¹äºå…·æœ‰æœ‰é™é‡å­å¯„å­˜å™¨çš„ NISQ è®¾å¤‡éå¸¸é‡è¦ã€‚ä¾‹å¦‚ï¼Œå›¾Â [9.1](#9.1)
    ä¸­æ‰€ç¤ºçš„ QCBM å‡è®¾å¯ä»¥ç”¨æ¥ç¼–ç ä¸¤ä¸ªè¿ç»­å˜é‡ï¼Œæ¯ä¸ªå˜é‡ä½¿ç”¨ 6 ä½äºŒè¿›åˆ¶ç²¾åº¦ã€‚æˆ–è€…ï¼Œæ›´é‡è¦çš„å˜é‡å¯ä»¥ä½¿ç”¨ä¾‹å¦‚ 8 ä½äºŒè¿›åˆ¶ç²¾åº¦è¿›è¡Œç¼–ç ï¼Œè€Œè¾ƒä¸é‡è¦çš„å˜é‡åˆ™åªä½¿ç”¨
    4 ä½äºŒè¿›åˆ¶ç²¾åº¦ã€‚
- en: 'FigureÂ [9.2](#9.4) illustrates how the readout from 12 quantum registers can
    be translated into a sample consisting of two continuous variables: the value
    of the first one is encoded as a 7-digit binary number and the value of the second
    one is encoded as a 5-digit binary number. In this example, we assume that both
    variables take values in the interval [âˆ’1*,*1].'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â [9.2](#9.4) æ¼”ç¤ºäº†å¦‚ä½•å°†æ¥è‡ª 12 ä¸ªé‡å­å¯„å­˜å™¨çš„è¯»å‡ºç»“æœè½¬åŒ–ä¸ºç”±ä¸¤ä¸ªè¿ç»­å˜é‡ç»„æˆçš„æ ·æœ¬ï¼šç¬¬ä¸€ä¸ªå˜é‡çš„å€¼è¢«ç¼–ç ä¸ºä¸€ä¸ª 7 ä½äºŒè¿›åˆ¶æ•°ï¼Œç¬¬äºŒä¸ªå˜é‡çš„å€¼è¢«ç¼–ç ä¸ºä¸€ä¸ª
    5 ä½äºŒè¿›åˆ¶æ•°ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å‡è®¾ä¸¤ä¸ªå˜é‡éƒ½å–å€¼äºåŒºé—´ [âˆ’1*,*1]ã€‚
- en: '![FigureÂ 9.4: Sample QCBM readout and data transformation for two continuous
    variables taking values in the interval [âˆ’1,1] and where we set ğœ€min = ğœ€max =
    0\. ](img/file814.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾Â 9.4ï¼šç¤ºä¾‹ QCBM è¯»å‡ºå’Œæ•°æ®å˜æ¢ï¼Œé’ˆå¯¹å–å€¼åœ¨åŒºé—´ [âˆ’1,1] çš„ä¸¤ä¸ªè¿ç»­å˜é‡ï¼Œå¹¶ä¸”æˆ‘ä»¬è®¾ç½®äº† ğœ–[min] = ğœ–[max] = 0\.](img/file814.jpg)'
- en: 'FigureÂ 9.4: Sample QCBM readout and data transformation for two continuous
    variables taking values in the interval [âˆ’1,1] and where we set ğœ€[min] = ğœ€[max]
    = 0\.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 9.4ï¼šç¤ºä¾‹ QCBM è¯»å‡ºå’Œæ•°æ®å˜æ¢ï¼Œé’ˆå¯¹å–å€¼åœ¨åŒºé—´ [âˆ’1,1] çš„ä¸¤ä¸ªè¿ç»­å˜é‡ï¼Œå¹¶ä¸”æˆ‘ä»¬è®¾ç½®äº† ğœ–[min] = ğœ–[max] = 0\ã€‚
- en: 9.2.2 Choosing the right cost function
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.2 é€‰æ‹©åˆé€‚çš„æˆæœ¬å‡½æ•°
- en: 'The differentiable learning of QCBM follows the same principles as that of
    training the quantum neural networks outlined in ChapterÂ [8](Chapter_8.xhtml#x1-1620008):
    minimisation of the cost function with the gradient descent method. The main difference
    is the form of the cost function. In the case of a QNN-based classifier, the cost
    function represents the classification error while the cost function for QCBM
    represents the distance between two probability distributions: the distribution
    of samples in the training dataset and the distribution of samples in the generated
    dataset.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: QCBM çš„å¯å¾®å­¦ä¹ éµå¾ªä¸è®­ç»ƒé‡å­ç¥ç»ç½‘ç»œç›¸åŒçš„åŸåˆ™ï¼Œè¯¦è§ç¬¬Â [8](Chapter_8.xhtml#x1-1620008) ç« ï¼šé€šè¿‡æ¢¯åº¦ä¸‹é™æ³•æœ€å°åŒ–æˆæœ¬å‡½æ•°ã€‚ä¸»è¦çš„åŒºåˆ«åœ¨äºæˆæœ¬å‡½æ•°çš„å½¢å¼ã€‚åœ¨åŸºäº
    QNN çš„åˆ†ç±»å™¨ä¸­ï¼Œæˆæœ¬å‡½æ•°è¡¨ç¤ºåˆ†ç±»é”™è¯¯ï¼Œè€Œ QCBM çš„æˆæœ¬å‡½æ•°è¡¨ç¤ºä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼šè®­ç»ƒæ•°æ®é›†ä¸­çš„æ ·æœ¬åˆ†å¸ƒå’Œç”Ÿæˆæ•°æ®é›†ä¸­çš„æ ·æœ¬åˆ†å¸ƒã€‚
- en: Let ğœƒ denote the set of adjustable QCBM parameters, *p*[ğœƒ](â‹…) the QCBM distribution,
    andÂ *Ï€*(â‹…) the data distribution. Then we can define the cost function *L*(ğœƒ)
    as
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¤ ğœƒ è¡¨ç¤ºå¯è°ƒèŠ‚çš„ QCBM å‚æ•°é›†åˆï¼Œ*p*[ğœƒ](â‹…) ä¸º QCBM åˆ†å¸ƒï¼Œ*Ï€*(â‹…) ä¸ºæ•°æ®åˆ†å¸ƒã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å°†æˆæœ¬å‡½æ•° *L*(ğœƒ) å®šä¹‰ä¸º
- en: '| ![ âˆ‘ L (ğœƒ ) := &#124;pğœƒ(x )âˆ’ Ï€(x)&#124;, x ](img/file815.jpg) |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘ L (ğœƒ ) := &#124;pğœƒ(x )âˆ’ Ï€(x)&#124;, x ](img/file815.jpg) |  |'
- en: 'where the sum goes over all samples x in the dataset. This cost function is
    a strong metric but may not be the easiest to deal withÂ Â [[73](Biblography.xhtml#XCoyle2019)].
    An efficient alternative choice of the cost function is the *maximum mean* *discrepancy*Â Â [[189](Biblography.xhtml#XLiuWang2018)]:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ±‚å’Œéå†æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ ·æœ¬xã€‚è¿™ä¸ªæˆæœ¬å‡½æ•°æ˜¯ä¸€ä¸ªå¼ºæœ‰åŠ›çš„åº¦é‡ï¼Œä½†å¯èƒ½ä¸æ˜¯æœ€å®¹æ˜“å¤„ç†çš„[[73](Biblography.xhtml#XCoyle2019)]ã€‚ä¸€ä¸ªæœ‰æ•ˆçš„æ›¿ä»£é€‰æ‹©æ˜¯*æœ€å¤§å‡å€¼*
    *åå·®* [[189](Biblography.xhtml#XLiuWang2018)]ï¼š
- en: '| ![L(ğœƒ) := ğ”¼ [K (x,y)]âˆ’ 2 ğ”¼ [K (x,y)]+ ğ”¼ [K (x,y)], xâˆ¼pğœƒ,yâˆ¼pğœƒ xâˆ¼pğœƒ,yâˆ¼Ï€ xâˆ¼Ï€,yâˆ¼Ï€
    ](img/file816.jpg) |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ![L(ğœƒ) := ğ”¼ [K (x,y)]âˆ’ 2 ğ”¼ [K (x,y)]+ ğ”¼ [K (x,y)], xâˆ¼pğœƒ,yâˆ¼pğœƒ xâˆ¼pğœƒ,yâˆ¼Ï€ xâˆ¼Ï€,yâˆ¼Ï€
    ](img/file816.jpg) |  |'
- en: 'where *K*(â‹…*,*â‹…) is a *kernel function*, i.e., a measure of similarity between
    points in the sample space. A popular choice of kernel function is the Gaussian
    mixture:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­*K*(â‹…*,*â‹…)æ˜¯*æ ¸å‡½æ•°*ï¼Œå³æ ·æœ¬ç©ºé—´ä¸­ç‚¹ä¹‹é—´ç›¸ä¼¼åº¦çš„åº¦é‡ã€‚ä¸€ä¸ªæµè¡Œçš„æ ¸å‡½æ•°é€‰æ‹©æ˜¯é«˜æ–¯æ··åˆæ¨¡å‹ï¼š
- en: '| ![ âˆ‘ c ( 2) K (x,y) = 1- exp âˆ’ âˆ¥xâˆ’-y2âˆ¥-- , c i=1 2Ïƒi ](img/file817.jpg) |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘ c ( 2) K (x,y) = 1- exp âˆ’ âˆ¥xâˆ’-y2âˆ¥-- , c i=1 2Ïƒi ](img/file817.jpg) |  |'
- en: for some *c* âˆˆâ„• and where (*Ïƒ*[i])[i=1,â€¦,c] are the bandwidth parameters of
    each Gaussian kernel and âˆ¥â‹…âˆ¥ is theÂ *L*[2] norm.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸäº›*c* âˆˆâ„•ï¼Œå¹¶ä¸”(*Ïƒ*[i])[i=1,â€¦,c]æ˜¯æ¯ä¸ªé«˜æ–¯æ ¸çš„å¸¦å®½å‚æ•°ï¼Œâˆ¥â‹…âˆ¥æ˜¯*L*[2]èŒƒæ•°ã€‚
- en: 'We can also explore the possibility of using *quantum kernels*. Quantum kernels
    can provide an advantage over classical methods for kernels that are difficult
    to compute on a classical device. For example, we can consider a non-variational
    quantum kernel methodÂ Â [[232](Biblography.xhtml#XPeters2021)], which uses a quantum
    circuitÂ U(x) to map real data into a quantum stateÂ |*Ï•*âŸ© via a *feature map*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥æ¢ç´¢ä½¿ç”¨*é‡å­æ ¸*çš„å¯èƒ½æ€§ã€‚é‡å­æ ¸åœ¨è®¡ç®—ä¸Šéš¾ä»¥å¤„ç†çš„æ ¸å‡½æ•°ä¸Šï¼Œç›¸æ¯”ç»å…¸æ–¹æ³•å¯ä»¥æä¾›ä¸€å®šçš„ä¼˜åŠ¿ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘ä¸€ç§éå˜åˆ†é‡å­æ ¸æ–¹æ³•[[232](Biblography.xhtml#XPeters2021)]ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é‡å­ç”µè·¯U(x)é€šè¿‡*ç‰¹å¾æ˜ å°„*å°†çœŸå®æ•°æ®æ˜ å°„åˆ°é‡å­æ€|*Ï•*âŸ©ï¼š
- en: '| ![&#124;Ï•(x)âŸ© = U(x) &#124;0âŸ©âŠ—n . ](img/file818.jpg) |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| ![&#124;Ï•(x)âŸ© = U(x) &#124;0âŸ©âŠ—n . ](img/file818.jpg) |  |'
- en: The kernel function is then defined as the squared inner product
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸å‡½æ•°è¢«å®šä¹‰ä¸ºå¹³æ–¹å†…ç§¯
- en: '| ![K (x,y ) = &#124;âŸ¨Ï•(x)&#124;Ï• (y )âŸ© &#124;2\. ](img/file819.jpg) |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| ![K (x,y ) = &#124;âŸ¨Ï•(x)&#124;Ï• (y )âŸ© &#124;2\. ](img/file819.jpg) |  |'
- en: This quantum kernel is evaluated on a quantum computer and is hard to compute
    on a classical oneÂ Â [[129](Biblography.xhtml#XHavlicek2019)]. We investigate the
    question of expressive power of various models in ChapterÂ [12](Chapter_12.xhtml#x1-22500012)
    and provide a detailed analysis of the quantum kernel approach in ChapterÂ [13](Chapter_13.xhtml#x1-23600013).
    Taking into account the mappingÂ ([9.2.2](#x1-1910002)) and denoting ![|0âŸ©](img/file820.jpg)
    = ![|0âŸ©](img/file821.jpg)^(âŠ—n), the kernel becomes
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé‡å­æ ¸åœ¨é‡å­è®¡ç®—æœºä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè€Œåœ¨ç»å…¸è®¡ç®—æœºä¸Šåˆ™éš¾ä»¥è®¡ç®—[[129](Biblography.xhtml#XHavlicek2019)]ã€‚æˆ‘ä»¬å°†åœ¨ç¬¬[12](Chapter_12.xhtml#x1-22500012)ç« æ¢è®¨å„ç§æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›é—®é¢˜ï¼Œå¹¶åœ¨ç¬¬[13](Chapter_13.xhtml#x1-23600013)ç« å¯¹é‡å­æ ¸æ–¹æ³•è¿›è¡Œè¯¦ç»†åˆ†æã€‚è€ƒè™‘åˆ°æ˜ å°„ï¼ˆ[9.2.2](#x1-1910002)ï¼‰å¹¶ä¸”è¡¨ç¤º![|0âŸ©](img/file820.jpg)
    = ![|0âŸ©](img/file821.jpg)^(âŠ—n)ï¼Œæ ¸å‡½æ•°å˜ä¸º
- en: '| ![ â€  2 K (x,y) = &#124;âŸ¨0&#124;U (x )U(y) &#124;0 âŸ©&#124;, ](img/file822.jpg)
    |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ![ â€  2 K (x,y) = &#124;âŸ¨0&#124;U (x )U(y) &#124;0 âŸ©&#124;, ](img/file822.jpg)
    |  |'
- en: which is the probability of measuring the all-zero outcome. It can be calculated
    by measuring, in the computational basis, the state which results from running
    the circuit given byÂ U(y), followed by that of U^â€ (x).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æµ‹é‡å…¨é›¶ç»“æœçš„æ¦‚ç‡ã€‚å®ƒå¯ä»¥é€šè¿‡åœ¨è®¡ç®—åŸºåº•ä¸Šæµ‹é‡è¿è¡Œç”µè·¯U(y)æ‰€å¾—åˆ°çš„çŠ¶æ€ï¼Œå†æµ‹é‡U^â€ (x)çš„ç»“æœæ¥è®¡ç®—ã€‚
- en: 9.3 Non-Differentiable Learning of QCBM
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3 éå¯å¾®åˆ†çš„QCBMå­¦ä¹ 
- en: The hardware-efficient ansatz we proposed for the QCBM architecture, while simple
    and intuitive, may be vulnerable to barren plateaus, or regions of exponentially
    vanishing gradient magnitudes that make training untenableÂ Â [[54](Biblography.xhtml#XCerezo2021),Â [139](Biblography.xhtml#XHolmes2021),Â [299](Biblography.xhtml#XWang2020)].
    This provides a strong motivation for exploring a non-differentiable learning
    alternative such as Genetic Algorithm.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ºQCBMæ¶æ„æå‡ºçš„ç¡¬ä»¶é«˜æ•ˆå‡è®¾è™½ç„¶ç®€å•ç›´è§‚ï¼Œä½†å¯èƒ½å®¹æ˜“å—åˆ°è´«ç˜ å¹³å°ï¼ˆå³æ¢¯åº¦æ¶ˆå¤±çš„æŒ‡æ•°åŒºåŸŸï¼‰çš„å½±å“ï¼Œè¿™ä½¿å¾—è®­ç»ƒå˜å¾—ä¸å¯è¡Œ[[54](Biblography.xhtml#XCerezo2021),
    [139](Biblography.xhtml#XHolmes2021), [299](Biblography.xhtml#XWang2020)]ã€‚è¿™ä¸ºæ¢ç´¢éå¯å¾®åˆ†å­¦ä¹ çš„æ›¿ä»£æ–¹æ³•æä¾›äº†å¼ºæœ‰åŠ›çš„åŠ¨æœºï¼Œæ¯”å¦‚é—ä¼ ç®—æ³•ã€‚
- en: 9.3.1 The principles of Genetic Algorithm
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.1 é—ä¼ ç®—æ³•åŸç†
- en: GA is a powerful evolutionary search heuristicÂ Â [[214](Biblography.xhtml#XMitchell1998)]
    that was introduced in ChapterÂ [3](Chapter_3.xhtml#x1-630003). It performs a multi-directional
    search by maintaining a population of proposed solutions (chromosomes) for a given
    problem. Each solution is represented in a fixed alphabet with an established
    meaning (genes). The population undergoes a simulated evolution with relatively
    good solutions producing offspring, which subsequently replace the worse ones,
    and the quality of a solution is estimated with some objective function (environment).
    GAs have found applications is such diverse fields as quantitative finance (for
    portfolio optimisation problemsÂ Â [[172](Biblography.xhtml#XKondratyev2017)]) and
    experiments with adiabatic quantum computing (as a classical benchmarkÂ Â [[296](Biblography.xhtml#XVenturelli2019)]).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: é—ä¼ ç®—æ³•ï¼ˆGAï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„è¿›åŒ–æœç´¢å¯å‘å¼æ–¹æ³•[[214](Biblography.xhtml#XMitchell1998)]ï¼Œå®ƒåœ¨ç¬¬[3](Chapter_3.xhtml#x1-630003)ç« ä¸­è¿›è¡Œäº†ä»‹ç»ã€‚GAé€šè¿‡ç»´æŠ¤ä¸€ä¸ªç»™å®šé—®é¢˜çš„å€™é€‰è§£ï¼ˆæŸ“è‰²ä½“ï¼‰ç§ç¾¤ï¼Œè¿›è¡Œå¤šæ–¹å‘æœç´¢ã€‚æ¯ä¸ªè§£åœ¨ä¸€ä¸ªå›ºå®šå­—æ¯è¡¨ä¸­è¡¨ç¤ºï¼Œå¹¶ä¸”å…·æœ‰æ—¢å®šçš„å«ä¹‰ï¼ˆåŸºå› ï¼‰ã€‚ç§ç¾¤ç»å†æ¨¡æ‹Ÿè¿›åŒ–ï¼Œè¾ƒå¥½çš„è§£å†³æ–¹æ¡ˆäº§ç”Ÿåä»£ï¼Œç»§è€Œæ›¿ä»£è¾ƒå·®çš„è§£ï¼Œå¹¶ä¸”è§£å†³æ–¹æ¡ˆçš„è´¨é‡é€šè¿‡æŸäº›ç›®æ ‡å‡½æ•°ï¼ˆç¯å¢ƒï¼‰è¿›è¡Œä¼°è®¡ã€‚GAå·²è¢«åº”ç”¨äºé‡‘èå®šé‡åˆ†æï¼ˆå¦‚æŠ•èµ„ç»„åˆä¼˜åŒ–é—®é¢˜[[172](Biblography.xhtml#XKondratyev2017)]ï¼‰ä»¥åŠç»çƒ­é‡å­è®¡ç®—å®éªŒï¼ˆä½œä¸ºç»å…¸åŸºå‡†[[296](Biblography.xhtml#XVenturelli2019)]ï¼‰ã€‚
- en: 'The simulation cycle is performed in three basic steps. During the selection
    step, a new population is formed by stochastic sampling (with replacement). Then,
    some of the members of the newly selected populations recombine. Finally, all
    new individuals are re-evaluated. The mating process (recombination) is based
    on the application of two operators: mutation and crossover. Mutation introduces
    random variability into the population, and crossover exchanges random pieces
    of two chromosomes in the hope of propagating partial solutions.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡æ‹Ÿè¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªåŸºæœ¬æ­¥éª¤ã€‚åœ¨é€‰æ‹©æ­¥éª¤ä¸­ï¼Œé€šè¿‡éšæœºæŠ½æ ·ï¼ˆæœ‰æ”¾å›ï¼‰å½¢æˆæ–°çš„ç§ç¾¤ã€‚ç„¶åï¼Œæ–°é€‰ä¸­çš„ç§ç¾¤æˆå‘˜ä¼šè¿›è¡Œé‡ç»„ã€‚æœ€åï¼Œæ‰€æœ‰æ–°çš„ä¸ªä½“ä¼šé‡æ–°è¯„ä¼°ã€‚é…å¯¹è¿‡ç¨‹ï¼ˆé‡ç»„ï¼‰åŸºäºä¸¤ç§æ“ä½œç¬¦ï¼šçªå˜å’Œäº¤å‰ã€‚çªå˜å‘ç§ç¾¤å¼•å…¥éšæœºå˜å¼‚ï¼Œè€Œäº¤å‰åˆ™äº¤æ¢ä¸¤ä¸ªæŸ“è‰²ä½“çš„éšæœºç‰‡æ®µï¼Œæ—¨åœ¨ä¼ æ’­éƒ¨åˆ†è§£å†³æ–¹æ¡ˆã€‚
- en: The training of the QCBM specified in FigureÂ [9.1](#9.1) consists of finding
    an optimal configuration of the rotation angles (*ğœƒ*[i]^j)[i=1,â€¦,12; j=1,â€¦,7]
    that would minimise a chosen cost function given a particular choice of the fixed
    2-qubit gates. Since we only deal with 84 adjustable parameters (rather than tens
    of thousands), we do not need to implement the crossover mechanism and can rely
    on parameter mutations to achieve GA convergence to the minimum of the cost function.
    This significantly simplifies the algorithm.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[9.1](#9.1)æ‰€ç¤ºçš„QCBMè®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬æ‰¾åˆ°æ—‹è½¬è§’åº¦çš„æœ€ä½³é…ç½®ï¼ˆ*ğœƒ*[i]^j)[i=1,â€¦,12; j=1,â€¦,7]ï¼Œä»¥æœ€å°åŒ–ç»™å®šç‰¹å®š2é‡å­æ¯”ç‰¹é—¨çš„æˆæœ¬å‡½æ•°ã€‚ç”±äºæˆ‘ä»¬ä»…å¤„ç†84ä¸ªå¯è°ƒå‚æ•°ï¼ˆè€Œä¸æ˜¯æ•°ä¸‡ä¸ªï¼‰ï¼Œæˆ‘ä»¬ä¸éœ€è¦å®ç°äº¤å‰æœºåˆ¶ï¼Œå¯ä»¥ä¾èµ–å‚æ•°çªå˜æ¥å®ç°é—ä¼ ç®—æ³•æ”¶æ•›åˆ°æˆæœ¬å‡½æ•°çš„æœ€å°å€¼ã€‚è¿™å¤§å¤§ç®€åŒ–äº†ç®—æ³•ã€‚
- en: 9.3.2 Training QCBM with a Genetic Algorithm
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.2 ä½¿ç”¨é—ä¼ ç®—æ³•è®­ç»ƒQCBM
- en: AlgorithmÂ [8](#x1-194004r8) outlines the proposed approach. However, before
    we provide a formal description of the algorithm, we have to specify the main
    individual components.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³•[8](#x1-194004r8)æ¦‚è¿°äº†æå‡ºçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨æˆ‘ä»¬æä¾›ç®—æ³•çš„æ­£å¼æè¿°ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ˜ç¡®ç®—æ³•çš„ä¸»è¦ç»„æˆéƒ¨åˆ†ã€‚
- en: '**Solution.** The solution is a 12 Ã— 7 matrix of rotation angles:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§£å†³æ–¹æ¡ˆã€‚** è§£å†³æ–¹æ¡ˆæ˜¯ä¸€ä¸ª12 Ã— 7çš„æ—‹è½¬è§’åº¦çŸ©é˜µï¼š'
- en: '| ![ âŒŠ 1 7âŒ‹ &#124; ğœƒ1 ... ğœƒ1&#124; ğœƒ = &#124; ... ... ...&#124; . âŒˆ âŒ‰ ğœƒ112
    ... ğœƒ712 ](img/file823.jpg) |  |'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![ âŒŠ 1 7âŒ‹ &#124; ğœƒ1 ... ğœƒ1&#124; ğœƒ = &#124; ... ... ...&#124; . âŒˆ âŒ‰ ğœƒ112
    ... ğœƒ712 ](img/file823.jpg) |  |'
- en: In GA language, the matrix ğœƒ plays the role of a chromosome and its components
    *ğœƒ*[i]^j play the roles of individual genes.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨é—ä¼ ç®—æ³•è¯­è¨€ä¸­ï¼ŒçŸ©é˜µğœƒå……å½“æŸ“è‰²ä½“çš„è§’è‰²ï¼Œè€Œå…¶ç»„æˆéƒ¨åˆ†*ğœƒ*[i]^jå……å½“å•ä¸ªåŸºå› çš„è§’è‰²ã€‚
- en: '**Mutation.** The genes can mutate from generation to generation. The mutation
    rate can be either constant or time dependent. For example, the mutation rate
    can start at some large value and then decrease exponentially such that it halves
    after eachÂ *Îº* generations. In AlgorithmÂ [8](#x1-194004r8), we adopt the following
    mutation dynamics:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**çªå˜ã€‚** åŸºå› å¯ä»¥åœ¨ä»£é™…é—´å‘ç”Ÿçªå˜ã€‚çªå˜ç‡å¯ä»¥æ˜¯å¸¸æ•°å€¼ï¼Œä¹Ÿå¯ä»¥éšæ—¶é—´å˜åŒ–ã€‚ä¾‹å¦‚ï¼Œçªå˜ç‡å¯ä»¥ä»ä¸€ä¸ªè¾ƒå¤§çš„å€¼å¼€å§‹ï¼Œç„¶åä»¥æŒ‡æ•°æ–¹å¼å‡å°ï¼Œä½¿å¾—æ¯ç»è¿‡*Îº*ä»£ï¼Œçªå˜ç‡å‡åŠã€‚åœ¨ç®—æ³•[8](#x1-194004r8)ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨ä»¥ä¸‹çªå˜åŠ¨æ€ï¼š'
- en: A rotation angle (gene) can mutate to any of the allowed discrete values with
    equal probability.
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ—‹è½¬è§’åº¦ï¼ˆåŸºå› ï¼‰å¯ä»¥ä»¥ç›¸ç­‰çš„æ¦‚ç‡çªå˜ä¸ºä»»ä½•å…è®¸çš„ç¦»æ•£å€¼ã€‚
- en: Mutation is controlled by a single global parameter *Î±* âˆˆ (0*,*1], which can
    be either constant or exponentially decreasing at some fixed rate *Î²* â‰¥ 0.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å˜å¼‚æ˜¯ç”±ä¸€ä¸ªå•ä¸€çš„å…¨å±€å‚æ•° *Î±* âˆˆ (0*,*1] æ§åˆ¶çš„ï¼Œè¯¥å‚æ•°å¯ä»¥æ˜¯å¸¸æ•°ï¼Œæˆ–è€…ä»¥æŸä¸ªå›ºå®šçš„é€Ÿç‡ *Î²* â‰¥ 0 é€æ¸å‡å°ã€‚
- en: Mutations happen independently for each column in ğœƒ.
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸€åˆ—çš„å˜å¼‚æ˜¯ç‹¬ç«‹å‘ç”Ÿçš„ã€‚
- en: For each column in ğœƒ, at each generation, a single rotation angle mutation happens
    with probability *Î±*. All rotation angles are equally likely to mutate. After
    that, one more mutation can happen with probability *Î±âˆ•*2\. Again, all rotation
    angles are equally likely to mutate. This ensures that we can have scenarios where
    two rotation angles within the same column can mutate simultaneously.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº ğœƒ ä¸­çš„æ¯ä¸€åˆ—ï¼Œåœ¨æ¯ä¸€ä»£ä¸­ï¼Œä»¥ *Î±* çš„æ¦‚ç‡å‘ç”Ÿä¸€æ¬¡å•ä¸€çš„æ—‹è½¬è§’åº¦å˜å¼‚ã€‚æ‰€æœ‰æ—‹è½¬è§’åº¦å‘ç”Ÿå˜å¼‚çš„æ¦‚ç‡ç›¸åŒã€‚ä¹‹åï¼Œä»¥ *Î±âˆ•*2 çš„æ¦‚ç‡å‘ç”Ÿå¦ä¸€æ¬¡å˜å¼‚ã€‚å†æ¬¡åœ°ï¼Œæ‰€æœ‰æ—‹è½¬è§’åº¦å‘ç”Ÿå˜å¼‚çš„æ¦‚ç‡ç›¸åŒã€‚è¿™ç¡®ä¿äº†æˆ‘ä»¬å¯ä»¥æœ‰è¿™æ ·çš„æƒ…å†µï¼Œå³åŒä¸€åˆ—ä¸­çš„ä¸¤ä¸ªæ—‹è½¬è§’åº¦å¯ä»¥åŒæ—¶å‘ç”Ÿå˜å¼‚ã€‚
- en: '**Search Space.** The rotation angles *ğœƒ*[i]^j are defined in [âˆ’*Ï€,Ï€*], which
    we split intoÂ 2^m equal subintervals, so that the possible values for *ğœƒ*[i]^j
    are (âˆ’*Ï€* + *nÏ€âˆ•*2^(mâˆ’1))[n=0,â€¦,2^mâˆ’1]. A rotation angle can mutate into any of
    these values. The search space can quickly become enormous even for the relatively
    small values of *m*. For example, for *m* = 7 we have 128 possible values for
    each rotation angle making the total number of possible configurations âˆ¼ 10^(177).
    The GA can only explore a tiny fraction of the search space. But due to the GAâ€™s
    ability to propagate best solutions and to avoid being trapped in local minima,
    the algorithm can achieve reasonably fast convergence to the solution in the vicinity
    of the global minimum. For a detailed analysis of the rate of convergence of genetic
    algorithms, we refer the interested reader toÂ Â [[130](Biblography.xhtml#XHe1999),Â [264](Biblography.xhtml#XSharapov2006)].'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœç´¢ç©ºé—´ã€‚** æ—‹è½¬è§’åº¦ *ğœƒ*[i]^j å®šä¹‰åœ¨ [âˆ’*Ï€,Ï€*] èŒƒå›´å†…ï¼Œæˆ‘ä»¬å°†å…¶åˆ†å‰²æˆ 2^m ä¸ªç›¸ç­‰çš„å­åŒºé—´ï¼Œä½¿å¾— *ğœƒ*[i]^j çš„å¯èƒ½å–å€¼ä¸º
    (âˆ’*Ï€* + *nÏ€âˆ•*2^(mâˆ’1))[n=0,â€¦,2^mâˆ’1]ã€‚ä¸€ä¸ªæ—‹è½¬è§’åº¦å¯ä»¥å˜å¼‚ä¸ºè¿™äº›å€¼ä¸­çš„ä»»ä½•ä¸€ä¸ªã€‚å³ä½¿å¯¹äºç›¸å¯¹è¾ƒå°çš„ *m* å€¼ï¼Œæœç´¢ç©ºé—´ä¹Ÿå¯ä»¥è¿…é€Ÿå˜å¾—åºå¤§ã€‚ä¾‹å¦‚ï¼Œå¯¹äº
    *m* = 7ï¼Œæ¯ä¸ªæ—‹è½¬è§’åº¦æœ‰ 128 ä¸ªå¯èƒ½çš„å–å€¼ï¼Œä½¿å¾—å¯èƒ½çš„é…ç½®æ€»æ•°å¤§çº¦ä¸º 10^(177)ã€‚é—ä¼ ç®—æ³•åªèƒ½æ¢ç´¢æœç´¢ç©ºé—´çš„æå°éƒ¨åˆ†ã€‚ä½†ç”±äºé—ä¼ ç®—æ³•èƒ½å¤Ÿä¼ æ’­æœ€ä½³è§£å¹¶é¿å…é™·å…¥å±€éƒ¨æœ€å°å€¼ï¼Œç®—æ³•èƒ½å¤Ÿåœ¨æ¥è¿‘å…¨å±€æœ€å°å€¼çš„åŒºåŸŸå†…è¿…é€Ÿæ”¶æ•›ã€‚å…³äºé—ä¼ ç®—æ³•æ”¶æ•›é€Ÿç‡çš„è¯¦ç»†åˆ†æï¼Œæ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥å‚è€ƒ
    [[130](Biblography.xhtml#XHe1999), [264](Biblography.xhtml#XSharapov2006)]ã€‚'
- en: '**Cost Function.** A cost function is a measure of how far the distribution
    of generated samples is from the distribution of original samples provided by
    the training dataset. Let u := (*u*[1]*,â€¦,u*[K]) be a sample from the training
    dataset and v(ğœƒ) := (*v*[1](ğœƒ)*,â€¦,v*[K](ğœƒ)) a sample from the QCBM generated dataset
    that corresponds to a particular configuration of rotation anglesÂ ğœƒ. Let us order
    these samples from the smallest to the largest with any suitable sort(â‹…) function:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æˆæœ¬å‡½æ•°ã€‚** æˆæœ¬å‡½æ•°æ˜¯è¡¡é‡ç”Ÿæˆæ ·æœ¬çš„åˆ†å¸ƒä¸è®­ç»ƒæ•°æ®é›†ä¸­åŸå§‹æ ·æœ¬åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„åº¦é‡ã€‚è®¾ u := (*u*[1]*,â€¦,u*[K]) ä¸ºæ¥è‡ªè®­ç»ƒæ•°æ®é›†çš„æ ·æœ¬ï¼Œv(ğœƒ)
    := (*v*[1](ğœƒ)*,â€¦,v*[K](ğœƒ)) ä¸ºæ¥è‡ª QCBM ç”Ÿæˆæ•°æ®é›†çš„æ ·æœ¬ï¼Œå¯¹åº”äºæŸä¸€ç‰¹å®šçš„æ—‹è½¬è§’åº¦é…ç½® ğœƒã€‚æˆ‘ä»¬å°†è¿™äº›æ ·æœ¬æŒ‰ä»å°åˆ°å¤§çš„é¡ºåºæ’åˆ—ï¼Œä½¿ç”¨ä»»ä½•åˆé€‚çš„
    sort(â‹…) å‡½æ•°ï¼š'
- en: '| ![-- -- u = sort(u), v(ğœƒ) = sort(v(ğœƒ )). ](img/file824.jpg) |  |'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![-- -- u = sort(u), v(ğœƒ) = sort(v(ğœƒ )). ](img/file824.jpg) |  |'
- en: The cost functionÂ *L*(â‹…) can then be defined as
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆæœ¬å‡½æ•° *L*(â‹…) å¯ä»¥å®šä¹‰ä¸º
- en: '| ![ Kâˆ‘ -- -- 2 L (ğœƒ ) := (uk âˆ’ vk(ğœƒ )) . k=1 ](img/file825.jpg) |  |'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![ Kâˆ‘ -- -- 2 L (ğœƒ ) := (uk âˆ’ vk(ğœƒ )) . k=1 ](img/file825.jpg) |  |'
- en: The sort(â‹…) function inÂ ([9.3.2](#x1-1940002)) can be, e.g., quicksortÂ Â [[137](Biblography.xhtml#XHoare1961)]
    or mergesortÂ Â [[166](Biblography.xhtml#XKnuth1998)], which belong to the class
    of divide-and-conquer algorithms. Alternatively, it can be, e.g., heapsortÂ Â [[303](Biblography.xhtml#XWilliams1964)]
    â€“ a comparison-based sorting algorithm.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ ([9.3.2](#x1-1940002)) ä¸­çš„ sort(â‹…) å‡½æ•°å¯ä»¥æ˜¯ï¼Œä¾‹å¦‚ï¼Œå¿«é€Ÿæ’åº [[137](Biblography.xhtml#XHoare1961)]
    æˆ–å½’å¹¶æ’åº [[166](Biblography.xhtml#XKnuth1998)]ï¼Œå®ƒä»¬å±äºåˆ†æ²»ç®—æ³•ç±»åˆ«ã€‚æˆ–è€…ï¼Œå®ƒå¯ä»¥æ˜¯ä¾‹å¦‚å †æ’åº [[303](Biblography.xhtml#XWilliams1964)]
    â€”â€” ä¸€ç§åŸºäºæ¯”è¾ƒçš„æ’åºç®—æ³•ã€‚
- en: '![--------------------------------------------------------------------- -Algorithm---8:-Genetic
    Algorithm------------------------------------ Result: Optimal con figuration of
    the set of QCBM parameters ğœƒâˆ— minimising the cost function. Input: â€¢ u âˆˆ â„K :
    vector of sample training dataset; â€¢ L: number of iterations (generations); â€¢
    M : number of best solutions in the given generation, chosen for further mutation;
    â€¢ N : number of solutions in each generation (N = DM , D âˆˆ â„•); â€¢ Î±, Î²: mutation
    parameters; â€¢ m: search space parameter. ( The poss)ible values of rotation angles
    are âˆ’ Ï€ + -Î½Ï€-- . 2mâˆ’ 1 Î½=0,...,2mâˆ’ 1 Initialise and evaluate the first generation
    of solutions: for n = 1,...,N do | Generate a configuration ğœƒ (0;n ) by randomly
    drawing each | rotation angle ğœƒj(0;n ) from the uniform distribution on the set
    | i | of possible values of rotation angles given by m. | | for k = 1,...,K do
    | | Run the quantum circuit with con figuration ğœƒ(0;n) and | | generate new sample
    v (ğœƒ(0;n)). | k | end | Evaluate the cost function L(ğœƒ(0;n)). end Order solutions
    from best (minimum of cost function) to worst (maximum of cost function ). ğœƒâˆ—
    â† con figuration corresponding to the minimum of cost function. ---------------------------------------------------------------------
    ](img/file826.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- -ç®—æ³•---8:-é—ä¼ ç®—æ³•------------------------------------
    ç»“æœï¼šæœ€ä¼˜é…ç½®çš„QCBMå‚æ•°é›†ğœƒâˆ—ï¼Œæœ€å°åŒ–ä»£ä»·å‡½æ•°ã€‚ è¾“å…¥ï¼š â€¢ u âˆˆ â„Kï¼šæ ·æœ¬è®­ç»ƒæ•°æ®é›†çš„å‘é‡ï¼› â€¢ Lï¼šè¿­ä»£æ¬¡æ•°ï¼ˆä»£æ•°ï¼‰ï¼› â€¢ Mï¼šå½“å‰ä»£ä¸­é€‰å‡ºçš„æœ€ä½³è§£çš„æ•°é‡ï¼Œç”¨äºè¿›ä¸€æ­¥å˜å¼‚ï¼›
    â€¢ Nï¼šæ¯ä»£ä¸­çš„è§£çš„æ•°é‡ï¼ˆN = DMï¼ŒD âˆˆ â„•ï¼‰ï¼› â€¢ Î±ï¼ŒÎ²ï¼šå˜å¼‚å‚æ•°ï¼› â€¢ mï¼šæœç´¢ç©ºé—´å‚æ•°ã€‚ï¼ˆæ—‹è½¬è§’åº¦çš„å¯èƒ½å€¼ä¸ºâˆ’ Ï€ + -Î½Ï€-- . 2mâˆ’
    1 Î½=0,...,2mâˆ’ 1 åˆå§‹åŒ–å¹¶è¯„ä¼°ç¬¬ä¸€ä»£è§£ï¼šå¯¹äºn = 1,...,Nï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œ | éšæœºæŠ½å–æ¯ä¸ªæ—‹è½¬è§’åº¦ğœƒj(0;n)åœ¨æ—‹è½¬è§’åº¦çš„å¯èƒ½å€¼é›†åˆä¸Šï¼Œ
    | å¹¶ç”Ÿæˆé…ç½®ğœƒ(0;n)ã€‚ | | å¯¹äºk = 1,...,Kï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œ | | è¿è¡Œé‡å­ç”µè·¯ï¼Œä½¿ç”¨é…ç½®ğœƒ(0;n)å¹¶ | | ç”Ÿæˆæ–°çš„æ ·æœ¬v(ğœƒ(0;n))ã€‚
    | k | ç»“æŸ | è¯„ä¼°ä»£ä»·å‡½æ•°L(ğœƒ(0;n))ã€‚ ç»“æŸ å°†è§£ä»æœ€ä¼˜ï¼ˆä»£ä»·å‡½æ•°æœ€å°ï¼‰åˆ°æœ€å·®ï¼ˆä»£ä»·å‡½æ•°æœ€å¤§ï¼‰è¿›è¡Œæ’åºã€‚ ğœƒâˆ— â† å¯¹åº”äºä»£ä»·å‡½æ•°æœ€å°å€¼çš„é…ç½®ã€‚
    --------------------------------------------------------------------- ](img/file826.jpg)'
- en: '![--------------------------------------------------------------------- ---------------------------------------------------------------------
    Iterations: for l = 1,...,L do | âˆ’Î² | Î± â† Î±e | | Select M best solutions from
    generation l âˆ’ 1 and generate new | | solutions (ğœƒ(l;n ))n=1,...,N by mutating
    the rotation angles using | the updated mutation rate Î±. Each of the M best solutions
    is | | used to produce D new solutions. | | for n = 1,...,N do | | for k = 1,...,K
    do | | | | | | Run the quantum circuit with ğœƒ(l;n ) and generate new | | | sample
    vk(ğœƒ (l;n)). | | end | | | | | Evaluate the cost function L(ğœƒ(l;n)). | end | |
    Order solutions from best (minimum of the cost function) to | | worst (maximum
    of the cost function). | | ğœƒâˆ—(l) â† configuration corresponding to the minimum
    of the cost | | function (l-th generation). | | if L(ğœƒâˆ—(l)) < L(ğœƒâˆ—) then | ğœƒ âˆ—
    â† ğœƒâˆ—(l) | end end ---------------------------------------------------------------------
    ](img/file827.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- ---------------------------------------------------------------------
    è¿­ä»£ï¼šå¯¹äºl = 1,...,Lï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œ | âˆ’Î² | Î± â† Î±e | | ä»ä¸Šä¸€ä»£l âˆ’ 1ä¸­é€‰æ‹©Mä¸ªæœ€ä½³è§£å¹¶ç”Ÿæˆæ–°çš„ | | è§£ï¼ˆğœƒ(l;n ))n=1,...,Nï¼Œæ–¹æ³•æ˜¯é€šè¿‡ä½¿ç”¨æ›´æ–°åçš„å˜å¼‚ç‡Î±æ¥å˜æ¢æ—‹è½¬è§’åº¦ã€‚æ¯ä¸€ä¸ªMä¸ªæœ€ä½³è§£éƒ½
    | | ç”¨äºç”ŸæˆDä¸ªæ–°çš„è§£ã€‚ | | å¯¹äºn = 1,...,Nï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œ | | å¯¹äºk = 1,...,Kï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œ | | | | | | è¿è¡Œé‡å­ç”µè·¯ï¼Œä½¿ç”¨ğœƒ(l;n)å¹¶ç”Ÿæˆæ–°çš„
    | | | æ ·æœ¬vk(ğœƒ (l;n))ã€‚ | | ç»“æŸ | | | | | è¯„ä¼°ä»£ä»·å‡½æ•°L(ğœƒ(l;n))ã€‚ | ç»“æŸ | | å°†è§£ä»æœ€ä¼˜ï¼ˆä»£ä»·å‡½æ•°æœ€å°ï¼‰åˆ°æœ€å·®ï¼ˆä»£ä»·å‡½æ•°æœ€å¤§ï¼‰è¿›è¡Œæ’åºã€‚
    | | ğœƒâˆ—(l) â† å¯¹åº”äºä»£ä»·å‡½æ•°æœ€å°å€¼çš„é…ç½® | | ï¼ˆç¬¬lä»£ï¼‰ã€‚ | | å¦‚æœL(ğœƒâˆ—(l)) < L(ğœƒâˆ—)ï¼Œåˆ™ | ğœƒâˆ— â† ğœƒâˆ—(l) | ç»“æŸ
    ç»“æŸ --------------------------------------------------------------------- ](img/file827.jpg)'
- en: Having described the training algorithm, we now specify the classical benchmark
    before comparing the results obtained by the quantum and the classical generative
    models on the sample datasets.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æè¿°äº†è®­ç»ƒç®—æ³•ä¹‹åï¼Œæˆ‘ä»¬ç°åœ¨æŒ‡å®šç»å…¸åŸºå‡†ï¼Œç„¶åå†æ¯”è¾ƒé‡å­ç”Ÿæˆæ¨¡å‹å’Œç»å…¸ç”Ÿæˆæ¨¡å‹åœ¨æ ·æœ¬æ•°æ®é›†ä¸Šçš„ç»“æœã€‚
- en: 9.4 Classical Benchmark
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4 ç»å…¸åŸºå‡†
- en: 'There is a deep connection between QCBM and its classical counterpart â€“ Restricted
    Boltzmann MachineÂ Â [[60](Biblography.xhtml#XCheng2017)]. RBM, introduced and discussed
    in ChapterÂ [5](Chapter_5.xhtml#x1-960005) in the context of quantum annealing,
    is a generative model inspired by statistical physics, where the probability of
    a particular data sample, v, is given by the Boltzmann distribution:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: QCBMä¸å…¶ç»å…¸å¯¹åº”ç‰©â€”â€”é™åˆ¶ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰ä¹‹é—´å­˜åœ¨æ·±åˆ»çš„è”ç³» [[60](Biblography.xhtml#XCheng2017)]ã€‚RBMåœ¨é‡å­é€€ç«çš„èƒŒæ™¯ä¸‹è¢«ä»‹ç»å¹¶è®¨è®ºï¼Œè¯¦è§ç¬¬[5](Chapter_5.xhtml#x1-960005)ç« ï¼Œæ˜¯ä¸€ç§å—åˆ°ç»Ÿè®¡ç‰©ç†å¯å‘çš„ç”Ÿæˆæ¨¡å‹ï¼Œå…¶ä¸­æŸä¸ªç‰¹å®šæ•°æ®æ ·æœ¬vçš„æ¦‚ç‡ç”±ç»å°”å…¹æ›¼åˆ†å¸ƒç»™å‡ºï¼š
- en: '| ![ 1 âˆ’ E(v) â„™(v) = Ze . ](img/file828.jpg) |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| ![ 1 âˆ’ E(v) â„™(v) = Ze . ](img/file828.jpg) |  |'
- en: 'Here, *E*(v) is the (positive) *energy* of the data sample (data samples with
    lower energy have higher probabilities) andÂ *Z* is the partition function, namely
    the normalisation factor of the probability density:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ*E*(v)æ˜¯æ•°æ®æ ·æœ¬çš„ï¼ˆæ­£ï¼‰*èƒ½é‡*ï¼ˆå…·æœ‰è¾ƒä½èƒ½é‡çš„æ•°æ®æ ·æœ¬å…·æœ‰è¾ƒé«˜çš„æ¦‚ç‡ï¼‰ï¼Œ*Z*æ˜¯é…åˆ†å‡½æ•°ï¼Œå³æ¦‚ç‡å¯†åº¦çš„å½’ä¸€åŒ–å› å­ï¼š
- en: '| ![ âˆ‘ Z = eâˆ’E(v). v ](img/file829.jpg) |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘ Z = eâˆ’E(v). v ](img/file829.jpg) |  |'
- en: 'Alternatively, we can use the inherent probabilistic nature of quantum mechanics
    that allows us to model the probability distribution using a quantum stateÂ ![|ÏˆâŸ©](img/file830.jpg):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨é‡å­åŠ›å­¦å›ºæœ‰çš„æ¦‚ç‡æ€§è´¨ï¼Œåˆ©ç”¨é‡å­æ€æ¥å»ºæ¨¡æ¦‚ç‡åˆ†å¸ƒÂ ![|ÏˆâŸ©](img/file830.jpg)ï¼š
- en: '| ![â„™(v) = âŸ¨Ïˆ&#124;ğ’«â€ vğ’«v &#124;Ïˆ âŸ©, ](img/file831.jpg) |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| ![â„™(v) = âŸ¨Ïˆ&#124;ğ’«â€ vğ’«v &#124;Ïˆ âŸ©, ](img/file831.jpg) |  |'
- en: where ğ’«[v] is the measurement operator introduced in SectionÂ [1.2.3](Chapter_1.xhtml#x1-380003)
    and, since the quantum state ![|Ïˆ âŸ©](img/file832.jpg) is a unit vector, we have
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ğ’«[v]æ˜¯ç¬¬[1.2.3](Chapter_1.xhtml#x1-380003)èŠ‚ä¸­ä»‹ç»çš„æµ‹é‡ç®—ç¬¦ï¼Œå¹¶ä¸”ç”±äºé‡å­æ€ ![|Ïˆ âŸ©](img/file832.jpg)
    æ˜¯å•ä½å‘é‡ï¼Œæˆ‘ä»¬æœ‰
- en: '| ![âŸ¨Ïˆ &#124;Ïˆ âŸ© = 1\. ](img/file833.jpg) |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ![âŸ¨Ïˆ &#124;Ïˆ âŸ© = 1\. ](img/file833.jpg) |  |'
- en: We realise this approach in the Quantum Circuit Born Machine, where generative
    modelling of probability density is translated into learning a quantum state.
    The sole purpose of QCBMâ€™s parameterised circuit is to create the quantum state
    ![|Ïˆ âŸ©](img/file834.jpg) that encodes the desired probability distribution starting
    from the initial state |0âŸ©^(âŠ—n), with sampling performed by applying the measurement
    operators.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨é‡å­ç”µè·¯Bornæœºå™¨ï¼ˆQuantum Circuit Born Machine, QCBMï¼‰ä¸­å®ç°äº†è¿™ç§æ–¹æ³•ï¼Œå…¶ä¸­ç”Ÿæˆçš„æ¦‚ç‡å¯†åº¦å»ºæ¨¡è¢«è½¬åŒ–ä¸ºå­¦ä¹ ä¸€ä¸ªé‡å­æ€ã€‚QCBMçš„å‚æ•°åŒ–ç”µè·¯çš„å”¯ä¸€ç›®çš„æ˜¯ç”Ÿæˆé‡å­æ€
    ![|Ïˆ âŸ©](img/file834.jpg)ï¼Œè¯¥æ€ç¼–ç äº†ä»åˆå§‹çŠ¶æ€|0âŸ©^(âŠ—n)å¼€å§‹çš„æœŸæœ›æ¦‚ç‡åˆ†å¸ƒï¼Œé‡‡æ ·æ˜¯é€šè¿‡æ–½åŠ æµ‹é‡ç®—ç¬¦æ¥æ‰§è¡Œçš„ã€‚
- en: 'Therefore, providing a classical benchmark for QCBM consists in finding a suitable
    RBM configuration that will allow us to compare two methods generating the probability
    distribution â„™(v): one given byÂ ([9.4](#x1-1950004)) for RBM and another one given
    byÂ ([9.4](#x1-1950004)) for QCBMÂ Â [[170](Biblography.xhtml#XKondratyev2020)].'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæä¾›QCBMçš„ç»å…¸åŸºå‡†çš„æ–¹æ³•æ˜¯æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„RBMé…ç½®ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ¯”è¾ƒç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒâ„™(v)çš„ä¸¤ç§æ–¹æ³•ï¼šä¸€ç§æ˜¯ç”±Â ([9.4](#x1-1950004))
    äº§ç”Ÿçš„RBMæ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯ç”±Â ([9.4](#x1-1950004)) äº§ç”Ÿçš„QCBMæ–¹æ³•  [[170](Biblography.xhtml#XKondratyev2020)]ã€‚
- en: FigureÂ [9.5](#9.5) shows an RBM with 12 stochastic binary visible activation
    units and 7 stochastic binary hidden activation units, where (*a*[i])[i=1,â€¦,12],
    (*b*[j])[j=1,â€¦,7], and (*w*[ij])[i=1,â€¦,12; j=1,â€¦,7] denote, respectively, the
    biases for the visible and hidden layers and the network weights.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[9.5](#9.5)å±•ç¤ºäº†ä¸€ä¸ªå…·æœ‰12ä¸ªéšæœºäºŒè¿›åˆ¶å¯è§æ¿€æ´»å•å…ƒå’Œ7ä¸ªéšæœºäºŒè¿›åˆ¶éšè—æ¿€æ´»å•å…ƒçš„RBMï¼Œå…¶ä¸­ (*a*[i])[i=1,â€¦,12]ï¼Œ(*b*[j])[j=1,â€¦,7]
    å’Œ (*w*[ij])[i=1,â€¦,12; j=1,â€¦,7] åˆ†åˆ«è¡¨ç¤ºå¯è§å±‚å’Œéšè—å±‚çš„åç½®ä»¥åŠç½‘ç»œæƒé‡ã€‚
- en: This network architecture makes RBM equivalent to QCBM as described in SectionÂ [9.1](#x1-1860001)
    in the sense that both generative models have the same number of adjustable parameters
    (the number of RBM weights is equal to the number of adjustable rotation angles
    in QCBM) and the number of visible activation units is equal to the number of
    quantum registers. The latter ensures that both generative models can learn the
    empirical distribution of a continuous random variable with the same precision
    (12-digit binary representation).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç½‘ç»œæ¶æ„ä½¿å¾—RBMç­‰ä»·äºQCBMï¼Œå¦‚ç¬¬[9.1](#x1-1860001)èŠ‚ä¸­æ‰€è¿°ï¼Œå› ä¸ºè¿™ä¸¤ç§ç”Ÿæˆæ¨¡å‹å…·æœ‰ç›¸åŒæ•°é‡çš„å¯è°ƒå‚æ•°ï¼ˆRBMçš„æƒé‡æ•°é‡ç­‰äºQCBMä¸­å¯è°ƒæ—‹è½¬è§’åº¦çš„æ•°é‡ï¼‰ï¼Œå¹¶ä¸”å¯è§æ¿€æ´»å•å…ƒçš„æ•°é‡ç­‰äºé‡å­å¯„å­˜å™¨çš„æ•°é‡ã€‚åè€…ç¡®ä¿è¿™ä¸¤ç§ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿä»¥ç›¸åŒçš„ç²¾åº¦ï¼ˆ12ä½äºŒè¿›åˆ¶è¡¨ç¤ºï¼‰å­¦ä¹ è¿ç»­éšæœºå˜é‡çš„ç»éªŒåˆ†å¸ƒã€‚
- en: '![FigureÂ 9.5: RBM(12, 7). ](img/file835.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾Â 9.5: RBM(12, 7). ](img/file835.jpg)'
- en: 'FigureÂ 9.5: RBM(12, 7).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾Â 9.5: RBM(12, 7).'
- en: QCBM performance should be compared against the performance of its classical
    counterpart, the Restricted Boltzmann Machine. Both models operate on the binary
    representation of the dataset and have a comparable number of adjustable parameters.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: QCBMçš„æ€§èƒ½åº”ä¸å…¶ç»å…¸å¯¹åº”ç‰©â€”â€”é™åˆ¶ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰çš„æ€§èƒ½è¿›è¡Œæ¯”è¾ƒã€‚ä¸¤ç§æ¨¡å‹éƒ½åœ¨æ•°æ®é›†çš„äºŒè¿›åˆ¶è¡¨ç¤ºä¸Šæ“ä½œï¼Œå¹¶ä¸”å…·æœ‰ç›¸ä¼¼æ•°é‡çš„å¯è°ƒå‚æ•°ã€‚
- en: 9.5 QCBM as a Market Generator
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5 QCBMä½œä¸ºå¸‚åœºç”Ÿæˆå™¨
- en: The most obvious financial application of QCBM is as a market generator. An
    efficient generation of realistic market scenarios, for example sampling from
    the joint distribution of risk factors, is one of the most important and challenging
    problems in quantitative finance today. We thus need to investigate how well QCBM
    can execute this task, and compare it to classical benchmarks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: QCBMæœ€æ˜æ˜¾çš„é‡‘èåº”ç”¨æ˜¯ä½œä¸ºå¸‚åœºç”Ÿæˆå™¨ã€‚ä¾‹å¦‚ï¼Œä»é£é™©å› å­çš„è”åˆåˆ†å¸ƒä¸­æŠ½æ ·ï¼Œè¿›è¡Œé«˜æ•ˆçš„ç°å®å¸‚åœºæƒ…å¢ƒç”Ÿæˆï¼Œæ˜¯ä»Šå¤©å®šé‡é‡‘èé¢†åŸŸä¸­æœ€é‡è¦å’Œæœ€å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ä¹‹ä¸€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è°ƒæŸ¥QCBMå¦‚ä½•æ‰§è¡Œè¿™ä¸€ä»»åŠ¡ï¼Œå¹¶å°†å…¶ä¸ç»å…¸åŸºå‡†è¿›è¡Œæ¯”è¾ƒã€‚
- en: 9.5.1 Non-parametric modelling of market risk factors
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.5.1 å¸‚åœºé£é™©å› å­çš„éå‚æ•°å»ºæ¨¡
- en: Historically, the problem of producing reliable synthetic market scenarios was
    solved through sampling from some easy-to-calibrate parametric models, such as
    the multivariate Normal distribution of risk factor log-returns (equities) or
    a Gaussian copula combining the multivariate Normal dependence structure with
    heavy-tailed univariate marginal distributions of individual risk factors (credit).
    However, there are well-known issues with this approach that often outweigh the
    benefits provided by simplicity and transparencyÂ Â [[217](Biblography.xhtml#XMorini2009)].
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å†å²ä¸Šçœ‹ï¼Œè§£å†³ç”Ÿæˆå¯é çš„åˆæˆå¸‚åœºæƒ…å¢ƒçš„é—®é¢˜é€šå¸¸æ˜¯é€šè¿‡ä»ä¸€äº›æ˜“äºæ ¡å‡†çš„å‚æ•°æ¨¡å‹ä¸­è¿›è¡ŒæŠ½æ ·æ¥å®Œæˆçš„ï¼Œä¾‹å¦‚é£é™©å› å­å¯¹æ•°æ”¶ç›Šï¼ˆè‚¡ç¥¨ï¼‰çš„å¤šå…ƒæ­£æ€åˆ†å¸ƒï¼Œæˆ–ç»“åˆå¤šå…ƒæ­£æ€ä¾èµ–ç»“æ„å’Œå•ä¸ªé£é™©å› å­çš„é‡å°¾å•å˜é‡è¾¹é™…åˆ†å¸ƒï¼ˆä¿¡ç”¨ï¼‰çš„é«˜æ–¯
    copulaã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨ä¸€äº›ä¼—æ‰€å‘¨çŸ¥çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¾€å¾€è¶…è¿‡äº†å…¶åœ¨ç®€æ´æ€§å’Œé€æ˜æ€§ä¸Šçš„ä¼˜åŠ¿[[217](Biblography.xhtml#XMorini2009)]ã€‚
- en: 'A parametric model is often a poor approximation of reality. To be useful,
    it has to be relatively simple: one should be able to describe the key features
    of the risk factor distribution with a handful of parameters achieving the best
    possible fit to either the empirical distribution derived from historical data
    or from prices of traded instruments observed in the market at the time of model
    calibration. Making the parametric model too complex would lead to overfitting
    and poor generalisation.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°æ¨¡å‹å¾€å¾€æ˜¯ç°å®çš„ä¸€ä¸ªè¾ƒå·®è¿‘ä¼¼ã€‚ä¸ºäº†æœ‰æ•ˆï¼Œå®ƒå¿…é¡»ç›¸å¯¹ç®€å•ï¼šåº”è¯¥èƒ½å¤Ÿç”¨å°‘é‡å‚æ•°æè¿°é£é™©å› å­åˆ†å¸ƒçš„å…³é”®ç‰¹å¾ï¼Œä»è€Œæœ€å¥½åœ°æ‹Ÿåˆä»å†å²æ•°æ®ä¸­æ¨å¯¼å‡ºçš„ç»éªŒåˆ†å¸ƒæˆ–ä»å¸‚åœºä¸­è§‚å¯Ÿåˆ°çš„äº¤æ˜“å·¥å…·ä»·æ ¼ï¼ˆåœ¨æ¨¡å‹æ ¡å‡†æ—¶ï¼‰ã€‚å°†å‚æ•°æ¨¡å‹åšå¾—è¿‡äºå¤æ‚ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆå’Œè¾ƒå·®çš„æ³›åŒ–èƒ½åŠ›ã€‚
- en: It is even more difficult to model a realistic dependence structure. A typical
    parametric approach used in most Monte Carlo risk engines starts with modelling
    the dynamics of various risk factors independently, and then imposes a dependence
    structure by correlating the corresponding stochastic drivers. These are, almost
    invariably, Brownian motions, and the linear correlations between them are supposed
    to be sufficient to construct the joint distribution of risk factors.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å»ºæ¨¡ä¸€ä¸ªç°å®çš„ä¾èµ–ç»“æ„æ›´ä¸ºå›°éš¾ã€‚å¤§å¤šæ•°è’™ç‰¹å¡æ´›é£é™©å¼•æ“ä¸­å¸¸ç”¨çš„å…¸å‹å‚æ•°åŒ–æ–¹æ³•æ˜¯é¦–å…ˆç‹¬ç«‹åœ°å»ºæ¨¡å„ç§é£é™©å› å­çš„åŠ¨æ€ï¼Œç„¶åé€šè¿‡ç›¸å…³ç›¸åº”çš„éšæœºé©±åŠ¨å› ç´ æ¥å¼ºåŠ ä¸€ä¸ªä¾èµ–ç»“æ„ã€‚è¿™äº›é©±åŠ¨å› ç´ å‡ ä¹æ€»æ˜¯å¸ƒæœ—è¿åŠ¨ï¼Œå®ƒä»¬ä¹‹é—´çš„çº¿æ€§ç›¸å…³æ€§åº”è¯¥è¶³ä»¥æ„å»ºé£é™©å› å­çš„è”åˆåˆ†å¸ƒã€‚
- en: An alternative approach is to use non-parametric modelling, where the joint
    and marginal distributions of risk factors are learned directly from the available
    datasets. Classically, we can realise this approach with the help of a Restricted
    Boltzmann Machine â€“ the classical benchmark of choice described in the previous
    section and successfully applied to a number of financial use casesÂ Â [[173](Biblography.xhtml#XKS2020),Â [174](Biblography.xhtml#XKSH2020)].
    Another possibility is to use the Generative Adversarial Network (GAN) framework,
    where the distribution learned from the dataset by a generative neural network
    is tested by a discriminative neural network trying to judge whether samples are
    coming from the true distribution (data) or from the reconstructed distribution
    (generated samples)Â Â [[114](Biblography.xhtml#XGoodfellow2014)].
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æ›¿ä»£æ–¹æ³•æ˜¯ä½¿ç”¨éå‚æ•°å»ºæ¨¡ï¼Œå…¶ä¸­é£é™©å› å­çš„è”åˆåˆ†å¸ƒå’Œè¾¹é™…åˆ†å¸ƒæ˜¯ç›´æ¥ä»å¯ç”¨æ•°æ®é›†å­¦ä¹ å¾—å‡ºçš„ã€‚ç»å…¸åœ°ï¼Œæˆ‘ä»¬å¯ä»¥å€ŸåŠ©é™åˆ¶ç»å°”å…¹æ›¼æœºï¼ˆRestricted Boltzmann
    Machineï¼‰æ¥å®ç°è¿™ä¸€æ–¹æ³•â€”â€”å‰ä¸€èŠ‚ä¸­æè¿°çš„ç»å…¸åŸºå‡†ï¼Œä¸”å·²æˆåŠŸåº”ç”¨äºå¤šä¸ªé‡‘èæ¡ˆä¾‹[[173](Biblography.xhtml#XKS2020), [174](Biblography.xhtml#XKSH2020)]ã€‚å¦ä¸€ç§å¯èƒ½æ€§æ˜¯ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¡†æ¶ï¼Œå…¶ä¸­é€šè¿‡ç”Ÿæˆç¥ç»ç½‘ç»œä»æ•°æ®é›†ä¸­å­¦ä¹ åˆ°çš„åˆ†å¸ƒè¢«åˆ¤åˆ«ç¥ç»ç½‘ç»œæµ‹è¯•ï¼Œåè€…è¯•å›¾åˆ¤æ–­æ ·æœ¬æ˜¯æ¥è‡ªçœŸå®åˆ†å¸ƒï¼ˆæ•°æ®ï¼‰è¿˜æ˜¯æ¥è‡ªé‡å»ºåˆ†å¸ƒï¼ˆç”Ÿæˆçš„æ ·æœ¬ï¼‰[[114](Biblography.xhtml#XGoodfellow2014)]ã€‚
- en: ChapterÂ [12](Chapter_12.xhtml#x1-22500012) explores the question of the larger
    expressive power of QCBM in comparison with classical neural networks (RBM). However,
    the first step should be an experimental verification of their performance characteristics.
    With this in mind, we would like to test the ability of both QCBM and RBM to learn
    relatively complex probability distributions and then efficiently sample from
    them.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬Â [12](Chapter_12.xhtml#x1-22500012)ç« æ¢è®¨äº†QCBMåœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šä¸ç»å…¸ç¥ç»ç½‘ç»œï¼ˆRBMï¼‰ç›¸æ¯”çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œç¬¬ä¸€æ­¥åº”è¯¥æ˜¯å¯¹å®ƒä»¬çš„æ€§èƒ½ç‰¹å¾è¿›è¡Œå®éªŒéªŒè¯ã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¸Œæœ›æµ‹è¯•QCBMå’ŒRBMåœ¨å­¦ä¹ ç›¸å¯¹å¤æ‚çš„æ¦‚ç‡åˆ†å¸ƒæ–¹é¢çš„èƒ½åŠ›ï¼Œç„¶åé«˜æ•ˆåœ°ä»ä¸­é‡‡æ ·ã€‚
- en: 9.5.2 Sampling from the learned probability distributions
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.5.2 ä»å­¦ä¹ åˆ°çš„æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·
- en: 'We are going to test the performance of QCBM and RBM on two datasets:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æµ‹è¯• QCBM å’Œ RBM åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼š
- en: '**Dataset A.** A heavy-tailed distribution of daily S&P 500 index returns observed
    between 5 January 2009 and 22 February 2011 (UCI Machine Learning RepositoryÂ Â [[10](Biblography.xhtml#XAkbilgic2013),Â [9](Biblography.xhtml#XUCI_SP)]).
    The dataset consists of 536 samples.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•°æ®é›† A**ã€‚2009å¹´1æœˆ5æ—¥è‡³2011å¹´2æœˆ22æ—¥ä¹‹é—´è§‚å¯Ÿåˆ°çš„S&P 500æŒ‡æ•°å›æŠ¥çš„é‡å°¾åˆ†å¸ƒï¼ˆUCIæœºå™¨å­¦ä¹ åº“Â [[10](Biblography.xhtml#XAkbilgic2013)ï¼ŒÂ [9](Biblography.xhtml#XUCI_SP)]ï¼‰ã€‚æ•°æ®é›†åŒ…å«536ä¸ªæ ·æœ¬ã€‚'
- en: '**Dataset B.** A specially constructed distribution of a continuous random
    variable with a highly spiky probability density function (pdf) modelled as a
    mixture of Normal distributions. The dataset consists of 5,000 generated samples
    from a mixture of four Normal distributions with the following means, standard
    deviations, and weights:'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•°æ®é›† B**ã€‚ä¸€ä¸ªç‰¹åˆ«æ„é€ çš„è¿ç»­éšæœºå˜é‡åˆ†å¸ƒï¼Œå…·æœ‰é«˜åº¦å°–é”çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆpdfï¼‰ï¼Œè¯¥åˆ†å¸ƒè¢«å»ºæ¨¡ä¸ºå¤šä¸ªæ­£æ€åˆ†å¸ƒçš„æ··åˆã€‚æ•°æ®é›†ç”±5,000ä¸ªæ ·æœ¬ç»„æˆï¼Œè¿™äº›æ ·æœ¬æ¥è‡ªå››ä¸ªæ­£æ€åˆ†å¸ƒçš„æ··åˆï¼Œå…·æœ‰ä»¥ä¸‹å‡å€¼ã€æ ‡å‡†å·®å’Œæƒé‡ï¼š'
- en: '| Mean | Standard deviation | Weight |'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| å‡å€¼ | æ ‡å‡†å·® | æƒé‡ |'
- en: '| âˆ’3 | 0.3 | 0.1 |'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| âˆ’3 | 0.3 | 0.1 |'
- en: '| âˆ’1 | 0.3 | 0.2 |'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| âˆ’1 | 0.3 | 0.2 |'
- en: '| 1 | 0.3 | 0.3 |'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1 | 0.3 | 0.3 |'
- en: '| 3 | 0.3 | 0.4 |'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 3 | 0.3 | 0.4 |'
- en: 'TableÂ 9.1: Parameters of the mixture of standard Normal distributions.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¡¨ 9.1ï¼šæ ‡å‡†æ­£æ€åˆ†å¸ƒæ··åˆçš„å‚æ•°ã€‚
- en: 'In both cases, we convert the continuous samples into the corresponding 12-digit
    binary representation as per AlgorithmÂ [6](#x1-190006r6). Once the networks are
    trained (QCBM with AlgorithmÂ [8](#x1-194004r8) and RBM with AlgorithmÂ [2](Chapter_5.xhtml#x1-106003r2)),
    we generate new samples: 536 new samples for DatasetÂ A and 5,000 new samples for
    DatasetÂ B. This allows us to visualise the quality of the generated samples (once
    they are converted into the corresponding continuous representation as per Algorithm
    [7](#x1-190009r7)) by producing the empirical pdf and the QQ-plots as shown in
    FiguresÂ [9.6](#9.6) andÂ [9.7](#9.7), which display sample simulation results for
    the fully trained models. We can see that both QCBM(12, 7) and RBM(12, 7) can
    successfully learn complex empirical distributions (heavy-tailed in the case of
    DatasetÂ A and light-tailed with spiky pdf in the case of DatasetÂ B). We have chosenÂ CX
    for the fixed gates in QCBM and used the Qiskit quantum simulator to simulate
    the quantum parts of the training and sampling algorithms.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†è¿ç»­æ ·æœ¬è½¬æ¢ä¸ºç›¸åº”çš„12ä½äºŒè¿›åˆ¶è¡¨ç¤ºï¼ŒæŒ‰ç…§ç®—æ³•Â [6](#x1-190006r6)ã€‚ä¸€æ—¦ç½‘ç»œè®­ç»ƒå®Œæˆï¼ˆQCBM ä½¿ç”¨ç®—æ³•Â [8](#x1-194004r8)ï¼ŒRBM
    ä½¿ç”¨ç®—æ³•Â [2](Chapter_5.xhtml#x1-106003r2)ï¼‰ï¼Œæˆ‘ä»¬ç”Ÿæˆæ–°æ ·æœ¬ï¼šæ•°æ®é›† A ç”Ÿæˆ 536 ä¸ªæ–°æ ·æœ¬ï¼Œæ•°æ®é›† B ç”Ÿæˆ 5,000
    ä¸ªæ–°æ ·æœ¬ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡ç”Ÿæˆç»éªŒpdfå’ŒQQå›¾æ¥å¯è§†åŒ–ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ï¼Œæ­£å¦‚å›¾Â [9.6](#9.6) å’ŒÂ [9.7](#9.7) æ‰€ç¤ºï¼Œå®ƒä»¬å±•ç¤ºäº†å®Œå…¨è®­ç»ƒæ¨¡å‹çš„æ ·æœ¬æ¨¡æ‹Ÿç»“æœã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒQCBM(12,
    7) å’Œ RBM(12, 7) éƒ½èƒ½æˆåŠŸå­¦ä¹ å¤æ‚çš„ç»éªŒåˆ†å¸ƒï¼ˆæ•°æ®é›† A ä¸ºé‡å°¾åˆ†å¸ƒï¼Œæ•°æ®é›† B ä¸ºè½»å°¾ä¸”å…·æœ‰å°–é”pdfçš„åˆ†å¸ƒï¼‰ã€‚æˆ‘ä»¬é€‰æ‹©äº†CXé—¨ä½œä¸ºQCBMä¸­çš„å›ºå®šé—¨ï¼Œå¹¶ä½¿ç”¨Qiskité‡å­æ¨¡æ‹Ÿå™¨æ¥æ¨¡æ‹Ÿè®­ç»ƒå’Œé‡‡æ ·ç®—æ³•ä¸­çš„é‡å­éƒ¨åˆ†ã€‚
- en: 'The following sets of hyperparameters were used to train the models:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹è¶…å‚æ•°é›†ç”¨äºè®­ç»ƒæ¨¡å‹ï¼š
- en: '**Genetic Algorithm for training QCBM (AlgorithmÂ **[**8**](#x1-194004r8)**)**
    *N* = 1000, *M* = 25, *m* = 7, *Î±* = 1*.*0, *Î²* = 0*.*013863, *Îº* = 50, *L* =
    200\. The value ofÂ *Î²* ensures that mutation rate halves after eachÂ *Îº* generations.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç”¨äºè®­ç»ƒ QCBM çš„é—ä¼ ç®—æ³•ï¼ˆç®—æ³•Â **[**8**](#x1-194004r8)**ï¼‰** *N* = 1000, *M* = 25, *m*
    = 7, *Î±* = 1*.*0, *Î²* = 0*.*013863, *Îº* = 50, *L* = 200ã€‚*Î²* çš„å€¼ç¡®ä¿æ¯éš” *Îº* ä»£å˜å¼‚ç‡å‡åŠã€‚'
- en: '**Contrastive Divergence algorithm for training RBM** (`sklearn.neural_network.BernoulliRBM`)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç”¨äºè®­ç»ƒ RBM çš„å¯¹æ¯”æ•£åº¦ç®—æ³•** (`sklearn.neural_network.BernoulliRBM`)'
- en: n_components = 7 â€“ number of hidden activation units for RBM(12, 7) learning_rate
    = 0.0005 â€“ learning rate *Î·* in AlgorithmÂ [2](Chapter_5.xhtml#x1-106003r2) batch_size
    = 10 â€“ size of the training minibatches *S* in AlgorithmÂ [2](Chapter_5.xhtml#x1-106003r2)
    n_iter = 40000 â€“ number of iterations
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'n_components = 7 â€“ RBM(12, 7) çš„éšè—æ¿€æ´»å•å…ƒæ•°  '
- en: Although a visual inspection of the pdf and QQ-plots in FiguresÂ [9.6](#9.6)
    andÂ [9.7](#9.7) suggests that both QCBM and RBM are doing a good job in generating
    high-quality samples from the learned empirical distributions encoded in model
    parameters, we would like to have a more objective measure of the model performance.
    This is especially important since we deal with generative models and very little
    can be concluded from a single model run.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ä»å›¾[9.6](#9.6)å’Œå›¾[9.7](#9.7)ä¸­çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆpdfï¼‰å’Œ QQ å›¾æ¥çœ‹ï¼ŒQCBM å’Œ RBM éƒ½åœ¨ä»å­¦ä¹ åˆ°çš„ç»éªŒåˆ†å¸ƒä¸­ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†æˆ‘ä»¬å¸Œæœ›èƒ½æœ‰ä¸€ä¸ªæ›´å®¢è§‚çš„æ¨¡å‹æ€§èƒ½è¡¡é‡æ ‡å‡†ã€‚ç‰¹åˆ«æ˜¯åœ¨æˆ‘ä»¬å¤„ç†ç”Ÿæˆæ¨¡å‹æ—¶ï¼Œå•æ¬¡æ¨¡å‹è¿è¡Œå¾—å‡ºçš„ç»“è®ºéå¸¸æœ‰é™ã€‚
- en: Running the quantum circuit multiple times for a particular configuration of
    model parameters (e.g., an optimal set of rotation angles found with the help
    of GA) results in the distribution of objective function values. This gives us
    an idea of what metrics can be used to measure the performance of QCBM and RBMÂ Â [[170](Biblography.xhtml#XKondratyev2020)].
    The cost functionÂ ([9.3.2](#x1-1940002)) we used for training QCBM can be calculated
    on the samples generated by RBM. In other words, we can compare the performance
    of QCBM and RBM by comparing the distributions of the cost function values calculated
    for the samples generated by these models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: é’ˆå¯¹æŸä¸€ç‰¹å®šæ¨¡å‹å‚æ•°é…ç½®ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡é—ä¼ ç®—æ³•ï¼ˆGAï¼‰æ‰¾åˆ°çš„æœ€ä¼˜æ—‹è½¬è§’åº¦é›†ï¼‰å¤šæ¬¡è¿è¡Œé‡å­ç”µè·¯ï¼Œä¼šäº§ç”Ÿç›®æ ‡å‡½æ•°å€¼çš„åˆ†å¸ƒã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿäº†è§£å¯ä»¥ç”¨æ¥è¡¡é‡ QCBM
    å’Œ RBM æ€§èƒ½çš„åº¦é‡æ ‡å‡†[[170](Biblography.xhtml#XKondratyev2020)]ã€‚æˆ‘ä»¬åœ¨è®­ç»ƒ QCBM æ—¶ä½¿ç”¨çš„æˆæœ¬å‡½æ•°ï¼ˆ[9.3.2](#x1-1940002)ï¼‰å¯ä»¥é€šè¿‡
    RBM ç”Ÿæˆçš„æ ·æœ¬è®¡ç®—å¾—åˆ°ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ¯”è¾ƒè¿™ä¸¤ç§æ¨¡å‹ç”Ÿæˆçš„æ ·æœ¬è®¡ç®—å‡ºçš„æˆæœ¬å‡½æ•°å€¼çš„åˆ†å¸ƒï¼Œæ¥æ¯”è¾ƒ QCBM å’Œ RBM çš„æ€§èƒ½ã€‚
- en: TableÂ [9.2](#x1-198004r2) shows the means and standard deviations of the cost
    functions calculated forÂ 100 runs of QCBM(12, 7) and RBM(12, 7). Each run generated
    5,000 samples from the learned empirical distribution (the models were trained
    on DatasetÂ B, which consists of 5,000 samples from the mixture of four Normal
    distributions).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨[9.2](#x1-198004r2)å±•ç¤ºäº†é’ˆå¯¹ QCBM(12, 7) å’Œ RBM(12, 7) è¿›è¡Œ 100 æ¬¡è®­ç»ƒçš„æˆæœ¬å‡½æ•°çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚æ¯æ¬¡è®­ç»ƒéƒ½ä»å­¦ä¹ åˆ°çš„ç»éªŒåˆ†å¸ƒä¸­ç”Ÿæˆ
    5000 ä¸ªæ ·æœ¬ï¼ˆæ¨¡å‹æ˜¯åœ¨æ•°æ®é›† B ä¸Šè®­ç»ƒçš„ï¼Œæ•°æ®é›† B åŒ…å« 5000 ä¸ªæ¥è‡ªå››ä¸ªæ­£æ€åˆ†å¸ƒæ··åˆçš„æ ·æœ¬ï¼‰ã€‚
- en: '| Model | Mean | Standard deviation |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | å¹³å‡å€¼ | æ ‡å‡†å·® |'
- en: '| QCBM(12, 7) | 30.5 | 23.6 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| QCBM(12, 7) | 30.5 | 23.6 |'
- en: '| RBM(12, 7) | 39.6 | 30.8 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| RBM(12, 7) | 39.6 | 30.8 |'
- en: 'TableÂ 9.2: Cost function statistics for the models trained on DatasetÂ B.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 9.2ï¼šåœ¨æ•°æ®é›† B ä¸Šè®­ç»ƒçš„æ¨¡å‹çš„æˆæœ¬å‡½æ•°ç»Ÿè®¡æ•°æ®ã€‚
- en: It is clear from TableÂ [9.2](#x1-198004r2) that QCBM(12, 7) with a weakly optimised
    set of hyperparameters performs better than RBM(12, 7) trained with equally weakly
    optimised hyperparameters (a small learning rate combined with a large number
    of iterations and the small size of the minibatchesÂ Â [[134](Biblography.xhtml#XHinton2010)]).
    Although this cannot be seen as proper evidence of quantum advantage, this nevertheless
    opens the gate to promising further research.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¡¨[9.2](#x1-198004r2)å¯ä»¥æ˜æ˜¾çœ‹å‡ºï¼ŒQCBM(12, 7) åœ¨ä½¿ç”¨å¼±ä¼˜åŒ–çš„è¶…å‚æ•°è®¾ç½®æ—¶ï¼Œæ¯”ä½¿ç”¨åŒæ ·å¼±ä¼˜åŒ–çš„è¶…å‚æ•°ï¼ˆè¾ƒå°çš„å­¦ä¹ ç‡ä¸è¾ƒå¤§çš„è¿­ä»£æ¬¡æ•°ä»¥åŠå°æ‰¹é‡çš„å¤§å°[[134](Biblography.xhtml#XHinton2010)]ï¼‰è®­ç»ƒçš„
    RBM(12, 7) è¡¨ç°æ›´å¥½ã€‚è™½ç„¶è¿™ä¸èƒ½è§†ä¸ºé‡å­ä¼˜åŠ¿çš„ç¡®å‡¿è¯æ®ï¼Œä½†å®ƒæ— ç–‘ä¸ºæœ‰å‰æ™¯çš„è¿›ä¸€æ­¥ç ”ç©¶å¼€è¾Ÿäº†å¤§é—¨ã€‚
- en: '![FigureÂ 9.6: Mixture of Normal distributions. ](img/file836.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 9.6ï¼šæ­£æ€åˆ†å¸ƒæ··åˆæ¨¡å‹ã€‚](img/file836.png)'
- en: 'FigureÂ 9.6: Mixture of Normal distributions.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.6ï¼šæ­£æ€åˆ†å¸ƒæ··åˆæ¨¡å‹ã€‚
- en: '![FigureÂ 9.7: Distribution of S&P 500 index returns. ](img/file837.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 9.7ï¼šæ ‡å‡†æ™®å°” 500 æŒ‡æ•°å›æŠ¥åˆ†å¸ƒã€‚](img/file837.png)'
- en: 'FigureÂ 9.7: Distribution of S&P 500 index returns.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.7ï¼šæ ‡å‡†æ™®å°” 500 æŒ‡æ•°å›æŠ¥åˆ†å¸ƒã€‚
- en: Let us now turn our attention to DatasetÂ A. The dataset consists of just 536
    samples and, as we can see in FigureÂ [9.7](#9.7), the empirical pdf displays pronounced
    heavy tails which are also clearly seen in the QQ-plot against the Normal distribution.
    The relatively small number of samples means that we have to deal with a substantial
    amount of noise. Therefore, we need to use a robust statistical test to compare
    QCBM and RBM. Since we are working with a univariate distribution, we can estimate
    the quality of generated samples with the Kolmogorov-Smirnov testÂ Â [[233](Biblography.xhtml#XPfaffenberger1987)].
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å…³æ³¨æ•°æ®é›†Aã€‚è¯¥æ•°æ®é›†ä»…åŒ…å«536ä¸ªæ ·æœ¬ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨å›¾[9.7](#9.7)ä¸­çœ‹åˆ°çš„ï¼Œç»éªŒæ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„é‡å°¾ç°è±¡ï¼Œè¿™åœ¨ä¸æ­£æ€åˆ†å¸ƒçš„QQå›¾ä¸­ä¹Ÿæ¸…æ™°å¯è§ã€‚æ ·æœ¬æ•°é‡ç›¸å¯¹è¾ƒå°‘æ„å‘³ç€æˆ‘ä»¬å¿…é¡»å¤„ç†å¤§é‡å™ªå£°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ç¨³å¥çš„ç»Ÿè®¡æ£€éªŒæ¥æ¯”è¾ƒQCBMå’ŒRBMã€‚ç”±äºæˆ‘ä»¬å¤„ç†çš„æ˜¯å•å˜é‡åˆ†å¸ƒï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Kolmogorov-Smirnovæ£€éªŒ[[233](Biblography.xhtml#XPfaffenberger1987)]æ¥ä¼°ç®—ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚
- en: TableÂ [9.3](#x1-198011r3) provides the p-values and Kolmogorov-Smirnov statistics
    for the RBM and the QCBM generated samples as well as a Normal distribution fitted
    to the original dataset (by matching the first two moments). The p-value represents
    the probability of obtaining test results supporting the null hypothesis of the
    two datasets coming from the same distribution. In the context of our numerical
    experiments, the larger the p-value the more likely the generated samples were
    drawn from the correct distribution.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æ ¼[9.3](#x1-198011r3)æä¾›äº†RBMå’ŒQCBMç”Ÿæˆçš„æ ·æœ¬çš„på€¼å’ŒKolmogorov-Smirnovç»Ÿè®¡é‡ï¼Œä»¥åŠé€šè¿‡åŒ¹é…å‰ä¸¤ä¸ªçŸ©å¾—åˆ°çš„æ‹Ÿåˆåˆ°åŸå§‹æ•°æ®é›†çš„æ­£æ€åˆ†å¸ƒã€‚på€¼è¡¨ç¤ºè·å¾—æ”¯æŒé›¶å‡è®¾çš„æµ‹è¯•ç»“æœçš„æ¦‚ç‡ï¼Œå³ä¸¤ä¸ªæ•°æ®é›†æ¥è‡ªç›¸åŒåˆ†å¸ƒçš„æ¦‚ç‡ã€‚åœ¨æˆ‘ä»¬çš„æ•°å€¼å®éªŒä¸­ï¼Œpå€¼è¶Šå¤§ï¼Œç”Ÿæˆçš„æ ·æœ¬è¶Šå¯èƒ½æ¥è‡ªæ­£ç¡®çš„åˆ†å¸ƒã€‚
- en: '| Distribution | p-value | K-S statistic |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| åˆ†å¸ƒ | på€¼ | K-Sç»Ÿè®¡é‡ |'
- en: '| Normal | 0.004 Â± 0.009 | 0.121 Â± 0.017 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| æ­£æ€åˆ†å¸ƒ | 0.004 Â± 0.009 | 0.121 Â± 0.017 |'
- en: '| RBM generated samples | 0.46 Â± 0.23 | 0.055 Â± 0.011 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| RBMç”Ÿæˆæ ·æœ¬ | 0.46 Â± 0.23 | 0.055 Â± 0.011 |'
- en: '| QCBM generated samples | 0.46 Â± 0.11 | 0.053 Â± 0.005 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| QCBMç”Ÿæˆæ ·æœ¬ | 0.46 Â± 0.11 | 0.053 Â± 0.005 |'
- en: 'TableÂ 9.3: p-value and K-S statistic for Normal, RBM and QCBM generated samples
    in the format: mean Â± standard deviation. Number of Normal, RBM and QCBM generated
    datasets: 20\. Number of samples in each generated dataset: 536 (equal to the
    number of samples in the original dataset).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æ ¼9.3ï¼šæ­£æ€åˆ†å¸ƒã€RBMå’ŒQCBMç”Ÿæˆæ ·æœ¬çš„på€¼å’ŒK-Sç»Ÿè®¡é‡ï¼Œæ ¼å¼ä¸ºï¼šå‡å€¼ Â± æ ‡å‡†å·®ã€‚æ­£æ€åˆ†å¸ƒã€RBMå’ŒQCBMç”Ÿæˆçš„æ•°æ®é›†æ•°é‡ï¼š20ã€‚æ¯ä¸ªç”Ÿæˆçš„æ•°æ®é›†çš„æ ·æœ¬æ•°ï¼š536ï¼ˆä¸åŸå§‹æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°ç›¸ç­‰ï¼‰ã€‚
- en: The K-S statistic takes the largest absolute difference between the two distribution
    functions across all values of the random variable. The larger the K-S statistic
    the less likely the generated samples were drawn from the correct distribution.
    The K-S statistic can be compared with the critical values calculated for the
    given confidence level and number of samples. For example, the critical value
    corresponding to the 95th percentile confidence level andÂ 536 samples in both
    datasets is 0*.*0587\. If the K-S statistic is larger then, with 95% certainty,
    we can reject the null hypothesis thatÂ 536 generated samples were drawn from the
    right distribution.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: K-Sç»Ÿè®¡é‡å–æ‰€æœ‰éšæœºå˜é‡å€¼ä¸‹ä¸¤ä¸ªåˆ†å¸ƒå‡½æ•°ä¹‹é—´çš„æœ€å¤§ç»å¯¹å·®ã€‚K-Sç»Ÿè®¡é‡è¶Šå¤§ï¼Œç”Ÿæˆæ ·æœ¬æ¥è‡ªæ­£ç¡®åˆ†å¸ƒçš„å¯èƒ½æ€§å°±è¶Šå°ã€‚K-Sç»Ÿè®¡é‡å¯ä»¥ä¸ç»™å®šç½®ä¿¡æ°´å¹³å’Œæ ·æœ¬æ•°é‡è®¡ç®—å¾—åˆ°çš„ä¸´ç•Œå€¼è¿›è¡Œæ¯”è¾ƒã€‚ä¾‹å¦‚ï¼Œå¯¹åº”äº95%ç½®ä¿¡æ°´å¹³å’Œ536ä¸ªæ ·æœ¬çš„ä¸´ç•Œå€¼æ˜¯0*.*0587ã€‚å¦‚æœK-Sç»Ÿè®¡é‡è¾ƒå¤§ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ä»¥95%çš„ç½®ä¿¡åº¦æ‹’ç»â€œ536ä¸ªç”Ÿæˆæ ·æœ¬æ¥è‡ªæ­£ç¡®åˆ†å¸ƒâ€çš„é›¶å‡è®¾ã€‚
- en: 'The first observation is that we can definitely reject the null hypothesis
    that the daily S&PÂ 500 index returns are Normally distributed. The corresponding
    p-value is much smaller than 1, and the K-S statistic is twice the critical value.
    More importantly, QCBM performs at par with RBM in terms of both the p-value and
    the K-S statistic: we therefore cannot reject the null hypothesis that QCBM and
    RBM generated samples were drawn from the same distribution as the original dataset.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªè§‚å¯Ÿç»“æœæ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥æ˜ç¡®æ‹’ç»â€œæ¯æ—¥S&P 500æŒ‡æ•°å›æŠ¥æœä»æ­£æ€åˆ†å¸ƒâ€çš„é›¶å‡è®¾ã€‚ç›¸åº”çš„på€¼è¿œå°äº1ï¼ŒK-Sç»Ÿè®¡é‡æ˜¯ä¸´ç•Œå€¼çš„ä¸¤å€ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒQCBMåœ¨på€¼å’ŒK-Sç»Ÿè®¡é‡æ–¹é¢çš„è¡¨ç°ä¸RBMç›¸å½“ï¼šå› æ­¤æˆ‘ä»¬æ— æ³•æ‹’ç»QCBMå’ŒRBMç”Ÿæˆçš„æ ·æœ¬æ¥è‡ªä¸åŸå§‹æ•°æ®é›†ç›¸åŒåˆ†å¸ƒçš„é›¶å‡è®¾ã€‚
- en: 9.5.3 Training algorithm convergence and hyperparameter optimisation
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.5.3 è®­ç»ƒç®—æ³•æ”¶æ•›æ€§å’Œè¶…å‚æ•°ä¼˜åŒ–
- en: Next, we would like to explore the GA behaviour for various model configurations.
    In particular, it is interesting to investigate the algorithm convergence for
    different types of fixed gates, not justÂ CX, and for different choices of the
    mutation rate. The charts in FigureÂ [9.8](#9.8) confirm our intuition aboutÂ CX
    being the best choice of fixed gate given the configuration of one-qubit gates
    (FigureÂ [9.1](#9.1)) and the exponentially decreasing mutation rate performing
    better than the constant mutation rates. Here, we continue working with DatasetÂ B.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¸Œæœ›æ¢è®¨ GA åœ¨ä¸åŒæ¨¡å‹é…ç½®ä¸‹çš„è¡¨ç°ã€‚ç‰¹åˆ«æ˜¯ï¼Œè°ƒæŸ¥ä¸åŒç±»å‹å›ºå®šé—¨ï¼ˆä¸ä»…ä»…æ˜¯ CXï¼‰å’Œä¸åŒå˜å¼‚ç‡é€‰æ‹©ä¸‹çš„ç®—æ³•æ”¶æ•›æ€§æ˜¯éå¸¸æœ‰è¶£çš„ã€‚å›¾ [9.8](#9.8)
    ä¸­çš„å›¾è¡¨ç¡®è®¤äº†æˆ‘ä»¬çš„ç›´è§‰ï¼šç»™å®šå•é‡å­æ¯”ç‰¹é—¨çš„é…ç½®ï¼ˆå›¾ [9.1](#9.1)ï¼‰ï¼ŒCX æ˜¯æœ€ä½³çš„å›ºå®šé—¨é€‰æ‹©ï¼Œè€ŒæŒ‡æ•°è¡°å‡çš„å˜å¼‚ç‡ä¼˜äºå¸¸æ•°å˜å¼‚ç‡ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ç»§ç»­ä½¿ç”¨æ•°æ®é›†
    Bã€‚
- en: 'As we can see in FigureÂ [9.1](#9.1), the fixed gates are flanked by one-qubit
    gates performing rotations around the *z*-axis. Therefore, adding another rotation
    around the *z*-axis by *Ï•* = *Ï€* (Z = R[Z](*Ï€*)) may not offer the same flexibility
    as rotation around the *x*-axis by *Ï•* = *Ï€* (X = R[X](*Ï€*)). Controlled rotations
    around the *z*-axis by an angle *Ï• < Ï€* are likely to perform even worse. This
    is exactly what we see in FigureÂ [9.8](#9.8) (left chart) for three different
    types of fixed gates: CX, CZ, and CR[Z](*Ï€âˆ•*4).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨å›¾ [9.1](#9.1) ä¸­æ‰€è§ï¼Œå›ºå®šé—¨ä¸¤ä¾§æ˜¯æ‰§è¡Œç»• *z* è½´æ—‹è½¬çš„å•é‡å­æ¯”ç‰¹é—¨ã€‚å› æ­¤ï¼Œé€šè¿‡ *Ï•* = *Ï€* è¿›è¡Œçš„ç»• *z* è½´æ—‹è½¬ï¼ˆZ
    = R[Z](*Ï€*)ï¼‰å¯èƒ½æ— æ³•æä¾›ä¸ç»• *x* è½´æ—‹è½¬ï¼ˆX = R[X](*Ï€*)ï¼‰ç›¸åŒçš„çµæ´»æ€§ã€‚ç»• *z* è½´çš„æ§åˆ¶æ—‹è½¬è§’åº¦ *Ï• < Ï€* å¯èƒ½è¡¨ç°å¾—æ›´å·®ã€‚è¿™æ­£æ˜¯æˆ‘ä»¬åœ¨å›¾
    [9.8](#9.8)ï¼ˆå·¦å›¾ï¼‰ä¸­å¯¹ä¸‰ç§ä¸åŒç±»å‹çš„å›ºå®šé—¨ï¼ˆCXã€CZ å’Œ CR[Z](*Ï€âˆ•*4)ï¼‰æ‰€çœ‹åˆ°çš„æƒ…å†µã€‚
- en: Our intuition about the optimal choice of mutation rate suggests that it should
    be productive to start the algorithm with a really large mutation rate in order
    to explore the search space as broadly as possible (the "exploration" phase).
    Then, as the algorithm finds progressively better solutions, it should be useful
    to reduce the mutation rate in order to perform a more detailed search in the
    vicinity of the best solutions found so far (the "exploitation" phase). As the
    algorithm converges, we may want to perform more and more refined searches by
    only mutating one or two parameters. FigureÂ [9.8](#9.8) (right chart) shows that
    this is indeed the case. Here, the maximum value of the mutation rate is *Î±* =
    1*.*0 and the minimum value is *Î±* = 0*.*0625 â€“ the value reached after *L* =
    200 algorithm iterations when the algorithm is run with the initial value of mutation
    rate *Î±* = 1*.*0 and exponential decay factor *Î²* = 0*.*013863.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹å˜å¼‚ç‡æœ€ä½³é€‰æ‹©çš„ç›´è§‰è¡¨æ˜ï¼Œå¼€å§‹æ—¶ä½¿ç”¨ä¸€ä¸ªéå¸¸å¤§çš„å˜å¼‚ç‡åº”è¯¥æ˜¯æœ‰ç›Šçš„ï¼Œä»¥ä¾¿å°½å¯èƒ½å¹¿æ³›åœ°æ¢ç´¢æœç´¢ç©ºé—´ï¼ˆâ€œæ¢ç´¢â€é˜¶æ®µï¼‰ã€‚ç„¶åï¼Œéšç€ç®—æ³•é€æ­¥æ‰¾åˆ°æ›´å¥½çš„è§£å†³æ–¹æ¡ˆï¼Œå‡å°‘å˜å¼‚ç‡åº”è¯¥ä¼šæ›´æœ‰åˆ©ï¼Œä»¥ä¾¿åœ¨æ‰¾åˆ°çš„æœ€ä½³è§£å†³æ–¹æ¡ˆé™„è¿‘è¿›è¡Œæ›´è¯¦ç»†çš„æœç´¢ï¼ˆâ€œå¼€å‘â€é˜¶æ®µï¼‰ã€‚éšç€ç®—æ³•çš„æ”¶æ•›ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›é€šè¿‡ä»…å˜å¼‚ä¸€ä¸ªæˆ–ä¸¤ä¸ªå‚æ•°æ¥è¿›è¡Œè¶Šæ¥è¶Šç²¾ç»†çš„æœç´¢ã€‚å›¾
    [9.8](#9.8)ï¼ˆå³å›¾ï¼‰æ˜¾ç¤ºäº†è¿™ä¸€ç‚¹ã€‚è¿™é‡Œï¼Œå˜å¼‚ç‡çš„æœ€å¤§å€¼ä¸º *Î±* = 1*.*0ï¼Œæœ€å°å€¼ä¸º *Î±* = 0*.*0625â€”â€”è¿™æ˜¯åœ¨å˜å¼‚ç‡åˆå§‹å€¼ *Î±*
    = 1*.*0 å’ŒæŒ‡æ•°è¡°å‡å› å­ *Î²* = 0*.*013863 çš„æ¡ä»¶ä¸‹ï¼Œç®—æ³•ç»è¿‡ *L* = 200 æ¬¡è¿­ä»£åè¾¾åˆ°çš„å€¼ã€‚
- en: '![FigureÂ 9.8: Left: GA convergence as a function of fixed gate type. Right:
    GA convergence as a function of mutation rate forÂ CX fixed gates. Dots indicate
    mean values and error bars indicate the 10th and the 90th percentiles. GA parameters:
    N = 1000, M = 25, m = 7, 20 GA runs. ](img/file838.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 9.8ï¼šå·¦å›¾ï¼šGA æ”¶æ•›æ€§ä¸å›ºå®šé—¨ç±»å‹çš„å…³ç³»ã€‚å³å›¾ï¼šGA æ”¶æ•›æ€§ä¸ CX å›ºå®šé—¨çš„å˜å¼‚ç‡å…³ç³»ã€‚åœ†ç‚¹è¡¨ç¤ºå‡å€¼ï¼Œè¯¯å·®æ¡è¡¨ç¤ºç¬¬ 10 å’Œç¬¬ 90 ç™¾åˆ†ä½æ•°ã€‚GA
    å‚æ•°ï¼šN = 1000ï¼ŒM = 25ï¼Œm = 7ï¼Œè¿›è¡Œ 20 æ¬¡ GA è¿è¡Œã€‚](img/file838.jpg)'
- en: 'FigureÂ 9.8: Left: GA convergence as a function of fixed gate type. Right: GA
    convergence as a function of mutation rate forÂ CX fixed gates. Dots indicate mean
    values and error bars indicate the 10th and the 90th percentiles. GA parameters:
    N = 1000, M = 25, m = 7, 20 GA runs.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.8ï¼šå·¦å›¾ï¼šGA æ”¶æ•›æ€§ä¸å›ºå®šé—¨ç±»å‹çš„å…³ç³»ã€‚å³å›¾ï¼šGA æ”¶æ•›æ€§ä¸ CX å›ºå®šé—¨çš„å˜å¼‚ç‡å…³ç³»ã€‚åœ†ç‚¹è¡¨ç¤ºå‡å€¼ï¼Œè¯¯å·®æ¡è¡¨ç¤ºç¬¬ 10 å’Œç¬¬ 90 ç™¾åˆ†ä½æ•°ã€‚GA
    å‚æ•°ï¼šN = 1000ï¼ŒM = 25ï¼Œm = 7ï¼Œè¿›è¡Œ 20 æ¬¡ GA è¿è¡Œã€‚
- en: Finally, we need to investigate the convergence of the algorithm as a function
    of the rotation angle discretisation scheme. In principle, an arbitrary rotation
    poses a problem as it must be approximated by a sequence of discrete gates because
    only discrete sets of gates can be implemented fault-tolerantlyÂ Â [[180](Biblography.xhtml#XKudrow2013)].
    Since a GA operates on a discrete set of rotation angles, we face a trade-off
    between higher accuracy achieved through a finer discretisation scheme and implementation
    efficiency in the case of a less granular set of rotation angles. Additionally,
    all rotation gates can be executed with finite precision and the discretisation
    scheme should take this into account. Hence, in order to facilitate the efficient
    implementation of the rotation gates R[X](*ğœƒ*) and R[Z](*ğœƒ*), the GA operates
    on the rotation angles *ğœƒ* that take discrete values (âˆ’*Ï€* + *Î½Ï€âˆ•*2^(mâˆ’1))[Î½=0,â€¦,2^mâˆ’1],
    thus splitting the [âˆ’*Ï€,Ï€*] interval intoÂ 2^m equal subintervals.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬éœ€è¦ç ”ç©¶ç®—æ³•çš„æ”¶æ•›æ€§ï¼Œä½œä¸ºæ—‹è½¬è§’åº¦ç¦»æ•£åŒ–æ–¹æ¡ˆçš„å‡½æ•°ã€‚ä»åŸåˆ™ä¸Šè®²ï¼Œä»»æ„æ—‹è½¬éƒ½ä¼šå¸¦æ¥é—®é¢˜ï¼Œå› ä¸ºå®ƒå¿…é¡»é€šè¿‡ä¸€ç³»åˆ—ç¦»æ•£é—¨è¿›è¡Œé€¼è¿‘ï¼Œå› ä¸ºåªæœ‰ç¦»æ•£çš„é—¨é›†åˆæ‰èƒ½å¤Ÿå®¹é”™å®ç°
    [[180](Biblography.xhtml#XKudrow2013)]ã€‚ç”±äºé—ä¼ ç®—æ³•ï¼ˆGAï¼‰åœ¨ç¦»æ•£çš„æ—‹è½¬è§’åº¦é›†åˆä¸Šè¿è¡Œï¼Œæˆ‘ä»¬é¢ä¸´ä¸€ä¸ªæŠ˜è¡·é—®é¢˜ï¼Œå³é€šè¿‡æ›´ç²¾ç»†çš„ç¦»æ•£åŒ–æ–¹æ¡ˆå®ç°æ›´é«˜çš„ç²¾åº¦ï¼Œè¿˜æ˜¯åœ¨è¾ƒä¸ç²¾ç»†çš„æ—‹è½¬è§’åº¦é›†åˆä¸‹æé«˜å®ç°æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰æ—‹è½¬é—¨éƒ½å¯ä»¥åœ¨æœ‰é™ç²¾åº¦ä¸‹æ‰§è¡Œï¼Œç¦»æ•£åŒ–æ–¹æ¡ˆåº”è¯¥è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ã€‚å› æ­¤ï¼Œä¸ºäº†ä¿ƒè¿›æ—‹è½¬é—¨
    R[X](*ğœƒ*) å’Œ R[Z](*ğœƒ*) çš„é«˜æ•ˆå®ç°ï¼ŒGA åœ¨å–ç¦»æ•£å€¼çš„æ—‹è½¬è§’åº¦ *ğœƒ* ä¸Šè¿è¡Œï¼Œè¿™äº›å€¼ä¸º (âˆ’*Ï€* + *Î½Ï€âˆ•*2^(mâˆ’1))[Î½=0,â€¦,2^mâˆ’1]ï¼Œä»è€Œå°†
    [âˆ’*Ï€,Ï€*] åŒºé—´åˆ†æˆ 2^m ä¸ªç›¸ç­‰çš„å­åŒºé—´ã€‚
- en: 'Therefore, we must answer the question of GA convergence for various values
    of *m*. FigureÂ [9.9](#9.9) shows the minimum values of the objective functionÂ ([9.3.2](#x1-1940002))
    as a function of the number of algorithm iterations for three different values
    of *m*:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»å›ç­”å…³äº GA æ”¶æ•›æ€§çš„é—®é¢˜ï¼Œé’ˆå¯¹ä¸åŒçš„ *m* å€¼ã€‚å›¾ [9.9](#9.9) æ˜¾ç¤ºäº†ç›®æ ‡å‡½æ•°çš„æœ€å°å€¼ï¼ˆ[9.3.2](#x1-1940002)ï¼‰ä½œä¸ºç®—æ³•è¿­ä»£æ¬¡æ•°çš„å‡½æ•°ï¼Œå¯¹äºä¸‰ç§ä¸åŒçš„
    *m* å€¼ï¼š
- en: '*m* = 3, rotation angle step Î”*ğœƒ* = *Ï€âˆ•*4;'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m* = 3ï¼Œæ—‹è½¬è§’åº¦æ­¥é•¿ Î”*ğœƒ* = *Ï€âˆ•*4ï¼›'
- en: '*m* = 5, rotation angle step Î”*ğœƒ* = *Ï€âˆ•*16;'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m* = 5ï¼Œæ—‹è½¬è§’åº¦æ­¥é•¿ Î”*ğœƒ* = *Ï€âˆ•*16ï¼›'
- en: '*m* = 7, rotation angle step Î”*ğœƒ* = *Ï€âˆ•*64.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m* = 7ï¼Œæ—‹è½¬è§’åº¦æ­¥é•¿ Î”*ğœƒ* = *Ï€âˆ•*64ã€‚'
- en: We can see that the GA performance improves only marginally for *m >* 5\. This
    is good news suggesting that it may be sufficient to operate with rotation angle
    step Î”*ğœƒ* = *Ï€âˆ•*16 to achieve the desired precision in learning the target distribution.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå½“ *m >* 5 æ—¶ï¼ŒGA çš„æ€§èƒ½ä»…æœ‰è½»å¾®æå‡ã€‚è¿™æ˜¯ä¸ªå¥½æ¶ˆæ¯ï¼Œè¡¨æ˜ä½¿ç”¨æ—‹è½¬è§’åº¦æ­¥é•¿ Î”*ğœƒ* = *Ï€âˆ•*16 å°±è¶³ä»¥åœ¨å­¦ä¹ ç›®æ ‡åˆ†å¸ƒæ—¶è¾¾åˆ°æœŸæœ›çš„ç²¾åº¦ã€‚
- en: '![FigureÂ 9.9: GA convergence as a function of the rotation angle discretisation
    scheme forÂ CX fixed gates and exponentially decreasing mutation rate. Dots indicate
    mean values and error bars indicate the 10th and the 90th percentiles. GA parameters:
    N = 1000, M = 25, Î± = 1.0, Î² = 0.013863, 20Â GA runs. ](img/file839.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 9.9ï¼šGA æ”¶æ•›æ€§ä½œä¸ºæ—‹è½¬è§’åº¦ç¦»æ•£åŒ–æ–¹æ¡ˆçš„å‡½æ•°ï¼Œé’ˆå¯¹å›ºå®šçš„ CX é—¨å’ŒæŒ‡æ•°ä¸‹é™çš„çªå˜ç‡ã€‚ç‚¹è¡¨ç¤ºå¹³å‡å€¼ï¼Œè¯¯å·®æ¡è¡¨ç¤º 10th å’Œ 90th ç™¾åˆ†ä½æ•°ã€‚GA
    å‚æ•°ï¼šN = 1000ï¼ŒM = 25ï¼ŒÎ± = 1.0ï¼ŒÎ² = 0.013863ï¼Œè¿›è¡Œ 20 æ¬¡ GA è¿è¡Œã€‚](img/file839.jpg)'
- en: 'FigureÂ 9.9: GA convergence as a function of the rotation angle discretisation
    scheme forÂ CX fixed gates and exponentially decreasing mutation rate. Dots indicate
    mean values and error bars indicate the 10th and the 90th percentiles. GA parameters:
    N = 1000, M = 25, Î± = 1.0, Î² = 0.013863, 20Â GA runs.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.9ï¼šGA æ”¶æ•›æ€§ä½œä¸ºæ—‹è½¬è§’åº¦ç¦»æ•£åŒ–æ–¹æ¡ˆçš„å‡½æ•°ï¼Œé’ˆå¯¹å›ºå®šçš„ CX é—¨å’ŒæŒ‡æ•°ä¸‹é™çš„çªå˜ç‡ã€‚ç‚¹è¡¨ç¤ºå¹³å‡å€¼ï¼Œè¯¯å·®æ¡è¡¨ç¤º 10th å’Œ 90th ç™¾åˆ†ä½æ•°ã€‚GA
    å‚æ•°ï¼šN = 1000ï¼ŒM = 25ï¼ŒÎ± = 1.0ï¼ŒÎ² = 0.013863ï¼Œè¿›è¡Œ 20 æ¬¡ GA è¿è¡Œã€‚
- en: The non-differentiable learning of QCBM with Genetic Algorithm is a viable approach
    to the training of PQCs. QCBM trained with GA performs at least as well as an
    equivalent classical neural network (RBM). The performance of QCBM and its classical
    counterpart were tested on two different datasets (heavy-tail distributed samples
    derived from the financial time series and light-tail distributed samples drawn
    from the specially constructed distribution with spiky pdf) and in both cases
    QCBM demonstrated its ability to learn the empirical distribution and generate
    new synthetic samples that have the same statistical properties as the original
    ones, as can be seen in the pdf and QQ-plots.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é—ä¼ ç®—æ³•ï¼ˆGAï¼‰è¿›è¡Œ QCBM çš„éå¯å¾®å­¦ä¹ æ˜¯ä¸€ç§å¯è¡Œçš„ PQC è®­ç»ƒæ–¹æ³•ã€‚ç”¨ GA è®­ç»ƒçš„ QCBM è‡³å°‘èƒ½å¤Ÿä¸ç­‰æ•ˆçš„ç»å…¸ç¥ç»ç½‘ç»œï¼ˆRBMï¼‰è¡¨ç°ç›¸å½“ã€‚QCBM
    å’Œå…¶ç»å…¸å¯¹æ¯”æ¨¡å‹åœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼ˆåˆ†åˆ«æ¥æºäºé‡‘èæ—¶é—´åºåˆ—çš„é‡å°¾åˆ†å¸ƒæ ·æœ¬å’Œä»ç‰¹åˆ«æ„é€ çš„åˆ†å¸ƒä¸­æå–çš„è½»å°¾åˆ†å¸ƒæ ·æœ¬ï¼Œå¹¶ä¸”å…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºå°–å³°å‹ï¼‰ï¼Œåœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼ŒQCBM
    éƒ½å±•ç¤ºäº†å®ƒå­¦ä¹ ç»éªŒåˆ†å¸ƒå¹¶ç”Ÿæˆæ–°çš„åˆæˆæ ·æœ¬çš„èƒ½åŠ›ï¼Œè¿™äº›æ–°æ ·æœ¬å…·æœ‰ä¸åŸå§‹æ ·æœ¬ç›¸åŒçš„ç»Ÿè®¡ç‰¹æ€§ï¼Œå¦‚æ¦‚ç‡å¯†åº¦å‡½æ•°å’Œ QQ å›¾æ‰€ç¤ºã€‚
- en: Analysing the GA convergence for different sets of hyperparameters, we observe
    that the best results were achieved withÂ CX fixed gates and an exponentially decreasing
    mutation rate (starting from the maximum value of the mutation rate and setting
    the decay rate at a reasonably small value). More importantly, we see that more
    granular rotation angle discretisation schemes provide progressively less incremental
    value beyond some point. This means that for many practical purposes it is sufficient
    to implement rotations with the step Î”*ğœƒ* = *Ï€âˆ•*16 in order to encode target distribution
    with the desired accuracy for deep enough QCBM architectures (at least two layers
    of fixed 2-qubit gates). Since qubit rotations on NISQ devices can be implemented
    with finite precision, this ensures that QCBMs can be used productively for many
    real-world use cases.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†æä¸åŒè¶…å‚æ•°é›†çš„é—ä¼ ç®—æ³•æ”¶æ•›æ€§æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æœ€ä½³ç»“æœæ˜¯åœ¨CXå›ºå®šé—¨å’ŒæŒ‡æ•°è¡°å‡çš„å˜å¼‚ç‡ä¸‹å®ç°çš„ï¼ˆä»å˜å¼‚ç‡çš„æœ€å¤§å€¼å¼€å§‹ï¼Œå¹¶å°†è¡°å‡ç‡è®¾ç½®ä¸ºä¸€ä¸ªåˆç†çš„å°å€¼ï¼‰ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ï¼Œæ›´ç»†ç²’åº¦çš„æ—‹è½¬è§’åº¦ç¦»æ•£åŒ–æ–¹æ¡ˆåœ¨æŸä¸ªä¸´ç•Œç‚¹ä¹‹åæä¾›çš„å¢é‡ä»·å€¼é€æ¸å‡å°ã€‚è¿™æ„å‘³ç€ï¼Œå¯¹äºè®¸å¤šå®é™…åº”ç”¨è€Œè¨€ï¼Œå®ç°Î”*ğœƒ*
    = *Ï€âˆ•*16çš„æ—‹è½¬å³å¯ï¼Œåœ¨è¶³å¤Ÿæ·±çš„QCBMæ¶æ„ä¸­ï¼ˆè‡³å°‘ä¸¤å±‚å›ºå®š2é‡å­æ¯”ç‰¹é—¨ï¼‰å°±èƒ½ä»¥æ‰€éœ€çš„å‡†ç¡®åº¦ç¼–ç ç›®æ ‡åˆ†å¸ƒã€‚ç”±äºåœ¨NISQè®¾å¤‡ä¸Šé‡å­æ¯”ç‰¹æ—‹è½¬å¯ä»¥å®ç°æœ‰é™ç²¾åº¦ï¼Œè¿™ç¡®ä¿äº†QCBMå¯ä»¥åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­æœ‰æ•ˆä½¿ç”¨ã€‚
- en: QCBM is a viable choice for building market generators. It performs at least
    as well as its classical counterpart, RBM, and demonstrates potential for achieving
    quantum advantage on near-term quantum processors.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: QCBMæ˜¯æ„å»ºå¸‚åœºç”Ÿæˆå™¨çš„å¯è¡Œé€‰æ‹©ã€‚å®ƒçš„è¡¨ç°è‡³å°‘ä¸å…¶ç»å…¸å¯¹æ‰‹RBMç›¸å½“ï¼Œå¹¶å±•ç¤ºäº†åœ¨è¿‘ç«¯é‡å­å¤„ç†å™¨ä¸Šå®ç°é‡å­ä¼˜åŠ¿çš„æ½œåŠ›ã€‚
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: In this chapter, we learned how to construct and train a generative QML model
    â€“ Quantum Circuit Born Machine. We started with the general concept of a PQC as
    a generative model, where the readout operation produces a sample from the probability
    distribution encoded in the PQC parameters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•æ„å»ºå’Œè®­ç»ƒä¸€ä¸ªç”Ÿæˆå‹QMLæ¨¡å‹â€”â€”é‡å­ç”µè·¯å‡ºç”Ÿæœºã€‚æˆ‘ä»¬ä»PQCä½œä¸ºç”Ÿæˆæ¨¡å‹çš„æ€»ä½“æ¦‚å¿µå¼€å§‹ï¼Œåœ¨è¯¥æ¨¡å‹ä¸­ï¼Œè¯»å–æ“ä½œä»PQCå‚æ•°ä¸­ç¼–ç çš„æ¦‚ç‡åˆ†å¸ƒä¸­ç”Ÿæˆä¸€ä¸ªæ ·æœ¬ã€‚
- en: Next, we introduced the concept of a hardware-efficient PQC ansatz. Additionally,
    to build a model that is compatible with QPU connectivity and can easily be embedded
    into a QPU graph, we tried to use adjustable (one-qubit) and fixed (two-qubit)
    gates from the set of the native quantum gates for the given system.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç¡¬ä»¶é«˜æ•ˆPQCå‡è®¾çš„æ¦‚å¿µã€‚æ­¤å¤–ï¼Œä¸ºäº†æ„å»ºä¸€ä¸ªä¸QPUè¿æ¥æ€§å…¼å®¹å¹¶ä¸”èƒ½è½»æ¾åµŒå…¥QPUå›¾çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°è¯•ä½¿ç”¨ç»™å®šç³»ç»Ÿçš„åŸç”Ÿé‡å­é—¨é›†ä¸­çš„å¯è°ƒï¼ˆå•é‡å­æ¯”ç‰¹ï¼‰å’Œå›ºå®šï¼ˆåŒé‡å­æ¯”ç‰¹ï¼‰é—¨ã€‚
- en: Then, we studied differentiable and non-differentiable learning algorithms and
    experimented with the QCBM trained using Genetic Algorithm. Comparison with the
    classical benchmark (RBM) demonstrated a realistic possibility of quantum advantage
    for generative QML models.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¯å¾®åˆ†å’Œä¸å¯å¾®åˆ†çš„å­¦ä¹ ç®—æ³•ï¼Œå¹¶è¿›è¡Œäº†ä½¿ç”¨é—ä¼ ç®—æ³•è®­ç»ƒçš„QCBMå®éªŒã€‚ä¸ç»å…¸åŸºå‡†ï¼ˆRBMï¼‰çš„æ¯”è¾ƒå±•ç¤ºäº†é‡å­ç”ŸæˆQMLæ¨¡å‹å®ç°é‡å­ä¼˜åŠ¿çš„ç°å®å¯èƒ½æ€§ã€‚
- en: Finally, we explored the question of training algorithm convergence for various
    sets of model parameters.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†ä¸åŒæ¨¡å‹å‚æ•°é›†çš„è®­ç»ƒç®—æ³•æ”¶æ•›æ€§é—®é¢˜ã€‚
- en: In the next chapter, we will study another important and exceptionally promising
    QML model â€“ Variational Quantum Eigensolver.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶å¦ä¸€ç§é‡è¦ä¸”æå…·æ½œåŠ›çš„QMLæ¨¡å‹â€”â€”å˜åˆ†é‡å­ç‰¹å¾æ±‚è§£å™¨ã€‚
- en: Join our bookâ€™s Discord space
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬ä¹¦ç±çš„Discordç©ºé—´
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/quantum](https://packt.link/quantum)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬çš„Discordç¤¾åŒºï¼Œä¸å¿—åŒé“åˆçš„äººäº¤æµï¼Œå¹¶ä¸è¶…è¿‡2000åæˆå‘˜ä¸€èµ·å­¦ä¹ ï¼Œç½‘å€ï¼š[https://packt.link/quantum](https://packt.link/quantum)
- en: '![PIC](img/file1.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
