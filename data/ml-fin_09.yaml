- en: Chapter 9. Fighting Bias
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第九章：反对偏见
- en: 'We like to think that machines are more rational than us: heartless silicon
    applying cold logic. Thus, when computer science introduced automated decision
    making into the economy, many hoped that computers would reduce prejudice and
    discrimination. Yet, as we mentioned earlier when looking at mortgage applications
    and ethnicity, computers are made and trained by humans, and the data that those
    machines use stems from an unjust world. Simply put, if we are not careful, our programs
    will amplify human biases.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们喜欢认为机器比我们更理性：冷酷的硅芯片应用冷逻辑。因此，当计算机科学将自动决策引入经济时，许多人希望计算机会减少偏见和歧视。然而，正如我们在查看抵押贷款申请和种族时提到的，计算机是由人类制造和训练的，而这些机器使用的数据来自一个不公正的世界。简单来说，如果我们不小心，我们的程序将放大人类的偏见。
- en: In the financial industry, anti-discrimination is not only a matter of morality.
    Take, for instance, the **Equal Credit Opportunity Act** (**ECOA**), which came
    into force in 1974 in the United States. This law explicitly forbids creditors
    from discriminating applicants based on race, sex, marital status, and several
    other attributes. It also requires creditors to inform applicants about the reasons
    for denial.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融行业，反歧视不仅是道德问题。例如，1974年在美国生效的**平等信贷机会法**（**ECOA**）明确禁止贷方根据种族、性别、婚姻状况和其他几个属性歧视申请人。它还要求贷方告知申请人拒绝的理由。
- en: The algorithms discussed in this book are discrimination machines. Given an
    objective, these machines will find the features that it’s best to discriminate
    on. Yet, as we’ve discussed discrimination is not always okay.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本书讨论的算法是歧视机器。给定一个目标，这些机器会找到最适合进行歧视的特征。然而，正如我们所讨论的，歧视并不总是可以接受的。
- en: While it's okay to target ads for books from a certain country to people who
    are also from that country, it's usually not okay, and thanks to the ECOA, often
    illegal, to deny a loan to people from a certain country. Within the financial
    domain, there are much stricter rules for discrimination than those seen in book
    sales. This is because decisions in the financial domain have a much more severe
    impact on people's lives than those of book sales.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然将某个国家的书籍广告定向投放给同样来自该国的人是可以的，但通常情况下，根据ECOA，这样做往往不合法，拒绝来自某个国家的人贷款通常是不允许的。在金融领域，比在图书销售中看到的歧视规则要严格得多。这是因为金融领域的决策对人们生活的影响远比图书销售更为严峻。
- en: Equally, discrimination in this context is **feature specific**. For example,
    while it's okay to discriminate against loan applicants based on their history
    of repaying loans, it's not okay to do so based on their country of origin, unless
    there are sanctions against that country or similar overarching laws in place.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在这种情况下，歧视是**特征特定的**。例如，虽然可以根据贷款申请人还款历史进行歧视，但如果仅根据其原籍国进行歧视，则不应该这样做，除非该国受到制裁或有类似的总体法律规定。
- en: 'Throughout this chapter, we''ll discuss the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下内容：
- en: Where bias in machines comes from
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏见在机器中的来源
- en: The legal implications of biased **machine learning** (**ML**) models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏见的法律影响：**机器学习**（**ML**）模型
- en: How observed unfairness can be reduced
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何减少观察到的不公平
- en: How models can be inspected for bias and unfairness
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何检查模型中的偏见和不公平
- en: How causal modeling can reduce bias
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果建模如何减少偏见
- en: How unfairness is a complex systems failure that needs to be addressed in non-technical
    ways
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何理解不公平是一个复杂的系统性失败，且需要通过非技术手段来解决
- en: The algorithms discussed in this book are feature extraction algorithms. Even
    if regulated features are omitted, an algorithm might infer them from proxy features
    and then discriminate based on them anyway. As an example of this, ZIP codes can
    be used to predict race reasonably well in many cities in the United States. Therefore,
    omitting regulated features is not enough when it comes to combating bias.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本书讨论的算法是特征提取算法。即使忽略了受管制的特征，算法也可能通过代理特征推断这些特征，并基于它们进行歧视。以美国许多城市为例，邮政编码可以合理地预测种族。因此，单单省略受管制的特征在应对偏见时是不够的。
- en: Sources of unfairness in machine learning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的不公平来源
- en: As we have discussed many times throughout this book, models are a function
    of the data that they are trained on. Generally speaking, more data will lead
    to smaller errors. So, by definition, there is less data on minority groups, simply
    because there are fewer people in those groups.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中多次讨论过的那样，模型是它们所训练数据的函数。一般来说，更多的数据会导致更小的误差。因此，按定义，少数群体的数据较少，因为这些群体的人数较少。
- en: This **disparate sample size** can lead to worse model performance for the minority
    group. As a result, this increased error is often known as a **systematic error**.
    The model might have to overfit the majority group data so that the relationships
    it found do not apply to the minority group data. Since there is little minority
    group data, this is not punished as much.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种**不均衡的样本量**可能会导致少数群体的模型表现更差。因此，这种增加的误差通常被称为**系统性误差**。模型可能不得不对多数群体数据进行过度拟合，以至于它找到的关系不适用于少数群体的数据。由于少数群体的数据很少，这种情况的惩罚并不严重。
- en: Imagine you are training a credit scoring model, and the clear majority of your
    data comes from people living in lower Manhattan, and a small minority of it comes
    from people living in rural areas. Manhattan housing is much more expensive, so
    the model might learn that you need a very high income to buy an apartment. However,
    rural housing is much cheaper in comparison. Even so, because the model is largely
    trained on data from Manhattan, it might deny loan applications to rural applicants
    because they tend to have lower incomes than their Manhattan peers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在训练一个信用评分模型，并且你大部分数据来自居住在下曼哈顿的人，而只有少部分数据来自生活在农村地区的人。曼哈顿的住房价格要贵得多，因此模型可能会学习到购买公寓需要非常高的收入。然而，相比之下，农村地区的住房价格要便宜得多。即便如此，由于模型大部分是基于曼哈顿的数据进行训练的，它可能会拒绝农村申请者的贷款申请，因为他们的收入通常低于曼哈顿的同龄人。
- en: Aside from sample size issues, our data can be biased by itself. For example,
    "raw data" does not exist. Data does not appear naturally, instead it's measured
    by humans using human-made measurement protocols, which in themselves can be biased
    in many different ways.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了样本量问题外，我们的数据本身也可能存在偏见。例如，“原始数据”并不存在。数据并不是自然产生的，而是由人类使用人工测量协议进行测量的，这些测量协议本身可能以多种方式存在偏见。
- en: Biases could include having **sampling biases**, such as in the Manhattan housing
    example, or having **measurement biases**, which is when your sample might not
    measure what it is intended to measure, or may even discriminate against one group.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见可能包括**抽样偏见**，就像曼哈顿住房例子中的情况，或**测量偏见**，即你的样本可能无法准确测量它所要测量的内容，甚至可能对某一群体产生歧视。
- en: Another bias that's possible is **pre-existing social biases**. These are visible
    in word vectors, for instance, in Word2Vec, where the mapping from father to doctor
    in latent space maps from mother to nurse. Likewise, the vector from man to computer
    programmer maps from woman to homemaker. This is because sexism is encoded within
    the written language of our sexist society. Until today, typically speaking doctors
    have usually been men and nurses have usually been women. Likewise, tech companies'
    diversity statistics reveal that far more men are computer programmers than women,
    and these biases get encoded into models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能的偏见是**既存的社会偏见**。这些偏见可以在词向量中显现出来，例如在Word2Vec中，从父亲到医生的映射与从母亲到护士的映射相似。同样，从男性到计算机程序员的向量，也映射到女性到家庭主妇。这是因为性别歧视已经深深烙印在我们性别歧视的社会中。直到今天，通常情况下，医生多为男性，护士多为女性。同样，科技公司的人才多样性统计显示，计算机程序员中男性远多于女性，这些偏见也被编码进了模型中。
- en: Legal perspectives
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 法律视角
- en: 'There are two doctrines in anti-discrimination law: *disparate treatment*,
    and *disparate impact*. Let''s take a minute to look at each of these:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 反歧视法中有两个原则：*不均等对待*和*不均等影响*。我们花一点时间来看看这两个概念：
- en: '**Disparate treatment**: This is one kind of unlawful discrimination. Intentionally
    discriminating against ZIP codes with the hope of discriminating against race
    is not legal. Disparate treatment problems have less to do with the algorithm
    and more to do with the organization running it.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不均等对待**：这是一种非法歧视。故意歧视特定邮政编码区域，以期歧视某一族群，这是不合法的。不均等对待问题更多的是与运行算法的组织有关，而非算法本身。'
- en: '**Disparate impact**: This can be a problem if an algorithm is deployed that
    has a different impact on different groups, even without the organization knowing
    about it. Let''s walk through a lending scenario in which disparate impact could
    be a problem. Firstly, the plaintiff must establish that there is a disparate
    impact. Assessing if there''s a disparate impact is usually done with the **four-fifths
    rule**, which says that if the selection rate of a group is less than 80% of the
    group, then it is regarded as evidence of adverse impact. If a lender has 150
    loan applicants from group A, of which 100, or 67%, are accepted, and 50 applicants
    from group B, of which 25 are accepted, the difference in selection is 0.5/0.67
    = 0.746, which qualifies as evidence for discrimination against group B. The defendant
    can counter this by showing that the decision procedure is justified as necessary.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不平等影响**：如果部署的算法对不同群体产生不同的影响，即使组织并不知情，这也可能成为一个问题。让我们以一个借贷场景为例，看看不平等影响如何成为问题。首先，原告必须证明存在不平等影响。通常，评估是否存在不平等影响是通过**四分之五规则**来进行的，该规则指出，如果一个群体的选择率低于其他群体的80%，则被视为不利影响的证据。如果某贷款人有150名来自A群体的贷款申请者，其中100人被接受，比例为67%，以及50名来自B群体的申请者，其中25人被接受，则选择差异为0.5/0.67
    = 0.746，这构成了对B群体歧视的证据。被告可以通过证明该决策程序是必要的，从而对此作出反驳。'
- en: After this is done, the plaintiff has the opportunity to show that the goal
    of the procedure could also be achieved with a different procedure that shows
    a smaller disparity.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完成此步骤后，原告有机会证明，通过采用另一种可以展示更小差距的程序，也能够实现该程序的目标。
- en: Note
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: For a more in-depth overview of these topics, see Moritz Hardt''s
    2017 NeurIPS presentation on the topic at [http://mrtz.org/nips17/#/11](http://mrtz.org/nips17/#/11).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：欲了解更深入的相关内容，请参见Moritz Hardt 2017年在[http://mrtz.org/nips17/#/11](http://mrtz.org/nips17/#/11)的NeurIPS演讲。'
- en: The disparate treatment doctrine tries to achieve procedural fairness and equal
    opportunity. The disparate impact doctrine aims for distributive justice and minimized
    inequality in outcomes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不平等待遇原则旨在实现程序公平和机会平等。不同影响原则则旨在追求分配公正并最小化结果的不平等。
- en: There is an intrinsic tension between the two doctrines, as illustrated by the
    Ricci V. DeStefano case from 2009\. In this case, 19 white firefighters and 1
    Hispanic firefighter sued their employer, the New Haven Fire Department. The firefighters
    had all passed their test for promotion, yet their black colleagues did not score
    the mark required for the promotion. Fearing a disparate impact lawsuit, the city
    invalidated the test results and did not promote the firefighters. Because the
    evidence for disparate impact was not strong enough, the Supreme Court of the
    United States eventually ruled that the firefighters should have been promoted.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种原则之间存在内在的张力，正如2009年Ricci V. DeStefano案件所展示的那样。在这个案件中，19名白人消防员和1名西班牙裔消防员起诉了他们的雇主——新哈文消防局。这些消防员都通过了晋升测试，但他们的黑人同事未达到晋升所需的分数。担心会被起诉为存在不平等影响，城市当局宣布无效这些测试结果，并未晋升这些消防员。由于不平等影响的证据不足，美国最高法院最终裁定，这些消防员应该被晋升。
- en: Given the complex legal and technical situation around fairness in machine learning,
    we're going to dive into how we can define and quantify fairness, before using
    this insight to create fairer models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于机器学习中的公平性面临着复杂的法律和技术情况，我们将深入探讨如何定义和量化公平性，并利用这一见解来构建更公平的模型。
- en: Observational fairness
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察公平性
- en: Equality is often seen as a purely qualitative issue, and as such, it's often
    dismissed by quantitative-minded modelers. As this section will show, equality
    can be seen from a quantitative perspective, too. Consider a classifier, *c,*
    with input *X*, some sensitive input, *A*, a target, *Y* and output *C*. Usually,
    we would denote the classifier output as ![Observational fairness](img/B10354_09_001.jpg),
    but for readability, we follow CS 294 and name it *C*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 平等常被视为一个纯粹的定性问题，因此通常会被定量思维的建模者忽视。正如本节所展示的，平等也可以从定量角度来看待。考虑一个分类器，*c*，其输入为*X*，某些敏感输入为*A*，目标为*Y*，输出为*C*。通常情况下，我们会将分类器输出表示为![Observational
    fairness](img/B10354_09_001.jpg)，但为了可读性，我们遵循CS 294，并将其命名为*C*。
- en: 'Let''s say that our classifier is being used to decide who gets a loan. When
    would we consider this classifier to be fair and free of bias? To answer this
    question, picture two demographics, group A and B, both loan applicants. Given
    a credit score, our classifier must find a cutoff point. Let''s look at the distribution
    of applicants in this graph:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的分类器用于决定谁能获得贷款。我们何时可以认为这个分类器是公平的且没有偏见的呢？为了回答这个问题，我们可以想象两个人群，A组和B组，都是贷款申请人。根据信用评分，我们的分类器必须找到一个截止点。我们来看一下申请人的分布情况：
- en: Note
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: The data for this example is synthetic; you can find the Excel file
    used for these calculations in the GitHub repository of this book, [https://github.com/PacktPublishing/Machine-Learning-for-Finance/blob/master/9.1_parity.xlsx](https://github.com/PacktPublishing/Machine-Learning-for-Finance/blob/master/9.1_parity.xlsx).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：本示例的数据是合成的；你可以在本书的GitHub仓库中找到用于这些计算的Excel文件，[https://github.com/PacktPublishing/Machine-Learning-for-Finance/blob/master/9.1_parity.xlsx](https://github.com/PacktPublishing/Machine-Learning-for-Finance/blob/master/9.1_parity.xlsx)。'
- en: '![Observational fairness](img/B10354_09_01.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![观察公平性](img/B10354_09_01.jpg)'
- en: Max profits
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化利润
- en: 'For this exercise, we assume that a successful applicant yields a profit of
    $300, while a defaulting successful applicant costs $700\. The cutoff point here
    has been chosen to maximize profits:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们假设一个成功的申请人可以带来300美元的利润，而一个违约的成功申请人则会造成700美元的损失。这里的截止点被选定为最大化利润：
- en: 'So, what can we see? We can see the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们可以看到什么呢？我们可以看到以下几点：
- en: 'In orange are applicants who would not have repaid the loan and did not get
    accepted: **true negatives** (**TNs**).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 橙色部分是那些本不打算偿还贷款且未获得批准的申请人：**真负例**（**TNs**）。
- en: 'In blue are applicants who would have repaid the loan but did not get accepted:
    **false negatives** (**FNs**).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝色部分是那些本应偿还贷款但未被批准的申请人：**假负例**（**FNs**）。
- en: 'In yellow are applicants who did get the loan but did not pay it back: **false
    positives** (**FPs**).'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄色部分是那些获得了贷款但未偿还的申请人：**假正例**（**FPs**）。
- en: 'In gray are applicants who did receive the loan and paid it back: **true positives**
    (**TPs**).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灰色部分是那些成功获得贷款并且按时偿还的申请人：**真正例**（**TPs**）。
- en: As you can see, there are several issues with this choice of cutoff point. **Group
    B** applicants need to have a better score to get a loan than **Group A** applicants,
    indicating disparate treatment. At the same time, only around 51% of **Group A** applicants
    get a loan but only 37% of **Group B** applicants do, indicating disparate impact.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，选择这个截止点存在若干问题。**B组**申请人需要比**A组**申请人有更高的评分才能获得贷款，这表明存在不同的对待。同时，只有大约51%的**A组**申请人获得贷款，而只有37%的**B组**申请人获得贷款，这表明存在影响差异。
- en: 'A *group unaware threshold*, which we can see below, would give both groups
    the same minimum score:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*不考虑群体的阈值*，如下面所示，会给两个群体相同的最低评分：'
- en: '![Observational fairness](img/B10354_09_02.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![观察公平性](img/B10354_09_02.jpg)'
- en: Equal cutoff
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的截止点
- en: In the preceding graph, while both groups have the same cutoff rate, **Group
    A** has been given fewer loans. At the same time, predictions for **Group A**
    have a lower accuracy than the predictions given for **Group B**. It seems that
    although both groups face the same score threshold, **Group A** is at a disadvantage.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，虽然两个组的截止率相同，但**A组**获得的贷款较少。同时，**A组**的预测准确性低于**B组**的预测准确性。似乎虽然两个组面临相同的评分门槛，但**A组**处于不利位置。
- en: 'Demographic parity aims to achieve fairness by ensuring that both groups have
    the same chance of receiving the loan. This method aims to achieve the same selection
    rate for both groups, which is what impact disparity is measured by. Mathematically,
    this process can be expressed as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 人口平等旨在通过确保两个群体有相同的贷款机会来实现公平。这种方法旨在使两个群体的选择率相同，这也是衡量影响差异的标准。从数学上来说，这一过程可以表示为：
- en: '![Observational fairness](img/B10354_09_002.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![观察公平性](img/B10354_09_002.jpg)'
- en: 'If we apply this rule to the same context as we used previously, we''ll arrive
    at the following cutoff points:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这个规则应用到之前的同一情境中，我们将得到以下的截止点：
- en: '![Observational fairness](img/B10354_09_03.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![观察公平性](img/B10354_09_03.jpg)'
- en: Equal pick rate
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的选择率
- en: While this method cannot be blamed for statistical discrimination and disparate
    impact, it can be blamed for disparate treatment. In the equal pick rate graphic
    we can see how **Group A** is given a lower threshold score; meanwhile, there
    are more successful **Group A** applicants who default on their loans. In fact,
    **Group A** is not profitable and gets subsidized by **Group B**. Accepting a
    worse economic outcome to favor a certain group is also known as taste-based discrimination.
    It could be said that the higher thresholds for **Group B** are unfair, as they
    have a lower FP rate.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法不能被归咎于统计歧视和不同影响，但它可以被归咎于不同待遇。在平等选择率的图示中，我们可以看到**A组**被赋予了一个更低的阈值分数；与此同时，更多的成功的**A组**申请人未能偿还贷款。实际上，**A组**并不盈利，而是由**B组**进行补贴。接受更糟糕的经济结果以偏袒某个群体，这也被称为基于偏好的歧视。可以说，**B组**的较高阈值是不公平的，因为它们的假阳性率较低。
- en: 'TP parity, which is also called equal opportunity, means that both demographics
    have the same TP rate. For people who can pay back the loan, the same chance of
    getting a loan should exist. Mathematically, this can be expressed as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: TP平等，也叫做平等机会，意味着两个群体有相同的TP（真阳性）率。对于能够偿还贷款的人来说，应该有相同的获得贷款的机会。从数学上讲，这可以表示为：
- en: '![Observational fairness](img/B10354_09_003.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![观察公平性](img/B10354_09_003.jpg)'
- en: 'Applied to our data, this policy looks similar to demographic parity, except
    that the group cutoff point is even lower:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 应用到我们的数据中，这一政策看起来类似于人口平等，只是群体的分界点更低：
- en: '![Observational fairness](img/B10354_09_04.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![观察公平性](img/B10354_09_04.jpg)'
- en: Equal opportunity
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 平等机会
- en: Equal opportunity can address many of the problems of demographic parity, as
    most people believe that everyone should be given the same opportunities. Still,
    our classifier is less accurate for **Group A**, and there is a form of disparate
    treatment in place.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 平等机会可以解决人口平等的许多问题，因为大多数人认为每个人都应该有相同的机会。然而，我们的分类器对于**A组**的准确性较低，且存在某种形式的不平等待遇。
- en: 'Accuracy parity tells us that the accuracy of predictions should be the same
    for both groups. Mathematically, this can be expressed as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性平等告诉我们，两个群体的预测准确性应该相同。从数学上讲，这可以表示为：
- en: '![Observational fairness](img/B10354_09_004.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![观察公平性](img/B10354_09_004.jpg)'
- en: 'The probability that the classifier is correct should be the same for the two
    possible values of the sensitive variable *A*. When we apply this criteria to
    our data, we arrive at the following output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器正确的概率对于敏感变量*A*的两个可能值应该是相同的。当我们将这一标准应用于我们的数据时，得到了以下结果：
- en: '![Observational fairness](img/B10354_09_05.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![观察公平性](img/B10354_09_05.jpg)'
- en: Equal accuracy
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 平等准确性
- en: From the preceding diagram, the downside becomes apparent. In order to satisfy
    the accuracy constraint, members of **Group B** are given much easier access to
    loans.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图示可以看出其缺点。为了满足准确性约束，**B组**的成员能够更容易地获得贷款。
- en: 'Therefore to solve this, trade-offs are necessary because no classifier can
    have precision parity, TP parity, and FP parity unless the classifier is perfect.
    *C = Y,* or both demographics have the same base rates:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要解决这一问题，需要进行权衡，因为没有任何分类器能够同时实现精确度平等、TP平等和FP平等，除非分类器是完美的。*C = Y*，即两个群体具有相同的基准率：
- en: '![Observational fairness](img/B10354_09_005.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![观察公平性](img/B10354_09_005.jpg)'
- en: There are many more ways to express fairness. The key takeaway, however, is
    that none of them perfectly satisfies all of the fairness criteria. For any two
    populations with unequal base rates, and unequal chances of repaying their loan,
    establishing statistical parity requires the introduction of a treatment disparity.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表达公平性的方式有很多种。然而，关键的结论是，它们都无法完美地满足所有公平标准。对于任何具有不同基准率和不同贷款偿还机会的两个群体，建立统计平等就需要引入待遇差异。
- en: This fact has led to a number of debates, with the best practice to express
    and eliminate discrimination having not been agreed on yet. With that being said,
    even if the perfect mathematical expression of fairness was found, it would not
    immediately lead to perfectly fair systems.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这一事实引发了许多辩论，目前尚未就表达和消除歧视的最佳实践达成共识。也就是说，即使找到了完美的数学表达式来表示公平，它也不会立即导致完全公平的系统。
- en: Any machine learning algorithm is part of a bigger system. Inputs *X* are often
    not as clearly defined as a different algorithm in the same system that might
    use different inputs. Demographic groups *A* are often not clearly defined or
    inferred. Even the output, *C,* of the classifier can often not be clearly distinguished,
    as many algorithms together might perform the classification task while each algorithm
    is predicting a different output, such as a credit score or a profitability estimate.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习算法都是更大系统的一部分。输入 *X* 通常不像同一系统中可能使用不同输入的其他算法那样明确。人口群体 *A* 往往不明确或无法推断。即使分类器的输出
    *C* 也常常无法明确区分，因为多个算法可能共同执行分类任务，而每个算法预测的输出不同，例如信用评分或盈利估算。
- en: Good technology is not a substitute for good policy. Blindly following an algorithm
    without the opportunity for individual consideration or appeal will always lead
    to unfairness. With that being said, while mathematical fairness criteria cannot
    solve all the fairness issues that we face, it is surely worth trying to make
    machine learning algorithms fairer, which is what the next section is about.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的技术不能替代良好的政策。盲目地遵循一个算法而没有个别考虑或申诉的机会，始终会导致不公平。话虽如此，尽管数学公平性标准不能解决我们面临的所有公平性问题，但尝试让机器学习算法更公平无疑是值得的，这也是下一节的内容。
- en: Training to be fair
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公平训练
- en: There are multiple ways to train models to be fairer. A simple approach could
    be using the different fairness measures that we have listed in the previous section
    as an additional loss. However, in practice, this approach has turned out to have
    several issues, such as having poor performance on the actual classification task.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以训练模型以实现更公平的结果。一种简单的方法是使用我们在前一节中列出的不同公平性度量作为附加损失。然而，在实践中，这种方法已被证明存在一些问题，例如在实际分类任务中的表现较差。
- en: An alternative approach is to use an adversarial network. Back in 2016, Louppe,
    Kagan, and Cranmer published the paper *Learning to Pivot with Adversarial Networks*,
    available at [https://arxiv.org/abs/1611.01046](https://arxiv.org/abs/1611.01046).
    This paper showed how to use an adversarial network to train a classifier to ignore
    a nuisance parameter, such as a sensitive feature.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用对抗网络。早在2016年，Louppe、Kagan 和 Cranmer 发表了论文 *使用对抗网络学习转向*，可以在[https://arxiv.org/abs/1611.01046](https://arxiv.org/abs/1611.01046)找到。这篇论文展示了如何使用对抗网络训练分类器忽略干扰参数，例如敏感特征。
- en: In this example, we will train a classifier to predict whether an adult makes
    over $50,000 in annual income. The challenge here is to make our classifier unbiased
    from the influences of race and gender, with it only focusing on features that
    we can discriminate on, including their occupation and the gains they make from
    their capital.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将训练一个分类器来预测一个成年人是否年收入超过 5 万美元。这里的挑战是让我们的分类器不受种族和性别的影响，只关注我们可以区分的特征，包括他们的职业和资本收益。
- en: 'To this end, we must train a classifier and an adversarial network. The adversarial
    network aims to classify the sensitive attributes, *a*, gender and race, from
    the predictions of the classifier:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们必须训练一个分类器和一个对抗网络。对抗网络旨在从分类器的预测中分类敏感属性，如 *a*、性别和种族：
- en: '![Training to be fair](img/B10354_09_06.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![公平训练](img/B10354_09_06.jpg)'
- en: Making an unbiased classifier to detect the income of an adult
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个无偏的分类器来检测成年人的收入
- en: 'The classifier aims to classify by income but also aims to fool the adversarial
    network. The classifier''s minimization objective formula is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的目标是按收入分类，同时也要欺骗对抗网络。分类器的最小化目标公式如下：
- en: '![Training to be fair](img/B10354_09_006.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![公平训练](img/B10354_09_006.jpg)'
- en: Within that formula, ![Training to be fair](img/B10354_09_007.jpg) is a binary
    cross-entropy loss of the classification, while ![Training to be fair](img/B10354_09_008.jpg)
    is the adversarial loss. ![Training to be fair](img/B10354_09_009.jpg) represents
    a hyperparameter that we can use to amplify or reduce the impact of the adversarial
    loss.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在该公式中，![公平训练](img/B10354_09_007.jpg)是分类的二元交叉熵损失，而![公平训练](img/B10354_09_008.jpg)是对抗损失。![公平训练](img/B10354_09_009.jpg)表示一个超参数，我们可以使用它来放大或减少对抗损失的影响。
- en: Note
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: This implementation of the adversarial fairness method follows an
    implementation by Stijn Tonk and Henk Griffioen. You can find the code to this
    chapter on Kaggle at [https://www.kaggle.com/jannesklaas/learning-how-to-be-fair](https://www.kaggle.com/jannesklaas/learning-how-to-be-fair).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：这种对抗性公平性方法的实现参考了 Stijn Tonk 和 Henk Griffioen 的实现。你可以在 Kaggle 上找到本章的代码：[https://www.kaggle.com/jannesklaas/learning-how-to-be-fair](https://www.kaggle.com/jannesklaas/learning-how-to-be-fair)。'
- en: 'Stijn''s and Henk''s original blogpost can be found here: [https://blog.godatadriven.com/fairness-in-ml](https://blog.godatadriven.com/fairness-in-ml).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Stijn 和 Henk 原始的博客文章可以在这里找到：[https://blog.godatadriven.com/fairness-in-ml](https://blog.godatadriven.com/fairness-in-ml)。
- en: 'To train this model fairly, we not only need data *X* and targets *y*, but
    also data about the sensitive attributes, *A*. In the example we''re going to
    work on, we''ll be taking data from the 1994 US census provided by the UCI repository:
    [https://archive.ics.uci.edu/ml/datasets/Adult](https://archive.ics.uci.edu/ml/datasets/Adult).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公平地训练这个模型，我们不仅需要数据 *X* 和目标 *y*，还需要关于敏感属性的数据，*A*。在我们将要处理的示例中，我们将使用 UCI 仓库提供的
    1994 年美国人口普查数据：[https://archive.ics.uci.edu/ml/datasets/Adult](https://archive.ics.uci.edu/ml/datasets/Adult)。
- en: To make loading the data easier, it has been transformed into a CSV file with
    column headers. As a side note, please refer to the online version to see the
    data as viewing the data would be difficult in the format of the book.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于加载数据，它已经被转换为带有列标题的 CSV 文件。顺便提一下，请参考在线版本查看数据，因为在书籍格式中查看数据会很困难。
- en: 'First, we load the data. The dataset contains data about people from a number
    of different races, but for the simplicity of this task, we will only be focusing
    on white and black people for the `race` attribute. To do this, we need to run
    the following code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载数据集。该数据集包含来自不同种族的人的数据，但为了简化任务，我们只关注 `种族` 属性中的白人和黑人。为此，我们需要运行以下代码：
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we select the sensitive attributes, in this case we''re focusing on race
    and gender, into our sensitive dataset, `A`. We one-hot encode the data so that
    "Male" equals one for the `gender` attribute and `White` equals one for the `race`
    attribute. We can achieve this by running the following code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择敏感属性，在这种情况下我们专注于种族和性别，将它们放入我们的敏感数据集 `A` 中。我们对数据进行独热编码，使得 `性别` 属性中的 "Male"
    等于 1，`种族` 属性中的 "White" 等于 1。我们可以通过运行以下代码来实现这一点：
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Our target is the `income` attribute. Therefore, we need to encode `>50K` as
    1 and everything else as zero, which is achieved by writing this code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是 `收入` 属性。因此，我们需要将 `>50K` 编码为 1，将其他所有值编码为 0，可以通过编写以下代码实现：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To get our training data, we firstly remove the sensitive and target attributes.
    Then we fill all of the missing values and one-hot encode all of the data, as
    you can see in the following code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取我们的训练数据，首先去除敏感属性和目标属性。然后填补所有缺失值，并对所有数据进行独热编码，正如你在下面的代码中看到的那样：
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we split the data into train and test sets. As seen in the following
    code, we then stratify the data to ensure that the same number of high earners
    are in both the test and training data:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将数据拆分为训练集和测试集。如以下代码所示，我们然后进行分层处理，确保在测试数据和训练数据中都有相同数量的高收入者：
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To ensure the data works nicely with the neural network, we''re now going to
    scale the data using scikit-learn''s `StandardScaler`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保数据能够良好地与神经网络配合使用，我们现在将使用 scikit-learn 的 `StandardScaler` 对数据进行缩放：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We need a metric of how fair our model is. We are using the disparate impact
    selection rule. The `p_rule` method calculates the share of people classified
    to have over $50,000 income from both groups and then returns the ratio of selections
    in the disadvantaged demographic over the ratio of selections in the advantaged
    group.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个衡量模型公平性的指标。我们使用不平等影响选择规则。`p_rule` 方法计算两个群体中被分类为年收入超过 50,000 美元的人的比例，然后返回弱势群体的选择比例与优势群体的选择比例之比。
- en: 'The goal is for the `p_rule` method to return at least 80% in order to meet
    the four-fifths rule for both race and gender. The following code shows how this
    function is only used for monitoring, and not as a loss function:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是让 `p_rule` 方法返回至少 80%，以满足种族和性别的四分之一规则。以下代码展示了这个函数仅用于监控，而不是作为损失函数：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s explore this code in some more detail. As you can see from the preceding
    code block, it''s created with two key features, which are as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探讨一下这段代码。正如你从前面的代码块中看到的，它是通过两个关键特征创建的，具体如下：
- en: Firstly, we select who is given a selected threshold. Here, we classify everyone
    whom the model assigns a chance of over 50% of making $50,000 or more as a high
    earner.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们选择哪些人被赋予一个选定的阈值。在这里，我们将模型为那些被分配有超过50%机会赚取$50,000或更多的人分类为高收入者。
- en: Secondly, we calculate the selection ratio of both demographics. We divide the
    ratio of the one group by the ratio of the other group. By returning the minimum
    of either the odds or one divided by the odds, we ensure the return of a value
    below one.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，我们计算两个人群的选择比例。我们将一组的比例除以另一组的比例。通过返回两者中的最小值，无论是赔率本身还是赔率的倒数，我们确保返回一个小于1的值。
- en: 'To make the model setup a bit easier, we need to define the number of input
    features and the number of sensitive features. This is something that is simply
    done by running these two lines:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让模型设置更简单，我们需要定义输入特征的数量和敏感特征的数量。这可以通过运行以下两行轻松完成：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now we set up our classifier. Note how this classifier is a standard classification
    neural network. It features three hidden layers, some dropout, and a final output
    layer with a sigmoid activation, which occurs since this is a binary classification
    task. This classifier is written in the Keras functional API.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们设置我们的分类器。注意，这个分类器是一个标准的分类神经网络。它包含三个隐藏层、一些丢弃层（dropout），以及一个带有sigmoid激活的最终输出层，因为这是一个二分类任务。这个分类器是用Keras功能性API编写的。
- en: 'To make sure you understand how the API works, go through the following code
    example and ensure you understand why the steps are taken:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保你理解API的工作原理，请通过以下代码示例，确保你理解为什么要采取这些步骤：
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The adversarial network is a classifier with two heads: one to predict the
    applicant''s race from the model output, and one to predict the applicant''s gender:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗网络是一个具有两个头的分类器：一个用来从模型输出预测申请人的种族，另一个用来预测申请人的性别：
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As with generative adversarial networks, we have to make the networks trainable and
    untrainable multiple times. To make this easier, the following function will create
    a function that makes a network and all its layers either trainable or untrainable:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与生成对抗网络一样，我们需要多次将网络设置为可训练或不可训练。为了简化这一过程，以下函数将创建一个函数，使网络及其所有层变得可训练或不可训练：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'From the preceding code, there are four key features that we should take a
    moment to explore:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，有四个关键特性我们应该花点时间探讨：
- en: The function accepts a Keras neural network, for which the train switch function
    will be created.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该函数接受一个Keras神经网络，接着会创建一个训练开关函数。
- en: Inside the function, a second function is created. This second function accepts
    a Boolean flag (`True`/`False`).
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在函数内部，创建了第二个函数。这个第二个函数接受一个布尔标志（`True`/`False`）。
- en: When called, the second function sets the network's trainability to the flag.
    If `False` is passed, the network is not trainable. Since the layers of the network
    can also be used in other networks, we ensure that each individual layer is not
    trainable, too.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当调用时，第二个函数将网络的可训练性设置为标志。如果传入`False`，则网络不可训练。由于网络的层也可以在其他网络中使用，我们确保每个独立的层也不可训练。
- en: Finally, we return the function.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们返回该函数。
- en: 'Using a function to create another function might seem convoluted at first,
    but this allows us to create "switches" for the neural network easily. The following
    code snippet shows us how to create switch functions for the classifier and the
    adversarial network:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个函数来创建另一个函数，刚开始可能看起来有些复杂，但这使我们能够轻松地为神经网络创建“开关”。以下代码片段展示了如何为分类器和对抗网络创建开关函数：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To make the classifier trainable, we can use the function with the `True` flag:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要使分类器可训练，我们可以使用带有`True`标志的函数：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we can compile our classifier. As you will see later on in this chapter,
    it is useful to keep the classifier network as a separate variable from the compiled
    classifier with which we make predictions:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编译我们的分类器。如你在本章后面将看到的，保持分类器网络作为一个独立的变量，与我们用来进行预测的已编译分类器分开是非常有用的：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Remember that to train our classifier, we need to run its predictions through
    the adversary as well as obtaining the adversary loss and applying the negative
    adversary loss to the classifier. This is best done by packing the classifier
    and adversary into one network.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，要训练我们的分类器，我们需要将其预测结果传递给对抗者，并获得对抗者的损失值，并将负的对抗者损失应用到分类器。这最好通过将分类器和对抗者打包成一个网络来完成。
- en: 'To do this, we must first create a new model that maps from the classifier
    inputs to the classifier and adversary outputs. We define the adversary output
    to be a nested function of the adversarial network and the classifier network.
    This way, the predictions of the classifier get immediately passed on to the adversary:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们必须首先创建一个新模型，将分类器输入映射到分类器和对抗者输出。我们定义对抗者的输出为对抗网络和分类器网络的嵌套函数。这样，分类器的预测会立即传递给对抗者：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We then define the classifier output to be the output of the classifier network,
    just as we would for classification:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义分类器的输出为分类器网络的输出，就像我们在分类任务中一样：
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, we define the combined model to map from the classifier input, that is,
    the data about an applicant, to the classifier output and adversary output:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义组合模型，从分类器输入（即关于申请人的数据）映射到分类器输出和对抗者输出：
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'When training the combined model, we only want to update the weights of the
    classifier, as we will train the adversary separately. We can use our switch functions
    to make the classifier network trainable and the adversarial network untrainable:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练组合模型时，我们只想更新分类器的权重，因为我们将单独训练对抗网络。我们可以使用切换函数使分类器网络可训练，而对抗网络不可训练：
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Remember the hyperparameter, ![Training to be fair](img/B10354_09_010.jpg),
    from the preceding minimization objective. We need to set this parameter manually
    for both sensitive attributes. As it turns out, the networks train best if lambda
    for race is set much higher than lambda for gender.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 记住前面最小化目标中的超参数，![公平训练](img/B10354_09_010.jpg)。我们需要手动为这两个敏感属性设置该参数。事实证明，网络在种族的lambda值远高于性别的lambda值时表现最佳。
- en: 'With the lambda values in hand, we can create the weighted loss:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有lambda值后，我们可以创建加权损失：
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The preceding expression leads to loss weights of [1.,-130,-30]. This means
    the classification error has a weight of 1, the race prediction error of the adversary
    has a weight of -130, and the gender prediction error of the adversary has a weight
    of -30\. Since the losses of the adversarial's prediction have negative weights,
    gradient descent will optimize the parameters of the classifier to *increase*
    these losses.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 上述表达式导致了损失权重为[1., -130, -30]。这意味着分类错误的权重为1，对抗者的种族预测错误权重为-130，对抗者的性别预测错误权重为-30。由于对抗预测的损失权重为负，梯度下降将优化分类器的参数以*增加*这些损失。
- en: 'Finally, we can compile the combined network:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以编译组合网络：
- en: '[PRE19]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With the classifier and combined classifier-adversarial model in place, the
    only thing missing is a compiled adversarial model. To get this, we''ll first
    define the adversarial model to map from the classifier inputs to the outputs
    of the nested adversarial-classifier model:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 有了分类器和组合的分类器-对抗模型后，唯一缺少的是一个编译好的对抗模型。为此，我们首先定义对抗模型，从分类器输入映射到嵌套的对抗-分类器模型输出：
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, when training the adversarial model, we want to optimize the weights
    of the adversarial network and not of the classifier network, so we use our switch
    functions to make the adversarial trainable and the classifier not trainable:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在训练对抗模型时，我们希望优化对抗网络的权重，而不是分类器网络的权重，因此我们使用切换函数使对抗者可训练，而分类器不可训练：
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we compile the adversarial model just like we would with a regular
    Keras model:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们像编译常规的Keras模型一样编译对抗模型：
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'With all the pieces in hand, we can now pretrain the classifier. This means
    we train the classifier without any special fairness considerations:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有所有组件后，我们现在可以对分类器进行预训练。这意味着我们在没有任何特殊公平性考虑的情况下训练分类器：
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After we have trained the model, we can make predictions on the validation
    set to evaluate both the model''s fairness and accuracy:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练完模型后，可以对验证集进行预测，以评估模型的公平性和准确性：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we''ll calculate the model''s accuracy and `p_rule` for both gender and
    race. In all calculations, we''re going to use a cutoff point of 0.5:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将计算模型的准确性和`p_rule`，分别针对性别和种族。在所有计算中，我们将使用0.5作为临界点：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see, the classifier achieves a respectable accuracy, 85.44%, in predicting
    incomes. However, it is deeply unfair. It gives women only a 29.4% chance to make
    over $50,000 than it does men.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，分类器在预测收入方面取得了相当不错的准确率85.44%。然而，它是不公平的。它给女性超过50,000美元的机会只有男性的29.4%。
- en: Equally, it discriminates strongly on race. If we used this classifier to judge
    loan applications, for instance, we would be vulnerable to discrimination lawsuits.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，它在种族上有强烈的歧视。如果我们使用这个分类器来评判贷款申请，比如说，我们将面临歧视诉讼的风险。
- en: Note
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: Neither gender or race was included in the features of the classifier.
    Yet, the classifier discriminates strongly on them. If the features can be inferred,
    dropping sensitive columns is not enough.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：分类器的特征中没有包含性别或种族。然而，分类器在这些特征上有强烈的歧视。如果这些特征能够被推断出来，那么仅仅去除敏感列是不够的。'
- en: 'To get out of this mess, we will pretrain the adversarial network before training
    both networks to make fair predictions. Once again, we use our switch functions
    to make the classifier untrainable and the adversarial trainable:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了摆脱这个困境，我们将在训练这两个网络之前预训练对抗网络，以便做出公平的预测。再次使用我们的开关函数，使分类器无法训练而对抗网络可以训练：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As the distributions for race and gender in the data might be skewed, we''re
    going to use weighted classes to adjust for this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据中的种族和性别分布可能存在偏斜，我们将使用加权类来调整这一点：
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We then train the adversary to predict race and gender from the training data
    through the predictions of the classifier:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们训练对抗网络，通过分类器的预测从训练数据中预测种族和性别：
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: NumPy's `hsplit` function splits the 2D `A_train` matrix into two vectors that
    are then used to train the two model heads.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy的`hsplit`函数将2D的`A_train`矩阵拆分为两个向量，随后用于训练两个模型头部。
- en: 'With the classifier and adversary pretrained, we will now train the classifier
    to fool the adversary in order to get better at spotting the classifier''s discrimination.
    Before we start, we need to do some setup. We want to train for 250 epochs, with
    a batch size of 128, with two sensitive attributes:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类器和对抗网络预训练后，我们将训练分类器以欺骗对抗网络，从而更好地识别分类器的歧视性。在开始之前，我们需要做一些设置。我们希望训练250个周期，批次大小为128，包含两个敏感属性：
- en: '[PRE30]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The combined network of the classifier and adversarial also needs some class
    weights. The weights for the income predictions, less/more than $50,000, are both
    one. For the adversarial heads of the combined model, we use the preceding computed
    adversarial class weights:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 结合网络中的分类器和对抗网络同样需要一些类别权重。对于收入预测（高于/低于$50,000），这两个类别的权重均为1。对于结合模型中的对抗网络头部，我们使用前面计算的对抗类权重：
- en: '[PRE31]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To keep track of metrics, we set up one DataFrame for validation metrics, accuracy,
    and area under the curve, as well as for the fairness metrics. The fairness metrics
    are the `p_rule` values for race and gender:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟踪指标，我们设置了一个包含验证指标、准确率和曲线下面积的DataFrame，以及公平性指标。公平性指标是种族和性别的`p_rule`值：
- en: '[PRE32]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Inside the main training loop, three steps are performed: training the adversarial
    network, training the classifier to be fair, and printing out validation metrics.
    For better explanations, all three are printed separately here.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在主训练循环中，执行三个步骤：训练对抗网络，训练分类器使其公平，并打印出验证指标。为了更好的解释，下面会单独打印这三个步骤。
- en: 'Within the code, you will find them in the same loop, where `idx` is the current
    iteration:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，您会看到它们在同一个循环中，其中`idx`是当前的迭代：
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The first step is to train the adversarial network. To this end, we''re going
    to make the classifier untrainable, the adversarial network trainable, and then
    train the adversarial network just as we did before. To do this, we need to run
    the following code block:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是训练对抗网络。为此，我们将使分类器不可训练，对抗网络可训练，然后像之前一样训练对抗网络。为了实现这一点，我们需要运行以下代码块：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Training the classifier to be a good classifier but also to fool the adversary
    and be fair involves three steps. Firstly, we make the adversary untrainable and
    the classifier trainable:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 训练分类器使其成为一个好的分类器，同时也能欺骗对抗网络并保持公平，涉及三个步骤。首先，我们使对抗网络不可训练，而分类器可以训练：
- en: '[PRE35]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then we sample a batch from `X`, `y`, and `A`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从`X`、`y`和`A`中采样一个批次：
- en: '[PRE36]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, we train the combined adversary and classifier. Since the adversarial
    network is set to not be trainable, only the classifier network will be trained.
    However, the loss from the adversarial network''s predictions of race and gender
    gets backpropagated through the entire network, so that the classifier learns
    to fool the adversarial network:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们训练结合的对抗网络和分类器。由于对抗网络被设置为不可训练，只有分类器网络会被训练。然而，从对抗网络对种族和性别的预测所得损失会通过整个网络进行反向传播，因此分类器会学会欺骗对抗网络：
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Finally, we want to keep track of progress by first making predictions on the
    test:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们希望通过首先对测试集进行预测来跟踪进展：
- en: '[PRE38]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We then calculate the area under the curve (`ROC AUC`) and the accuracy of
    the predictions, and save them in the `val_metrics` DataFrame:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算曲线下面积（`ROC AUC`）和预测的准确性，并将它们保存在`val_metrics`数据框中：
- en: '[PRE39]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next up, we calculate `p_rule` for both race and gender and save those values
    in the fairness metrics:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算种族和性别的`p_rule`，并将这些值保存在公平性指标中：
- en: '[PRE40]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'If we plot both the fairness and validation metrics, we''ll arrive at the following
    plot:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制公平性和验证指标的图表，我们将得到以下图示：
- en: '![Training to be fair](img/B10354_09_07.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![训练以实现公平](img/B10354_09_07.jpg)'
- en: Pivot train progress
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 透视训练进展
- en: As you can see, the fairness scores of the classifier steadily increase with
    training. After about 150 epochs, the classifier satisfies the four-fifths rule.
    At the same time, the p-values are well over 90%. This increase in fairness comes
    at only a small decrease in accuracy and area under the curve. The classifier
    trained in this manner is clearly a fairer classifier with similar performance,
    and is thus preferred over a classifier trained without fairness criteria.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，分类器的公平性得分随着训练的进行稳定提高。大约经过150个训练周期后，分类器满足了四分之三规则。同时，p值也远超过90%。这种公平性的提升仅伴随着准确性和曲线下面积的轻微下降。以这种方式训练的分类器显然是一个更加公平的分类器，且性能相似，因此比没有公平性标准的分类器更受青睐。
- en: The pivot approach to fair machine learning has a number of advantages. Yet,
    it cannot rule out unfairness entirely. What if, for example, there was a group
    that the classifier discriminates against that we did not think of yet? What if
    it discriminates on treatment, instead of impact? To make sure our models are
    not biased, we need more technical and social tools, namely *interpretability*,
    *causality*, and *diverse development teams*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 公平机器学习的透视方法有很多优点。然而，它并不能完全排除不公平的情况。例如，如果存在一个被分类器歧视的群体，而我们尚未想到这个群体会如何？如果它是在处理而不是影响上进行歧视呢？为了确保我们的模型不偏倚，我们需要更多的技术和社会工具，即*可解释性*、*因果性*和*多元化的开发团队*。
- en: In the next section, we'll discuss how to train machine learning models that
    learn causal relationships, instead of just statistical associations.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何训练能够学习因果关系的机器学习模型，而不仅仅是统计关联。
- en: Causal learning
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 因果学习
- en: This book is by and large a book about statistical learning. Given data *X*
    and targets *Y*, we aim to estimate ![Causal learning](img/B10354_09_011.jpg),
    the distribution of target values given certain data points. Statistical learning
    allows us to create a number of great models with useful applications, but it
    doesn't allow us to claim that *X* being *x* caused *Y* to be *y*.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 本书主要是一本关于统计学习的书。给定数据*X*和目标*Y*，我们的目标是估计![因果学习](img/B10354_09_011.jpg)，即在给定某些数据点的情况下目标值的分布。统计学习使我们能够创建许多有用的应用程序的优秀模型，但它不能让我们声称*X*的取值为*x*导致了*Y*的取值为*y*。
- en: This statement is critical if we intend to manipulate *X*. For instance, if
    we want to know whether giving insurance to someone leads to them behaving recklessly,
    we are not going to be satisfied with the statistical relationship that people
    with insurance behave more reckless than those without. For instance, there could
    be a self-selection bias present about the number of reckless people getting insurance,
    while those who are not marked as reckless don't.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打算操控*X*，这一声明至关重要。例如，如果我们想知道给某人提供保险是否会导致他们行为鲁莽，我们不会仅仅满足于统计关系——有保险的人比没有保险的人行为更鲁莽。例如，可能存在自我选择偏差，表现在鲁莽的人更容易获得保险，而没有被标记为鲁莽的人则没有。
- en: Judea Pearl, a famous computer scientist, invented a notation for causal models
    called do-calculus; we are interested in ![Causal learning](img/B10354_09_012.jpg),
    which is the probability of someone behaving recklessly after we manipulated *P*
    to be *p*. In a causal notation, *X* usually stands for observed features, while
    *P* stands for the policy features that we can manipulate. This notation can be
    a bit confusing, as *p* now expresses both a probability and a policy. Yet, it
    is important to distinguish between observed and influenced features. So, if you
    see ![Causal learning](img/B10354_09_013.jpg), *p* is a feature that is influenced,
    and if you see ![Causal learning](img/B10354_09_014.jpg), *p* is a probability
    function.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 朱迪亚·珀尔（Judea Pearl），一位著名的计算机科学家，发明了一种用于因果模型的符号系统，称为do-calculus；我们关注的是![因果学习](img/B10354_09_012.jpg)，它表示在我们将*P*操控为*p*之后，一个人行为鲁莽的概率。在因果符号系统中，*X*通常代表观察到的特征，而*P*则代表我们可以操控的政策特征。这个符号系统可能有些令人困惑，因为此时*p*既表示概率，也表示政策。然而，区分观察到的特征和受影响的特征非常重要。因此，如果你看到![因果学习](img/B10354_09_013.jpg)，*p*是一个受影响的特征；而如果你看到![因果学习](img/B10354_09_014.jpg)，*p*是一个概率函数。
- en: So, the formula ![Causal learning](img/B10354_09_015.jpg) expresses the statistical
    relationship that insurance holders are more reckless on average. This is what
    supervised models learn. ![Causal learning](img/B10354_09_016.jpg) expresses the
    causal relationship that people who get insurance become more reckless because
    they are insured.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，公式![因果学习](img/B10354_09_015.jpg)表示了保险持有者平均更鲁莽的统计关系。这是监督模型所学习的内容。![因果学习](img/B10354_09_016.jpg)表示了因果关系，说明获得保险的人变得更鲁莽，因为他们已经投保。
- en: Causal models are a great tool for fair learning. If we only build our models
    in a causal way, then we'll avoid most of the statistical discrimination that
    occurs in statistical models. Do females statistically earn less than males? Yes.
    Do females earn less because they are females and females are somehow undeserving
    of high salaries? No. Instead, the earnings difference is caused by other factors,
    such as different jobs being offered to males and females, discrimination in the
    workplace, cultural stereotypes, and so on.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因果模型是公平学习的一个重要工具。如果我们仅以因果方式构建模型，那么我们将避免大多数统计模型中出现的统计歧视。女性的收入统计上是否低于男性？是的。女性收入低是因为她们是女性，并且女性在某种程度上不配获得高薪吗？不是。相反，收入差异是由其他因素造成的，比如男性和女性获得的工作不同，职场中的歧视，文化偏见等等。
- en: That does not mean we have to throw statistical models out of the window. They
    are great for the many cases where causality is not as much of an important factor
    and where we do not intend to set the values of *X*. For instance, if we are creating
    a natural language model, then we are not interested in whether the occurrence
    of a word caused the sentence to be about a certain topic. Knowing that the topic
    and the word are related is enough to make predictions about the content of the
    text.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着我们必须抛弃统计模型。它们在很多情况下是很有用的，尤其是当因果关系不是那么重要，且我们不打算设置*X*的值时。例如，如果我们正在创建一个自然语言模型，那么我们并不关心一个词的出现是否导致句子涉及某个特定话题。了解话题和单词之间的关系，就足以预测文本的内容。
- en: Obtaining causal models
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取因果模型
- en: The golden route to obtaining information about ![Obtaining causal models](img/B10354_09_017.jpg)
    is to actually go and manipulate the policy, *P,* in a randomized control trial.
    Many websites, for instance, measure the impact of different ads by showing different
    ads to different customers, a process known as A/B testing. Equally, a trader
    might choose different routes to market to figure out which one is the best. Yet,
    it's not always possible or even ethical to do an A/B test. For instance, in our
    focus on finance, a bank cannot deny a loan with the explanation, "Sorry, but
    you are the control group."
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 获取![获取因果模型](img/B10354_09_017.jpg)信息的黄金路线，是通过随机对照试验（RCT）实际去操控政策*P*。许多网站，例如，通过向不同客户展示不同的广告来衡量不同广告的影响，这个过程被称为A/B测试。同样，交易员可能会选择不同的市场路线，以找出最佳的那一条。然而，并非总是能够或甚至合乎道德地进行A/B测试。例如，在我们关注的金融领域，银行不能以“抱歉，你是控制组”作为拒绝贷款的理由。
- en: Yet, often causal inference can be made without the need for an A/B test. Using
    do-calculus, we can infer the effect of our policy on our outcome. Take the example
    of us wondering whether giving people insurance makes them reckless; the applicant's
    moral hazard, if you will. Given features *X* and a policy, *P*, we want to predict
    the outcome distribution, ![Obtaining causal models](img/B10354_09_018.jpg).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常情况下，我们可以在不需要A/B测试的情况下进行因果推断。通过使用do-计算法，我们可以推断政策对结果的影响。以我们是否认为给人们提供保险会让他们变得鲁莽为例；如果你愿意，可以称之为申请人的道德风险。给定特征*X*和政策*P*，我们希望预测结果分布，![获取因果模型](img/B10354_09_018.jpg)。
- en: In this case, given observed information about the applicant, such as their
    age or history of risky behavior, we want to predict the probability of the applicant
    behaving recklessly, ![Obtaining causal models](img/B10354_09_019.jpg), given
    that we manipulate the policy, *P,* of granting insurance. The observed features
    often end up influencing both the policy and the response. An applicant with a
    high-risk appetite might, for example, not be given insurance, but might also
    be more likely to behave recklessly.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，考虑到申请人提供的信息，例如他们的年龄或冒险行为历史，我们希望预测在我们调整授予保险的政策（*P*）下，申请人可能表现出鲁莽行为的概率，![获取因果模型](img/B10354_09_019.jpg)。观察到的特征通常最终会影响政策和反应。例如，一个具有较高风险偏好的申请人，可能不会获得保险，但也更有可能表现出鲁莽行为。
- en: Additionally, we have to deal with unobserved, confounding variables, *e,* which
    often influence both policy and response. A prominent media article titled *Freestyle
    skiing is safe, and you should not get insurance*, for example, would reduce the
    number of people taking insurance as well as the number of reckless skiers.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要处理未观察到的混杂变量（*e*），这些变量通常会同时影响政策和反应。例如，一篇名为*自由式滑雪很安全，你不需要购买保险*的媒体文章，可能会减少购买保险的人数以及鲁莽滑雪者的数量。
- en: Instrument variables
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具变量
- en: 'To distinguish the influence on policy and response, we need access to an **instrument,
    Z**. An instrument is a variable that influences the policy, but nothing else.
    The reinsurance cost, for example, could prompt the insurance company to give out
    fewer insurance policies. This relationship can be seen in the flowchart below,
    where the relationship has been mapped:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了区分政策和反应的影响，我们需要一个**工具变量，Z**。工具变量是一个只影响政策而不影响其他任何因素的变量。例如，再保险成本可能促使保险公司发放更少的保险单。这一关系可以通过下方的流程图看到，其中已经映射了这种关系：
- en: '![Instrument variables](img/B10354_09_08.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![工具变量](img/B10354_09_08.jpg)'
- en: Causal flowchart
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 因果流程图
- en: The field of econometrics already has a built a method to work with these kinds
    of situations called **instrumental variables two-stage least squares** (**IV2SLS,**
    or just **2SLS**). In a nutshell, 2SLS first fits a linear regression model between
    the instrument, *z,* and the policy, *p*, which in econometrics called the endogenous
    or treatment variable.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 计量经济学领域已经建立了一种方法来处理这类情况，称为**工具变量两阶段最小二乘法**（**IV2SLS**，简称**2SLS**）。简而言之，2SLS首先拟合工具变量（*z*）与政策（*p*）之间的线性回归模型，在计量经济学中称为内生变量或处理变量。
- en: From this linear regression, it then estimates an "adjusted treatment variable,"
    which is the treatment variable as it can be explained by the instrument. The
    idea is that this adjustment removes the influence of all other factors on the
    treatment. A second linear regression model then creates a linear model mapping
    from the features, *x,* and the adjusted treatment variable, ![Instrument variables](img/B10354_09_020.jpg),
    to the outcome, *y*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个线性回归模型中，它随后估计出一个“调整后的处理变量”，即可以通过工具变量来解释的处理变量。这个调整的目的是消除所有其他因素对处理的影响。然后，第二个线性回归模型通过映射特征（*x*）和调整后的处理变量，![工具变量](img/B10354_09_020.jpg)，到结果（*y*）来创建一个线性模型。
- en: 'In the following diagram, you can see an overview of how 2SLS works:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，您可以看到2SLS如何工作的概览：
- en: '![Instrument variables](img/B10354_09_09.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![工具变量](img/B10354_09_09.jpg)'
- en: IV2SLS
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: IV2SLS
- en: 2SLS is probably what the insurance company in our case would use since it is
    an established method. We won't go into details here, beyond giving you a brief
    overview of how to use 2SLS in Python. The `linear model` package in Python features
    an easy way to run 2SLS.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 2SLS很可能是我们案例中的保险公司使用的方法，因为它是一种成熟的方法。我们在这里不会深入讨论，除了简要介绍如何在Python中使用2SLS。Python中的`linear
    model`包提供了一种简便的方法来运行2SLS。
- en: Note
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: You can find the package on GitHub at [https://github.com/bashtage/linearmodels](https://github.com/bashtage/linearmodels).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：你可以在 GitHub 上找到该包：[https://github.com/bashtage/linearmodels](https://github.com/bashtage/linearmodels)。'
- en: 'You can install the package by running:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行以下命令安装该包：
- en: '[PRE41]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'If you have data `X`, `y`, `P`, and `Z`, you can run a 2SLS regression as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有数据 `X`、`y`、`P` 和 `Z`，可以按如下方式运行 2SLS 回归：
- en: '[PRE42]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Non-linear causal models
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非线性因果模型
- en: What if the relationships between features, the treatment, and the outcome are
    complex and non-linear? In this case, we need to perform a process similar to
    2SLS, but with a non-linear model, such as a neural network, instead of linear
    regression.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征、处理和结果之间的关系是复杂的非线性的，怎么办？在这种情况下，我们需要执行一个类似于 2SLS 的过程，但使用非线性模型（如神经网络）来代替线性回归。
- en: 'Ignoring the confounding variables for a minute, function *g* determines the
    recklessness of behavior *y* given insurance policy ![Non-linear causal models](img/B10354_09_022.jpg)
    and a set of applicant''s features, *x*:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 暂时忽略混杂变量，函数 *g* 根据保险政策 ![非线性因果模型](img/B10354_09_022.jpg) 和一组申请者的特征 *x* 来确定行为
    *y* 的鲁莽程度：
- en: '![Non-linear causal models](img/B10354_09_023.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![非线性因果模型](img/B10354_09_023.jpg)'
- en: 'Function *f* determines policy ![Non-linear causal models](img/B10354_09_024.jpg)
    given the applicant''s features, *x,* as well as the instrument, *z*:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 *f* 根据申请者的特征 *x* 以及工具 *z* 确定政策 ![非线性因果模型](img/B10354_09_024.jpg)：
- en: '![Non-linear causal models](img/B10354_09_025.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![非线性因果模型](img/B10354_09_025.jpg)'
- en: 'Given these two functions, the following identity holds, if the confounding
    variable has a mean of zero overall features:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这两个函数，如果混杂变量在所有特征上的均值为零，以下恒等式成立：
- en: '![Non-linear causal models](img/B10354_09_026.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![非线性因果模型](img/B10354_09_026.jpg)'
- en: 'This means that if we can reliably estimate the function, *g,* and distribution,
    *F*, we can make causal statements about the effects of policy ![Non-linear causal
    models](img/B10354_09_027.jpg). If we have data about the actual outcome, *y*,
    features *x*, policy ![Non-linear causal models](img/B10354_09_028.jpg), and instrument
    *z*, we can optimize the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果我们能够可靠地估计函数 *g* 和分布 *F*，我们就能够对政策的影响做出因果声明 ![非线性因果模型](img/B10354_09_027.jpg)。如果我们有关于实际结果
    *y*、特征 *x*、政策 ![非线性因果模型](img/B10354_09_028.jpg) 和工具 *z* 的数据，我们可以优化以下内容：
- en: '![Non-linear causal models](img/B10354_09_029.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![非线性因果模型](img/B10354_09_029.jpg)'
- en: The preceding function is the squared error between the predicted outcome using
    the prediction function, *g,* and the actual outcome, *y*.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数是使用预测函数 *g* 预测的结果与实际结果 *y* 之间的平方误差。
- en: 'Notice the similarity to 2SLS. In 2SLS, we estimated *F* and *g* with two separate
    linear regressions. For the more complex functions, we can also estimate them
    with two separate neural networks. Back in 2017, Jason Hartfort and others presented
    just such an approach with their paper, *Deep IV: A Flexible Approach for Counterfactual
    Prediction*, - available at: [http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf](http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf)
    - the overview of which you can see in the following diagram:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '注意与 2SLS 的相似性。在 2SLS 中，我们通过两个独立的线性回归来估计 *F* 和 *g*。对于更复杂的函数，我们也可以通过两个独立的神经网络来估计它们。早在
    2017 年，Jason Hartford 等人就在他们的论文《*Deep IV: A Flexible Approach for Counterfactual
    Prediction*》中提出了这种方法，- 该论文可在以下网址找到：[http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf](http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf)
    - 你可以在下面的图示中看到该方法的概述：'
- en: '![Non-linear causal models](img/B10354_09_10.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![非线性因果模型](img/B10354_09_10.jpg)'
- en: Deep IV
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Deep IV
- en: The idea of Deep IV is to first train a neural network to express a distribution,
    *F(z,x),* which describes the distribution of policies given certain features,
    *x*, and instrument values, *z*. A second neural network is predicting the response,
    *y*, from the estimated policy distribution and features. Deep IV's advantage
    is that it can learn complex, non-linear relationships from complex data, such
    as text.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Deep IV 的思路是首先训练一个神经网络来表达分布 *F(z,x)*，它描述了在给定特征 *x* 和工具值 *z* 的情况下，政策的分布。第二个神经网络则根据估计的政策分布和特征来预测响应
    *y*。Deep IV 的优势在于，它能够从复杂数据（例如文本）中学习复杂的非线性关系。
- en: 'The authors of the *Deep IV* paper have also published a custom Keras model
    which is used for handling sampling and learning from a distribution part, which
    you can find on GitHub: [https://github.com/jhartford/DeepIV](https://github.com/jhartford/DeepIV).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '*Deep IV* 论文的作者们还发布了一个自定义的 Keras 模型，用于处理采样和从分布部分学习，你可以在 GitHub 上找到： [https://github.com/jhartford/DeepIV](https://github.com/jhartford/DeepIV)。'
- en: While their code is too long to be discussed in depth here, it is interesting
    to think about what the source of our causal claim is, both in Deep IV and IV2SLS.
    In our insurance case, we assumed that either having or not having an insurance
    would influence behavior, not the other way around. We never showed or tested
    the truth behind this direction of causality.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然他们的代码太长，无法在这里深入讨论，但思考我们因果主张的来源是很有趣的，既在 Deep IV 中，也在 IV2SLS 中。在我们的保险案例中，我们假设拥有或没有保险会影响行为，而不是反过来。我们从未展示或验证过因果关系方向背后的真相。
- en: In our case, assuming that insurance influences behavior is justified because
    the insurance contract is signed before the behavior is observed. However, the
    direction of causality is not always as straightforward. There is no way to establish
    the direction of causality other than logical reasoning or experiments. In the
    absence of experiments, we have to assume and logically reason, for example, through
    the sequence of events. Another important assumption that we make is that the
    instrument is actually an independent instrument. If it is not independent, our
    estimation of the policy will break down.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，假设保险会影响行为是合理的，因为保险合同是在行为被观察之前签署的。然而，因果关系的方向并不总是如此直接。除了逻辑推理或实验之外，没有其他方法能够确定因果关系的方向。在没有实验的情况下，我们只能假设并通过逻辑推理，例如通过事件的顺序。我们做出的另一个重要假设是工具实际上是一个独立的工具。如果它不是独立的，我们的政策估计将会失败。
- en: With these two limitations in mind, causal inference becomes a great tool and
    an active area of research from which we can hope to see great results in the
    future. In the best case, your discrimination-sensitive models would only contain
    causal variables. In practice, this is usually not possible. However, keeping
    the difference between statistical correlation in mind, as expressed by standard
    statistical models and causation, can help you avoid statistical biases and wrong
    associations.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这两种限制，因果推断成为了一个极好的工具，并且是一个活跃的研究领域，我们有理由期待未来能看到显著的成果。在最佳情况下，你的敏感性模型只会包含因果变量。实际上，这通常是不可行的。然而，牢记标准统计模型所表达的统计相关性和因果关系之间的差异，可以帮助你避免统计偏差和错误的关联。
- en: A final, more technical, method to reduce unfairness is to peek inside the model
    to ensure it is fair. We already looked at interpretability in the last chapter,
    mostly to debug data and spot overfitting, but now, we will give it another look,
    this time to justify the model's predictions.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一种更具技术性的减少不公平性的方法是窥视模型内部，以确保它是公平的。我们已经在上一章中看到了可解释性，主要是为了调试数据并发现过拟合，但现在，我们将再次审视它，这次是为了验证模型的预测。
- en: Interpreting models to ensure fairness
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释模型以确保公平性
- en: In [Chapter 8](ch08.xhtml "Chapter 8. Privacy, Debugging, and Launching Your
    Products"), *Privacy, Debugging, and Launching Your Products,* we discussed model
    interpretability as a debugging method. We used LIME to spot the features that
    the model is overfitting to.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第8章](ch08.xhtml "第8章。隐私、调试与产品发布")，*隐私、调试与产品发布* 中，我们将模型可解释性作为调试方法进行了讨论。我们使用
    LIME 来发现模型过拟合的特征。
- en: In this section, we will use a slightly more sophisticated method called **SHAP**
    (**SHapley Additive exPlanation**). SHAP combines several different explanation
    approaches into one neat method. This method lets us generate explanations for
    individual predictions as well as for entire datasets in order to understand the
    model better.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一种稍微复杂一点的方法，叫做 **SHAP**（**Shapley 加性解释**）。SHAP 将几种不同的解释方法结合成一个简洁的方式。这种方法让我们能够为单个预测以及整个数据集生成解释，从而更好地理解模型。
- en: You can find SHAP on GitHub at [https://github.com/slundberg/shap](https://github.com/slundberg/shap)
    and install it locally with `pip install shap`. Kaggle kernels have SHAP preinstalled.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 上找到 SHAP：[https://github.com/slundberg/shap](https://github.com/slundberg/shap)，并通过
    `pip install shap` 安装到本地。Kaggle 核心已经预装了 SHAP。
- en: Tip
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'The example code given here is from the SHAP example notebooks. You can find
    a slightly extended version of the notebook on Kaggle:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这里给出的示例代码来自 SHAP 示例笔记本。你可以在 Kaggle 上找到一个稍微扩展的版本：
- en: '[https://www.kaggle.com/jannesklaas/explaining-income-classification-with-keras](https://www.kaggle.com/jannesklaas/explaining-income-classification-with-keras)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/jannesklaas/explaining-income-classification-with-keras](https://www.kaggle.com/jannesklaas/explaining-income-classification-with-keras)'
- en: 'SHAP combines seven model interpretation methods, those being LIME, Shapley
    sampling values, DeepLIFT, **Quantitative Input Influence** (**QII**), layer-wise
    relevance propagation, Shapley regression values, and a tree interpreter that
    has two modules: a model-agnostic `KernelExplainer` and a `TreeExplainer` module
    specifically for tree-based methods such as `XGBoost`.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP结合了七种模型解释方法，分别是LIME、Shapley采样值、DeepLIFT、**定量输入影响**（**QII**）、逐层相关传播、Shapley回归值，以及一个树模型解释器，其中包含两个模块：一个是模型无关的`KernelExplainer`，另一个是专门针对树基方法如`XGBoost`的`TreeExplainer`模块。
- en: The mathematics of how and when the interpreters are used is not terribly relevant
    for using SHAP. In a nutshell, given a function, *f*, expressed through a neural
    network, for instance, and a data point, *x*, SHAP compares ![Interpreting models
    to ensure fairness](img/B10354_09_030.jpg) to ![Interpreting models to ensure
    fairness](img/B10354_09_031.jpg) where ![Interpreting models to ensure fairness](img/B10354_09_032.jpg)
    is the "expected normal output" generated for a larger sample. SHAP will then
    create smaller models, similar to LIME, to see which features explain the difference
    between ![Interpreting models to ensure fairness](img/B10354_09_033.jpg) and ![Interpreting
    models to ensure fairness](img/B10354_09_034.jpg).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 关于何时以及如何使用解释器的数学原理，对于使用SHAP并不是特别相关。简而言之，给定一个函数*f*，例如通过神经网络表示，并且给定一个数据点*x*，SHAP将对比![解释模型以确保公平性](img/B10354_09_030.jpg)与![解释模型以确保公平性](img/B10354_09_031.jpg)，其中![解释模型以确保公平性](img/B10354_09_032.jpg)是为更大样本生成的“预期正常输出”。然后，SHAP会创建类似于LIME的更小模型，来查看哪些特征解释了![解释模型以确保公平性](img/B10354_09_033.jpg)与![解释模型以确保公平性](img/B10354_09_034.jpg)之间的差异。
- en: In our loan example, this corresponds to having an applicant, *x,* and a distribution
    of many applicants, *z*, and trying to explain why the chance of getting a loan
    for applicant *x* is different from the expected chance for the other applicants,
    *z*.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的贷款示例中，这意味着有一个申请人，*x*，以及许多申请人，*z*，的分布，并试图解释为什么申请人*x*获得贷款的机会与其他申请人*z*的预期机会不同。
- en: SHAP does not only compare ![Interpreting models to ensure fairness](img/B10354_09_035.jpg)
    and ![Interpreting models to ensure fairness](img/B10354_09_036.jpg), but also
    compares ![Interpreting models to ensure fairness](img/B10354_09_037.jpg) to ![Interpreting
    models to ensure fairness](img/B10354_09_038.jpg).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP不仅比较了![解释模型以确保公平性](img/B10354_09_035.jpg)与![解释模型以确保公平性](img/B10354_09_036.jpg)，还比较了![解释模型以确保公平性](img/B10354_09_037.jpg)与![解释模型以确保公平性](img/B10354_09_038.jpg)。
- en: This means it compares the importance of certain features that are held constant,
    which allows it to better estimate the interactions between features.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它比较了某些特征保持不变时的重要性，从而更好地估计特征之间的相互作用。
- en: Explaining a single prediction can very important, especially in the world of
    finance. Your customers might ask you, "Why did you deny me a loan?" You'll remember
    from earlier on that the ECOA act stipulates that you must give the customer a
    valid reason, and if you have no good explanation, you might find yourself in
    a tough situation. In this example, we are once again working with the income
    prediction dataset, with the objective of explaining why our model made a single
    decision. This process works in three steps.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 解释单一预测可能非常重要，尤其是在金融领域。您的客户可能会问你：“为什么拒绝了我的贷款？”您会记得之前提到的，ECOA法案规定您必须给客户一个有效的理由，如果没有合理解释，您可能会陷入困境。在这个示例中，我们再次使用收入预测数据集，目的是解释为什么我们的模型做出了这个单一决策。这个过程分为三个步骤。
- en: 'Firstly, we need to define the explainer and provide it with a prediction method
    and values, *z*, to estimate a "normal outcome." Here we are using a wrapper,
    `f`, for Keras'' prediction function, which makes working with SHAP much easier.
    We provide 100 rows of the dataset as values for `z`:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义解释器，并为其提供一个预测方法和数值*z*，以估计“正常结果”。在这里，我们使用了Keras预测函数的包装器`f`，这使得与SHAP的工作变得更加简单。我们提供了数据集的100行作为`z`的值：
- en: '[PRE43]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we need to calculate the SHAP values indicating the importance of different
    features for a single example. We let SHAP create 500 permutations of each sample
    from *z* so that SHAP has a total of 50,000 examples to compare the one example
    to:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要计算 SHAP 值，指示单个示例中不同特征的重要性。我们让 SHAP 对每个样本进行500次排列组合，以便 SHAP 总共有50,000个示例来与该单个示例进行比较：
- en: '[PRE44]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Finally, we can plot the influence of the features with SHAP''s own plotting
    tool. This time, we provide a row from `X_display`, not `X`. `X_display`, which
    contains the unscaled values and is only used for annotation of the plot to make
    it easier to read:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用 SHAP 自带的绘图工具来展示特征的影响。这次，我们提供的是来自`X_display`的一行数据，而不是`X`。`X_display`包含了未经缩放的值，仅用于绘图的注释，使其更易于阅读：
- en: '[PRE45]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We can see the output of the code in the following graph:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图表中看到代码的输出：
- en: '![Interpreting models to ensure fairness](img/B10354_09_11.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![解读模型以确保公平性](img/B10354_09_11.jpg)'
- en: The influence of features with the SHAP plotting tool
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SHAP 绘图工具展示特征的影响
- en: If you look at the preceding plot, the predictions of the model seem, by and
    large, reasonable. The model gives the applicant a high chance of having a high
    income because they have a master's degree, and because they're an executive manager
    who works 65 hours a week. The applicant could have an even higher expected income
    score were it not for a capital loss. Likewise, the model seems to take the fact
    that the applicant is married as a big factor of a high income. In fact, in our
    example, it seems that marriage is more important than either the long hours or
    the job title.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看前面的图表，模型的预测似乎大致合理。模型根据申请人拥有硕士学位，并且作为一名每周工作65小时的执行经理，给出了较高的收入预期。若没有资本损失，申请人可能会有更高的收入预期分数。同样，模型似乎将申请人已婚的事实视为高收入的重要因素。事实上，在我们的示例中，似乎婚姻比长工时或职位头衔更为重要。
- en: 'Our model also has some problems that become clear once we calculate and plot
    the SHAP values of another applicant:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型也存在一些问题，这些问题在我们计算并绘制另一个申请人的 SHAP 值时变得更加明显：
- en: '[PRE46]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The following outputted graph is then shown. This also shows some of the problems
    that we''ve encountered:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来显示的是输出的图表。这也显示了我们遇到的一些问题：
- en: '![Interpreting models to ensure fairness](img/B10354_09_12.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![解读模型以确保公平性](img/B10354_09_12.jpg)'
- en: The SHAP values showing some of the problems we can encounter
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 显示我们可能遇到的一些问题的 SHAP 值
- en: In this example, the applicant also has a good education, and works 48 hours
    a week in the technology industry, but the model gives her a much lower chance
    of having a high income because of the fact that she's a female, an Asian-Pacific
    islander who has never been married and has no other family relationship. A loan
    rejection on these grounds is a lawsuit waiting to happen as per the ECOA act.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，申请人也有良好的教育背景，每周工作48小时，并从事技术行业的工作，但模型却因为她是女性，且是亚太岛裔、未婚且没有其他家庭关系而给了她较低的高收入概率。基于这些理由拒绝贷款，根据《公平信贷法案》（ECOA），这将是一起即将发生的诉讼案件。
- en: The two individual cases that we just looked at might have been unfortunate
    glitches by the model. It might have overfitted to some strange combination that
    gave an undue importance to marriage. To investigate whether our model is biased,
    we should investigate a number of different predictions. Fortunately for us, the
    SHAP library has a number of tools that can do just that.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才看到的两个个案可能只是模型的意外故障。它可能过拟合了一些奇怪的组合，给了婚姻不应有的重要性。为了调查我们的模型是否存在偏差，我们应该检查多个不同的预测。幸运的是，SHAP
    库提供了一些工具，可以帮助我们完成这个任务。
- en: 'We can use the SHAP value calculations for multiple rows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 SHAP 值计算多个数据行：
- en: '[PRE47]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then, we can plot a forced plot for all of these values as well:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们也可以为这些所有值绘制强制图：
- en: '[PRE48]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Again, this code produces a SHAP dataset graph, which we can see in the following
    graphic:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这段代码会生成一个 SHAP 数据集图表，我们可以在以下图形中看到它：
- en: '![Interpreting models to ensure fairness](img/B10354_09_13.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![解读模型以确保公平性](img/B10354_09_13.jpg)'
- en: SHAP dataset
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 数据集
- en: The preceding plot shows 230 rows of the dataset, grouped by similarity with
    the forces of each feature that matter to them. In your live version, if you move
    the mouse over the graph, you'll be able to read the features and their values.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了数据集的230行数据，按特征的相似性分组，显示了各个特征对它们的重要性。如果你在实际版本中将鼠标移到图表上，你将能够看到特征及其值。
- en: By exploring this graph, you can get an idea of what kind of people the model
    classifies as either high or low earners. On the very left, for example, you'll
    see most people with low education who work as cleaners. The big red block between
    40 and 60 are mostly highly educated people who work a high number of hours.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 通过探索这个图表，你可以大致了解模型如何将人群分类为高收入或低收入。例如，在最左边，你会看到大多数低学历的人从事清洁工工作。在40到60之间的那个大红块大多是受过高等教育并且工作时长很长的人。
- en: 'To further examine the impact of marital status, you can change what SHAP displays
    on the *y*-axis. Let''s look at the impact of marriage:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步检查婚姻状况的影响，你可以更改SHAP在*y*轴上显示的内容。我们来看看婚姻的影响：
- en: '![Interpreting models to ensure fairness](img/B10354_09_14.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![解释模型以确保公平性](img/B10354_09_14.jpg)'
- en: SHAP marriage outcome
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP婚姻结果
- en: As you can see in this chart, marriage status either strongly positively or
    negatively impacts people from different groups. If you move your mouse over the
    chart, you can see that the positive influences all stem from civic marriages.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在这个图表中看到的，婚姻状况对不同群体的人有着强烈的正向或负向影响。如果你把鼠标悬停在图表上，你会看到正向影响都来源于民事婚姻。
- en: 'Using a summary plot, we can see which features matter the most to our model:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 使用总结图，我们可以看到哪些特征对我们的模型最为重要：
- en: '[PRE49]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This code then outputs the final summary plot graph, which we can see below:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码会输出最终的总结图，我们可以在下面看到：
- en: '![Interpreting models to ensure fairness](img/B10354_09_15.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![解释模型以确保公平性](img/B10354_09_15.jpg)'
- en: SHAP summary plot
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP总结图
- en: As you can see, education is the most important influence on our model. It also
    has the widest spread of influence. Low education levels really drag predictions
    down, while strong education levels really boost predictions up. Marital status
    is the second most important predictor. Interestingly, though, capital losses
    are important to the model, but capital gains are not.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，教育是我们模型中最重要的影响因素。它的影响范围也最广。低学历水平会大大降低预测值，而高学历水平则能显著提升预测值。婚姻状况是第二重要的预测因素。有趣的是，资本损失对模型有重要影响，但资本收益则没有。
- en: 'To dig deeper into the effects of marriage, we have one more tool at our disposal,
    a dependence plot, which can show the SHAP values of an individual feature together
    with a feature for which SHAP suspects high interaction. With the following code
    snippet, we can inspect the effect of marriage on our model''s predictions:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入探讨婚姻的影响，我们还可以使用另一个工具——依赖图，它可以展示某个特征的SHAP值以及SHAP怀疑具有高互动的其他特征。使用以下代码片段，我们可以检查婚姻对我们模型预测的影响：
- en: '[PRE50]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'As a result of running this code, we can now see a visualized representation
    of the effect of marriage in the following graph:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码的结果是，我们现在可以在以下图表中看到婚姻的影响可视化表示：
- en: '![Interpreting models to ensure fairness](img/B10354_09_16.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![解释模型以确保公平性](img/B10354_09_16.jpg)'
- en: SHAP marriage dependence
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP婚姻依赖性
- en: As you can see, **Married-civ-spouse**, the census code for a civilian marriage
    with no partner in the armed forces, stands out with a positive influence on model
    outcomes. Meanwhile, every other type of arrangement has slightly negative scores,
    especially never married.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，**Married-civ-spouse**（即没有军人配偶的民事婚姻的普查代码）在模型结果中有着显著的正向影响。与此同时，其他所有类型的婚姻状况得分略显负面，尤其是从未结过婚的情况。
- en: Statistically, rich people tend to stay married for longer, and younger people
    are more likely to have never been married. Our model correctly correlated that
    marriage goes hand in hand with high income, but not because marriage causes high
    income. The model is correct in making the correlation, but it would be false
    to make decisions based on the model. By selecting, we effectively manipulate
    the features on which we select. We are no longer interested in just ![Interpreting
    models to ensure fairness](img/B10354_09_039.jpg), but in ![Interpreting models
    to ensure fairness](img/B10354_09_040.jpg).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 从统计学上讲，富人往往会维持更长时间的婚姻，而年轻人则更可能从未结过婚。我们的模型正确地关联了婚姻与高收入，但这并不是因为婚姻导致了高收入。模型在建立关联时是正确的，但如果基于这个模型做出决策则是不正确的。通过选择，我们实际上是在操控我们选择的特征。我们不再仅仅对![解释模型以确保公平性](img/B10354_09_039.jpg)感兴趣，而是对![解释模型以确保公平性](img/B10354_09_040.jpg)感兴趣。
- en: Unfairness as complex system failure
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不公平作为复杂系统的失败
- en: In this chapter, you have been equipped with an arsenal of technical tools to
    make machine learning models fairer. However, a model does not operate in a vacuum.
    Models are embedded in complex socio-technical systems. There are humans developing
    and monitoring the model, sourcing the data and creating the rules for what to
    do with the model output. There are also other machines in place, producing data
    or using outputs from the model. Different players might try to game the system
    in different ways.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你已经获得了一套技术工具，来使机器学习模型更具公平性。然而，模型并不是在真空中运行的。模型嵌入在复杂的社会技术系统中。开发和监控模型的有人的参与，数据来源及处理规则也都影响着模型的输出。同时，也有其他机器在运行，生产数据或使用模型输出。不同的参与者可能会以不同的方式试图操纵系统。
- en: Unfairness is equally complex. We've already discussed the two general definitions
    of unfairness, *disparate impact* and *disparate treatment*. Disparate treatment
    can occur against any combination of features (age, gender, race, nationality,
    income, and so on), often in complex and non-linear ways. This section examines
    Richard Cook's 1998 paper, *How complex systems fail* - available at [https://web.mit.edu/2.75/resources/random/How%20Complex%20Systems%20Fail.pdf](https://web.mit.edu/2.75/resources/random/How%20Complex%20Systems%20Fail.pdf)
    - which looks at how complex machine learning-driven systems fail to be fair.
    Cook lists 18 points, some of which will be discussed in the following sections.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 不公平同样是复杂的。我们已经讨论了两种不公平的一般定义——*不平等影响*和*不平等待遇*。不平等待遇可能发生在任何特征组合上（如年龄、性别、种族、国籍、收入等），并且通常以复杂和非线性的方式发生。本节将讨论理查德·库克（Richard
    Cook）1998年的论文，*复杂系统如何失败* —— 该文可在[https://web.mit.edu/2.75/resources/random/How%20Complex%20Systems%20Fail.pdf](https://web.mit.edu/2.75/resources/random/How%20Complex%20Systems%20Fail.pdf)查阅——库克在文中探讨了复杂的机器学习驱动系统如何未能实现公平。库克列出了18个要点，其中一些将在以下章节中讨论。
- en: Complex systems are intrinsically hazardous systems
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复杂系统本质上是具有风险的系统
- en: Systems are usually complex because they are hazardous, and many safeguards
    have been created because of that fact. The financial system is a hazardous system;
    if it goes off the rails, it can break the economy or ruin people's lives. Thus,
    many regulations have been created and many players in the market work to make
    the system safer.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 系统通常是复杂的，因为它们具有危险性，很多保障措施正是基于这一事实而设立的。金融系统就是一个具有风险的系统；如果它脱轨，可能会摧毁经济或毁掉人们的生活。因此，许多规定已被制定，市场中的许多参与者也在努力使系统更安全。
- en: Since the financial system is so hazardous, it is important to make sure it
    is safe against unfairness, too. Luckily, there are a number of safeguards in
    place to keep the system fair. Naturally, these safeguards can break, and they
    do so constantly in a number of small ways.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 由于金融系统本身存在很大的风险，因此确保其公平性同样重要。幸运的是，系统中有多项保障措施来保持其公平性。自然，这些保障措施也可能会失效，而且它们在很多小的方面不断出现故障。
- en: Catastrophes are caused by multiple failures
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 灾难通常由多个故障引起
- en: In a complex system, no single point of failure can cause catastrophes since
    there are many safeguards in place. Failure usually results from multiple points
    of failure. In the financial crises, banks created risky products, but regulators
    didn't stop them.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个复杂系统中，单一的故障点无法导致灾难，因为系统中有许多保障措施。故障通常是由多个故障点共同导致的。在金融危机中，银行创造了有风险的产品，但监管机构并没有阻止它们。
- en: For widespread discrimination to happen, not only does the model have to make
    unfair predictions, but employees must blindly follow the model and criticism
    must be suppressed. On the flip side, just fixing your model will not magically
    keep all unfairness away. The procedures and culture inside and outside the firm
    can also cause discrimination, even with a fair model.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 要让广泛的歧视发生，不仅模型必须做出不公平的预测，员工还必须盲目地服从模型，而批评意见则必须被压制。另一方面，单单修正模型并不能神奇地消除所有的不公平。公司内外的程序和文化也可能导致歧视，即便模型本身是公平的。
- en: Complex systems run in degraded mode
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复杂系统通常会以降级模式运行
- en: In most accident reports, there is a section that lists "proto-accidents," which
    are instances in the past where the same accident nearly happened but did not
    happen. The model might have made erratic predictions before, but a human operator
    stepped in, for example.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数事故报告中，会有一个部分列出“原始事故”，即过去差点发生但最终未发生的相似事故。例如，模型可能曾做出过不稳定的预测，但有人工操作员介入进行了修正。
- en: It is important to know that in a complex system, failures that nearly lead
    to catastrophe always occur. The complexity of the system makes it prone to error,
    but the heavy safeguards against catastrophe keep them from happening. However,
    once these safeguards fail, catastrophe is right around the corner. Even if your
    system seems to run smoothly, check for proto-accidents and strange behavior before
    it is too late.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 需要知道的是，在复杂系统中，几乎导致灾难的失败总是会发生。系统的复杂性使得它容易出错，但严密的灾难防护措施能防止灾难的发生。然而，一旦这些防护措施失效，灾难便迫在眉睫。即使你的系统看似运行顺利，也要在为时已晚之前检查潜在的事故和异常行为。
- en: Human operators both cause and prevent accidents
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类操作员既是事故的制造者，也是预防者
- en: Once things have gone wrong, blame is often put at the human operators who "must
    have known" that their behavior would "inevitably" lead to an accident. On the
    other hand, it is usually humans who step in at the last minute to prevent accidents
    from happening. Counterintuitively, it is rarely one human and one action that
    causes the accident, but the behavior of many humans over many actions. For models
    to be fair, the entire team has to work to keep it fair.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦出现问题，通常会将责任归咎于操作人员，认为他们“肯定知道”自己的行为会“不可避免”地导致事故。另一方面，通常是人类在最后一刻介入，防止事故发生。违反直觉的是，事故通常不是由单一的个体或单一行为引起的，而是许多人在多个行动中的行为共同导致的。为了保证模型的公平性，整个团队必须共同努力保持其公平。
- en: Accident-free operation requires experience with failure
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无事故运行需要失败经验
- en: In fairness, the single biggest problem is often that the designers of a system
    do not experience the system discriminating against them. It is thus important
    to get the insights of a diverse group of people into the development process.
    Since your system constantly fails, you should capture the learning from these
    small failures before bigger accidents happen.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在公平性方面，最大的一个问题往往是系统设计者没有亲身经历系统对他们的歧视。因此，将多元化的人员视角融入开发过程至关重要。由于你的系统不断失败，你应当在大规模事故发生之前，从这些小规模的失败中吸取教训。
- en: A checklist for developing fair models
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公平模型开发清单
- en: With the preceding information, we can create a short checklist that can be
    used when creating fair models. Each issue comes with several sub-issues.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 基于以上信息，我们可以创建一个简短的清单，用于在创建公平模型时参考。每个问题都有几个子问题。
- en: What is the goal of the model developers?
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型开发者的目标是什么？
- en: Is fairness an explicit goal?
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公平是否是一个明确的目标？
- en: Is the model evaluation metric chosen to reflect the fairness of the model?
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估指标是否选择反映模型的公平性？
- en: How do model developers get promoted and rewarded?
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型开发者如何获得晋升和奖励？
- en: How does the model influence business results?
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型如何影响业务结果？
- en: Would the model discriminate against the developer's demographic?
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型会不会对开发者的群体产生歧视？
- en: How diverse is the development team?
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发团队的多样性如何？
- en: Who is responsible when things go wrong?
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当事情出错时，谁负责？
- en: Is the data biased?
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据是否存在偏见？
- en: How was the data collected?
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是如何收集的？
- en: Are there statistical misrepresentations in the sample?
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本中是否存在统计误差？
- en: Are sample sizes for minorities adequate?
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 少数群体的样本量是否充足？
- en: Are sensitive variables included?
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否包括敏感变量？
- en: Can sensitive variables be inferred from the data?
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 敏感变量是否能从数据中推断出来？
- en: Are there interactions between features that might only affect subgroups?
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征之间是否存在只影响子群体的交互作用？
- en: Are errors biased?
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误是否存在偏差？
- en: What are the error rates for different subgroups?
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同子群体的错误率是多少？
- en: What is the error rate of a simple, rule-based alternative?
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的基于规则的替代方法的错误率是多少？
- en: How do the errors in the model lead to different outcomes?
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型中的错误是如何导致不同结果的？
- en: How is feedback incorporated?
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何纳入反馈？
- en: Is there an appeals/reporting process?
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有申诉/报告流程？
- en: Can mistakes be attributed back to a model?
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误是否可以追溯到模型？
- en: Do model developers get insight into what happens with their model's predictions?
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型开发者是否能够了解其模型预测结果的实际情况？
- en: Can the model be audited?
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否可以审计？
- en: Is the model open source?
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否开源？
- en: Do people know which features are used to make predictions about them?
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人们是否知道哪些特征被用来预测他们的情况？
- en: Can the model be interpreted?
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型是否可解释？
- en: Is a model interpretation, for example, individual results, in place?
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有模型解释，例如个体结果？
- en: Can the interpretation be understood by those it matters to?
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释是否能被相关方理解？
- en: Can findings from the interpretation lead to changes in the model?
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释结果是否可以导致对模型的修改？
- en: What happens to models after deployment?
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型部署后会发生什么？
- en: Is there a central repository to keep track of all the models deployed?
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有一个中央存储库来跟踪所有已部署的模型？
- en: Are input assumptions checked continuously?
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入假设是否在持续检查中？
- en: Are accuracy and fairness metrics monitored continuously?
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确性和公平性指标是否在持续监控中？
- en: Exercises
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'In this chapter, you have learned a lot about both the technical and non-technical
    considerations of fairness in machine learning. These exercises will help you
    think much more deeply about the topic:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你已经了解了机器学习中公平性问题的技术性和非技术性考量。这些练习将帮助你更深入地思考这一话题：
- en: Think about the organization you work for. How is fairness incorporated in your
    organization? What works well and what could be improved?
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想想你所在的组织。公平性是如何在你的组织中被融入的？哪些做得好，哪些可以改进？
- en: Revisit any of the models developed in this book. Are they fair? How would you
    test them for fairness?
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新审视本书中开发的任何模型。它们公平吗？你将如何测试它们的公平性？
- en: Fairness is only one of the many complex issues large models can have. Can you
    think of an issue in your area of work that could be tackled with the tools discussed
    in this chapter?
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公平性只是大模型可能面临的众多复杂问题之一。你能想到在你的工作领域中，能够利用本章讨论的工具解决的某个问题吗？
- en: Summary
  id: totrans-355
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have learned about fairness in machine learning in different
    aspects. First, we discussed legal definitions of fairness and quantitative ways
    to measure these definitions. We then discussed technical methods to train models
    to meet fairness criteria. We also discussed causal models. We learned about SHAP
    as a powerful tool to interpret models and find unfairness in a model. Finally,
    we learned how fairness is a complex systems issue and how lessons from complex
    systems management can be applied to make models fair.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经从不同的角度学习了机器学习中的公平性。首先，我们讨论了公平性的法律定义和量化这些定义的方式。接着，我们探讨了训练模型以满足公平标准的技术方法。我们还讨论了因果模型，学习了如何通过SHAP这一强大的工具来解释模型并发现模型中的不公平。最后，我们了解到公平性是一个复杂的系统问题，且如何将复杂系统管理的经验应用于使模型公平。
- en: There is no guarantee that following all the steps outlined here will make your
    model fair, but these tools vastly increase your chances of creating a fair model.
    Remember that models in finance operate in high-stakes environments and need to
    meet many regulatory demands. If you fail to do so, damage could be severe.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 不能保证按照此处列出的所有步骤会让你的模型变得公平，但这些工具大大提高了你创建公平模型的机会。请记住，金融领域的模型操作在高风险环境中，需要满足许多监管要求。如果未能做到这一点，可能会造成严重损害。
- en: In the next, and final, chapter of this book, we will be looking at probabilistic
    programming and Bayesian inference.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的下一章，也是最后一章，我们将讨论概率编程和贝叶斯推理。
