- en: '8'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '8'
- en: Quantum Neural Network
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 量子神经网络
- en: Quantum neural networks  [[100](Biblography.xhtml#XFarhi2018)] are parameterised
    quantum circuits that can be trained as either generative or discriminative machine
    learning models in direct analogy with their classical counterparts. In this chapter,
    we will consider parameterised quantum circuits trained as classifiers. In the
    most general case, a classifier is a function that takes an *N*-dimensional input
    and returns one of *M* possible class values. The classifier can be trained on
    a dataset of samples with known class labels by adjusting the configurable model
    parameters in such a way as to minimise the classification error. Once the classifier
    is fully trained, it can be exposed to new unseen samples for which correct class
    labels are unknown. Therefore, it is critically important to avoid overfitting
    to the training dataset and ensure that the classifier generalises well to the
    new data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 量子神经网络 [[100](Biblography.xhtml#XFarhi2018)] 是参数化的量子电路，可以作为生成型或判别型机器学习模型进行训练，直接类比于其经典对应物。在本章中，我们将考虑将量子电路作为分类器进行训练的情况。在最一般的情况下，分类器是一个函数，它接受一个
    *N* 维的输入，并返回 *M* 个可能类别值中的一个。分类器可以通过调整可配置的模型参数，在已知类别标签的样本数据集上进行训练，从而最小化分类误差。一旦分类器完全训练好，它就可以接受新的、未见过的样本，而这些样本的正确类别标签是未知的。因此，避免过拟合训练数据集并确保分类器能很好地推广到新数据是至关重要的。
- en: There are many similarities between quantum and classical neural networks. In
    both cases, the key element is the forward propagation of the signal (input),
    which is transformed by the network activation functions. Both quantum and classical
    neural networks can be trained through the backpropagation of error (differentiable
    learning) as well as through various non-differentiable learning techniques. However,
    there are also fundamental differences. For example, classical neural networks
    derive their power from the non-linear transformation of input. In contrast, all
    quantum gates are linear operators and the power of quantum neural networks comes
    from the mapping of the input into the high-dimensional Hilbert space where classification
    can be more easily done.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 量子神经网络和经典神经网络之间有许多相似之处。在这两种情况下，关键元素是信号（输入）的前向传播，这个信号通过网络的激活函数进行转换。量子神经网络和经典神经网络都可以通过误差反向传播（可微分学习）以及各种非可微分学习技术进行训练。然而，它们也存在一些根本性的差异。例如，经典神经网络的强大之处在于输入的非线性变换。相比之下，所有的量子门都是线性算符，量子神经网络的力量来自于将输入映射到高维的希尔伯特空间，在这个空间中，分类更容易进行。
- en: 8.1 Quantum Neural Networks
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 量子神经网络
- en: Figure [8.1](#x1-1630001) provides a schematic representation of a typical Quantum
    Neural Network (QNN) trained as a classifier. Let us have a look at the quantum
    circuit and understand how it operates. The network consists of *n* quantum registers,
    a number of one-qubit and two-qubit gates, and *m* measurement operators. The
    input is a quantum state ![|ψ ⟩ k](img/file747.jpg) encoding the *k*-th sample
    from the dataset. If our dataset is classical, then every classical sample should
    first be encoded in the input quantum state (as explained in the previous chapter).
    With *m* measurement operators, the output is a bitstring that can encode up to 2^m
    integer values (class labels). In the case of a binary classifier, it is sufficient
    to perform measurement on a single qubit.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [8.1](#x1-1630001) 提供了典型的量子神经网络（QNN）作为分类器训练的示意图。让我们来看一下量子电路，了解它是如何工作的。网络由
    *n* 个量子寄存器、一系列的单量子比特门和双量子比特门，以及 *m* 个测量算符组成。输入是一个量子态 ![|ψ ⟩ k](img/file747.jpg)，它编码了来自数据集的第
    *k* 个样本。如果我们的数据集是经典的，那么每个经典样本应首先编码成输入量子态（如前一章所述）。通过 *m* 个测量算符，输出是一个比特串，可以编码最多
    2^m 个整数值（类别标签）。对于二分类器，只需要对单个量子比特进行测量即可。
- en: '![Figure 8.1: Schematic representation of a quantum neural network – parameterised
    quantum circuit – consisting of 1-qubit and 2-qubit gates and measurement operators
    on one or more quantum registers. The initial state |ψk⟩ encodes the k-th sample
    from the dataset. ](img/file749.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1：量子神经网络的示意图 —— 参数化量子电路 —— 由 1 量子比特和 2 量子比特门以及一个或多个量子寄存器上的测量算符组成。初始状态
    |ψk⟩ 编码了来自数据集的第 k 个样本。](img/file749.jpg)'
- en: 'Figure 8.1: Schematic representation of a quantum neural network – parameterised
    quantum circuit – consisting of 1-qubit and 2-qubit gates and measurement operators
    on one or more quantum registers. The initial state ![|ψk⟩](img/file748.jpg) encodes
    the k-th sample from the dataset.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：量子神经网络的示意图——带参数的量子电路——由单量子比特和双量子比特门以及在一个或多个量子寄存器上的测量算符组成。初始状态![|ψk⟩](img/file748.jpg)编码了数据集中第k个样本。
- en: The measurement process produces a single sample from the probability distribution
    encoded in the quantum state. Therefore, we need to run the quantum circuit many
    times for the same input in order to collect sufficient statistics for each qubit
    on which we perform measurement.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 测量过程从量子态中编码的概率分布中产生一个单一的样本。因此，我们需要对相同的输入多次运行量子电路，以便为我们进行测量的每个量子比特收集足够的统计数据。
- en: 'For example, if our QNN is organised as a classifier that should be able to
    predict one of the four possible classes ("0", "1", "2", and "3"), then we would
    need to perform measurement on 2 qubits with possible outcomes ![|00⟩](img/file750.jpg)
    corresponding to class "0", ![|01⟩](img/file751.jpg) corresponding to class "1",
    ![|10⟩](img/file752.jpg) corresponding to class "2", and ![|11⟩](img/file753.jpg)
    corresponding to class "3". Let us assume that we have run the quantum circuit
    1,000 times and observed the following results as shown in Table [8.1](#x1-163003r1):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的QNN被组织为一个分类器，能够预测四个可能的类别之一（“0”，“1”，“2”和“3”），那么我们需要对2个量子比特进行测量，可能的结果是![|00⟩](img/file750.jpg)对应类别“0”，![|01⟩](img/file751.jpg)对应类别“1”，![|10⟩](img/file752.jpg)对应类别“2”，以及![|11⟩](img/file753.jpg)对应类别“3”。假设我们已经运行了量子电路1,000次，并观察到如表[8.1](#x1-163003r1)所示的结果：
- en: '| Measured bitstring | Class label | Number of observations |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 测量位串 | 类别标签 | 观察次数 |'
- en: '| 00 | 0 | 100 |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 00 | 0 | 100 |'
- en: '| 01 | 1 | 550 |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 01 | 1 | 550 |'
- en: '| 10 | 2 | 200 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 2 | 200 |'
- en: '| 11 | 3 | 150 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 3 | 150 |'
- en: 'Table 8.1: 1,000 runs of the quantum circuit.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1：量子电路的1,000次运行。
- en: Then we can conclude that the most likely class label for the given input is
    class "1" (with probability 55%). At the same time, we also obtain probabilities
    for all other possible class values, which may be useful in some cases.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以得出结论，对于给定的输入，最可能的类别标签是类别“1”（概率为55%）。同时，我们还获得了所有其他可能类别值的概率，这在某些情况下可能会有用。
- en: The network is organised as *l* layers of one-qubit and two-qubit gates. The
    gates can be *adjustable*, meaning that they can be controlled by adjustable parameters,
    such as rotation angles, or they can be *fixed*. The 2-qubit gates in Figure [8.1](#x1-1630001)
    are fixed CX gates but, in principle, they can be adjustable controlled rotation
    gates. Although the network shown schematically in Figure [8.1](#x1-1630001) can
    have up to *n* × *l* adjustable parameters (*𝜃*[i]^j)[i=1,…,n; j=1,…,l], it is
    often the case that the two-qubit gates are fixed and we only have one-qubit rotations
    as available degrees of freedom in training the network.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 网络组织为 *l* 层的单量子比特和双量子比特门。门可以是*可调的*，意味着它们可以通过可调参数（如旋转角度）来控制，或者它们也可以是*固定的*。图[8.1](#x1-1630001)中的双量子比特门是固定的CX门，但原则上，它们可以是可调的受控旋转门。尽管图[8.1](#x1-1630001)中示意性地展示的网络最多可以有
    *n* × *l* 个可调参数（*𝜃*[i]^j)[i=1,…,n; j=1,…,l]），但通常情况下，双量子比特门是固定的，我们在训练网络时只有单量子比特旋转作为可用的自由度。
- en: Similar to classical neural networks, QNNs can be trained through either differentiable
    learning (for example, backpropagation of error with gradient descent) or non-differentiable
    learning (e.g., evolutionary search heuristics). Both approaches have their relative
    strengths and weaknesses. In theory, differentiable learning can be faster, but
    convergence is not guaranteed due to the well-known problem of "barren plateaus"
    associated with the gradients becoming vanishingly small  [[207](Biblography.xhtml#XMcClean2018)]
    and is problem dependent. Non-differentiable learning is, as a rule, slower but
    avoids being trapped in local minima and works well in situations where the cost
    function is not smooth. Sections [8.2](#x1-1640002) and [8.3](#x1-1680003) provide
    detailed descriptions of the QNN training procedures.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于经典神经网络，QNNs可以通过可微分学习（例如，使用梯度下降的误差反向传播）或非可微分学习（例如，进化搜索启发式算法）进行训练。两种方法各有其相对的优缺点。从理论上讲，可微分学习可能更快，但由于著名的“荒原平台”问题（即梯度变得极其微小[[207](Biblography.xhtml#XMcClean2018)]），其收敛性并不能得到保证，且受具体问题的影响。非可微分学习通常较慢，但避免了被困在局部最小值，并且在代价函数不光滑的情况下表现良好。第[8.2](#x1-1640002)节和第[8.3](#x1-1680003)节提供了QNN训练过程的详细描述。
- en: Obviously, the strongest motivation for using quantum classifiers is their ability
    to process quantum data. The input quantum states that must be classified may
    be outputs of some other quantum circuits. As we may not be able to store the
    information encoded in these quantum states classically, a quantum classifier
    becomes an indispensable tool. However, quantum classifiers have a realistic chance
    to demonstrate their advantage on purely classical data too. There are several
    considerations that motivate our interest in trying to apply QNNs to classical
    datasets.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，使用量子分类器的最强动力是它们能够处理量子数据。必须分类的输入量子态可能是其他量子电路的输出。由于我们可能无法将这些量子态中编码的信息以经典方式存储，量子分类器成为了一个不可或缺的工具。然而，量子分类器也有可能在纯经典数据上展示其优势。以下是几个动机，促使我们有兴趣尝试将QNNs应用于经典数据集。
- en: First, parameterised quantum circuits possess a larger expressive power than
    equivalent classical neural networks. Second, they are structurally able to efficiently
    fight overfitting. Finally, quantum speedup is achievable on some types of quantum
    hardware for specific use cases even at these very early stages of quantum computing
    development. Chapter [12](Chapter_12.xhtml#x1-22500012) investigates these questions
    in more detail.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，参数化量子电路的表达能力大于等效的经典神经网络。其次，量子电路结构上能够高效地应对过拟合问题。最后，在量子计算发展的初期阶段，对于特定的使用案例，某些类型的量子硬件上是可以实现量子加速的。第[12](Chapter_12.xhtml#x1-22500012)章更详细地探讨了这些问题。
- en: In this chapter, we focus on using QNNs to efficiently solve specific finance-related
    classification use cases and provide a comparison with a number of standard classical
    classifiers. While experimentally proving quantum speedup and larger expressive
    power of QNNs requires powerful quantum hardware, the way QNNs fight overfitting
    can be verified on relatively small and shallow quantum circuits with the help
    of open-source quantum simulators.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们重点介绍如何使用量子神经网络（QNNs）高效地解决特定的与金融相关的分类问题，并与多种标准的经典分类器进行对比。虽然通过实验证明量子加速和量子神经网络更大表达能力的优势需要强大的量子硬件，但QNNs如何应对过拟合的问题可以在相对较小且浅的量子电路上进行验证，并借助开源量子模拟器实现。
- en: QNNs are PQCs trained as ML models such as classifiers. QNNs have a natural
    advantage over classical neural networks when it comes to classifying quantum
    data. However, classical datasets can also be encoded as quantum states and processed
    by QNNs with their larger expressive power, their ability to efficiently fight
    overfitting, and, ultimately, with their quantum speedup.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: QNNs是作为机器学习模型（如分类器）进行训练的参数化量子电路。量子神经网络在处理量子数据时，相比经典神经网络具有天然的优势。然而，经典数据集也可以编码为量子态，并通过QNNs进行处理，借助它们更强大的表达能力、有效应对过拟合的能力，最终实现量子加速。
- en: As we learned from Chapter [5](Chapter_5.xhtml#x1-960005), to specify the architecture
    of the neural network is not sufficient to build the working ML model – it is
    also necessary to specify the training algorithm. In the following sections, we
    show how a quantum neural network can be trained with the help of differentiable
    and non-differentiable learning methods.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第[5章](Chapter_5.xhtml#x1-960005)中学到的，单单指定神经网络的架构不足以构建有效的机器学习模型——还需要指定训练算法。在接下来的章节中，我们将展示如何通过可微和不可微的学习方法来训练量子神经网络。
- en: 8.2 Training QNN with Gradient Descent
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 使用梯度下降训练量子神经网络
- en: Since we are not only interested in building QNNs as standalone QML tools but
    also in comparing and contrasting them with classical neural networks, we start
    our review of QNN training methods with gradient descent – a ubiquitous classical
    ML algorithm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不仅仅关注构建作为独立工具的量子神经网络，还希望将其与经典神经网络进行比较和对比，因此我们首先回顾使用梯度下降法训练量子神经网络——这是一个普遍使用的经典机器学习算法。
- en: 8.2.1 The finite difference scheme
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1 有限差分方案
- en: 'Training QNNs consists of specifying and executing a procedure that finds an
    optimal configuration of the adjustable rotation parameters 𝜃. Assume that a QNN
    is specified on *n* quantum registers with *l* layers of adjustable quantum gates,
    where each adjustable gate is controlled by a single parameter (*𝜃*[i]^j)[i=1,…,n;
    j=1,…,l]. In this case, 𝜃 ∈ℳ[n,l] is an *n*×*l* matrix of adjustable network parameters:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 训练量子神经网络包括指定并执行一个程序，该程序可以找到可调旋转参数𝜃的最优配置。假设一个量子神经网络被指定在*n*个量子寄存器上，并具有*l*层可调量子门，其中每个可调门由一个单一参数控制（*𝜃*[i]^j)[i=1,…,n;
    j=1,…,l]）。在这种情况下，𝜃 ∈ℳ[n,l]是一个*n*×*l*的可调网络参数矩阵：
- en: '| ![ ⌊ 1 l⌋ 𝜃1 ... 𝜃1 𝜃 = &#124;&#124; .. ... .. &#124;&#124; . ⌈ . . ⌉ 𝜃1n
    ... 𝜃ln ](img/file754.jpg) |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| ![ ⌊ 1 l⌋ 𝜃1 ... 𝜃1 𝜃 = &#124;&#124; .. ... .. &#124;&#124; . ⌈ . . ⌉ 𝜃1n
    ... 𝜃ln ](img/file754.jpg) |  |'
- en: Without loss of generality, we assume that we work with a binary classifier.
    The latter takes an input (a quantum state that encodes a sample from the dataset),
    applies a sequence of quantum gates (the parameterised quantum circuit controlled
    by at most *n* × *l* adjustable parameters), and performs the measurement of an
    observable *M* on the chosen quantum register. An example of an observable is
    the Pauli Z gate and the result of a single measurement is ±1 for a qubit found
    in the state ![|0⟩](img/file755.jpg) or ![|1⟩](img/file756.jpg), respectively.
    The value of the measured observable is mapped into a value of a binary variable
    {0, 1}. This process is repeated *N* times for each sample in order to collect
    sufficient statistics for the classification result.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在不失一般性的前提下，我们假设使用的是二分类器。该分类器接收一个输入（一个量子态，编码了来自数据集的一个样本），应用一系列量子门（由最多*n* × *l*
    可调参数控制的参数化量子电路），并对选定的量子寄存器进行可观察量*M*的测量。一个可观察量的例子是保利Z门，对于一个量子比特，若其处于态![|0⟩](img/file755.jpg)或![|1⟩](img/file756.jpg)，单次测量的结果分别为±1。测量到的可观察量值会映射为二元变量{0，1}的值。这个过程会针对每个样本重复*N*次，以便收集足够的统计数据用于分类结果。
- en: 'The first step in finding an optimal configuration of adjustable parameters 𝜃
    is to choose an appropriate cost function – an objective function that represents
    the total error in classifying samples from the training dataset and which can
    be minimised by changing the adjustable network parameters. Let y := (*y*[1]*,…,y*[K])
    be a vector of binary labels and f(𝜃) := (*f*[1](𝜃)*,…,f*[K](𝜃)) a vector of binary
    classifier predictions for the training dataset consisting of *K* samples. The
    cost function *L*(𝜃) can then be defined, for example, as the sum of squared errors
    across all samples in the training dataset:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找可调参数𝜃的最优配置的第一步是选择一个合适的成本函数——一个表示在分类训练数据集样本时总误差的目标函数，且可以通过改变可调网络参数来最小化。设y :=
    (*y*[1]*,…,y*[K])为二元标签向量，f(𝜃) := (*f*[1](𝜃)*,…,f*[K](𝜃))为训练数据集中*K*个样本的二分类器预测结果向量。那么，成本函数*L*(𝜃)可以定义为，举例来说，训练数据集所有样本的平方误差之和：
- en: '| ![ ∑K L (𝜃) := 1 (yk − fk(𝜃))2\. 2 k=1 ](img/file757.jpg) |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∑K L (𝜃) := 1 (yk − fk(𝜃))2\. 2 k=1 ](img/file757.jpg) |  |'
- en: The next step is an iterative update of the adjustable parameters in the direction
    that reduces the value of the cost function. That direction is given by the cost
    function gradient – hence the name of the method. The parameters are updated towards
    the direction of the steepest descent of the cost function. At step *u* + 1, we
    update the system to
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是沿着能够减少代价函数值的方向进行可调参数的迭代更新。这个方向由代价函数的梯度给出——因此该方法得名。参数沿着代价函数的最速下降方向进行更新。在第
    *u* + 1 步，我们将系统更新为：
- en: '| ![ ∂L(𝜃) u+1 𝜃ij← − u𝜃ji − η---j-, for each i = 1,...,n, j = 1,...,l, ∂𝜃i
    ](img/file758.jpg) |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∂L(𝜃) u+1 𝜃ij← − u𝜃ji − η---j-, for each i = 1,...,n, j = 1,...,l, ∂𝜃i
    ](img/file758.jpg) |  |'
- en: 'where *η* is the learning rate, namely a hyperparameter controlling the magnitude
    of the update. For each *i* = 1*,…,n*, *j* = 1*,…,l*, the derivative can be calculated
    numerically using a finite difference scheme:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *η* 是学习率，即控制更新幅度的超参数。对于每个 *i* = 1*,…,n*，*j* = 1*,…,l*，可以使用有限差分法来数值计算导数：
- en: '| ![ j j j j ∂L-(𝜃)- L(𝜃11,...,𝜃i-+-Δ𝜃i,...,𝜃ln)−-L-(𝜃11,...,𝜃i-−-Δ𝜃i,...,𝜃ln)-
    ∂𝜃j ≈ 2Δ 𝜃j , i i ](img/file759.jpg) |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| ![ j j j j ∂L-(𝜃)- L(𝜃11,...,𝜃i-+-Δ𝜃i,...,𝜃ln)−-L-(𝜃11,...,𝜃i-−-Δ𝜃i,...,𝜃ln)-
    ∂𝜃j ≈ 2Δ 𝜃j , i i ](img/file759.jpg) |  |'
- en: 'with an error of order 𝒪((Δ*𝜃*[i]^j)²), where Δ*𝜃*[i]^j is a small rotation
    angle increment. The physical characteristics of the NISQ devices put restrictions
    on how small this increment can be: in most cases Δ*𝜃*[i]^j should not be smaller
    than 0*.*1 radians. The rest of the training routine follows the standard classical
    algorithm of training neural networks through the backpropagation of error with
    gradient descent.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 误差的阶数为 𝒪((Δ*𝜃*[i]^j)²)，其中 Δ*𝜃*[i]^j 是一个小的旋转角度增量。NISQ设备的物理特性对这一增量的最小值有所限制：在大多数情况下，Δ*𝜃*[i]^j
    不应小于 0*.*1 弧度。其余的训练过程遵循经典的神经网络训练算法，通过反向传播误差并使用梯度下降法进行优化。
- en: 8.2.2 The analytic gradient approach
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2 分析梯度方法
- en: An alternative to the finite difference method, which can be unstable and ill-conditioned
    due to truncation and round-off errors (for parameterised quantum circuits  [[29](Biblography.xhtml#XBenedetti2019)]
    or, in fact, for classical neural networks  [[27](Biblography.xhtml#XBaydin)]),
    is the analytic gradient approach. It can be a viable choice for parameterised
    quantum circuits with adjustable one-qubit gates and fixed multi-qubit gates.
    From ([8.2.1](#x1-1650001)), the cost function gradient with respect to the parameter *𝜃*[i]^j
    is given by
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代有限差分法的方法是分析梯度法，它能有效避免由于截断和舍入误差导致的不稳定和病态情况（无论是对于参数化量子电路 [[29](Biblography.xhtml#XBenedetti2019)]，还是对于经典神经网络
    [[27](Biblography.xhtml#XBaydin)]），它对于具有可调单量子比特门和固定多量子比特门的参数化量子电路是一个可行的选择。从([8.2.1](#x1-1650001))中可以得出，代价函数相对于参数
    *𝜃*[i]^j 的梯度为：
- en: '| ![ K ∂L-(𝜃)= − ∑ (y − f (𝜃)) ∂fk-(𝜃), ∂ 𝜃ji k k ∂𝜃ji k=1 ](img/file760.jpg)
    |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| ![ K ∂L-(𝜃)= − ∑ (y − f (𝜃)) ∂fk-(𝜃), ∂ 𝜃ji k k ∂𝜃ji k=1 ](img/file760.jpg)
    |  |'
- en: so that the task of calculating the gradient of the cost function is reduced
    to the task of calculating the partial derivative of the expected value of the
    measurement operator for each sample quantum state that encodes the classical
    sample from the training dataset. Let ![|ψk⟩](img/file761.jpg) be the quantum
    state that encodes the *k*-th sample from the training dataset and let U(𝜃) denote
    the unitary operator that represents the sequence of QNN gates transforming the
    initial state ![|ψk⟩](img/file762.jpg). Then the expected value of the measurement
    operator M is given by
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，计算代价函数梯度的任务可以简化为计算每个样本量子态的测量算符的期望值的偏导数，该量子态编码了来自训练数据集的经典样本。设 ![|ψk⟩](img/file761.jpg)
    为编码训练数据集中的第 *k* 个样本的量子态，且设 U(𝜃) 为表示一系列QNN门的幺正算符，这些门将初始态 ![|ψk⟩](img/file762.jpg)
    转换为最终状态。那么，测量算符 M 的期望值为：
- en: '| ![fk(𝜃) = ⟨ψk &#124;U †(𝜃)MU(𝜃) &#124;ψk ⟩. ](img/file763.jpg) |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| ![fk(𝜃) = ⟨ψk &#124;U †(𝜃)MU(𝜃) &#124;ψk ⟩. ](img/file763.jpg) |  |'
- en: According to the conventions we used in constructing the QNN ansatz, the parameter *𝜃*[i]^j
    only affects a single gate, which we will denote as G(*𝜃*[i]^j). Therefore, the
    sequence of gates U(𝜃) can be represented as
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们在构造QNN假设时使用的约定，参数 *𝜃*[i]^j 仅影响一个单独的门，我们将其表示为 G(*𝜃*[i]^j)。因此，门序列 U(𝜃) 可以表示为：
- en: '| ![U(𝜃) = VG(𝜃ji)W, ](img/file764.jpg) |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ![U(𝜃) = VG(𝜃ji)W, ](img/file764.jpg) |  |'
- en: 'where W and V are gate sequences that precede and follow gate G(*𝜃*[i]^j).
    Let us absorb V into the Hermitian observable Q = V^†MV and W into the quantum
    state ![|ϕ ⟩ k](img/file765.jpg) = W![|ψ ⟩ k](img/file766.jpg):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 W 和 V 是在门 G(*𝜃*[i]^j) 之前和之后的门序列。我们可以将 V 吸收到厄米观测量 Q = V^†MV 中，并将 W 吸收到量子态
    ![|ϕ ⟩ k](img/file765.jpg) = W![|ψ ⟩ k](img/file766.jpg) 中：
- en: '| ![ † j j fk(𝜃) = ⟨ϕk&#124;G (𝜃i)QG(𝜃i) &#124;ϕk⟩. ](img/file767.jpg) |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| ![ † j j fk(𝜃) = ⟨ϕk&#124;G (𝜃i)QG(𝜃i) &#124;ϕk⟩. ](img/file767.jpg) |  |'
- en: Then the partial derivative of *f*[k](𝜃) with respect to parameter *𝜃*[i]^j
    is calculated as
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，*f*[k](𝜃) 关于参数 *𝜃*[i]^j 的偏导数计算为
- en: '| ![∂fk(𝜃) ----j-- ∂ 𝜃i](img/file768.jpg) | = ![ ∂ ---j ∂ 𝜃i](img/file769.jpg)
    ⟨*ϕ*[k]&#124;G^†(*𝜃* [i]^j)QG(*𝜃* [i]^j)![&#124;ϕk⟩](img/file770.jpg) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ![∂fk(𝜃) ----j-- ∂ 𝜃i](img/file768.jpg) | = ![ ∂ ---j ∂ 𝜃i](img/file769.jpg)
    ⟨*ϕ*[k]&#124;G^†(*𝜃* [i]^j)QG(*𝜃* [i]^j)![&#124;ϕk⟩](img/file770.jpg) |'
- en: '|  | = ⟨*ϕ*[k]&#124;![( ) ∂G (𝜃j) ----ij- ∂𝜃i](img/file771.jpg)^†QG(*𝜃* [i]^j)![&#124;ϕk⟩](img/file772.jpg)
    + ⟨*ϕ*[k]&#124;G^†(*𝜃* [i]^j)Q![( ) ∂G(𝜃j) ---ji- ∂𝜃i](img/file773.jpg)![&#124;ϕk⟩](img/file774.jpg)*.*
    | (8.2.1) |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | = ⟨*ϕ*[k]&#124;![( ) ∂G (𝜃j) ----ij- ∂𝜃i](img/file771.jpg)^†QG(*𝜃* [i]^j)![&#124;ϕk⟩](img/file772.jpg)
    + ⟨*ϕ*[k]&#124;G^†(*𝜃* [i]^j)Q![( ) ∂G(𝜃j) ---ji- ∂𝜃i](img/file773.jpg)![&#124;ϕk⟩](img/file774.jpg)*.*
    | (8.2.1) |  |'
- en: Let us denote
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们表示
- en: '| ![ ∂G (𝜃j) B := G(𝜃ji) and C :=----ji, ∂𝜃i ](img/file775.jpg) |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| ![ ∂G (𝜃j) B := G(𝜃ji) 和 C :=----ji, ∂𝜃i ](img/file775.jpg) |  |'
- en: and notice that
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 并且注意到
- en: '| ![ † † ⟨ϕk&#124;C( QB &#124;ϕk ⟩+ ⟨ϕk&#124;B QC &#124;ϕk ⟩ ) 1- † † = 2 ⟨ϕk
    &#124;(B + C) Q(B+ C) &#124;ϕk ⟩− ⟨ϕk&#124;(B− C) Q(B − C) &#124;ϕk⟩ . ](img/file776.jpg)
    |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| ![ † † ⟨ϕk&#124;C( QB &#124;ϕk ⟩+ ⟨ϕk&#124;B QC &#124;ϕk ⟩ ) 1- † † = 2 ⟨ϕk
    &#124;(B + C) Q(B+ C) &#124;ϕk ⟩− ⟨ϕk&#124;(B− C) Q(B − C) &#124;ϕk⟩ . ](img/file776.jpg)
    |  |'
- en: Therefore, if we can find the way to implement the operator B±C as part of an
    overall unitary evolution then we can evaluate ([8.2.1](#x1-166003r1)) directly.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们能够找到实现操作符 B±C 作为整体单位演化的一部分的方法，那么我们可以直接计算 ([8.2.1](#x1-166003r1))。
- en: 8.2.3 The parameter shift rule for analytic gradient calculation
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.3 分析梯度计算的参数偏移规则
- en: Following  [[257](Biblography.xhtml#XSchuld2018)], we outline the parameter
    shift rule for gates with generators with two distinct eigenvalues – this covers
    all one-qubit gates. Being unitary, the gate G(*𝜃*[i]^j) above can be represented
    as
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [[257](Biblography.xhtml#XSchuld2018)]，我们概述了具有两个不同特征值的生成元的门的参数偏移规则——这涵盖了所有单量子比特门。作为单位操作，门
    G(*𝜃*[i]^j) 可以表示为
- en: '| ![ ( ) G(𝜃ji) = exp − i𝜃jiΓ , ](img/file777.jpg) |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ) G(𝜃ji) = exp − i𝜃jiΓ , ](img/file777.jpg) |  |'
- en: for some Hermitian operator Γ (Theorem [6](Chapter_1.xhtml#x1-29009r6)). The
    partial derivative with respect to *𝜃*[i]^j reads
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些厄米算符 Γ（定理 [6](Chapter_1.xhtml#x1-29009r6)）。关于 *𝜃*[i]^j 的偏导数为
- en: '| ![ j ( ) ∂G-(𝜃i) = − iΓ exp − i𝜃jiΓ = − iΓ G(𝜃ji). ∂𝜃ji ](img/file778.jpg)
    |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| ![ j ( ) ∂G-(𝜃i) = − iΓ exp − i𝜃jiΓ = − iΓ G(𝜃ji). ∂𝜃ji ](img/file778.jpg)
    |  |'
- en: Substituting ([8.2.3](#x1-1670003)) into ([8.2.1](#x1-166003r1)) yields
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将 ([8.2.3](#x1-1670003)) 代入 ([8.2.1](#x1-166003r1)) 得到
- en: '| ![ ⟩ ⟩ ∂fk(𝜃)-= ⟨ϕ′&#124;iΓ Q &#124;ϕ′ + ⟨ϕ′&#124;Q(− iΓ ) &#124;ϕ ′ , ∂𝜃ji
    k k k k ](img/file779.jpg) |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ![ ⟩ ⟩ ∂fk(𝜃)-= ⟨ϕ′&#124;iΓ Q &#124;ϕ′ + ⟨ϕ′&#124;Q(− iΓ ) &#124;ϕ ′ , ∂𝜃ji
    k k k k ](img/file779.jpg) |  |'
- en: where ![ ′⟩ |ϕ k](img/file780.jpg) = G(*𝜃*[i]^j)![|ϕk⟩](img/file781.jpg). If Γ
    has just two distinct eigenvalues we can shift the eigenvalues to ±*r*, since
    the global phase is unobservable  [[257](Biblography.xhtml#XSchuld2018)]. With
    I denoting the identity operator we can rewrite ([8.2.3](#x1-1670003)) as
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![ ′⟩ |ϕ k](img/file780.jpg) = G(*𝜃*[i]^j)![|ϕk⟩](img/file781.jpg)。如果 Γ 只有两个不同的特征值，我们可以将特征值移至
    ±*r*，因为全局相位是不可观察的 [[257](Biblography.xhtml#XSchuld2018)]。用 I 表示单位算符，我们可以将 ([8.2.3](#x1-1670003))
    重写为
- en: '| ![ ( ) ∂fk(𝜃)- ′ iΓ- ′⟩ ′ iΓ- ′⟩ j = r ⟨ϕ k&#124;r QI &#124;ϕk − ⟨ϕk&#124;IQ
    r &#124;ϕk . ∂𝜃i ](img/file782.jpg) |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ) ∂fk(𝜃)- ′ iΓ- ′⟩ ′ iΓ- ′⟩ j = r ⟨ϕ k&#124;r QI &#124;ϕk − ⟨ϕk&#124;IQ
    r &#124;ϕk . ∂𝜃i ](img/file782.jpg) |  |'
- en: Denoting
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表示
- en: '| ![ i B := I and C := − rΓ , ](img/file783.jpg) |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ![ i B := I 和 C := − rΓ , ](img/file783.jpg) |  |'
- en: 'and using ([8.2.2](#x1-166003r2)) we obtain from ([8.2.3](#x1-1670003)):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用 ([8.2.2](#x1-166003r2)) 我们从 ([8.2.3](#x1-1670003)) 中得到：
- en: '| ![ [ ] ∂f (𝜃) r ′ ( i ) † ( i ) ′⟩ ′ ( i )† ( i ) ′⟩ --k-j--= -- ⟨ϕk&#124;
    I− - Γ Q I− -Γ &#124;ϕk − ⟨ϕk&#124; I + -Γ Q I + - Γ &#124;ϕk . ∂𝜃i 2 r r r r
    ](img/file784.jpg) |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| ![ [ ] ∂f (𝜃) r ′ ( i ) † ( i ) ′⟩ ′ ( i )† ( i ) ′⟩ --k-j--= -- ⟨ϕk&#124;
    I− - Γ Q I− -Γ &#124;ϕk − ⟨ϕk&#124; I + -Γ Q I + - Γ &#124;ϕk . ∂𝜃i 2 r r r r
    ](img/file784.jpg) |  |'
- en: A straightforward computation  [[257](Biblography.xhtml#XSchuld2018), Theorem
    1] shows that if the Hermitian generator Γ of the unitary operator G(*𝜃*) = exp(−i*𝜃*Γ)
    has at most two unique eigenvalues ±*r*, then
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个直接的计算 [[257](Biblography.xhtml#XSchuld2018), Theorem 1] 显示，如果单位算符 G(*𝜃*) =
    exp(−i*𝜃*Γ) 的厄米生成元 Γ 至多具有两个唯一特征值 ±*r*，那么
- en: '| ![ ( ) ( -π-) -1- i G ∓ 4r = √2-- I± rΓ . ](img/file785.jpg) |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ) ( -π-) -1- i G ∓ 4r = √2-- I± rΓ . ](img/file785.jpg) |  |'
- en: 'In this case the gradient can be estimated using two additional evaluations
    of the quantum circuit. Either the gate G(*π∕*(4*r*)) or the gate G(−*π∕*(4*r*))
    should be placed in the original circuit next to the gate we are differentiating.
    Since for unitarily generated one-parameter gates G(*a*)G(*b*) = G(*a* + *b*),
    this is equivalent to shifting the gate parameter, and we obtain the “parameter
    shift rule”  [[257](Biblography.xhtml#XSchuld2018)] with the shift *s* = *π∕*(4*r*):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可以使用两个额外的量子电路评估来估算梯度。应该在原始电路中将门G(*π∕*(4*r*))或门G(−*π∕*(4*r*))放置在我们要微分的门旁边。由于对于单位生成的单参数门G(*a*)G(*b*)
    = G(*a* + *b*)，这相当于偏移门参数，因此我们获得了“参数偏移规则”[[257](Biblography.xhtml#XSchuld2018)]，其中偏移量*s*
    = *π∕*(4*r*)：
- en: '| ![ ( ) ∂fk(𝜃)-= r ⟨ϕk &#124;G†(𝜃j + s)QG(𝜃j+ s) &#124;ϕk⟩− ⟨ϕk &#124;G †(𝜃j
    − s)QG (𝜃j − s) &#124;ϕk⟩ . ∂𝜃ji i i i i ](img/file786.jpg) |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ) ∂fk(𝜃)-= r ⟨ϕk &#124;G†(𝜃j + s)QG(𝜃j+ s) &#124;ϕk⟩− ⟨ϕk &#124;G †(𝜃j
    − s)QG (𝜃j − s) &#124;ϕk⟩ . ∂𝜃ji i i i i ](img/file786.jpg) |  |'
- en: 'If Γ is a one-qubit rotation generator given by Pauli X, Y, and Z operators,
    then *r* = 1*∕*2 and *s* = *π∕*2  [[213](Biblography.xhtml#XMitarai2018), [257](Biblography.xhtml#XSchuld2018)]:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Γ是由保利X、Y和Z算符给出的单量子比特旋转生成器，则*r* = 1*∕*2，*s* = *π∕*2[[213](Biblography.xhtml#XMitarai2018)，[257](Biblography.xhtml#XSchuld2018)]：
- en: '| ![∂fk(𝜃) 1( ( j π) ( j π ) ---j--= -- ⟨ϕk &#124;G† 𝜃i + -- QG 𝜃i +-- &#124;ϕk⟩
    ∂𝜃i 2 ( 2 ) ( 2 ) ) − ⟨ϕ &#124;G† 𝜃j− π- QG 𝜃j − π- &#124;ϕ ⟩ . k i 2 i 2 k ](img/file787.jpg)
    |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| ![∂fk(𝜃) 1( ( j π) ( j π ) ---j--= -- ⟨ϕk &#124;G† 𝜃i + -- QG 𝜃i +-- &#124;ϕk⟩
    ∂𝜃i 2 ( 2 ) ( 2 ) ) − ⟨ϕ &#124;G† 𝜃j− π- QG 𝜃j − π- &#124;ϕ ⟩ . k i 2 i 2 k ](img/file787.jpg)
    |  |'
- en: Therefore, what we need to do in order to estimate the gradient is to execute
    two circuits *N* times to collect statistics and to calculate the expectations
    on the right-hand side of ([8.2.3](#x1-1670003)). The first circuit will have
    the gate parameter shifted by *π∕*2 and the second circuit will have the gate
    parameter shifted by −*π∕*2.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要做的事情是执行两次电路*N*次以收集统计数据，并计算([8.2.3](#x1-1670003))右侧的期望值。第一个电路将门参数偏移*π∕*2，第二个电路将门参数偏移−*π∕*2。
- en: Although this procedure is not necessarily faster than the finite difference
    scheme, it can produce a more accurate estimate of the cost function gradient.
    The main argument here is the fact that the NISQ hardware operates with limited
    precision. The state-of-the-art superconducting qubits have one-qubit gate fidelity
    ≤ 99*.*9% and two-qubit gate fidelity ≤ 99*.*7% with rotation angle precision
    of order 0.05 radians. Therefore, the finite difference scheme cannot assume infinitesimal
    rotation angles Δ*𝜃* – they should not be smaller than about 0.1 radians (and,
    probably, materially larger in most cases). This means that gradients obtained
    with the finite difference scheme have some degree of built-in uncertainty that
    can only be fixed with further improvements in the NISQ hardware.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个过程不一定比有限差分方案更快，但它可以提供更准确的代价函数梯度估计。这里的主要论点是NISQ硬件在有限精度下运行。最先进的超导量子比特具有单量子比特门保真度≤99*.*9%和双量子比特门保真度≤99*.*7%，且旋转角度精度为0.05弧度。因此，有限差分方案不能假设无穷小的旋转角度Δ*𝜃*
    – 它们不应小于大约0.1弧度（并且在大多数情况下，可能要大得多）。这意味着使用有限差分方案获得的梯度具有一定程度的固有不确定性，只有通过进一步改善NISQ硬件才能解决。
- en: QNNs can be trained with the gradient descent algorithm in full analogy with
    the backpropagation of error in classical neural networks. The gradients can be
    either calculated analytically or estimated numerically.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: QNN可以通过梯度下降算法进行训练，完全类比于经典神经网络中的误差反向传播。梯度可以通过解析方法计算，或者通过数值方法估算。
- en: 8.3 Training QNN with Particle Swarm Optimisation
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 使用粒子群优化训练QNN
- en: Having specified the gradient descent scheme for training QNNs in the previous
    section, we now turn our attention to a non-differentiable learning method based
    on the powerful evolutionary search algorithm.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中已经指定了用于训练QNN的梯度下降方案，接下来我们将注意力转向一种基于强大进化搜索算法的不可微学习方法。
- en: 8.3.1 The Particle Swarm Optimisation algorithm
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.1 粒子群优化算法
- en: The Particle Swarm Optimisation (PSO) algorithm belongs to a wide class of evolutionary
    search heuristics where at each algorithm iteration ("generation" in the language
    of evolutionary algorithms), the population of solutions ("chromosomes" or "particles")
    is evaluated in terms of their fitness with respect to the environment. In the
    standard PSO formulation  [[236](Biblography.xhtml#XPoli2008)], a number of particles
    are placed in the solution space of some problem and each evaluates the fitness
    at its current location. Each particle then determines its movement through the
    solution space by combining some aspects of the history of its own fitness values
    with those of one or more members of the swarm, and then moves through the solution
    space with a velocity determined by the locations and processed fitness values
    of those other members, along with some random perturbations.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 粒子群优化（PSO）算法属于广泛的进化搜索启发式算法类，每次算法迭代（在进化算法的术语中称为“代”）时，解的种群（“染色体”或“粒子”）会根据其与环境的适应度进行评估。在标准的
    PSO 公式中 [[236](Biblography.xhtml#XPoli2008)]，一组粒子被放置在某个问题的解空间中，并且每个粒子会评估其当前位置的适应度。每个粒子接着结合自己适应度值的历史记录和一个或多个群体成员的适应度历史，来决定其在解空间中的移动，并以由这些成员的位置和处理后的适应度值以及一些随机扰动所决定的速度进行移动。
- en: 'It is a standard procedure  [[127](Biblography.xhtml#XHassan2005), [172](Biblography.xhtml#XKondratyev2017)]
    to follow three steps in specifying the PSO algorithm. First, we initialise the
    positions x[k]^i := (*x*[k]^i(1)*,…,x*[k]^i(*n*)) ∈ℝ^n of each particle *i* at
    time *k* moving through the *n*-dimensional search space and taking values in
    some range [x[min]*,*x[max]]. Next we initialise the velocities v[k]^i := (*v*[k]^i(1)*,…,v*[k]^i(*n*))
    ∈ℝ^n of each particle in the swarm. The initialisation process consists of distributing
    swarm particles randomly across the solution space:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个标准过程 [[127](Biblography.xhtml#XHassan2005), [172](Biblography.xhtml#XKondratyev2017)]，在指定
    PSO 算法时需要遵循三个步骤。首先，我们初始化每个粒子 *i* 在时间 *k* 时刻的位置信息 x[k]^i := (*x*[k]^i(1)*,…,x*[k]^i(*n*))
    ∈ℝ^n，粒子在 *n* 维搜索空间中移动并取值于某个范围 [x[min]*,*x[max]]。接下来，我们初始化群体中每个粒子的速度 v[k]^i :=
    (*v*[k]^i(1)*,…,v*[k]^i(*n*)) ∈ℝ^n。初始化过程包括将群体粒子随机分布到解空间中：
- en: '| ![ i i xmin + ωv(xmax − xmin) x0 = xmin + ωx (xmax − xmin), v0 = ----------Δt----------,
    ](img/file788.jpg) |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ![ i i xmin + ωv(xmax − xmin) x0 = xmin + ωx (xmax − xmin), v0 = ----------Δt----------,
    ](img/file788.jpg) |  |'
- en: where *ω*[x] and *ω*[v] are uniformly distributed random variables on [0*,*1]
    and Δ*t* is the time step between algorithm iterations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *ω*[x] 和 *ω*[v] 是均匀分布在 [0*,*1] 区间上的随机变量，Δ*t* 是算法迭代之间的时间步长。
- en: 'We then update the velocities of all particles at time *k* + 1 according to
    the specified objective function which depends on the particles’ current positions
    in the solution space at time *k*. The value of the objective function determines
    which particle has the best position p[k]^(global) in the current swarm and also
    determines the best position p^i of each particle over time, i.e., in the current
    and all previous moves. The velocity update formula uses these two pieces of information
    for each particle in the swarm along with the effect of the current motion v[k]^i
    to provide a search direction p[k+1]^i for the next iteration. The velocity update
    formula includes random parameters to ensure good coverage of the solution space
    and to avoid entrapment in local optima. The three values that affect the new
    search direction are the current motion, the particle’s own memory, and the swarm
    influence. They are incorporated via a summation approach with three weight factors:
    inertia *w*, self-confidence *c*[1], and swarm confidence *c*[2]:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们根据指定的目标函数更新所有粒子在时间 *k* + 1 时刻的速度，该目标函数依赖于粒子在解空间中时间 *k* 时刻的当前位置。目标函数的值决定了当前群体中哪个粒子拥有最佳位置
    p[k]^(global)，并且还决定了每个粒子随时间变化的最佳位置 p^i，即在当前及所有先前的移动中。速度更新公式使用这两部分信息（粒子的当前运动 v[k]^i）来提供下一个迭代的搜索方向
    p[k+1]^i。该公式还包括随机参数，以确保解空间的良好覆盖并避免陷入局部最优。影响新搜索方向的三个值是当前运动、粒子自身的记忆和群体的影响。这些通过求和方法与三个权重因子结合：惯性
    *w*、自信度 *c*[1] 和群体自信度 *c*[2]：
- en: '| ![ ( ) ( i i) pglobal− xi vik+1 = wvik + c1ω1-p-−-xk-+ c2ω2 --k-------k--,
    Δt Δt ](img/file789.jpg) |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ) ( i i) pglobal− xi vik+1 = wvik + c1ω1-p-−-xk-+ c2ω2 --k-------k--,
    Δt Δt ](img/file789.jpg) |  |'
- en: where *ω*[1] and *ω*[2] are uniformly distributed random variables on [0*,*1].
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *ω*[1] 和 *ω*[2] 是均匀分布在 [0*,*1] 区间上的随机变量。
- en: 'Finally, the position of each particle is updated using its velocity vector:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用每个粒子的速度向量来更新其位置：
- en: '| ![xik+1 = xik + vik+1Δt. ](img/file790.jpg) |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ![xik+1 = xik + vik+1Δt. ](img/file790.jpg) |  |'
- en: 'These steps are repeated until either a desired convergence criterion is met
    or until we reach the maximum number of iterations. Various reflection rules (stopping
    at the boundary, mirror reflection back into the allowed domain, etc.)  [[190](Biblography.xhtml#XLiuYangWang2010)]
    can be designed for the new position x[k+1]^i falling outside the [x[min]*,*x[max]]
    bounds and the dynamics can be normalised with Δ*t* ≡ 1\. If *K* is the last iteration
    of the algorithm, then the best solution found by the PSO is p[K]^(global). Figure [8.2](#8.2)
    provides a schematic illustration of the particle movement through the solution
    space under the influence of three forces: momentum, attraction to the globally
    best solution found by all particles at the previous iteration, and attraction
    to the best solution found by the given particle across all previous iterations.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤会重复进行，直到满足期望的收敛标准或达到最大迭代次数。可以设计各种反射规则（在边界处停止，镜像反射回允许的领域等）[[190](Biblography.xhtml#XLiuYangWang2010)]，以应对新的位置x[k+1]^i超出[x[min]*,*x[max]]边界的情况，并且可以通过Δ*t*
    ≡ 1来规范化动态。如果*K*是算法的最后一次迭代，那么PSO找到的最佳解是p[K]^(global)。图[8.2](#8.2)提供了粒子在三种力的作用下通过解空间的示意图：动量、吸引全体粒子在上一次迭代中找到的全局最佳解，以及吸引给定粒子在所有前次迭代中找到的最佳解。
- en: '![Figure 8.2: Schematic illustration of the PSO algorithm. Each particle moves
    through the solution space under the influence of three forces: momentum, own
    memory, and swarm influence. ](img/file791.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2：PSO算法的示意图。每个粒子在三种力的作用下在解空间中移动：动量、粒子的记忆和群体影响。](img/file791.jpg)'
- en: 'Figure 8.2: Schematic illustration of the PSO algorithm. Each particle moves
    through the solution space under the influence of three forces: momentum, own
    memory, and swarm influence.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：PSO算法的示意图。每个粒子在三种力的作用下在解空间中移动：动量、对前一次迭代中所有粒子找到的全局最佳解的吸引力，以及对给定粒子在所有前次迭代中找到的最佳解的吸引力。
- en: 8.3.2 PSO algorithm for training quantum neural networks
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.2 用于训练量子神经网络的PSO算法
- en: We are now ready to specify the PSO algorithm to train QNNs. We consider the
    most general case of an *n* × *l* matrix of adjustable parameters (rotations) 𝜃,
    where *n* is the number of quantum registers and *l* is the number of network
    layers. The solution we look for is the matrix ([8.2.1](#x1-1650001)) of adjustable
    parameters that minimises the chosen cost function.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备指定PSO算法来训练量子神经网络（QNN）。我们考虑一个* n * × * l *的可调参数（旋转）矩阵𝜃，其中*n*是量子寄存器的数量，*l*是网络层数。我们寻找的解决方案是可调参数矩阵([8.2.1](#x1-1650001))，它最小化所选的成本函数。
- en: 'The cost function can be specified in many different ways depending on what
    particular aspects we want to encourage or penalise. Given the training dataset,
    we would like to find a configuration of adjustable parameters 𝜃 such that as
    many samples as possible are classified correctly. One possible choice of cost
    function, for example, may be the ratio of incorrect to correct classification
    decisions. However, the classification process is probabilistic in nature – we
    decide on the sample label after many runs of the quantum circuit, which generate
    sufficient statistics. Therefore, each classification decision is not just right
    or wrong but can be seen as "more right" or "more wrong". If the correct sample
    label is "1" and we get "0" 51% of the time then the classifier is slightly wrong:
    the chances are that similar samples would be classified correctly or only a small
    change to the adjustable network parameters is required to rectify the classification
    process. But if we get "0", say, 90% of the time, then the classifier is "very
    wrong" and we need to penalise the outcome more aggressively.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数可以通过多种方式指定，具体取决于我们希望鼓励或惩罚哪些特定方面。给定训练数据集，我们希望找到一种可调参数𝜃的配置，使得尽可能多的样本被正确分类。例如，一种可能的成本函数选择是错误分类与正确分类决策的比率。然而，分类过程本质上是概率性的——我们在多次运行量子电路后决定样本标签，这些运行产生了足够的统计数据。因此，每个分类决策不仅仅是对错的问题，它也可以被看作是“更对”或“更错”。如果正确的样本标签是“1”，而我们51%的时间得到“0”，那么分类器略微错误：这种情况下，类似的样本有可能被正确分类，或者只需对可调的网络参数进行小的调整即可纠正分类过程。但如果我们90%的时间得到“0”，那么分类器就是“非常错”，我们需要更积极地惩罚这一结果。
- en: 'One possible realisation of the cost function that takes into account the above
    argument is as follows: Without loss of generality, assume that we work with the
    binary class labels "0" and "1", and let y := (*y*[1]*,…,y*[K]) be a vector of
    sample labels (either "0" or "1") from the training dataset. Further, let ℙ(𝜃)
    := (ℙ[1](𝜃)*,…,*ℙ[K](𝜃)) be a vector of QNN estimated probabilities of predicting
    class "1" for the given sample (i.e., the number of quantum circuit runs that
    returned "1" after measurement divided by the total number of quantum circuit
    runs). Then the cost function *L*(𝜃) is given by the following pseudo code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的代价函数实现方式，考虑到上述参数如下：不失一般性，假设我们使用二分类标签"0"和"1"，并且设 y := (*y*[1]*,…,y*[K])
    为一个样本标签向量（标签为"0"或"1"），来自训练数据集。进一步，设 ℙ(𝜃) := (ℙ[1](𝜃)*,…,*ℙ[K](𝜃)) 为QNN估计的样本预测为类别"1"的概率向量（即量子电路运行中返回"1"的次数除以总运行次数）。那么，代价函数
    *L*(𝜃) 由以下伪代码给出：
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This cost function penalises large errors in the class probability estimate
    more than small errors and represents the total error across all samples in the
    training dataset.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 该代价函数对类别概率估计的较大误差比小误差惩罚更重，且表示训练数据集中所有样本的总误差。
- en: 'We can now formulate the QNN training algorithm, which has the following inputs:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以制定QNN训练算法，该算法的输入如下：
- en: '| Variable | Meaning |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | 含义 |'
- en: '| X := (X[1]*,…,*X[K]) ∈ℝ^(M×K) | training dataset of features encoded as |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| X := (X[1]*,…,*X[K]) ∈ℝ^(M×K) | 编码为特征的训练数据集 |'
- en: '|  | rotation angles on [0*,π*] |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | 旋转角度范围为 [0*,π*] |'
- en: '| y := (*y*[1]*,…,y*[K]) ∈{0*,*1}^K | vector of binary labels |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| y := (*y*[1]*,…,y*[K]) ∈{0*,*1}^K | 二进制标签向量 |'
- en: '| *N*[iter] | number of iterations |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| *N*[iter] | 迭代次数 |'
- en: '| *N*[runs] | number of quantum circuit runs |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| *N*[runs] | 量子电路运行次数 |'
- en: '| *M* | number of particles (solutions) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| *M* | 粒子（解）的数量 |'
- en: '| *w* | momentum coefficient |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| *w* | 动量系数 |'
- en: '| *c* [1] | particle memory coefficient |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| *c* [1] | 粒子记忆系数 |'
- en: '| *c*[2] | swarm influence coefficient |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| *c*[2] | 群体影响系数 |'
- en: '| *n* | number of quantum registers |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| *n* | 量子寄存器的数量 |'
- en: '| *l* | number of QNN layers |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| *l* | QNN层数 |'
- en: 'Table 8.2: Inputs of the QNN training algorithm'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.2：QNN训练算法的输入
- en: 'The algorithm operates on the following objects, where *m* = 1*,…,M* denotes
    the *m*-th particle, and *t* = 0*,…,N*[iter] represents the algorithm iteration
    step:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法作用于以下对象，其中 *m* = 1*,…,M* 表示第 *m* 个粒子，*t* = 0*,…,N*[iter] 表示算法的迭代步骤：
- en: '𝜃(*t*;*m*) ∈ℳ[nl]([−*π,π*]): position of particle *m* at time *t*;'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '𝜃(*t*;*m*) ∈ℳ[nl]([−*π,π*]): 第 *m* 个粒子在时间 *t* 的位置；'
- en: 'v(*t*;*m*) ∈ℳ[nl]([−*π,π*]): velocity of particle *m* at time *t*;'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'v(*t*;*m*) ∈ℳ[nl]([−*π,π*]): 第 *m* 个粒子在时间 *t* 的速度；'
- en: 'Ξ(*m*) ∈ ℳ[nl]([−*π,π*]): best position found by particle *m* across all iterations;'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ξ(*m*) ∈ ℳ[nl]([−*π,π*]): 第 *m* 个粒子在所有迭代中的最佳位置；'
- en: 'Φ(*t*) ∈ ℳ[nl]([−*π,π*]): the globally best position found by all particles
    at time *t*;'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Φ(*t*) ∈ ℳ[nl]([−*π,π*]): 所有粒子在时间 *t* 找到的全局最佳位置；'
- en: '*L*(𝜃): value of the cost function for the solution 𝜃.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L*(𝜃): 解 𝜃 的代价函数值。'
- en: '![--------------------------------------------------------------------- -Algorithm---5:-Particle
    Swarm-Optimisation--------------------------- Result: Optimal con figuration of
    adjustable QNN parameters ∗ 𝜃 := argmin L(𝜃). Initialisation and evaluation of
    the first set of solutions (we set Δt in (8.3.1) equal to 1): for each particle
    m = 1,...,M do | for i = 1,...,n, j = 1,...,l do | | Randomly draw the rotation
    angle 𝜃j(0;m ) from 𝒰 ([− π,π]). | | i | | Randomly draw the rotation angle vji(0;m
    ) from 𝒰 ([− π,π]). | end | | | Initialise the individually best solution: | |
    Ξ(m ) ← 𝜃(0;m ) | | for k = 1,...,K do | | Run the quantum circuit Nruns times
    with configuration | | | | 𝜃(0;m ) on sample Xk to estimate the probability ℙk
    of | | reading out "1" on the target qubit. | | end | | Evaluate the cost function
    L(𝜃(0;m )) given the probabilities | ℙ := (ℙ ,...,ℙ ). 1 K end Order solutions
    from best (minimal cost function ) to worst (maximal cost function). Φ (0) ← configuration
    corresponding to the minimum of the cost function. Initialise the optimal configuration:
    ∗ 𝜃 ← Φ (0) ---------------------------------------------------------------------
    ](img/file792.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- -算法---5:-粒子群优化---------------------------
    结果：可调 QNN 参数的最优配置 ∗ 𝜃 := argmin L(𝜃)。初始解集的初始化与评估（我们在（8.3.1）中将 Δt 设置为 1）：对每个粒子
    m = 1,...,M 做 | 对 i = 1,...,n, j = 1,...,l 做 | | 从 𝒰 ([− π,π]) 中随机抽取旋转角度 𝜃j(0;m
    )。 | | i | | 从 𝒰 ([− π,π]) 中随机抽取旋转角度 vji(0;m )。 | 结束 | | | 初始化单独的最佳解： | | Ξ(m
    ) ← 𝜃(0;m ) | | 对 k = 1,...,K 做 | | 使用配置 | | | 𝜃(0;m ) 在样本 Xk 上运行量子电路 Nruns 次，以估算
    | | 读取目标量子比特为 "1" 的概率 ℙk。 | | 结束 | | 评估给定概率的代价函数 L(𝜃(0;m )) | ℙ := (ℙ1,...,ℙK
    ) 结束 按代价函数值从最优（最小代价函数）到最差（最大代价函数）排序解。Φ (0) ← 对应代价函数最小值的配置。初始化最优配置： ∗ 𝜃 ← Φ (0)
    --------------------------------------------------------------------- ](img/file792.jpg)'
- en: '![--------------------------------------------------------------------- Iterations:
    for t = 1,...,Niter do | | for m = 1,...,M do | | for i = 1,...,n, j = 1,...,l
    do | | | Generate independent random numbers ω ∼ U [0,1 ] and | | | 1 | | | ω2
    ∼ U[0,1]. | | | momentum ← wvj (t − 1;m ) | | | i | | | particle ← c1ω1[Ξji(m
    )− 𝜃ji(t− 1;m)] | | | swarm ← c ω [Φj (t − 1)− 𝜃j(t− 1;m )] | | | 2 2 i i | |
    | vji(t;m ) ← momentum + particle+ swarm | | | j j j | | | 𝜃i(t;m ) ← 𝜃i(t− 1;m
    )+ vi(t;m ) | | end | | | | for k = 1,...,K do | | | Run the quantum circuit Nruns
    times with configuration | | | 𝜃(t;m ) on sample X to estimate the probability
    ℙ of | | | k k | | reading out "1" on the target qubit. | | end | | | | Evaluate
    the cost function L(𝜃(t;m )) given | | | | ℙ := (ℙ1,...,ℙK ). | | if L(𝜃(t;m))
    < L(Ξ (m )) then | | | Ξ(m ) ← 𝜃(t;m ) | | | | end | | end | Order solutions from
    best (minimum value of the cost function) | | to worst (maximum value of the cost
    function). | | Φ (t) ← con figuration corresponding to the minimum of the cost
    | function. | | if L(𝜃∗) < L (Φ(t)) then | 𝜃 ∗ ← Φ(t) | | end end ---------------------------------------------------------------------
    ](img/file793.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- 迭代：对于
    t = 1,...,Niter 做 | | 对 m = 1,...,M 做 | | 对 i = 1,...,n, j = 1,...,l 做 | | | 生成独立的随机数
    ω ∼ U [0,1 ] 和 | | | 1 | | | ω2 ∼ U[0,1]。 | | | 动量 ← wvj (t − 1;m ) | | | i |
    | | 粒子 ← c1ω1[Ξji(m )− 𝜃ji(t− 1;m)] | | | 群体 ← c ω [Φj (t − 1)− 𝜃j(t− 1;m )] |
    | | 2 2 i i | | | vji(t;m ) ← 动量 + 粒子 + 群体 | | | j j j | | | 𝜃i(t;m ) ← 𝜃i(t−
    1;m ) + vi(t;m ) | | 结束 | | | | 对 k = 1,...,K 做 | | 使用配置 | | | 𝜃(t;m ) 在样本 X 上运行量子电路
    Nruns 次，以估算 | | | k k | | 读取目标量子比特为 "1" 的概率 ℙ。 | | 结束 | | | | 评估给定概率的代价函数 L(𝜃(t;m
    )) | | | | ℙ := (ℙ1,...,ℙK )。 | | 如果 L(𝜃(t;m)) < L(Ξ (m ))，则 | | | Ξ(m ) ← 𝜃(t;m
    ) | | | | 结束 | | 结束 | 按代价函数值从最优（代价函数最小值）到最差（代价函数最大值）排序解。 | | Φ (t) ← 对应代价函数最小值的配置。
    | | 如果 L(𝜃∗) < L (Φ(t))，则 | 𝜃 ∗ ← Φ(t) | | 结束 结束 ---------------------------------------------------------------------
    ](img/file793.jpg)'
- en: The non-differentiable learning based on the evolutionary search heuristic works
    well for irregular, non-convex objective functions with many local minima.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基于进化搜索启发式的不可微学习在处理具有多个局部极小值的不规则、非凸目标函数时效果良好。
- en: 8.4 QNN Embedding on NISQ QPU
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4 QNN 在 NISQ QPU 上的嵌入
- en: Ideally, parameterised quantum circuits should be constructed in a hardware-agnostic
    way, only driven by the characteristics of the problem being solved. This, however,
    would require the existence of large and exceptionally well-connected quantum
    computing systems with very high qubit fidelity and coherence time. In other words,
    we would need QPUs with capabilities that significantly exceed those of existing
    NISQ devices. The time for such powerful quantum computing systems may come sooner
    than one may expect but we still have to find a way of running PQCs efficiently
    on NISQ QPUs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，参数化量子电路应该以与硬件无关的方式构建，仅由待解决问题的特征驱动。然而，这将需要存在大型且连接极为良好的量子计算系统，具有非常高的量子比特保真度和相干时间。换句话说，我们需要的
    QPU 功能远远超过现有 NISQ 设备的能力。这样的强大量子计算系统的到来可能比预期的更早，但我们仍然需要找到一种在 NISQ QPU 上高效运行 PQC
    的方法。
- en: 8.4.1 NISQ QPU connectivity
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.1 NISQ QPU 连接性
- en: 'A typical approach to designing a PQC executable on the NISQ QPU would start
    with observing two main characteristics of the quantum computing systems: the
    graph (qubit connectivity) and the set of native gates. We can illustrate these
    points by looking at Rigetti’s Aspen system  [[72](Biblography.xhtml#XCoyle2020)]
    in Figure [8.3](#8.3).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 设计可在 NISQ QPU 上执行的 PQC 的典型方法是，从观察量子计算系统的两个主要特征开始：图（量子比特连接性）和原生门集。我们可以通过查看图 8.3
    中 Rigetti 的 Aspen 系统 [[72](Biblography.xhtml#XCoyle2020)] 来说明这些要点。
- en: '![Figure 8.3: Rigetti’s Aspen system. ](img/file794.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3: Rigetti 的 Aspen 系统。](img/file794.jpg)'
- en: 'Figure 8.3: Rigetti’s Aspen system.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8.3: Rigetti 的 Aspen 系统。'
- en: As we can see, most qubits are only connected to their nearest neighbours on
    the linear grid, with only four qubits having three connections. These extra connections
    form a bridge between two 8-qubit islands that, otherwise, would be completely
    independent.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，大多数量子比特仅与线性网格上的最近邻量子比特连接，只有四个量子比特有三个连接。这些额外的连接形成了两个 8 量子比特岛之间的桥梁，否则它们将是完全独立的。
- en: 8.4.2 QNN embedding scheme
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.2 QNN 嵌入方案
- en: The shaded qubits in Figure [8.3](#8.3) can be used to construct the 8-qubit
    tree network capable of processing a dataset with up to 16 continuous features
    (two features per quantum register) as shown in Figure [8.4](#8.4). The thick
    lines in Figure [8.3](#8.3) represent qubits connectivity used in constructing
    the QNN. The thin lines represent all other available qubit connections that have
    not been utilised in the QNN ansatz.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 中的阴影量子比特可以用于构建 8 量子比特树网络，能够处理最多具有 16 个连续特征（每个量子寄存器两个特征）的数据集，如图 8.4 所示。图
    8.3 中的粗线表示用于构建 QNN 的量子比特连接。细线表示所有其他未在 QNN 假设中使用的量子比特连接。
- en: '![Figure 8.4: QNN for the Aspen system; the gate G is any of the {X,Y,Z} gates.
    ](img/file795.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4: Aspen 系统的 QNN；门 G 是 {X,Y,Z} 任意门。](img/file795.jpg)'
- en: 'Figure 8.4: QNN for the Aspen system; the gate G is any of the {X,Y,Z} gates.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8.4: Aspen 系统的 QNN；门 G 是 {X,Y,Z} 任意门。'
- en: With the limited connectivity of existing QPUs, we need to fully utilise the
    graph structure of the quantum chips to implement the most efficient QNN embedding
    and extract the best possible performance.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现有量子处理单元（QPU）的连接性有限，我们需要充分利用量子芯片的图结构来实现最有效的 QNN 嵌入，并提取最佳性能。
- en: 8.5 QNN Trained as a Classifier
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5 QNN 作为分类器的训练
- en: We now demonstrate how a binary QNN classifier can be trained on a classical
    credit approval dataset using the non-differentiable learning approach.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在演示如何使用非可微分学习方法，在经典信用审批数据集上训练二元 QNN 分类器。
- en: 8.5.1 The ACA dataset and QNN ansatz
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.5.1 ACA 数据集与 QNN 假设
- en: One of the most fundamental use cases for a binary classifier in finance is
    credit approval. The UCI Machine Learning Database  [[241](Biblography.xhtml#XUCI_ACA), [242](Biblography.xhtml#XQuinlan1987)]
    holds the Australian Credit Approval (ACA) dataset consisting of 690 samples.
    There are 14 features (binary, integer, continuous) representing various attributes
    of potential borrowers and a binary class label (accept/reject credit application).
    The dataset is reasonably hard for classical classifiers due to the limited predictive
    power of the features and its relatively small size. This makes it ideal for testing
    and benchmarking the QNN performance.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类器在金融领域最基本的应用之一是信用批准。UCI机器学习数据库[[241](Biblography.xhtml#XUCI_ACA), [242](Biblography.xhtml#XQuinlan1987)]包含澳大利亚信用批准（ACA）数据集，该数据集由690个样本组成。数据集包含14个特征（包括二进制、整数、连续特征），代表潜在借款人的各种属性，以及一个二进制类别标签（接受/拒绝信用申请）。由于特征的预测能力有限以及数据集相对较小，经典分类器处理起来相当困难。这使得它成为测试和基准化QNN性能的理想选择。
- en: We start with the simplest tree network that can be mapped onto Rigetti’s Aspen
    system graph described in the previous section. Figure [8.5](#8.5) shows the full
    quantum circuit consisting of sample encoding and sample processing modules  [[171](Biblography.xhtml#XKondratyev2021)].
    The proposed scheme allows us to encode up to two continuous features per quantum
    register with the help of rotations around the *x*- and the *y*-axes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从最简单的树形网络开始，该网络可以映射到前一节中描述的Rigetti Aspen系统图。图[8.5](#8.5)显示了完整的量子电路，包括样本编码和样本处理模块[[171](Biblography.xhtml#XKondratyev2021)]。该方案使我们能够利用绕*x*轴和*y*轴的旋转，在每个量子寄存器中最多编码两个连续特征。
- en: '![Figure 8.5: PQC for the credit approvals classifier. ](img/file796.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5：用于信用批准分类器的PQC。](img/file796.jpg)'
- en: 'Figure 8.5: PQC for the credit approvals classifier.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：用于信用批准分类器的PQC。
- en: The features are encoded as rotation angles *ϕ* ∈ [0*,π*] according to the encoding
    scheme described in Section [7.2](Chapter_7.xhtml#x1-1520002). With all qubits
    initialised as ![|0⟩](img/file797.jpg) in the computational basis, this ensures
    the uniqueness of the encoded samples. The sample processing module consists of
    layers of adjustable one-qubit gates (rotations around the *x*- and the *y*-axes)
    and fixed two-qubit gates (CZ). We split the ACA dataset 50:50 into a training
    and a testing dataset using the train_test_split() function provided by the `sklearn.model_selection`
    module. Our objective is to train the QNN and various classical classifiers (classical
    benchmarks) on the training dataset and compare their out-of-sample performance
    on the testing dataset. The classical classifiers have a number of hyperparameters
    that can be fine-tuned to optimise the classifier performance on the given dataset.
    In contrast, the QNN architecture (location and types of one-qubit and two-qubit
    gates) is fixed.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 特征作为旋转角度*ϕ* ∈ [0*,π*]进行编码，按照第[7.2](Chapter_7.xhtml#x1-1520002)节中描述的编码方案进行。所有量子比特初始化为计算基态的![|0⟩](img/file797.jpg)，这确保了编码样本的唯一性。样本处理模块由可调的一量子比特门（绕*x*轴和*y*轴的旋转）和固定的二量子比特门（CZ）组成。我们使用`sklearn.model_selection`模块提供的train_test_split()函数，将ACA数据集按50:50分为训练集和测试集。我们的目标是训练QNN和各种经典分类器（经典基准）在训练集上的表现，并比较它们在测试集上的外推性能。经典分类器具有一些超参数，可以进行微调，以优化分类器在给定数据集上的性能。相比之下，QNN架构（单量子比特和双量子比特门的位置和类型）是固定的。
- en: 8.5.2 Training an ACA classifier with the PSO algorithm
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.5.2 使用PSO算法训练ACA分类器
- en: We first verify that the QNN can be efficiently trained with the Particle Swarm
    Optimisation algorithm – a non-differentiable learning approach. Figure [8.6](#8.6)
    illustrates PSO convergence for the set of PSO parameters given in Table [8.3](#x1-176001r3).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先验证QNN是否能够通过粒子群优化算法（PSO）高效训练——一种不可微分的学习方法。图[8.6](#8.6)展示了PSO算法在表[8.3](#x1-176001r3)中给定PSO参数集下的收敛情况。
- en: '| Parameter | Notation | Value |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 符号 | 值 |'
- en: '| Inertia coefficient | *w* | 0.25 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 惯性系数 | *w* | 0.25 |'
- en: '| Self-confidence coefficient | *c* [1] | 0.25 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 自信系数 | *c* [1] | 0.25 |'
- en: '| Swarm confidence coefficient | *c* [2] | 0.25 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 群体自信系数 | *c* [2] | 0.25 |'
- en: '| Number of particles | *M* | 10 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 粒子数量 | *M* | 10 |'
- en: '| Number of iterations | *N*[iter] | 20 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 迭代次数 | *N*[iter] | 20 |'
- en: '| Number of quantum circuit runs | *N*[runs] | 1000 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 量子电路运行次数 | *N*[runs] | 1000 |'
- en: 'Table 8.3: PSO parameters.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.3：PSO参数。
- en: The sample algorithm run has reached the minimum of the objective function in
    just four iterations with only ten particles, exploring the search space using
    the `Qiskit` quantum simulator.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 样本算法运行在仅用十个粒子的情况下，通过`Qiskit`量子模拟器探索搜索空间，仅在四次迭代中就达到了目标函数的最小值。
- en: '![Figure 8.6: Minimum objective function values found by individual particles.
    ](img/file798.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6：各个粒子找到的目标函数最小值。](img/file798.jpg)'
- en: 'Figure 8.6: Minimum objective function values found by individual particles.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6：各个粒子找到的目标函数最小值。
- en: The configuration of adjustable parameters (rotations) that corresponds to the
    minimum of the objective function found by the PSO algorithm is given by ([8.5.2](#x1-176003r2)).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: PSO算法找到的对应于目标函数最小值的可调参数（旋转）的配置由([8.5.2](#x1-176003r2))给出。
- en: '| ![ ⌊ ⌋ 0.16π &#124; &#124; &#124;&#124;− 0.55π 0.66π &#124;&#124; &#124;&#124;−
    0.13π &#124;&#124; &#124;&#124; &#124;&#124; &#124;&#124; 0.08π 0.72π 0.02π &#124;&#124;
    𝜃 = &#124;&#124; 0.33π &#124;&#124; . &#124; &#124; &#124;&#124; 0.06π 0.95π &#124;&#124;
    &#124;&#124; 0.48π &#124;&#124; ⌈ ⌉ 0.19π − 0.91π − 0.83π 0.59π ](img/file799.jpg)
    |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| ![ ⌊ ⌋ 0.16π &#124; &#124; &#124;&#124;− 0.55π 0.66π &#124;&#124; &#124;&#124;−
    0.13π &#124;&#124; &#124;&#124; &#124;&#124; &#124;&#124; 0.08π 0.72π 0.02π &#124;&#124;
    𝜃 = &#124;&#124; 0.33π &#124;&#124; . &#124; &#124; &#124;&#124; 0.06π 0.95π &#124;&#124;
    &#124;&#124; 0.48π &#124;&#124; ⌈ ⌉ 0.19π − 0.91π − 0.83π 0.59π ](img/file799.jpg)
    |  |'
- en: Figure [8.7](#8.7) displays the in- and out-of-sample confusion matrices for
    the QNN classifier obtained with the Qiskit quantum simulator assuming that Class 0
    is the positive class.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图[8.7](#8.7)显示了通过Qiskit量子模拟器获得的QNN分类器的内外样本混淆矩阵，假设类0为正类。
- en: '![Figure 8.7: Confusion matrix for the QNN classifier (ACA dataset). ](img/file800.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7：QNN分类器的混淆矩阵（ACA数据集）。](img/file800.jpg)'
- en: 'Figure 8.7: Confusion matrix for the QNN classifier (ACA dataset).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7：QNN分类器的混淆矩阵（ACA数据集）。
- en: The results are robust with an in-sample accuracy of 0.86 and an out-of-sample
    accuracy of 0.85\. Interestingly, the in-sample and out-of-sample results are
    very close, indicating that the QNN provides strong regularisation. The question
    of quantum and classical neural networks regularisation will be tackled in Chapter [12](Chapter_12.xhtml#x1-22500012).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 结果具有鲁棒性，内样本准确率为0.86，外样本准确率为0.85。值得注意的是，内样本和外样本结果非常接近，表明QNN提供了强有力的正则化。量子与经典神经网络正则化的问题将在第[12](Chapter_12.xhtml#x1-22500012)章中讨论。
- en: 8.6 Classical Benchmarks
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6 经典基准
- en: 'In Chapter [4](Chapter_4.xhtml#x1-820004), we introduced two classical classifiers:
    a feedforward artificial neural network (Multi-Layer Perceptron) and a decision
    tree algorithm. We now expand the range of classical benchmark classifiers by
    adding Support Vector Machine (SVM)  [[70](Biblography.xhtml#XCortes1995)], Logistic
    Regression  [[31](Biblography.xhtml#XBerkson1944)], and Random Forest  [[136](Biblography.xhtml#XHo1995)].
    The SVM approach based on the *kernel method* is covered in Chapter [13](Chapter_13.xhtml#x1-23600013).
    Here, we briefly explain the main principles of logistic regression and random
    forest classifiers.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[4](Chapter_4.xhtml#x1-820004)章中，我们介绍了两种经典分类器：前馈人工神经网络（多层感知器）和决策树算法。现在，我们通过添加支持向量机（SVM）[[70](Biblography.xhtml#XCortes1995)]、逻辑回归[[31](Biblography.xhtml#XBerkson1944)]和随机森林[[136](Biblography.xhtml#XHo1995)]来扩展经典基准分类器的范围。基于*核方法*的SVM方法将在第[13](Chapter_13.xhtml#x1-23600013)章中讨论。这里，我们简要解释逻辑回归和随机森林分类器的主要原理。
- en: 8.6.1 Logistic Regression and Random Forest
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.6.1 逻辑回归与随机森林
- en: Logistic regression can be seen as a special case of a feedforward neural network
    with a single hidden layer consisting of an activation unit with the logistic
    activation function. The model operates as shown in Figure [4.3](Chapter_4.xhtml#4.3)
    with
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归可以看作是一个具有单个隐藏层的前馈神经网络的特例，隐藏层由具有逻辑激活函数的激活单元组成。该模型的运作如图[4.3](Chapter_4.xhtml#4.3)所示。
- en: '![ ( ) y(s) = 1 + e−s −1 . ](img/file801.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![ ( ) y(s) = 1 + e−s −1 . ](img/file801.jpg)'
- en: The standard logistic regression model is a *linear classifier* because the
    outcome always depends on the sum of the (weighted) inputs. Therefore, logistic
    regression performs well when working with a dataset where the classes are more
    or less linearly separable.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的逻辑回归模型是一个*线性分类器*，因为结果始终依赖于（加权）输入的总和。因此，逻辑回归在处理类之间或多或少是线性可分的数据集时表现良好。
- en: Random forest is an *ensemble learning* model and, as the name suggests, is
    based on combining the classification results of multiple decision trees. The
    ensemble technique used by random forest is known as *bootstrap aggregation*,
    or *bagging*, by choosing random subsets from the dataset. Hence, each decision
    tree is generated from samples drawn from the original dataset with replacement
    (row sampling). This step of row sampling with replacement is called the *bootstrap*.
    Each decision tree is trained independently. The final output for the given samples
    is based on *majority voting* after combining the results of all individual decision
    trees. This is the *aggregation* step.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种*集成学习*模型，顾名思义，它基于结合多个决策树的分类结果。随机森林使用的集成技术称为*自助聚合*（*bootstrap aggregation*），或*bagging*，通过从数据集中选择随机子集。因此，每棵决策树都是从原始数据集的样本中通过有放回抽样生成的（行抽样）。这种有放回的行抽样步骤被称为*自助法*（*bootstrap*）。每棵决策树都是独立训练的。给定样本的最终输出是基于所有单独决策树结果的*多数投票*，这是*聚合*步骤。
- en: 8.6.2 Benchmarking against standard classical classifiers
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.6.2 与标准经典分类器的基准测试
- en: 'The classical benchmarking can be done by training several popular `scikit-learn`
    models. Table [8.4](#x1-179001r4) provides classical benchmarking results in terms
    of out-of-sample *F*[1] scores for the following (weakly) optimised scikit-learn
    classifiers:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 经典基准测试可以通过训练几个流行的`scikit-learn`模型来完成。表[8.4](#x1-179001r4)提供了几个（弱优化的）`scikit-learn`分类器的样本外*F*[1]分数的经典基准测试结果：
- en: 'a feedforward neural network (MLP) classifier: `neural_network.MLPClassifier`'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈神经网络（MLP）分类器：`neural_network.MLPClassifier`
- en: 'a support vector machine classifier: `svm.SVC`'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机分类器：`svm.SVC`
- en: 'an ensemble learning model: `ensemble.RandomForestClassifier`'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种集成学习模型：`ensemble.RandomForestClassifier`
- en: 'a logistic regression classifier: `linear_model.LogisticRegression`'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归分类器：`linear_model.LogisticRegression`
- en: 'The *F*[1] score is a harmonic average of two performance metrics, precision
    and recall:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*F*[1] 分数是两个性能指标——精度和召回率——的调和平均值：'
- en: '| ![ Precision × Recall F1 := 2 -----------------, Precision + Recall ](img/file802.jpg)
    |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| ![ 精度 × 召回率 F1 := 2 -----------------, 精度 + 召回率 ](img/file802.jpg) |  |'
- en: both introduced in Chapter [4](Chapter_4.xhtml#x1-820004). In the context of
    credit approvals, optimising for recall helps with minimising the chance of approving
    a credit application that should be rejected. However, this comes at the cost
    of not approving credit applications for some high-quality borrowers. If we optimise
    for precision, then we improve the overall correctness of our decisions at the
    cost of approving some applicants with bad credits. The *F*[1] score is used to
    balance the positives and negatives in optimising precision and recall.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 两者都在第[4](Chapter_4.xhtml#x1-820004)章中介绍。在信用审批的背景下，优化召回率有助于最小化错误批准应被拒绝的信用申请的机会。然而，这会导致一些高质量借款人的信用申请未能获得批准。如果我们优化精度，那么我们会提高决策的整体正确性，但这也可能导致一些信用不良的申请者被批准。*F*[1]
    分数用于平衡精度和召回率优化中的正负效果。
- en: '| Classifier | Average *F*[1] score |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 分类器 | 平均 *F*[1] 分数 |'
- en: '| Logistic Regression Classifier | 0.88 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归分类器 | 0.88 |'
- en: '| Random Forest Classifier | 0.87 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林分类器 | 0.87 |'
- en: '| MLP Classifier | 0.86 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| MLP 分类器 | 0.86 |'
- en: '| QNN Classifier | 0.85 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| QNN 分类器 | 0.85 |'
- en: '| Support Vector Classifier | 0.84 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量分类器 | 0.84 |'
- en: 'Table 8.4: Out-of-sample F[1] scores for the classical and QNN classifiers
    trained on the ACA dataset.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.4：在ACA数据集上训练的经典和QNN分类器的样本外*F*[1]分数。
- en: The QNN classifier performance, as measured by the average *F*[1] score for
    Class 0 and Class 1, falls somewhere in the middle of the range of out-of-sample
    *F*[1] scores for the chosen classical benchmarks. This is encouraging since the
    QNN ansatz was fixed and we did not optimise the QNN hyperparameters – the placement
    and types of the two-qubit gates. The classifier performance can be further improved
    by deploying the standard ensemble learning techniques, as explained in the following
    section.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: QNN分类器的性能，依据类别0和类别1的平均*F*[1]分数来衡量，位于所选经典基准测试样本外*F*[1]分数的范围中间。这是令人鼓舞的，因为QNN的初始设置是固定的，我们没有优化QNN的超参数——即两量子比特门的放置和类型。通过部署标准的集成学习技术（如下一节所述），可以进一步提高分类器的性能。
- en: QNNs can be productively used for classification tasks on classical finance-related
    datasets.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: QNN 可有效用于经典金融相关数据集的分类任务。
- en: 8.7 Improving Performance with Ensemble Learning
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7 使用集成学习提升性能
- en: The ensemble learning methods combine different weak classifiers into a strong
    classifier that has better generalisation capabilities than each individual standalone
    classifier. In Chapter [4](Chapter_4.xhtml#x1-820004), we saw how the principles
    of ensemble learning can be used in combination with the methods of quantum annealing.
    Here, we look at them from the QNN perspective.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习方法将不同的弱分类器组合成一个强分类器，其泛化能力优于每个单独的独立分类器。在第[4章](Chapter_4.xhtml#x1-820004)中，我们看到集成学习的原理如何与量子退火方法结合使用。这里，我们从QNN的角度来看待这些方法。
- en: 8.7.1 Majority voting
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.7.1 多数投票
- en: 'The popular ensemble learning methods are majority voting (binary classification)
    and plurality voting (multiclass classification). Majority voting means what it
    says: the class label for the given sample is the one that receives more than
    half of the individual votes. Plurality voting chooses the class that receives
    the largest number of votes (the mode).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的集成学习方法包括多数投票（用于二分类）和多数投票（用于多类分类）。多数投票的意思就是：给定样本的类别标签是获得超过一半个体投票的类别。多数投票选择获得最多投票的类别（即众数）。
- en: The ensemble of the individual classifiers can be built from different classification
    algorithms. For example, by combining neural network classifiers, support vector
    machines, decision trees, etc. On the other hand, the same basic classification
    algorithm can be used to produce multiple classifiers by choosing different configurations
    of hyperparameters and different subsets of the training dataset. The random forest
    classifier, which combines different decision tree classifiers, illustrates the
    latter approach.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 个别分类器的集成可以通过不同的分类算法来构建。例如，通过组合神经网络分类器、支持向量机、决策树等。另一方面，可以通过选择不同的超参数配置和不同的训练数据集子集，使用相同的基本分类算法来生成多个分类器。随机森林分类器就是通过结合不同的决策树分类器来说明后一种方法。
- en: With these considerations in mind, we build a strong classifier from several
    individual QNN classifiers by changing the QNN ansatz within the restrictions
    imposed by the QPU qubit connectivity. In order to test the majority voting approach,
    we build two new QNN classifiers by adding a few more two-qubit CZ gates to the
    baseline parameterised quantum circuit, as shown in Figures [8.8](#8.8) and [8.9](#8.9).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些因素，我们通过在QPU量子比特连接性所限制的范围内改变QNN ansatz，构建了一个由多个独立QNN分类器组成的强分类器。为了测试多数投票方法，我们通过在基准参数化量子电路中添加一些额外的两量子比特CZ门，构建了两个新的QNN分类器，如图[8.8](#8.8)和[8.9](#8.9)所示。
- en: In the case of PQC #2, we add two extra CZ gates, exploiting the "bridge" structure
    of the Aspen system (Figure [8.3](#8.3)). This improves the overall system entaglement
    and allows for a richer set of achievable quantum states. PQC #3 has three extra CZ
    gates in comparison with the baseline circuit. The new classifiers can be trained
    with the same algorithm (PSO) on the same training dataset but will have different
    optimal configurations of the adjustable parameters and will make slightly different
    classification decisions on the testing dataset.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '在PQC #2的情况下，我们增加了两个额外的CZ门，利用了Aspen系统的“桥接”结构（图[8.3](#8.3)）。这改善了整个系统的纠缠，并允许实现更丰富的量子态。相比基准电路，PQC
    #3增加了三个额外的CZ门。这些新的分类器可以使用相同的算法（PSO）在相同的训练数据集上进行训练，但会有不同的可调参数的最佳配置，并在测试数据集上做出略有不同的分类决策。'
- en: With three QNN classifiers, the majority voting leads to either a unanimous
    or a 2:1 decision. Performance on the ACA dataset improves marginally with all
    three classifiers generally in full agreement with each other. There are only
    a handful of instances where majority voting adds value, but this improves the
    average out-of-sample *F*[1] score from 0.85 to 0.87 – on par with the random
    forest classifier trained on the same dataset.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用三个QNN分类器时，多数投票会导致一致的或2:1的决策。当三个分类器完全一致时，ACA数据集的表现略有提升。多数投票的增值作用仅体现在少数几个实例上，但这使得平均外样本*F*[1]得分从0.85提高到0.87——与在相同数据集上训练的随机森林分类器持平。
- en: '![Figure 8.8: PQC #2 for the credit approvals classifier. New fixed 2-qubit
    gates are shaded grey. ](img/file803.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8：用于信用审批分类器的PQC #2。新的固定2量子比特门以灰色阴影显示。](img/file803.jpg)'
- en: 'Figure 8.8: PQC #2 for the credit approvals classifier. New fixed 2-qubit gates
    are shaded grey.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '图8.8：用于信用审批分类器的PQC #2。新的固定2量子比特门以灰色阴影显示。'
- en: '![Figure 8.9: PQC #3 for the credit approvals classifier. New fixed 2-qubit
    gates are shaded grey. ](img/file804.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.9：用于信用审批分类器的 PQC #3。新的固定 2 量子比特门以灰色阴影显示。](img/file804.jpg)'
- en: 'Figure 8.9: PQC #3 for the credit approvals classifier. New fixed 2-qubit gates
    are shaded grey.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8.9：用于信用审批分类器的 PQC #3。新的固定 2 量子比特门以灰色阴影显示。'
- en: Similar results can be achieved with the original QNN classifier trained on
    different subsets of the training dataset. These subsets are produced by drawing
    the bootstrap samples – random samples with replacement – from the original training
    dataset. The differently trained QNN classifiers can then be combined into a single
    strong classifier using the majority voting approach as described above.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对不同子集的训练数据集进行训练，可以实现类似的结果。这些子集是通过从原始训练数据集中抽取自助样本（带有替换的随机样本）产生的。然后，可以将这些不同训练的
    QNN 分类器结合成一个强大的单一分类器，使用上述的多数投票方法。
- en: 8.7.2 Quantum boosting
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.7.2 量子提升
- en: 'We started by introducing the concept of ensemble learning where predictions
    produced by various QNNs are combined into a more robust unified prediction via
    a classical majority voting method. However, we can take a different approach
    to ensemble learning: predictions of various classical classifiers can be treated
    as an input into the QNN that performs their aggregation and comes up with a unified
    prediction. In other words, the QNN operates as a quantum booster similar to the
    QUBO-based QBoost model introduced in Chapter [4](Chapter_4.xhtml#x1-820004).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍了集成学习的概念，其中通过经典的多数投票方法将由多个 QNN 产生的预测结果合并成一个更为稳健的统一预测。然而，我们也可以采取另一种集成学习的方法：将多个经典分类器的预测结果作为输入传递给
    QNN，QNN 会对这些结果进行聚合并给出统一的预测。换句话说，QNN 作为量子增强器，类似于第 [4](Chapter_4.xhtml#x1-820004)
    章中介绍的基于 QUBO 的 QBoost 模型。
- en: Let us come back to the classical benchmarks used in Section [8.5](#x1-1740005).
    There are four different machine learning models performing binary classifications.
    Their outputs ("0" for Class 0 and "1" for Class 1) are inputs into a 4-qubit
    QNN classifier. Since all quantum registers are initialised as ![|0⟩](img/file805.jpg),
    the outputs of individual classifiers can be encoded by either doing nothing for
    Class 0 output (which is equivalent to applying an identity operator I) or by
    applying a NOT gate X for Class 1 output.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到第 [8.5](#x1-1740005) 节中使用的经典基准。这里有四种不同的机器学习模型执行二元分类。它们的输出（“0”表示类别 0，“1”表示类别
    1）作为输入传递给 4 量子比特 QNN 分类器。由于所有量子寄存器都初始化为 ![|0⟩](img/file805.jpg)，所以各个分类器的输出可以通过以下方式进行编码：对于类别
    0 输出不做任何操作（等同于应用恒等操作 I），或者对于类别 1 输出应用 NOT 门 X。
- en: '![Figure 8.10: Embedding of a 4-qubit QNN onto the bridge section of Rigetti’s
    Aspen system. ](img/file806.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.10：将 4 量子比特 QNN 嵌入到 Rigetti 的 Aspen 系统的桥接部分。](img/file806.jpg)'
- en: 'Figure 8.10: Embedding of a 4-qubit QNN onto the bridge section of Rigetti’s
    Aspen system.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10：将 4 量子比特 QNN 嵌入到 Rigetti 的 Aspen 系统的桥接部分。
- en: Figure [8.10](#8.10) shows how a 4-qubit QNN can be efficiently embedded on
    the QPU and Figure [8.11](#8.11) shows the corresponding parameterised quantum
    circuit with adjustable one-qubit gates (R[X]*,*R[Y]) and fixed two-qubit gates
    (CZ).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [8.10](#8.10) 显示了如何将 4 量子比特 QNN 高效地嵌入到 QPU 中，图 [8.11](#8.11) 显示了对应的参数化量子电路，具有可调的单量子比特门（R[X]*,*R[Y]）和固定的两量子比特门（CZ）。
- en: '![Figure 8.11: QBoost circuit. The sample encoding gate G is either an identity
    gate I if the input is "0", or a NOT gate X if the input is "1". ](img/file807.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11：QBoost 电路。样本编码门 G 如果输入为“0”则为恒等门 I，如果输入为“1”则为 NOT 门 X。](img/file807.jpg)'
- en: 'Figure 8.11: QBoost circuit. The sample encoding gate G is either an identity
    gate I if the input is "0", or a NOT gate X if the input is "1".'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11：QBoost 电路。样本编码门 G 如果输入为“0”则为恒等门 I，如果输入为“1”则为 NOT 门 X。
- en: Ensemble learning can improve QNN performance in the same way it improves performance
    of the classical weak learners.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习可以像提高经典弱学习者的性能一样，提高 QNN 的性能。
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we introduced the concept of a quantum neural network as a
    parameterised quantum circuit trained as a classifier. We considered two approaches
    to training QNNs: differentiable (gradient descent) and non-differentiable (Particle
    Swarm Optimisation) methods. Gradient descent is generally faster but can face
    the problem of barren plateaus (vanishing gradients). The evolutionary search
    heuristics may be slower but can handle the presence of multiple local minima
    and strike the right balance between exploration and exploitation.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章，我们介绍了量子神经网络的概念，它是一个作为分类器训练的参数化量子电路。我们考虑了两种训练 QNN 的方法：可微（梯度下降）和不可微（粒子群优化）方法。梯度下降通常更快，但可能会遇到荒漠高原（梯度消失）问题。进化搜索启发式方法可能较慢，但能够处理多个局部最小值，并在探索与利用之间找到合适的平衡。
- en: We also explored the embedding of QNNs on the NISQ QPUs with limited connectivity
    between the qubits. As an example, we considered Rigetti’s Aspen system and proposed
    an efficient embedding scheme that mirrors the "tree structure" architecture of
    the QNN.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了在具有有限量子比特连接性的 NISQ QPU 上嵌入 QNN 的问题。作为示例，我们考虑了 Rigetti 的 Aspen 系统，并提出了一种高效的嵌入方案，镜像了
    QNN 的“树结构”架构。
- en: Once our QNN was fully specified and embedded into a QPU graph, we investigated
    its performance on a real-world dataset of credit approvals and provided comparisons
    with several standard classical classifiers.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的量子神经网络（QNN）完全定义并嵌入到量子处理单元（QPU）图中，我们就会研究其在实际信用审批数据集上的表现，并与几种标准的经典分类器进行比较。
- en: Finally, we looked at several ensemble learning techniques that assist in improving
    QNN performance in the context of a hybrid quantum-classical protocol.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们介绍了几种集成学习技术，这些技术有助于在混合量子经典协议的背景下提升 QNN 的表现。
- en: In the next chapter, we will study a powerful generative QML model – the Quantum
    Circuit Born Machine – which is a direct quantum counterpart of the classical
    Restricted Boltzmann Machine we considered in Chapter [5](Chapter_5.xhtml#x1-960005).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习一个强大的生成型 QML 模型——量子电路 Born 机，这是经典受限玻尔兹曼机（Restricted Boltzmann Machine）在量子领域的直接对应物，我们在第[5](Chapter_5.xhtml#x1-960005)章中讨论过该经典模型。
- en: Join our book’s Discord space
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 社区
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/quantum](https://packt.link/quantum)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，结识志同道合的人，并与超过 2000 名成员一起学习，地址为：[https://packt.link/quantum](https://packt.link/quantum)
- en: '![PIC](img/file1.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1.png)'
