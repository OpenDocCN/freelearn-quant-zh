- en: '8'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '8'
- en: Quantum Neural Network
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­ç¥ç»ç½‘ç»œ
- en: Quantum neural networks Â [[100](Biblography.xhtml#XFarhi2018)] are parameterised
    quantum circuits that can be trained as either generative or discriminative machine
    learning models in direct analogy with their classical counterparts. In this chapter,
    we will consider parameterised quantum circuits trained as classifiers. In the
    most general case, a classifier is a function that takes an *N*-dimensional input
    and returns one ofÂ *M* possible class values. The classifier can be trained on
    a dataset of samples with known class labels by adjusting the configurable model
    parameters in such a way as to minimise the classification error. Once the classifier
    is fully trained, it can be exposed to new unseen samples for which correct class
    labels are unknown. Therefore, it is critically important to avoid overfitting
    to the training dataset and ensure that the classifier generalises well to the
    new data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­ç¥ç»ç½‘ç»œ [[100](Biblography.xhtml#XFarhi2018)] æ˜¯å‚æ•°åŒ–çš„é‡å­ç”µè·¯ï¼Œå¯ä»¥ä½œä¸ºç”Ÿæˆå‹æˆ–åˆ¤åˆ«å‹æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œç›´æ¥ç±»æ¯”äºå…¶ç»å…¸å¯¹åº”ç‰©ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†è€ƒè™‘å°†é‡å­ç”µè·¯ä½œä¸ºåˆ†ç±»å™¨è¿›è¡Œè®­ç»ƒçš„æƒ…å†µã€‚åœ¨æœ€ä¸€èˆ¬çš„æƒ…å†µä¸‹ï¼Œåˆ†ç±»å™¨æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒæ¥å—ä¸€ä¸ª
    *N* ç»´çš„è¾“å…¥ï¼Œå¹¶è¿”å› *M* ä¸ªå¯èƒ½ç±»åˆ«å€¼ä¸­çš„ä¸€ä¸ªã€‚åˆ†ç±»å™¨å¯ä»¥é€šè¿‡è°ƒæ•´å¯é…ç½®çš„æ¨¡å‹å‚æ•°ï¼Œåœ¨å·²çŸ¥ç±»åˆ«æ ‡ç­¾çš„æ ·æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œæœ€å°åŒ–åˆ†ç±»è¯¯å·®ã€‚ä¸€æ—¦åˆ†ç±»å™¨å®Œå…¨è®­ç»ƒå¥½ï¼Œå®ƒå°±å¯ä»¥æ¥å—æ–°çš„ã€æœªè§è¿‡çš„æ ·æœ¬ï¼Œè€Œè¿™äº›æ ·æœ¬çš„æ­£ç¡®ç±»åˆ«æ ‡ç­¾æ˜¯æœªçŸ¥çš„ã€‚å› æ­¤ï¼Œé¿å…è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®é›†å¹¶ç¡®ä¿åˆ†ç±»å™¨èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°æ–°æ•°æ®æ˜¯è‡³å…³é‡è¦çš„ã€‚
- en: There are many similarities between quantum and classical neural networks. In
    both cases, the key element is the forward propagation of the signal (input),
    which is transformed by the network activation functions. Both quantum and classical
    neural networks can be trained through the backpropagation of error (differentiable
    learning) as well as through various non-differentiable learning techniques. However,
    there are also fundamental differences. For example, classical neural networks
    derive their power from the non-linear transformation of input. In contrast, all
    quantum gates are linear operators and the power of quantum neural networks comes
    from the mapping of the input into the high-dimensional Hilbert space where classification
    can be more easily done.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­ç¥ç»ç½‘ç»œå’Œç»å…¸ç¥ç»ç½‘ç»œä¹‹é—´æœ‰è®¸å¤šç›¸ä¼¼ä¹‹å¤„ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œå…³é”®å…ƒç´ æ˜¯ä¿¡å·ï¼ˆè¾“å…¥ï¼‰çš„å‰å‘ä¼ æ’­ï¼Œè¿™ä¸ªä¿¡å·é€šè¿‡ç½‘ç»œçš„æ¿€æ´»å‡½æ•°è¿›è¡Œè½¬æ¢ã€‚é‡å­ç¥ç»ç½‘ç»œå’Œç»å…¸ç¥ç»ç½‘ç»œéƒ½å¯ä»¥é€šè¿‡è¯¯å·®åå‘ä¼ æ’­ï¼ˆå¯å¾®åˆ†å­¦ä¹ ï¼‰ä»¥åŠå„ç§éå¯å¾®åˆ†å­¦ä¹ æŠ€æœ¯è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¹Ÿå­˜åœ¨ä¸€äº›æ ¹æœ¬æ€§çš„å·®å¼‚ã€‚ä¾‹å¦‚ï¼Œç»å…¸ç¥ç»ç½‘ç»œçš„å¼ºå¤§ä¹‹å¤„åœ¨äºè¾“å…¥çš„éçº¿æ€§å˜æ¢ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ‰€æœ‰çš„é‡å­é—¨éƒ½æ˜¯çº¿æ€§ç®—ç¬¦ï¼Œé‡å­ç¥ç»ç½‘ç»œçš„åŠ›é‡æ¥è‡ªäºå°†è¾“å…¥æ˜ å°„åˆ°é«˜ç»´çš„å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼Œåœ¨è¿™ä¸ªç©ºé—´ä¸­ï¼Œåˆ†ç±»æ›´å®¹æ˜“è¿›è¡Œã€‚
- en: 8.1 Quantum Neural Networks
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 é‡å­ç¥ç»ç½‘ç»œ
- en: FigureÂ [8.1](#x1-1630001) provides a schematic representation of a typical Quantum
    Neural Network (QNN) trained as a classifier. Let us have a look at the quantum
    circuit and understand how it operates. The network consists ofÂ *n* quantum registers,
    a number of one-qubit and two-qubit gates, andÂ *m* measurement operators. The
    input is a quantum state ![|Ïˆ âŸ© k](img/file747.jpg) encoding the *k*-th sample
    from the dataset. If our dataset is classical, then every classical sample should
    first be encoded in the input quantum state (as explained in the previous chapter).
    WithÂ *m* measurement operators, the output is a bitstring that can encode up toÂ 2^m
    integer values (class labels). In the case of a binary classifier, it is sufficient
    to perform measurement on a single qubit.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ [8.1](#x1-1630001) æä¾›äº†å…¸å‹çš„é‡å­ç¥ç»ç½‘ç»œï¼ˆQNNï¼‰ä½œä¸ºåˆ†ç±»å™¨è®­ç»ƒçš„ç¤ºæ„å›¾ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹é‡å­ç”µè·¯ï¼Œäº†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚ç½‘ç»œç”±
    *n* ä¸ªé‡å­å¯„å­˜å™¨ã€ä¸€ç³»åˆ—çš„å•é‡å­æ¯”ç‰¹é—¨å’ŒåŒé‡å­æ¯”ç‰¹é—¨ï¼Œä»¥åŠ *m* ä¸ªæµ‹é‡ç®—ç¬¦ç»„æˆã€‚è¾“å…¥æ˜¯ä¸€ä¸ªé‡å­æ€ ![|Ïˆ âŸ© k](img/file747.jpg)ï¼Œå®ƒç¼–ç äº†æ¥è‡ªæ•°æ®é›†çš„ç¬¬
    *k* ä¸ªæ ·æœ¬ã€‚å¦‚æœæˆ‘ä»¬çš„æ•°æ®é›†æ˜¯ç»å…¸çš„ï¼Œé‚£ä¹ˆæ¯ä¸ªç»å…¸æ ·æœ¬åº”é¦–å…ˆç¼–ç æˆè¾“å…¥é‡å­æ€ï¼ˆå¦‚å‰ä¸€ç« æ‰€è¿°ï¼‰ã€‚é€šè¿‡ *m* ä¸ªæµ‹é‡ç®—ç¬¦ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªæ¯”ç‰¹ä¸²ï¼Œå¯ä»¥ç¼–ç æœ€å¤š
    2^m ä¸ªæ•´æ•°å€¼ï¼ˆç±»åˆ«æ ‡ç­¾ï¼‰ã€‚å¯¹äºäºŒåˆ†ç±»å™¨ï¼Œåªéœ€è¦å¯¹å•ä¸ªé‡å­æ¯”ç‰¹è¿›è¡Œæµ‹é‡å³å¯ã€‚
- en: '![FigureÂ 8.1: Schematic representation of a quantum neural network â€“ parameterised
    quantum circuit â€“ consisting of 1-qubit and 2-qubit gates and measurement operators
    on one or more quantum registers. The initial state |ÏˆkâŸ© encodes the k-th sample
    from the dataset. ](img/file749.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 8.1ï¼šé‡å­ç¥ç»ç½‘ç»œçš„ç¤ºæ„å›¾ â€”â€” å‚æ•°åŒ–é‡å­ç”µè·¯ â€”â€” ç”± 1 é‡å­æ¯”ç‰¹å’Œ 2 é‡å­æ¯”ç‰¹é—¨ä»¥åŠä¸€ä¸ªæˆ–å¤šä¸ªé‡å­å¯„å­˜å™¨ä¸Šçš„æµ‹é‡ç®—ç¬¦ç»„æˆã€‚åˆå§‹çŠ¶æ€
    |ÏˆkâŸ© ç¼–ç äº†æ¥è‡ªæ•°æ®é›†çš„ç¬¬ k ä¸ªæ ·æœ¬ã€‚](img/file749.jpg)'
- en: 'FigureÂ 8.1: Schematic representation of a quantum neural network â€“ parameterised
    quantum circuit â€“ consisting of 1-qubit and 2-qubit gates and measurement operators
    on one or more quantum registers. The initial state ![|ÏˆkâŸ©](img/file748.jpg) encodes
    the k-th sample from the dataset.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.1ï¼šé‡å­ç¥ç»ç½‘ç»œçš„ç¤ºæ„å›¾â€”â€”å¸¦å‚æ•°çš„é‡å­ç”µè·¯â€”â€”ç”±å•é‡å­æ¯”ç‰¹å’ŒåŒé‡å­æ¯”ç‰¹é—¨ä»¥åŠåœ¨ä¸€ä¸ªæˆ–å¤šä¸ªé‡å­å¯„å­˜å™¨ä¸Šçš„æµ‹é‡ç®—ç¬¦ç»„æˆã€‚åˆå§‹çŠ¶æ€![|ÏˆkâŸ©](img/file748.jpg)ç¼–ç äº†æ•°æ®é›†ä¸­ç¬¬kä¸ªæ ·æœ¬ã€‚
- en: The measurement process produces a single sample from the probability distribution
    encoded in the quantum state. Therefore, we need to run the quantum circuit many
    times for the same input in order to collect sufficient statistics for each qubit
    on which we perform measurement.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹é‡è¿‡ç¨‹ä»é‡å­æ€ä¸­ç¼–ç çš„æ¦‚ç‡åˆ†å¸ƒä¸­äº§ç”Ÿä¸€ä¸ªå•ä¸€çš„æ ·æœ¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯¹ç›¸åŒçš„è¾“å…¥å¤šæ¬¡è¿è¡Œé‡å­ç”µè·¯ï¼Œä»¥ä¾¿ä¸ºæˆ‘ä»¬è¿›è¡Œæµ‹é‡çš„æ¯ä¸ªé‡å­æ¯”ç‰¹æ”¶é›†è¶³å¤Ÿçš„ç»Ÿè®¡æ•°æ®ã€‚
- en: 'For example, if our QNN is organised as a classifier that should be able to
    predict one of the four possible classes ("0", "1", "2", and "3"), then we would
    need to perform measurement on 2 qubits with possible outcomes ![|00âŸ©](img/file750.jpg)
    corresponding to class "0", ![|01âŸ©](img/file751.jpg) corresponding to class "1",
    ![|10âŸ©](img/file752.jpg) corresponding to class "2", and ![|11âŸ©](img/file753.jpg)
    corresponding to class "3". Let us assume that we have run the quantum circuit
    1,000 times and observed the following results as shown in TableÂ [8.1](#x1-163003r1):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬çš„QNNè¢«ç»„ç»‡ä¸ºä¸€ä¸ªåˆ†ç±»å™¨ï¼Œèƒ½å¤Ÿé¢„æµ‹å››ä¸ªå¯èƒ½çš„ç±»åˆ«ä¹‹ä¸€ï¼ˆâ€œ0â€ï¼Œâ€œ1â€ï¼Œâ€œ2â€å’Œâ€œ3â€ï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦å¯¹2ä¸ªé‡å­æ¯”ç‰¹è¿›è¡Œæµ‹é‡ï¼Œå¯èƒ½çš„ç»“æœæ˜¯![|00âŸ©](img/file750.jpg)å¯¹åº”ç±»åˆ«â€œ0â€ï¼Œ![|01âŸ©](img/file751.jpg)å¯¹åº”ç±»åˆ«â€œ1â€ï¼Œ![|10âŸ©](img/file752.jpg)å¯¹åº”ç±»åˆ«â€œ2â€ï¼Œä»¥åŠ![|11âŸ©](img/file753.jpg)å¯¹åº”ç±»åˆ«â€œ3â€ã€‚å‡è®¾æˆ‘ä»¬å·²ç»è¿è¡Œäº†é‡å­ç”µè·¯1,000æ¬¡ï¼Œå¹¶è§‚å¯Ÿåˆ°å¦‚è¡¨[8.1](#x1-163003r1)æ‰€ç¤ºçš„ç»“æœï¼š
- en: '| Measured bitstring | Class label | Number of observations |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| æµ‹é‡ä½ä¸² | ç±»åˆ«æ ‡ç­¾ | è§‚å¯Ÿæ¬¡æ•° |'
- en: '| 00 | 0 | 100 |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 00 | 0 | 100 |'
- en: '| 01 | 1 | 550 |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 01 | 1 | 550 |'
- en: '| 10 | 2 | 200 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 2 | 200 |'
- en: '| 11 | 3 | 150 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 3 | 150 |'
- en: 'TableÂ 8.1: 1,000 runs of the quantum circuit.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨8.1ï¼šé‡å­ç”µè·¯çš„1,000æ¬¡è¿è¡Œã€‚
- en: Then we can conclude that the most likely class label for the given input is
    class "1" (with probability 55%). At the same time, we also obtain probabilities
    for all other possible class values, which may be useful in some cases.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œå¯¹äºç»™å®šçš„è¾“å…¥ï¼Œæœ€å¯èƒ½çš„ç±»åˆ«æ ‡ç­¾æ˜¯ç±»åˆ«â€œ1â€ï¼ˆæ¦‚ç‡ä¸º55%ï¼‰ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜è·å¾—äº†æ‰€æœ‰å…¶ä»–å¯èƒ½ç±»åˆ«å€¼çš„æ¦‚ç‡ï¼Œè¿™åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½ä¼šæœ‰ç”¨ã€‚
- en: The network is organised as *l* layers of one-qubit and two-qubit gates. The
    gates can be *adjustable*, meaning that they can be controlled by adjustable parameters,
    such as rotation angles, or they can be *fixed*. The 2-qubit gates in FigureÂ [8.1](#x1-1630001)
    are fixedÂ CX gates but, in principle, they can be adjustable controlled rotation
    gates. Although the network shown schematically in FigureÂ [8.1](#x1-1630001) can
    have up to *n* Ã— *l* adjustable parameters (*ğœƒ*[i]^j)[i=1,â€¦,n; j=1,â€¦,l], it is
    often the case that the two-qubit gates are fixed and we only have one-qubit rotations
    as available degrees of freedom in training the network.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œç»„ç»‡ä¸º *l* å±‚çš„å•é‡å­æ¯”ç‰¹å’ŒåŒé‡å­æ¯”ç‰¹é—¨ã€‚é—¨å¯ä»¥æ˜¯*å¯è°ƒçš„*ï¼Œæ„å‘³ç€å®ƒä»¬å¯ä»¥é€šè¿‡å¯è°ƒå‚æ•°ï¼ˆå¦‚æ—‹è½¬è§’åº¦ï¼‰æ¥æ§åˆ¶ï¼Œæˆ–è€…å®ƒä»¬ä¹Ÿå¯ä»¥æ˜¯*å›ºå®šçš„*ã€‚å›¾[8.1](#x1-1630001)ä¸­çš„åŒé‡å­æ¯”ç‰¹é—¨æ˜¯å›ºå®šçš„CXé—¨ï¼Œä½†åŸåˆ™ä¸Šï¼Œå®ƒä»¬å¯ä»¥æ˜¯å¯è°ƒçš„å—æ§æ—‹è½¬é—¨ã€‚å°½ç®¡å›¾[8.1](#x1-1630001)ä¸­ç¤ºæ„æ€§åœ°å±•ç¤ºçš„ç½‘ç»œæœ€å¤šå¯ä»¥æœ‰
    *n* Ã— *l* ä¸ªå¯è°ƒå‚æ•°ï¼ˆ*ğœƒ*[i]^j)[i=1,â€¦,n; j=1,â€¦,l]ï¼‰ï¼Œä½†é€šå¸¸æƒ…å†µä¸‹ï¼ŒåŒé‡å­æ¯”ç‰¹é—¨æ˜¯å›ºå®šçš„ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒç½‘ç»œæ—¶åªæœ‰å•é‡å­æ¯”ç‰¹æ—‹è½¬ä½œä¸ºå¯ç”¨çš„è‡ªç”±åº¦ã€‚
- en: Similar to classical neural networks, QNNs can be trained through either differentiable
    learning (for example, backpropagation of error with gradient descent) or non-differentiable
    learning (e.g., evolutionary search heuristics). Both approaches have their relative
    strengths and weaknesses. In theory, differentiable learning can be faster, but
    convergence is not guaranteed due to the well-known problem of "barren plateaus"
    associated with the gradients becoming vanishingly smallÂ Â [[207](Biblography.xhtml#XMcClean2018)]
    and is problem dependent. Non-differentiable learning is, as a rule, slower but
    avoids being trapped in local minima and works well in situations where the cost
    function is not smooth. SectionsÂ [8.2](#x1-1640002) andÂ [8.3](#x1-1680003) provide
    detailed descriptions of the QNN training procedures.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºç»å…¸ç¥ç»ç½‘ç»œï¼ŒQNNså¯ä»¥é€šè¿‡å¯å¾®åˆ†å­¦ä¹ ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨æ¢¯åº¦ä¸‹é™çš„è¯¯å·®åå‘ä¼ æ’­ï¼‰æˆ–éå¯å¾®åˆ†å­¦ä¹ ï¼ˆä¾‹å¦‚ï¼Œè¿›åŒ–æœç´¢å¯å‘å¼ç®—æ³•ï¼‰è¿›è¡Œè®­ç»ƒã€‚ä¸¤ç§æ–¹æ³•å„æœ‰å…¶ç›¸å¯¹çš„ä¼˜ç¼ºç‚¹ã€‚ä»ç†è®ºä¸Šè®²ï¼Œå¯å¾®åˆ†å­¦ä¹ å¯èƒ½æ›´å¿«ï¼Œä½†ç”±äºè‘—åçš„â€œè’åŸå¹³å°â€é—®é¢˜ï¼ˆå³æ¢¯åº¦å˜å¾—æå…¶å¾®å°[[207](Biblography.xhtml#XMcClean2018)]ï¼‰ï¼Œå…¶æ”¶æ•›æ€§å¹¶ä¸èƒ½å¾—åˆ°ä¿è¯ï¼Œä¸”å—å…·ä½“é—®é¢˜çš„å½±å“ã€‚éå¯å¾®åˆ†å­¦ä¹ é€šå¸¸è¾ƒæ…¢ï¼Œä½†é¿å…äº†è¢«å›°åœ¨å±€éƒ¨æœ€å°å€¼ï¼Œå¹¶ä¸”åœ¨ä»£ä»·å‡½æ•°ä¸å…‰æ»‘çš„æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ã€‚ç¬¬[8.2](#x1-1640002)èŠ‚å’Œç¬¬[8.3](#x1-1680003)èŠ‚æä¾›äº†QNNè®­ç»ƒè¿‡ç¨‹çš„è¯¦ç»†æè¿°ã€‚
- en: Obviously, the strongest motivation for using quantum classifiers is their ability
    to process quantum data. The input quantum states that must be classified may
    be outputs of some other quantum circuits. As we may not be able to store the
    information encoded in these quantum states classically, a quantum classifier
    becomes an indispensable tool. However, quantum classifiers have a realistic chance
    to demonstrate their advantage on purely classical data too. There are several
    considerations that motivate our interest in trying to apply QNNs to classical
    datasets.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œä½¿ç”¨é‡å­åˆ†ç±»å™¨çš„æœ€å¼ºåŠ¨åŠ›æ˜¯å®ƒä»¬èƒ½å¤Ÿå¤„ç†é‡å­æ•°æ®ã€‚å¿…é¡»åˆ†ç±»çš„è¾“å…¥é‡å­æ€å¯èƒ½æ˜¯å…¶ä»–é‡å­ç”µè·¯çš„è¾“å‡ºã€‚ç”±äºæˆ‘ä»¬å¯èƒ½æ— æ³•å°†è¿™äº›é‡å­æ€ä¸­ç¼–ç çš„ä¿¡æ¯ä»¥ç»å…¸æ–¹å¼å­˜å‚¨ï¼Œé‡å­åˆ†ç±»å™¨æˆä¸ºäº†ä¸€ä¸ªä¸å¯æˆ–ç¼ºçš„å·¥å…·ã€‚ç„¶è€Œï¼Œé‡å­åˆ†ç±»å™¨ä¹Ÿæœ‰å¯èƒ½åœ¨çº¯ç»å…¸æ•°æ®ä¸Šå±•ç¤ºå…¶ä¼˜åŠ¿ã€‚ä»¥ä¸‹æ˜¯å‡ ä¸ªåŠ¨æœºï¼Œä¿ƒä½¿æˆ‘ä»¬æœ‰å…´è¶£å°è¯•å°†QNNsåº”ç”¨äºç»å…¸æ•°æ®é›†ã€‚
- en: First, parameterised quantum circuits possess a larger expressive power than
    equivalent classical neural networks. Second, they are structurally able to efficiently
    fight overfitting. Finally, quantum speedup is achievable on some types of quantum
    hardware for specific use cases even at these very early stages of quantum computing
    development. ChapterÂ [12](Chapter_12.xhtml#x1-22500012) investigates these questions
    in more detail.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå‚æ•°åŒ–é‡å­ç”µè·¯çš„è¡¨è¾¾èƒ½åŠ›å¤§äºç­‰æ•ˆçš„ç»å…¸ç¥ç»ç½‘ç»œã€‚å…¶æ¬¡ï¼Œé‡å­ç”µè·¯ç»“æ„ä¸Šèƒ½å¤Ÿé«˜æ•ˆåœ°åº”å¯¹è¿‡æ‹Ÿåˆé—®é¢˜ã€‚æœ€åï¼Œåœ¨é‡å­è®¡ç®—å‘å±•çš„åˆæœŸé˜¶æ®µï¼Œå¯¹äºç‰¹å®šçš„ä½¿ç”¨æ¡ˆä¾‹ï¼ŒæŸäº›ç±»å‹çš„é‡å­ç¡¬ä»¶ä¸Šæ˜¯å¯ä»¥å®ç°é‡å­åŠ é€Ÿçš„ã€‚ç¬¬[12](Chapter_12.xhtml#x1-22500012)ç« æ›´è¯¦ç»†åœ°æ¢è®¨äº†è¿™äº›é—®é¢˜ã€‚
- en: In this chapter, we focus on using QNNs to efficiently solve specific finance-related
    classification use cases and provide a comparison with a number of standard classical
    classifiers. While experimentally proving quantum speedup and larger expressive
    power of QNNs requires powerful quantum hardware, the way QNNs fight overfitting
    can be verified on relatively small and shallow quantum circuits with the help
    of open-source quantum simulators.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹ä»‹ç»å¦‚ä½•ä½¿ç”¨é‡å­ç¥ç»ç½‘ç»œï¼ˆQNNsï¼‰é«˜æ•ˆåœ°è§£å†³ç‰¹å®šçš„ä¸é‡‘èç›¸å…³çš„åˆ†ç±»é—®é¢˜ï¼Œå¹¶ä¸å¤šç§æ ‡å‡†çš„ç»å…¸åˆ†ç±»å™¨è¿›è¡Œå¯¹æ¯”ã€‚è™½ç„¶é€šè¿‡å®éªŒè¯æ˜é‡å­åŠ é€Ÿå’Œé‡å­ç¥ç»ç½‘ç»œæ›´å¤§è¡¨è¾¾èƒ½åŠ›çš„ä¼˜åŠ¿éœ€è¦å¼ºå¤§çš„é‡å­ç¡¬ä»¶ï¼Œä½†QNNså¦‚ä½•åº”å¯¹è¿‡æ‹Ÿåˆçš„é—®é¢˜å¯ä»¥åœ¨ç›¸å¯¹è¾ƒå°ä¸”æµ…çš„é‡å­ç”µè·¯ä¸Šè¿›è¡ŒéªŒè¯ï¼Œå¹¶å€ŸåŠ©å¼€æºé‡å­æ¨¡æ‹Ÿå™¨å®ç°ã€‚
- en: QNNs are PQCs trained as ML models such as classifiers. QNNs have a natural
    advantage over classical neural networks when it comes to classifying quantum
    data. However, classical datasets can also be encoded as quantum states and processed
    by QNNs with their larger expressive power, their ability to efficiently fight
    overfitting, and, ultimately, with their quantum speedup.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: QNNsæ˜¯ä½œä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚åˆ†ç±»å™¨ï¼‰è¿›è¡Œè®­ç»ƒçš„å‚æ•°åŒ–é‡å­ç”µè·¯ã€‚é‡å­ç¥ç»ç½‘ç»œåœ¨å¤„ç†é‡å­æ•°æ®æ—¶ï¼Œç›¸æ¯”ç»å…¸ç¥ç»ç½‘ç»œå…·æœ‰å¤©ç„¶çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œç»å…¸æ•°æ®é›†ä¹Ÿå¯ä»¥ç¼–ç ä¸ºé‡å­æ€ï¼Œå¹¶é€šè¿‡QNNsè¿›è¡Œå¤„ç†ï¼Œå€ŸåŠ©å®ƒä»¬æ›´å¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ã€æœ‰æ•ˆåº”å¯¹è¿‡æ‹Ÿåˆçš„èƒ½åŠ›ï¼Œæœ€ç»ˆå®ç°é‡å­åŠ é€Ÿã€‚
- en: As we learned from ChapterÂ [5](Chapter_5.xhtml#x1-960005), to specify the architecture
    of the neural network is not sufficient to build the working ML model â€“ it is
    also necessary to specify the training algorithm. In the following sections, we
    show how a quantum neural network can be trained with the help of differentiable
    and non-differentiable learning methods.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬[5ç« ](Chapter_5.xhtml#x1-960005)ä¸­å­¦åˆ°çš„ï¼Œå•å•æŒ‡å®šç¥ç»ç½‘ç»œçš„æ¶æ„ä¸è¶³ä»¥æ„å»ºæœ‰æ•ˆçš„æœºå™¨å­¦ä¹ æ¨¡å‹â€”â€”è¿˜éœ€è¦æŒ‡å®šè®­ç»ƒç®—æ³•ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•é€šè¿‡å¯å¾®å’Œä¸å¯å¾®çš„å­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒé‡å­ç¥ç»ç½‘ç»œã€‚
- en: 8.2 Training QNN with Gradient Descent
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 ä½¿ç”¨æ¢¯åº¦ä¸‹é™è®­ç»ƒé‡å­ç¥ç»ç½‘ç»œ
- en: Since we are not only interested in building QNNs as standalone QML tools but
    also in comparing and contrasting them with classical neural networks, we start
    our review of QNN training methods with gradient descent â€“ a ubiquitous classical
    ML algorithm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬ä¸ä»…ä»…å…³æ³¨æ„å»ºä½œä¸ºç‹¬ç«‹å·¥å…·çš„é‡å­ç¥ç»ç½‘ç»œï¼Œè¿˜å¸Œæœ›å°†å…¶ä¸ç»å…¸ç¥ç»ç½‘ç»œè¿›è¡Œæ¯”è¾ƒå’Œå¯¹æ¯”ï¼Œå› æ­¤æˆ‘ä»¬é¦–å…ˆå›é¡¾ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•è®­ç»ƒé‡å­ç¥ç»ç½‘ç»œâ€”â€”è¿™æ˜¯ä¸€ä¸ªæ™®éä½¿ç”¨çš„ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•ã€‚
- en: 8.2.1 The finite difference scheme
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1 æœ‰é™å·®åˆ†æ–¹æ¡ˆ
- en: 'Training QNNs consists of specifying and executing a procedure that finds an
    optimal configuration of the adjustable rotation parametersÂ ğœƒ. Assume that a QNN
    is specified on *n* quantum registers withÂ *l* layers of adjustable quantum gates,
    where each adjustable gate is controlled by a single parameter (*ğœƒ*[i]^j)[i=1,â€¦,n;
    j=1,â€¦,l]. In this case,Â ğœƒ âˆˆâ„³[n,l] is anÂ *n*Ã—*l* matrix of adjustable network parameters:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒé‡å­ç¥ç»ç½‘ç»œåŒ…æ‹¬æŒ‡å®šå¹¶æ‰§è¡Œä¸€ä¸ªç¨‹åºï¼Œè¯¥ç¨‹åºå¯ä»¥æ‰¾åˆ°å¯è°ƒæ—‹è½¬å‚æ•°ğœƒçš„æœ€ä¼˜é…ç½®ã€‚å‡è®¾ä¸€ä¸ªé‡å­ç¥ç»ç½‘ç»œè¢«æŒ‡å®šåœ¨*n*ä¸ªé‡å­å¯„å­˜å™¨ä¸Šï¼Œå¹¶å…·æœ‰*l*å±‚å¯è°ƒé‡å­é—¨ï¼Œå…¶ä¸­æ¯ä¸ªå¯è°ƒé—¨ç”±ä¸€ä¸ªå•ä¸€å‚æ•°æ§åˆ¶ï¼ˆ*ğœƒ*[i]^j)[i=1,â€¦,n;
    j=1,â€¦,l]ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œğœƒ âˆˆâ„³[n,l]æ˜¯ä¸€ä¸ª*n*Ã—*l*çš„å¯è°ƒç½‘ç»œå‚æ•°çŸ©é˜µï¼š
- en: '| ![ âŒŠ 1 lâŒ‹ ğœƒ1 ... ğœƒ1 ğœƒ = &#124;&#124; .. ... .. &#124;&#124; . âŒˆ . . âŒ‰ ğœƒ1n
    ... ğœƒln ](img/file754.jpg) |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| ![ âŒŠ 1 lâŒ‹ ğœƒ1 ... ğœƒ1 ğœƒ = &#124;&#124; .. ... .. &#124;&#124; . âŒˆ . . âŒ‰ ğœƒ1n
    ... ğœƒln ](img/file754.jpg) |  |'
- en: Without loss of generality, we assume that we work with a binary classifier.
    The latter takes an input (a quantum state that encodes a sample from the dataset),
    applies a sequence of quantum gates (the parameterised quantum circuit controlled
    by at most *n* Ã— *l* adjustable parameters), and performs the measurement of an
    observableÂ *M* on the chosen quantum register. An example of an observable is
    the PauliÂ Z gate and the result of a single measurement is Â±1 for a qubit found
    in the state ![|0âŸ©](img/file755.jpg) or ![|1âŸ©](img/file756.jpg), respectively.
    The value of the measured observable is mapped into a value of a binary variable
    {0,Â 1}. This process is repeatedÂ *N* times for each sample in order to collect
    sufficient statistics for the classification result.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸å¤±ä¸€èˆ¬æ€§çš„å‰æä¸‹ï¼Œæˆ‘ä»¬å‡è®¾ä½¿ç”¨çš„æ˜¯äºŒåˆ†ç±»å™¨ã€‚è¯¥åˆ†ç±»å™¨æ¥æ”¶ä¸€ä¸ªè¾“å…¥ï¼ˆä¸€ä¸ªé‡å­æ€ï¼Œç¼–ç äº†æ¥è‡ªæ•°æ®é›†çš„ä¸€ä¸ªæ ·æœ¬ï¼‰ï¼Œåº”ç”¨ä¸€ç³»åˆ—é‡å­é—¨ï¼ˆç”±æœ€å¤š*n* Ã— *l*
    å¯è°ƒå‚æ•°æ§åˆ¶çš„å‚æ•°åŒ–é‡å­ç”µè·¯ï¼‰ï¼Œå¹¶å¯¹é€‰å®šçš„é‡å­å¯„å­˜å™¨è¿›è¡Œå¯è§‚å¯Ÿé‡*M*çš„æµ‹é‡ã€‚ä¸€ä¸ªå¯è§‚å¯Ÿé‡çš„ä¾‹å­æ˜¯ä¿åˆ©Zé—¨ï¼Œå¯¹äºä¸€ä¸ªé‡å­æ¯”ç‰¹ï¼Œè‹¥å…¶å¤„äºæ€![|0âŸ©](img/file755.jpg)æˆ–![|1âŸ©](img/file756.jpg)ï¼Œå•æ¬¡æµ‹é‡çš„ç»“æœåˆ†åˆ«ä¸ºÂ±1ã€‚æµ‹é‡åˆ°çš„å¯è§‚å¯Ÿé‡å€¼ä¼šæ˜ å°„ä¸ºäºŒå…ƒå˜é‡{0ï¼Œ1}çš„å€¼ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šé’ˆå¯¹æ¯ä¸ªæ ·æœ¬é‡å¤*N*æ¬¡ï¼Œä»¥ä¾¿æ”¶é›†è¶³å¤Ÿçš„ç»Ÿè®¡æ•°æ®ç”¨äºåˆ†ç±»ç»“æœã€‚
- en: 'The first step in finding an optimal configuration of adjustable parametersÂ ğœƒ
    is to choose an appropriate cost function â€“Â an objective function that represents
    the total error in classifying samples from the training dataset and which can
    be minimised by changing the adjustable network parameters. Let y := (*y*[1]*,â€¦,y*[K])
    be a vector of binary labels and f(ğœƒ) := (*f*[1](ğœƒ)*,â€¦,f*[K](ğœƒ)) a vector of binary
    classifier predictions for the training dataset consisting of *K* samples. The
    cost functionÂ *L*(ğœƒ) can then be defined, for example, as the sum of squared errors
    across all samples in the training dataset:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¯»æ‰¾å¯è°ƒå‚æ•°ğœƒçš„æœ€ä¼˜é…ç½®çš„ç¬¬ä¸€æ­¥æ˜¯é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„æˆæœ¬å‡½æ•°â€”â€”ä¸€ä¸ªè¡¨ç¤ºåœ¨åˆ†ç±»è®­ç»ƒæ•°æ®é›†æ ·æœ¬æ—¶æ€»è¯¯å·®çš„ç›®æ ‡å‡½æ•°ï¼Œä¸”å¯ä»¥é€šè¿‡æ”¹å˜å¯è°ƒç½‘ç»œå‚æ•°æ¥æœ€å°åŒ–ã€‚è®¾y :=
    (*y*[1]*,â€¦,y*[K])ä¸ºäºŒå…ƒæ ‡ç­¾å‘é‡ï¼Œf(ğœƒ) := (*f*[1](ğœƒ)*,â€¦,f*[K](ğœƒ))ä¸ºè®­ç»ƒæ•°æ®é›†ä¸­*K*ä¸ªæ ·æœ¬çš„äºŒåˆ†ç±»å™¨é¢„æµ‹ç»“æœå‘é‡ã€‚é‚£ä¹ˆï¼Œæˆæœ¬å‡½æ•°*L*(ğœƒ)å¯ä»¥å®šä¹‰ä¸ºï¼Œä¸¾ä¾‹æ¥è¯´ï¼Œè®­ç»ƒæ•°æ®é›†æ‰€æœ‰æ ·æœ¬çš„å¹³æ–¹è¯¯å·®ä¹‹å’Œï¼š
- en: '| ![ âˆ‘K L (ğœƒ) := 1 (yk âˆ’ fk(ğœƒ))2\. 2 k=1 ](img/file757.jpg) |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘K L (ğœƒ) := 1 (yk âˆ’ fk(ğœƒ))2\. 2 k=1 ](img/file757.jpg) |  |'
- en: The next step is an iterative update of the adjustable parameters in the direction
    that reduces the value of the cost function. That direction is given by the cost
    function gradientÂ â€“ hence the name of the method. The parameters are updated towards
    the direction of the steepest descent of the cost function. At step *u* + 1, we
    update the system to
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯æ²¿ç€èƒ½å¤Ÿå‡å°‘ä»£ä»·å‡½æ•°å€¼çš„æ–¹å‘è¿›è¡Œå¯è°ƒå‚æ•°çš„è¿­ä»£æ›´æ–°ã€‚è¿™ä¸ªæ–¹å‘ç”±ä»£ä»·å‡½æ•°çš„æ¢¯åº¦ç»™å‡ºâ€”â€”å› æ­¤è¯¥æ–¹æ³•å¾—åã€‚å‚æ•°æ²¿ç€ä»£ä»·å‡½æ•°çš„æœ€é€Ÿä¸‹é™æ–¹å‘è¿›è¡Œæ›´æ–°ã€‚åœ¨ç¬¬
    *u* + 1 æ­¥ï¼Œæˆ‘ä»¬å°†ç³»ç»Ÿæ›´æ–°ä¸ºï¼š
- en: '| ![ âˆ‚L(ğœƒ) u+1 ğœƒijâ† âˆ’ uğœƒji âˆ’ Î·---j-, for each i = 1,...,n, j = 1,...,l, âˆ‚ğœƒi
    ](img/file758.jpg) |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‚L(ğœƒ) u+1 ğœƒijâ† âˆ’ uğœƒji âˆ’ Î·---j-, for each i = 1,...,n, j = 1,...,l, âˆ‚ğœƒi
    ](img/file758.jpg) |  |'
- en: 'whereÂ *Î·* is the learning rate, namely a hyperparameter controlling the magnitude
    of the update. For each *i* = 1*,â€¦,n*, *j* = 1*,â€¦,l*, the derivative can be calculated
    numerically using a finite difference scheme:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *Î·* æ˜¯å­¦ä¹ ç‡ï¼Œå³æ§åˆ¶æ›´æ–°å¹…åº¦çš„è¶…å‚æ•°ã€‚å¯¹äºæ¯ä¸ª *i* = 1*,â€¦,n*ï¼Œ*j* = 1*,â€¦,l*ï¼Œå¯ä»¥ä½¿ç”¨æœ‰é™å·®åˆ†æ³•æ¥æ•°å€¼è®¡ç®—å¯¼æ•°ï¼š
- en: '| ![ j j j j âˆ‚L-(ğœƒ)- L(ğœƒ11,...,ğœƒi-+-Î”ğœƒi,...,ğœƒln)âˆ’-L-(ğœƒ11,...,ğœƒi-âˆ’-Î”ğœƒi,...,ğœƒln)-
    âˆ‚ğœƒj â‰ˆ 2Î” ğœƒj , i i ](img/file759.jpg) |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| ![ j j j j âˆ‚L-(ğœƒ)- L(ğœƒ11,...,ğœƒi-+-Î”ğœƒi,...,ğœƒln)âˆ’-L-(ğœƒ11,...,ğœƒi-âˆ’-Î”ğœƒi,...,ğœƒln)-
    âˆ‚ğœƒj â‰ˆ 2Î” ğœƒj , i i ](img/file759.jpg) |  |'
- en: 'with an error of order ğ’ª((Î”*ğœƒ*[i]^j)Â²), whereÂ Î”*ğœƒ*[i]^j is a small rotation
    angle increment. The physical characteristics of the NISQ devices put restrictions
    on how small this increment can be: in most cases Î”*ğœƒ*[i]^j should not be smaller
    thanÂ 0*.*1 radians. The rest of the training routine follows the standard classical
    algorithm of training neural networks through the backpropagation of error with
    gradient descent.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¯å·®çš„é˜¶æ•°ä¸º ğ’ª((Î”*ğœƒ*[i]^j)Â²)ï¼Œå…¶ä¸­ Î”*ğœƒ*[i]^j æ˜¯ä¸€ä¸ªå°çš„æ—‹è½¬è§’åº¦å¢é‡ã€‚NISQè®¾å¤‡çš„ç‰©ç†ç‰¹æ€§å¯¹è¿™ä¸€å¢é‡çš„æœ€å°å€¼æœ‰æ‰€é™åˆ¶ï¼šåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒÎ”*ğœƒ*[i]^j
    ä¸åº”å°äº 0*.*1 å¼§åº¦ã€‚å…¶ä½™çš„è®­ç»ƒè¿‡ç¨‹éµå¾ªç»å…¸çš„ç¥ç»ç½‘ç»œè®­ç»ƒç®—æ³•ï¼Œé€šè¿‡åå‘ä¼ æ’­è¯¯å·®å¹¶ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œä¼˜åŒ–ã€‚
- en: 8.2.2 The analytic gradient approach
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2 åˆ†ææ¢¯åº¦æ–¹æ³•
- en: An alternative to the finite difference method, which can be unstable and ill-conditioned
    due to truncation and round-off errors (for parameterised quantum circuitsÂ Â [[29](Biblography.xhtml#XBenedetti2019)]
    or, in fact, for classical neural networksÂ Â [[27](Biblography.xhtml#XBaydin)]),
    is the analytic gradient approach. It can be a viable choice for parameterised
    quantum circuits with adjustable one-qubit gates and fixed multi-qubit gates.
    FromÂ ([8.2.1](#x1-1650001)), the cost function gradient with respect to the parameterÂ *ğœƒ*[i]^j
    is given by
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æ›¿ä»£æœ‰é™å·®åˆ†æ³•çš„æ–¹æ³•æ˜¯åˆ†ææ¢¯åº¦æ³•ï¼Œå®ƒèƒ½æœ‰æ•ˆé¿å…ç”±äºæˆªæ–­å’Œèˆå…¥è¯¯å·®å¯¼è‡´çš„ä¸ç¨³å®šå’Œç—…æ€æƒ…å†µï¼ˆæ— è®ºæ˜¯å¯¹äºå‚æ•°åŒ–é‡å­ç”µè·¯ [[29](Biblography.xhtml#XBenedetti2019)]ï¼Œè¿˜æ˜¯å¯¹äºç»å…¸ç¥ç»ç½‘ç»œ
    [[27](Biblography.xhtml#XBaydin)]ï¼‰ï¼Œå®ƒå¯¹äºå…·æœ‰å¯è°ƒå•é‡å­æ¯”ç‰¹é—¨å’Œå›ºå®šå¤šé‡å­æ¯”ç‰¹é—¨çš„å‚æ•°åŒ–é‡å­ç”µè·¯æ˜¯ä¸€ä¸ªå¯è¡Œçš„é€‰æ‹©ã€‚ä»([8.2.1](#x1-1650001))ä¸­å¯ä»¥å¾—å‡ºï¼Œä»£ä»·å‡½æ•°ç›¸å¯¹äºå‚æ•°
    *ğœƒ*[i]^j çš„æ¢¯åº¦ä¸ºï¼š
- en: '| ![ K âˆ‚L-(ğœƒ)= âˆ’ âˆ‘ (y âˆ’ f (ğœƒ)) âˆ‚fk-(ğœƒ), âˆ‚ ğœƒji k k âˆ‚ğœƒji k=1 ](img/file760.jpg)
    |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| ![ K âˆ‚L-(ğœƒ)= âˆ’ âˆ‘ (y âˆ’ f (ğœƒ)) âˆ‚fk-(ğœƒ), âˆ‚ ğœƒji k k âˆ‚ğœƒji k=1 ](img/file760.jpg)
    |  |'
- en: so that the task of calculating the gradient of the cost function is reduced
    to the task of calculating the partial derivative of the expected value of the
    measurement operator for each sample quantum state that encodes the classical
    sample from the training dataset. LetÂ ![|ÏˆkâŸ©](img/file761.jpg) be the quantum
    state that encodes the *k*-th sample from the training dataset and letÂ U(ğœƒ) denote
    the unitary operator that represents the sequence of QNN gates transforming the
    initial stateÂ ![|ÏˆkâŸ©](img/file762.jpg). Then the expected value of the measurement
    operatorÂ M is given by
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè®¡ç®—ä»£ä»·å‡½æ•°æ¢¯åº¦çš„ä»»åŠ¡å¯ä»¥ç®€åŒ–ä¸ºè®¡ç®—æ¯ä¸ªæ ·æœ¬é‡å­æ€çš„æµ‹é‡ç®—ç¬¦çš„æœŸæœ›å€¼çš„åå¯¼æ•°ï¼Œè¯¥é‡å­æ€ç¼–ç äº†æ¥è‡ªè®­ç»ƒæ•°æ®é›†çš„ç»å…¸æ ·æœ¬ã€‚è®¾Â ![|ÏˆkâŸ©](img/file761.jpg)
    ä¸ºç¼–ç è®­ç»ƒæ•°æ®é›†ä¸­çš„ç¬¬ *k* ä¸ªæ ·æœ¬çš„é‡å­æ€ï¼Œä¸”è®¾Â U(ğœƒ) ä¸ºè¡¨ç¤ºä¸€ç³»åˆ—QNNé—¨çš„å¹ºæ­£ç®—ç¬¦ï¼Œè¿™äº›é—¨å°†åˆå§‹æ€Â ![|ÏˆkâŸ©](img/file762.jpg)
    è½¬æ¢ä¸ºæœ€ç»ˆçŠ¶æ€ã€‚é‚£ä¹ˆï¼Œæµ‹é‡ç®—ç¬¦Â M çš„æœŸæœ›å€¼ä¸ºï¼š
- en: '| ![fk(ğœƒ) = âŸ¨Ïˆk &#124;U â€ (ğœƒ)MU(ğœƒ) &#124;Ïˆk âŸ©. ](img/file763.jpg) |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| ![fk(ğœƒ) = âŸ¨Ïˆk &#124;U â€ (ğœƒ)MU(ğœƒ) &#124;Ïˆk âŸ©. ](img/file763.jpg) |  |'
- en: According to the conventions we used in constructing the QNN ansatz, the parameterÂ *ğœƒ*[i]^j
    only affects a single gate, which we will denote as G(*ğœƒ*[i]^j). Therefore, the
    sequence of gatesÂ U(ğœƒ) can be represented as
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æˆ‘ä»¬åœ¨æ„é€ QNNå‡è®¾æ—¶ä½¿ç”¨çš„çº¦å®šï¼Œå‚æ•° *ğœƒ*[i]^j ä»…å½±å“ä¸€ä¸ªå•ç‹¬çš„é—¨ï¼Œæˆ‘ä»¬å°†å…¶è¡¨ç¤ºä¸º G(*ğœƒ*[i]^j)ã€‚å› æ­¤ï¼Œé—¨åºåˆ—Â U(ğœƒ) å¯ä»¥è¡¨ç¤ºä¸ºï¼š
- en: '| ![U(ğœƒ) = VG(ğœƒji)W, ](img/file764.jpg) |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ![U(ğœƒ) = VG(ğœƒji)W, ](img/file764.jpg) |  |'
- en: 'whereÂ W andÂ V are gate sequences that precede and follow gate G(*ğœƒ*[i]^j).
    Let us absorb V into the Hermitian observable Q = V^â€ MV and W into the quantum
    state ![|Ï• âŸ© k](img/file765.jpg) = W![|Ïˆ âŸ© k](img/file766.jpg):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ W å’Œ V æ˜¯åœ¨é—¨ G(*ğœƒ*[i]^j) ä¹‹å‰å’Œä¹‹åçš„é—¨åºåˆ—ã€‚æˆ‘ä»¬å¯ä»¥å°† V å¸æ”¶åˆ°å„ç±³è§‚æµ‹é‡ Q = V^â€ MV ä¸­ï¼Œå¹¶å°† W å¸æ”¶åˆ°é‡å­æ€
    ![|Ï• âŸ© k](img/file765.jpg) = W![|Ïˆ âŸ© k](img/file766.jpg) ä¸­ï¼š
- en: '| ![ â€  j j fk(ğœƒ) = âŸ¨Ï•k&#124;G (ğœƒi)QG(ğœƒi) &#124;Ï•kâŸ©. ](img/file767.jpg) |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| ![ â€  j j fk(ğœƒ) = âŸ¨Ï•k&#124;G (ğœƒi)QG(ğœƒi) &#124;Ï•kâŸ©. ](img/file767.jpg) |  |'
- en: Then the partial derivative of *f*[k](ğœƒ) with respect to parameterÂ *ğœƒ*[i]^j
    is calculated as
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œ*f*[k](ğœƒ) å…³äºå‚æ•° *ğœƒ*[i]^j çš„åå¯¼æ•°è®¡ç®—ä¸º
- en: '| ![âˆ‚fk(ğœƒ) ----j-- âˆ‚ ğœƒi](img/file768.jpg) | = ![ âˆ‚ ---j âˆ‚ ğœƒi](img/file769.jpg)
    âŸ¨*Ï•*[k]&#124;G^â€ (*ğœƒ* [i]^j)QG(*ğœƒ* [i]^j)![&#124;Ï•kâŸ©](img/file770.jpg) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ![âˆ‚fk(ğœƒ) ----j-- âˆ‚ ğœƒi](img/file768.jpg) | = ![ âˆ‚ ---j âˆ‚ ğœƒi](img/file769.jpg)
    âŸ¨*Ï•*[k]&#124;G^â€ (*ğœƒ* [i]^j)QG(*ğœƒ* [i]^j)![&#124;Ï•kâŸ©](img/file770.jpg) |'
- en: '|  | = âŸ¨*Ï•*[k]&#124;![( ) âˆ‚G (ğœƒj) ----ij- âˆ‚ğœƒi](img/file771.jpg)^â€ QG(*ğœƒ* [i]^j)![&#124;Ï•kâŸ©](img/file772.jpg)
    + âŸ¨*Ï•*[k]&#124;G^â€ (*ğœƒ* [i]^j)Q![( ) âˆ‚G(ğœƒj) ---ji- âˆ‚ğœƒi](img/file773.jpg)![&#124;Ï•kâŸ©](img/file774.jpg)*.*
    | (8.2.1) |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | = âŸ¨*Ï•*[k]&#124;![( ) âˆ‚G (ğœƒj) ----ij- âˆ‚ğœƒi](img/file771.jpg)^â€ QG(*ğœƒ* [i]^j)![&#124;Ï•kâŸ©](img/file772.jpg)
    + âŸ¨*Ï•*[k]&#124;G^â€ (*ğœƒ* [i]^j)Q![( ) âˆ‚G(ğœƒj) ---ji- âˆ‚ğœƒi](img/file773.jpg)![&#124;Ï•kâŸ©](img/file774.jpg)*.*
    | (8.2.1) |  |'
- en: Let us denote
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¡¨ç¤º
- en: '| ![ âˆ‚G (ğœƒj) B := G(ğœƒji) and C :=----ji, âˆ‚ğœƒi ](img/file775.jpg) |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‚G (ğœƒj) B := G(ğœƒji) å’Œ C :=----ji, âˆ‚ğœƒi ](img/file775.jpg) |  |'
- en: and notice that
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”æ³¨æ„åˆ°
- en: '| ![ â€  â€  âŸ¨Ï•k&#124;C( QB &#124;Ï•k âŸ©+ âŸ¨Ï•k&#124;B QC &#124;Ï•k âŸ© ) 1- â€  â€  = 2 âŸ¨Ï•k
    &#124;(B + C) Q(B+ C) &#124;Ï•k âŸ©âˆ’ âŸ¨Ï•k&#124;(Bâˆ’ C) Q(B âˆ’ C) &#124;Ï•kâŸ© . ](img/file776.jpg)
    |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| ![ â€  â€  âŸ¨Ï•k&#124;C( QB &#124;Ï•k âŸ©+ âŸ¨Ï•k&#124;B QC &#124;Ï•k âŸ© ) 1- â€  â€  = 2 âŸ¨Ï•k
    &#124;(B + C) Q(B+ C) &#124;Ï•k âŸ©âˆ’ âŸ¨Ï•k&#124;(Bâˆ’ C) Q(B âˆ’ C) &#124;Ï•kâŸ© . ](img/file776.jpg)
    |  |'
- en: Therefore, if we can find the way to implement the operator BÂ±C as part of an
    overall unitary evolution then we can evaluateÂ ([8.2.1](#x1-166003r1)) directly.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿæ‰¾åˆ°å®ç°æ“ä½œç¬¦ BÂ±C ä½œä¸ºæ•´ä½“å•ä½æ¼”åŒ–çš„ä¸€éƒ¨åˆ†çš„æ–¹æ³•ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ç›´æ¥è®¡ç®—Â ([8.2.1](#x1-166003r1))ã€‚
- en: 8.2.3 The parameter shift rule for analytic gradient calculation
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.3 åˆ†ææ¢¯åº¦è®¡ç®—çš„å‚æ•°åç§»è§„åˆ™
- en: FollowingÂ Â [[257](Biblography.xhtml#XSchuld2018)], we outline the parameter
    shift rule for gates with generators with two distinct eigenvaluesÂ â€“Â this covers
    all one-qubit gates. Being unitary, the gateÂ G(*ğœƒ*[i]^j) above can be represented
    as
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ® [[257](Biblography.xhtml#XSchuld2018)]ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†å…·æœ‰ä¸¤ä¸ªä¸åŒç‰¹å¾å€¼çš„ç”Ÿæˆå…ƒçš„é—¨çš„å‚æ•°åç§»è§„åˆ™â€”â€”è¿™æ¶µç›–äº†æ‰€æœ‰å•é‡å­æ¯”ç‰¹é—¨ã€‚ä½œä¸ºå•ä½æ“ä½œï¼Œé—¨
    G(*ğœƒ*[i]^j) å¯ä»¥è¡¨ç¤ºä¸º
- en: '| ![ ( ) G(ğœƒji) = exp âˆ’ iğœƒjiÎ“ , ](img/file777.jpg) |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ) G(ğœƒji) = exp âˆ’ iğœƒjiÎ“ , ](img/file777.jpg) |  |'
- en: for some Hermitian operatorÂ Î“ (TheoremÂ [6](Chapter_1.xhtml#x1-29009r6)). The
    partial derivative with respect toÂ *ğœƒ*[i]^j reads
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸäº›å„ç±³ç®—ç¬¦ Î“ï¼ˆå®šç†Â [6](Chapter_1.xhtml#x1-29009r6)ï¼‰ã€‚å…³äº *ğœƒ*[i]^j çš„åå¯¼æ•°ä¸º
- en: '| ![ j ( ) âˆ‚G-(ğœƒi) = âˆ’ iÎ“ exp âˆ’ iğœƒjiÎ“ = âˆ’ iÎ“ G(ğœƒji). âˆ‚ğœƒji ](img/file778.jpg)
    |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| ![ j ( ) âˆ‚G-(ğœƒi) = âˆ’ iÎ“ exp âˆ’ iğœƒjiÎ“ = âˆ’ iÎ“ G(ğœƒji). âˆ‚ğœƒji ](img/file778.jpg)
    |  |'
- en: SubstitutingÂ ([8.2.3](#x1-1670003)) intoÂ ([8.2.1](#x1-166003r1)) yields
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å°†Â ([8.2.3](#x1-1670003)) ä»£å…¥Â ([8.2.1](#x1-166003r1)) å¾—åˆ°
- en: '| ![ âŸ© âŸ© âˆ‚fk(ğœƒ)-= âŸ¨Ï•â€²&#124;iÎ“ Q &#124;Ï•â€² + âŸ¨Ï•â€²&#124;Q(âˆ’ iÎ“ ) &#124;Ï• â€² , âˆ‚ğœƒji
    k k k k ](img/file779.jpg) |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ![ âŸ© âŸ© âˆ‚fk(ğœƒ)-= âŸ¨Ï•â€²&#124;iÎ“ Q &#124;Ï•â€² + âŸ¨Ï•â€²&#124;Q(âˆ’ iÎ“ ) &#124;Ï• â€² , âˆ‚ğœƒji
    k k k k ](img/file779.jpg) |  |'
- en: where ![ â€²âŸ© |Ï• k](img/file780.jpg) = G(*ğœƒ*[i]^j)![|Ï•kâŸ©](img/file781.jpg). IfÂ Î“
    has just two distinct eigenvalues we can shift the eigenvalues to Â±*r*, since
    the global phase is unobservableÂ Â [[257](Biblography.xhtml#XSchuld2018)]. With
    I denoting the identity operator we can rewriteÂ ([8.2.3](#x1-1670003)) as
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ ![ â€²âŸ© |Ï• k](img/file780.jpg) = G(*ğœƒ*[i]^j)![|Ï•kâŸ©](img/file781.jpg)ã€‚å¦‚æœ Î“ åªæœ‰ä¸¤ä¸ªä¸åŒçš„ç‰¹å¾å€¼ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç‰¹å¾å€¼ç§»è‡³
    Â±*r*ï¼Œå› ä¸ºå…¨å±€ç›¸ä½æ˜¯ä¸å¯è§‚å¯Ÿçš„ [[257](Biblography.xhtml#XSchuld2018)]ã€‚ç”¨ I è¡¨ç¤ºå•ä½ç®—ç¬¦ï¼Œæˆ‘ä»¬å¯ä»¥å°†Â ([8.2.3](#x1-1670003))
    é‡å†™ä¸º
- en: '| ![ ( ) âˆ‚fk(ğœƒ)- â€² iÎ“- â€²âŸ© â€² iÎ“- â€²âŸ© j = r âŸ¨Ï• k&#124;r QI &#124;Ï•k âˆ’ âŸ¨Ï•k&#124;IQ
    r &#124;Ï•k . âˆ‚ğœƒi ](img/file782.jpg) |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ) âˆ‚fk(ğœƒ)- â€² iÎ“- â€²âŸ© â€² iÎ“- â€²âŸ© j = r âŸ¨Ï• k&#124;r QI &#124;Ï•k âˆ’ âŸ¨Ï•k&#124;IQ
    r &#124;Ï•k . âˆ‚ğœƒi ](img/file782.jpg) |  |'
- en: Denoting
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤º
- en: '| ![ i B := I and C := âˆ’ rÎ“ , ](img/file783.jpg) |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ![ i B := I å’Œ C := âˆ’ rÎ“ , ](img/file783.jpg) |  |'
- en: 'and usingÂ ([8.2.2](#x1-166003r2)) we obtain fromÂ ([8.2.3](#x1-1670003)):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä½¿ç”¨Â ([8.2.2](#x1-166003r2)) æˆ‘ä»¬ä»Â ([8.2.3](#x1-1670003)) ä¸­å¾—åˆ°ï¼š
- en: '| ![ [ ] âˆ‚f (ğœƒ) r â€² ( i ) â€  ( i ) â€²âŸ© â€² ( i )â€  ( i ) â€²âŸ© --k-j--= -- âŸ¨Ï•k&#124;
    Iâˆ’ - Î“ Q Iâˆ’ -Î“ &#124;Ï•k âˆ’ âŸ¨Ï•k&#124; I + -Î“ Q I + - Î“ &#124;Ï•k . âˆ‚ğœƒi 2 r r r r
    ](img/file784.jpg) |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| ![ [ ] âˆ‚f (ğœƒ) r â€² ( i ) â€  ( i ) â€²âŸ© â€² ( i )â€  ( i ) â€²âŸ© --k-j--= -- âŸ¨Ï•k&#124;
    Iâˆ’ - Î“ Q Iâˆ’ -Î“ &#124;Ï•k âˆ’ âŸ¨Ï•k&#124; I + -Î“ Q I + - Î“ &#124;Ï•k . âˆ‚ğœƒi 2 r r r r
    ](img/file784.jpg) |  |'
- en: A straightforward computationÂ Â [[257](Biblography.xhtml#XSchuld2018),Â Theorem
    1] shows that if the Hermitian generatorÂ Î“ of the unitary operator G(*ğœƒ*) = exp(âˆ’i*ğœƒ*Î“)
    has at most two unique eigenvaluesÂ Â±*r*, then
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç›´æ¥çš„è®¡ç®— [[257](Biblography.xhtml#XSchuld2018), Theorem 1] æ˜¾ç¤ºï¼Œå¦‚æœå•ä½ç®—ç¬¦ G(*ğœƒ*) =
    exp(âˆ’i*ğœƒ*Î“) çš„å„ç±³ç”Ÿæˆå…ƒ Î“ è‡³å¤šå…·æœ‰ä¸¤ä¸ªå”¯ä¸€ç‰¹å¾å€¼ Â±*r*ï¼Œé‚£ä¹ˆ
- en: '| ![ ( ) ( -Ï€-) -1- i G âˆ“ 4r = âˆš2-- IÂ± rÎ“ . ](img/file785.jpg) |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ) ( -Ï€-) -1- i G âˆ“ 4r = âˆš2-- IÂ± rÎ“ . ](img/file785.jpg) |  |'
- en: 'In this case the gradient can be estimated using two additional evaluations
    of the quantum circuit. Either the gate G(*Ï€âˆ•*(4*r*)) or the gate G(âˆ’*Ï€âˆ•*(4*r*))
    should be placed in the original circuit next to the gate we are differentiating.
    Since for unitarily generated one-parameter gates G(*a*)G(*b*) = G(*a* + *b*),
    this is equivalent to shifting the gate parameter, and we obtain the â€œparameter
    shift ruleâ€Â Â [[257](Biblography.xhtml#XSchuld2018)] with the shift *s* = *Ï€âˆ•*(4*r*):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨ä¸¤ä¸ªé¢å¤–çš„é‡å­ç”µè·¯è¯„ä¼°æ¥ä¼°ç®—æ¢¯åº¦ã€‚åº”è¯¥åœ¨åŸå§‹ç”µè·¯ä¸­å°†é—¨G(*Ï€âˆ•*(4*r*))æˆ–é—¨G(âˆ’*Ï€âˆ•*(4*r*))æ”¾ç½®åœ¨æˆ‘ä»¬è¦å¾®åˆ†çš„é—¨æ—è¾¹ã€‚ç”±äºå¯¹äºå•ä½ç”Ÿæˆçš„å•å‚æ•°é—¨G(*a*)G(*b*)
    = G(*a* + *b*)ï¼Œè¿™ç›¸å½“äºåç§»é—¨å‚æ•°ï¼Œå› æ­¤æˆ‘ä»¬è·å¾—äº†â€œå‚æ•°åç§»è§„åˆ™â€[[257](Biblography.xhtml#XSchuld2018)]ï¼Œå…¶ä¸­åç§»é‡*s*
    = *Ï€âˆ•*(4*r*)ï¼š
- en: '| ![ ( ) âˆ‚fk(ğœƒ)-= r âŸ¨Ï•k &#124;Gâ€ (ğœƒj + s)QG(ğœƒj+ s) &#124;Ï•kâŸ©âˆ’ âŸ¨Ï•k &#124;G â€ (ğœƒj
    âˆ’ s)QG (ğœƒj âˆ’ s) &#124;Ï•kâŸ© . âˆ‚ğœƒji i i i i ](img/file786.jpg) |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ) âˆ‚fk(ğœƒ)-= r âŸ¨Ï•k &#124;Gâ€ (ğœƒj + s)QG(ğœƒj+ s) &#124;Ï•kâŸ©âˆ’ âŸ¨Ï•k &#124;G â€ (ğœƒj
    âˆ’ s)QG (ğœƒj âˆ’ s) &#124;Ï•kâŸ© . âˆ‚ğœƒji i i i i ](img/file786.jpg) |  |'
- en: 'IfÂ Î“ is a one-qubit rotation generator given by PauliÂ X, Y, andÂ Z operators,
    then *r* = 1*âˆ•*2 and *s* = *Ï€âˆ•*2Â Â [[213](Biblography.xhtml#XMitarai2018),Â [257](Biblography.xhtml#XSchuld2018)]:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœÎ“æ˜¯ç”±ä¿åˆ©Xã€Yå’ŒZç®—ç¬¦ç»™å‡ºçš„å•é‡å­æ¯”ç‰¹æ—‹è½¬ç”Ÿæˆå™¨ï¼Œåˆ™*r* = 1*âˆ•*2ï¼Œ*s* = *Ï€âˆ•*2[[213](Biblography.xhtml#XMitarai2018)ï¼Œ[257](Biblography.xhtml#XSchuld2018)]ï¼š
- en: '| ![âˆ‚fk(ğœƒ) 1( ( j Ï€) ( j Ï€ ) ---j--= -- âŸ¨Ï•k &#124;Gâ€  ğœƒi + -- QG ğœƒi +-- &#124;Ï•kâŸ©
    âˆ‚ğœƒi 2 ( 2 ) ( 2 ) ) âˆ’ âŸ¨Ï• &#124;Gâ€  ğœƒjâˆ’ Ï€- QG ğœƒj âˆ’ Ï€- &#124;Ï• âŸ© . k i 2 i 2 k ](img/file787.jpg)
    |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| ![âˆ‚fk(ğœƒ) 1( ( j Ï€) ( j Ï€ ) ---j--= -- âŸ¨Ï•k &#124;Gâ€  ğœƒi + -- QG ğœƒi +-- &#124;Ï•kâŸ©
    âˆ‚ğœƒi 2 ( 2 ) ( 2 ) ) âˆ’ âŸ¨Ï• &#124;Gâ€  ğœƒjâˆ’ Ï€- QG ğœƒj âˆ’ Ï€- &#124;Ï• âŸ© . k i 2 i 2 k ](img/file787.jpg)
    |  |'
- en: Therefore, what we need to do in order to estimate the gradient is to execute
    two circuitsÂ *N* times to collect statistics and to calculate the expectations
    on the right-hand side ofÂ ([8.2.3](#x1-1670003)). The first circuit will have
    the gate parameter shifted by *Ï€âˆ•*2 and the second circuit will have the gate
    parameter shifted by âˆ’*Ï€âˆ•*2.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦åšçš„äº‹æƒ…æ˜¯æ‰§è¡Œä¸¤æ¬¡ç”µè·¯*N*æ¬¡ä»¥æ”¶é›†ç»Ÿè®¡æ•°æ®ï¼Œå¹¶è®¡ç®—([8.2.3](#x1-1670003))å³ä¾§çš„æœŸæœ›å€¼ã€‚ç¬¬ä¸€ä¸ªç”µè·¯å°†é—¨å‚æ•°åç§»*Ï€âˆ•*2ï¼Œç¬¬äºŒä¸ªç”µè·¯å°†é—¨å‚æ•°åç§»âˆ’*Ï€âˆ•*2ã€‚
- en: Although this procedure is not necessarily faster than the finite difference
    scheme, it can produce a more accurate estimate of the cost function gradient.
    The main argument here is the fact that the NISQ hardware operates with limited
    precision. The state-of-the-art superconducting qubits have one-qubit gate fidelity
    â‰¤ 99*.*9% and two-qubit gate fidelity â‰¤ 99*.*7% with rotation angle precision
    of order 0.05 radians. Therefore, the finite difference scheme cannot assume infinitesimal
    rotation angles Î”*ğœƒ*Â â€“ they should not be smaller than about 0.1 radians (and,
    probably, materially larger in most cases). This means that gradients obtained
    with the finite difference scheme have some degree of built-in uncertainty that
    can only be fixed with further improvements in the NISQ hardware.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™ä¸ªè¿‡ç¨‹ä¸ä¸€å®šæ¯”æœ‰é™å·®åˆ†æ–¹æ¡ˆæ›´å¿«ï¼Œä½†å®ƒå¯ä»¥æä¾›æ›´å‡†ç¡®çš„ä»£ä»·å‡½æ•°æ¢¯åº¦ä¼°è®¡ã€‚è¿™é‡Œçš„ä¸»è¦è®ºç‚¹æ˜¯NISQç¡¬ä»¶åœ¨æœ‰é™ç²¾åº¦ä¸‹è¿è¡Œã€‚æœ€å…ˆè¿›çš„è¶…å¯¼é‡å­æ¯”ç‰¹å…·æœ‰å•é‡å­æ¯”ç‰¹é—¨ä¿çœŸåº¦â‰¤99*.*9%å’ŒåŒé‡å­æ¯”ç‰¹é—¨ä¿çœŸåº¦â‰¤99*.*7%ï¼Œä¸”æ—‹è½¬è§’åº¦ç²¾åº¦ä¸º0.05å¼§åº¦ã€‚å› æ­¤ï¼Œæœ‰é™å·®åˆ†æ–¹æ¡ˆä¸èƒ½å‡è®¾æ— ç©·å°çš„æ—‹è½¬è§’åº¦Î”*ğœƒ*
    â€“ å®ƒä»¬ä¸åº”å°äºå¤§çº¦0.1å¼§åº¦ï¼ˆå¹¶ä¸”åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå¯èƒ½è¦å¤§å¾—å¤šï¼‰ã€‚è¿™æ„å‘³ç€ä½¿ç”¨æœ‰é™å·®åˆ†æ–¹æ¡ˆè·å¾—çš„æ¢¯åº¦å…·æœ‰ä¸€å®šç¨‹åº¦çš„å›ºæœ‰ä¸ç¡®å®šæ€§ï¼Œåªæœ‰é€šè¿‡è¿›ä¸€æ­¥æ”¹å–„NISQç¡¬ä»¶æ‰èƒ½è§£å†³ã€‚
- en: QNNs can be trained with the gradient descent algorithm in full analogy with
    the backpropagation of error in classical neural networks. The gradients can be
    either calculated analytically or estimated numerically.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: QNNå¯ä»¥é€šè¿‡æ¢¯åº¦ä¸‹é™ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œå®Œå…¨ç±»æ¯”äºç»å…¸ç¥ç»ç½‘ç»œä¸­çš„è¯¯å·®åå‘ä¼ æ’­ã€‚æ¢¯åº¦å¯ä»¥é€šè¿‡è§£ææ–¹æ³•è®¡ç®—ï¼Œæˆ–è€…é€šè¿‡æ•°å€¼æ–¹æ³•ä¼°ç®—ã€‚
- en: 8.3 Training QNN with Particle Swarm Optimisation
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 ä½¿ç”¨ç²’å­ç¾¤ä¼˜åŒ–è®­ç»ƒQNN
- en: Having specified the gradient descent scheme for training QNNs in the previous
    section, we now turn our attention to a non-differentiable learning method based
    on the powerful evolutionary search algorithm.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€èŠ‚ä¸­å·²ç»æŒ‡å®šäº†ç”¨äºè®­ç»ƒQNNçš„æ¢¯åº¦ä¸‹é™æ–¹æ¡ˆï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬å‘ä¸€ç§åŸºäºå¼ºå¤§è¿›åŒ–æœç´¢ç®—æ³•çš„ä¸å¯å¾®å­¦ä¹ æ–¹æ³•ã€‚
- en: 8.3.1 The Particle Swarm Optimisation algorithm
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.1 ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•
- en: The Particle Swarm Optimisation (PSO) algorithm belongs to a wide class of evolutionary
    search heuristics where at each algorithm iteration ("generation" in the language
    of evolutionary algorithms), the population of solutions ("chromosomes" or "particles")
    is evaluated in terms of their fitness with respect to the environment. In the
    standard PSO formulationÂ Â [[236](Biblography.xhtml#XPoli2008)], a number of particles
    are placed in the solution space of some problem and each evaluates the fitness
    at its current location. Each particle then determines its movement through the
    solution space by combining some aspects of the history of its own fitness values
    with those of one or more members of the swarm, and then moves through the solution
    space with a velocity determined by the locations and processed fitness values
    of those other members, along with some random perturbations.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç²’å­ç¾¤ä¼˜åŒ–ï¼ˆPSOï¼‰ç®—æ³•å±äºå¹¿æ³›çš„è¿›åŒ–æœç´¢å¯å‘å¼ç®—æ³•ç±»ï¼Œæ¯æ¬¡ç®—æ³•è¿­ä»£ï¼ˆåœ¨è¿›åŒ–ç®—æ³•çš„æœ¯è¯­ä¸­ç§°ä¸ºâ€œä»£â€ï¼‰æ—¶ï¼Œè§£çš„ç§ç¾¤ï¼ˆâ€œæŸ“è‰²ä½“â€æˆ–â€œç²’å­â€ï¼‰ä¼šæ ¹æ®å…¶ä¸ç¯å¢ƒçš„é€‚åº”åº¦è¿›è¡Œè¯„ä¼°ã€‚åœ¨æ ‡å‡†çš„
    PSO å…¬å¼ä¸­ [[236](Biblography.xhtml#XPoli2008)]ï¼Œä¸€ç»„ç²’å­è¢«æ”¾ç½®åœ¨æŸä¸ªé—®é¢˜çš„è§£ç©ºé—´ä¸­ï¼Œå¹¶ä¸”æ¯ä¸ªç²’å­ä¼šè¯„ä¼°å…¶å½“å‰ä½ç½®çš„é€‚åº”åº¦ã€‚æ¯ä¸ªç²’å­æ¥ç€ç»“åˆè‡ªå·±é€‚åº”åº¦å€¼çš„å†å²è®°å½•å’Œä¸€ä¸ªæˆ–å¤šä¸ªç¾¤ä½“æˆå‘˜çš„é€‚åº”åº¦å†å²ï¼Œæ¥å†³å®šå…¶åœ¨è§£ç©ºé—´ä¸­çš„ç§»åŠ¨ï¼Œå¹¶ä»¥ç”±è¿™äº›æˆå‘˜çš„ä½ç½®å’Œå¤„ç†åçš„é€‚åº”åº¦å€¼ä»¥åŠä¸€äº›éšæœºæ‰°åŠ¨æ‰€å†³å®šçš„é€Ÿåº¦è¿›è¡Œç§»åŠ¨ã€‚
- en: 'It is a standard procedure Â [[127](Biblography.xhtml#XHassan2005),Â [172](Biblography.xhtml#XKondratyev2017)]
    to follow three steps in specifying the PSO algorithm. First, we initialise the
    positionsÂ x[k]^i := (*x*[k]^i(1)*,â€¦,x*[k]^i(*n*)) âˆˆâ„^n of each particle *i* at
    time *k* moving through the *n*-dimensional search space and taking values in
    some range [x[min]*,*x[max]]. Next we initialise the velocitiesÂ v[k]^i := (*v*[k]^i(1)*,â€¦,v*[k]^i(*n*))
    âˆˆâ„^n of each particle in the swarm. The initialisation process consists of distributing
    swarm particles randomly across the solution space:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†è¿‡ç¨‹ [[127](Biblography.xhtml#XHassan2005), [172](Biblography.xhtml#XKondratyev2017)]ï¼Œåœ¨æŒ‡å®š
    PSO ç®—æ³•æ—¶éœ€è¦éµå¾ªä¸‰ä¸ªæ­¥éª¤ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆå§‹åŒ–æ¯ä¸ªç²’å­ *i* åœ¨æ—¶é—´ *k* æ—¶åˆ»çš„ä½ç½®ä¿¡æ¯ x[k]^i := (*x*[k]^i(1)*,â€¦,x*[k]^i(*n*))
    âˆˆâ„^nï¼Œç²’å­åœ¨ *n* ç»´æœç´¢ç©ºé—´ä¸­ç§»åŠ¨å¹¶å–å€¼äºæŸä¸ªèŒƒå›´ [x[min]*,*x[max]]ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆå§‹åŒ–ç¾¤ä½“ä¸­æ¯ä¸ªç²’å­çš„é€Ÿåº¦ v[k]^i :=
    (*v*[k]^i(1)*,â€¦,v*[k]^i(*n*)) âˆˆâ„^nã€‚åˆå§‹åŒ–è¿‡ç¨‹åŒ…æ‹¬å°†ç¾¤ä½“ç²’å­éšæœºåˆ†å¸ƒåˆ°è§£ç©ºé—´ä¸­ï¼š
- en: '| ![ i i xmin + Ï‰v(xmax âˆ’ xmin) x0 = xmin + Ï‰x (xmax âˆ’ xmin), v0 = ----------Î”t----------,
    ](img/file788.jpg) |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ![ i i xmin + Ï‰v(xmax âˆ’ xmin) x0 = xmin + Ï‰x (xmax âˆ’ xmin), v0 = ----------Î”t----------,
    ](img/file788.jpg) |  |'
- en: whereÂ *Ï‰*[x] andÂ *Ï‰*[v] are uniformly distributed random variables on [0*,*1]
    andÂ Î”*t* is the time step between algorithm iterations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *Ï‰*[x] å’Œ *Ï‰*[v] æ˜¯å‡åŒ€åˆ†å¸ƒåœ¨ [0*,*1] åŒºé—´ä¸Šçš„éšæœºå˜é‡ï¼ŒÎ”*t* æ˜¯ç®—æ³•è¿­ä»£ä¹‹é—´çš„æ—¶é—´æ­¥é•¿ã€‚
- en: 'We then update the velocities of all particles at time *k* + 1 according to
    the specified objective function which depends on the particlesâ€™ current positions
    in the solution space at timeÂ *k*. The value of the objective function determines
    which particle has the best positionÂ p[k]^(global) in the current swarm and also
    determines the best positionÂ p^i of each particle over time, i.e., in the current
    and all previous moves. The velocity update formula uses these two pieces of information
    for each particle in the swarm along with the effect of the current motionÂ v[k]^i
    to provide a search directionÂ p[k+1]^i for the next iteration. The velocity update
    formula includes random parameters to ensure good coverage of the solution space
    and to avoid entrapment in local optima. The three values that affect the new
    search direction are the current motion, the particleâ€™s own memory, and the swarm
    influence. They are incorporated via a summation approach with three weight factors:
    inertiaÂ *w*, self-confidenceÂ *c*[1], and swarm confidenceÂ *c*[2]:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®æŒ‡å®šçš„ç›®æ ‡å‡½æ•°æ›´æ–°æ‰€æœ‰ç²’å­åœ¨æ—¶é—´ *k* + 1 æ—¶åˆ»çš„é€Ÿåº¦ï¼Œè¯¥ç›®æ ‡å‡½æ•°ä¾èµ–äºç²’å­åœ¨è§£ç©ºé—´ä¸­æ—¶é—´ *k* æ—¶åˆ»çš„å½“å‰ä½ç½®ã€‚ç›®æ ‡å‡½æ•°çš„å€¼å†³å®šäº†å½“å‰ç¾¤ä½“ä¸­å“ªä¸ªç²’å­æ‹¥æœ‰æœ€ä½³ä½ç½®
    p[k]^(global)ï¼Œå¹¶ä¸”è¿˜å†³å®šäº†æ¯ä¸ªç²’å­éšæ—¶é—´å˜åŒ–çš„æœ€ä½³ä½ç½® p^iï¼Œå³åœ¨å½“å‰åŠæ‰€æœ‰å…ˆå‰çš„ç§»åŠ¨ä¸­ã€‚é€Ÿåº¦æ›´æ–°å…¬å¼ä½¿ç”¨è¿™ä¸¤éƒ¨åˆ†ä¿¡æ¯ï¼ˆç²’å­çš„å½“å‰è¿åŠ¨ v[k]^iï¼‰æ¥æä¾›ä¸‹ä¸€ä¸ªè¿­ä»£çš„æœç´¢æ–¹å‘
    p[k+1]^iã€‚è¯¥å…¬å¼è¿˜åŒ…æ‹¬éšæœºå‚æ•°ï¼Œä»¥ç¡®ä¿è§£ç©ºé—´çš„è‰¯å¥½è¦†ç›–å¹¶é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚å½±å“æ–°æœç´¢æ–¹å‘çš„ä¸‰ä¸ªå€¼æ˜¯å½“å‰è¿åŠ¨ã€ç²’å­è‡ªèº«çš„è®°å¿†å’Œç¾¤ä½“çš„å½±å“ã€‚è¿™äº›é€šè¿‡æ±‚å’Œæ–¹æ³•ä¸ä¸‰ä¸ªæƒé‡å› å­ç»“åˆï¼šæƒ¯æ€§
    *w*ã€è‡ªä¿¡åº¦ *c*[1] å’Œç¾¤ä½“è‡ªä¿¡åº¦ *c*[2]ï¼š
- en: '| ![ ( ) ( i i) pglobalâˆ’ xi vik+1 = wvik + c1Ï‰1-p-âˆ’-xk-+ c2Ï‰2 --k-------k--,
    Î”t Î”t ](img/file789.jpg) |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ) ( i i) pglobalâˆ’ xi vik+1 = wvik + c1Ï‰1-p-âˆ’-xk-+ c2Ï‰2 --k-------k--,
    Î”t Î”t ](img/file789.jpg) |  |'
- en: whereÂ *Ï‰*[1] andÂ *Ï‰*[2] are uniformly distributed random variables on [0*,*1].
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *Ï‰*[1] å’Œ *Ï‰*[2] æ˜¯å‡åŒ€åˆ†å¸ƒåœ¨ [0*,*1] åŒºé—´ä¸Šçš„éšæœºå˜é‡ã€‚
- en: 'Finally, the position of each particle is updated using its velocity vector:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä½¿ç”¨æ¯ä¸ªç²’å­çš„é€Ÿåº¦å‘é‡æ¥æ›´æ–°å…¶ä½ç½®ï¼š
- en: '| ![xik+1 = xik + vik+1Î”t. ](img/file790.jpg) |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ![xik+1 = xik + vik+1Î”t. ](img/file790.jpg) |  |'
- en: 'These steps are repeated until either a desired convergence criterion is met
    or until we reach the maximum number of iterations. Various reflection rules (stopping
    at the boundary, mirror reflection back into the allowed domain, etc.)Â Â [[190](Biblography.xhtml#XLiuYangWang2010)]
    can be designed for the new position x[k+1]^i falling outside the [x[min]*,*x[max]]
    bounds and the dynamics can be normalised with Î”*t* â‰¡ 1\. IfÂ *K* is the last iteration
    of the algorithm, then the best solution found by the PSO is p[K]^(global). FigureÂ [8.2](#8.2)
    provides a schematic illustration of the particle movement through the solution
    space under the influence of three forces: momentum, attraction to the globally
    best solution found by all particles at the previous iteration, and attraction
    to the best solution found by the given particle across all previous iterations.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ­¥éª¤ä¼šé‡å¤è¿›è¡Œï¼Œç›´åˆ°æ»¡è¶³æœŸæœ›çš„æ”¶æ•›æ ‡å‡†æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚å¯ä»¥è®¾è®¡å„ç§åå°„è§„åˆ™ï¼ˆåœ¨è¾¹ç•Œå¤„åœæ­¢ï¼Œé•œåƒåå°„å›å…è®¸çš„é¢†åŸŸç­‰ï¼‰[[190](Biblography.xhtml#XLiuYangWang2010)]ï¼Œä»¥åº”å¯¹æ–°çš„ä½ç½®x[k+1]^iè¶…å‡º[x[min]*,*x[max]]è¾¹ç•Œçš„æƒ…å†µï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡Î”*t*
    â‰¡ 1æ¥è§„èŒƒåŒ–åŠ¨æ€ã€‚å¦‚æœ*K*æ˜¯ç®—æ³•çš„æœ€åä¸€æ¬¡è¿­ä»£ï¼Œé‚£ä¹ˆPSOæ‰¾åˆ°çš„æœ€ä½³è§£æ˜¯p[K]^(global)ã€‚å›¾[8.2](#8.2)æä¾›äº†ç²’å­åœ¨ä¸‰ç§åŠ›çš„ä½œç”¨ä¸‹é€šè¿‡è§£ç©ºé—´çš„ç¤ºæ„å›¾ï¼šåŠ¨é‡ã€å¸å¼•å…¨ä½“ç²’å­åœ¨ä¸Šä¸€æ¬¡è¿­ä»£ä¸­æ‰¾åˆ°çš„å…¨å±€æœ€ä½³è§£ï¼Œä»¥åŠå¸å¼•ç»™å®šç²’å­åœ¨æ‰€æœ‰å‰æ¬¡è¿­ä»£ä¸­æ‰¾åˆ°çš„æœ€ä½³è§£ã€‚
- en: '![FigureÂ 8.2: Schematic illustration of the PSO algorithm. Each particle moves
    through the solution space under the influence of three forces: momentum, own
    memory, and swarm influence. ](img/file791.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾8.2ï¼šPSOç®—æ³•çš„ç¤ºæ„å›¾ã€‚æ¯ä¸ªç²’å­åœ¨ä¸‰ç§åŠ›çš„ä½œç”¨ä¸‹åœ¨è§£ç©ºé—´ä¸­ç§»åŠ¨ï¼šåŠ¨é‡ã€ç²’å­çš„è®°å¿†å’Œç¾¤ä½“å½±å“ã€‚](img/file791.jpg)'
- en: 'FigureÂ 8.2: Schematic illustration of the PSO algorithm. Each particle moves
    through the solution space under the influence of three forces: momentum, own
    memory, and swarm influence.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.2ï¼šPSOç®—æ³•çš„ç¤ºæ„å›¾ã€‚æ¯ä¸ªç²’å­åœ¨ä¸‰ç§åŠ›çš„ä½œç”¨ä¸‹åœ¨è§£ç©ºé—´ä¸­ç§»åŠ¨ï¼šåŠ¨é‡ã€å¯¹å‰ä¸€æ¬¡è¿­ä»£ä¸­æ‰€æœ‰ç²’å­æ‰¾åˆ°çš„å…¨å±€æœ€ä½³è§£çš„å¸å¼•åŠ›ï¼Œä»¥åŠå¯¹ç»™å®šç²’å­åœ¨æ‰€æœ‰å‰æ¬¡è¿­ä»£ä¸­æ‰¾åˆ°çš„æœ€ä½³è§£çš„å¸å¼•åŠ›ã€‚
- en: 8.3.2 PSO algorithm for training quantum neural networks
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.2 ç”¨äºè®­ç»ƒé‡å­ç¥ç»ç½‘ç»œçš„PSOç®—æ³•
- en: We are now ready to specify the PSO algorithm to train QNNs. We consider the
    most general case of an *n* Ã— *l* matrix of adjustable parameters (rotations)Â ğœƒ,
    whereÂ *n* is the number of quantum registers andÂ *l* is the number of network
    layers. The solution we look for is the matrixÂ ([8.2.1](#x1-1650001)) of adjustable
    parameters that minimises the chosen cost function.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å‡†å¤‡æŒ‡å®šPSOç®—æ³•æ¥è®­ç»ƒé‡å­ç¥ç»ç½‘ç»œï¼ˆQNNï¼‰ã€‚æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ª* n * Ã— * l *çš„å¯è°ƒå‚æ•°ï¼ˆæ—‹è½¬ï¼‰çŸ©é˜µğœƒï¼Œå…¶ä¸­*n*æ˜¯é‡å­å¯„å­˜å™¨çš„æ•°é‡ï¼Œ*l*æ˜¯ç½‘ç»œå±‚æ•°ã€‚æˆ‘ä»¬å¯»æ‰¾çš„è§£å†³æ–¹æ¡ˆæ˜¯å¯è°ƒå‚æ•°çŸ©é˜µ([8.2.1](#x1-1650001))ï¼Œå®ƒæœ€å°åŒ–æ‰€é€‰çš„æˆæœ¬å‡½æ•°ã€‚
- en: 'The cost function can be specified in many different ways depending on what
    particular aspects we want to encourage or penalise. Given the training dataset,
    we would like to find a configuration of adjustable parametersÂ ğœƒ such that as
    many samples as possible are classified correctly. One possible choice of cost
    function, for example, may be the ratio of incorrect to correct classification
    decisions. However, the classification process is probabilistic in nature â€“ we
    decide on the sample label after many runs of the quantum circuit, which generate
    sufficient statistics. Therefore, each classification decision is not just right
    or wrong but can be seen as "more right" or "more wrong". If the correct sample
    label is "1" and we get "0" 51% of the time then the classifier is slightly wrong:
    the chances are that similar samples would be classified correctly or only a small
    change to the adjustable network parameters is required to rectify the classification
    process. But if we get "0", say, 90% of the time, then the classifier is "very
    wrong" and we need to penalise the outcome more aggressively.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆæœ¬å‡½æ•°å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼æŒ‡å®šï¼Œå…·ä½“å–å†³äºæˆ‘ä»¬å¸Œæœ›é¼“åŠ±æˆ–æƒ©ç½šå“ªäº›ç‰¹å®šæ–¹é¢ã€‚ç»™å®šè®­ç»ƒæ•°æ®é›†ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ç§å¯è°ƒå‚æ•°ğœƒçš„é…ç½®ï¼Œä½¿å¾—å°½å¯èƒ½å¤šçš„æ ·æœ¬è¢«æ­£ç¡®åˆ†ç±»ã€‚ä¾‹å¦‚ï¼Œä¸€ç§å¯èƒ½çš„æˆæœ¬å‡½æ•°é€‰æ‹©æ˜¯é”™è¯¯åˆ†ç±»ä¸æ­£ç¡®åˆ†ç±»å†³ç­–çš„æ¯”ç‡ã€‚ç„¶è€Œï¼Œåˆ†ç±»è¿‡ç¨‹æœ¬è´¨ä¸Šæ˜¯æ¦‚ç‡æ€§çš„â€”â€”æˆ‘ä»¬åœ¨å¤šæ¬¡è¿è¡Œé‡å­ç”µè·¯åå†³å®šæ ·æœ¬æ ‡ç­¾ï¼Œè¿™äº›è¿è¡Œäº§ç”Ÿäº†è¶³å¤Ÿçš„ç»Ÿè®¡æ•°æ®ã€‚å› æ­¤ï¼Œæ¯ä¸ªåˆ†ç±»å†³ç­–ä¸ä»…ä»…æ˜¯å¯¹é”™çš„é—®é¢˜ï¼Œå®ƒä¹Ÿå¯ä»¥è¢«çœ‹ä½œæ˜¯â€œæ›´å¯¹â€æˆ–â€œæ›´é”™â€ã€‚å¦‚æœæ­£ç¡®çš„æ ·æœ¬æ ‡ç­¾æ˜¯â€œ1â€ï¼Œè€Œæˆ‘ä»¬51%çš„æ—¶é—´å¾—åˆ°â€œ0â€ï¼Œé‚£ä¹ˆåˆ†ç±»å™¨ç•¥å¾®é”™è¯¯ï¼šè¿™ç§æƒ…å†µä¸‹ï¼Œç±»ä¼¼çš„æ ·æœ¬æœ‰å¯èƒ½è¢«æ­£ç¡®åˆ†ç±»ï¼Œæˆ–è€…åªéœ€å¯¹å¯è°ƒçš„ç½‘ç»œå‚æ•°è¿›è¡Œå°çš„è°ƒæ•´å³å¯çº æ­£åˆ†ç±»è¿‡ç¨‹ã€‚ä½†å¦‚æœæˆ‘ä»¬90%çš„æ—¶é—´å¾—åˆ°â€œ0â€ï¼Œé‚£ä¹ˆåˆ†ç±»å™¨å°±æ˜¯â€œéå¸¸é”™â€ï¼Œæˆ‘ä»¬éœ€è¦æ›´ç§¯æåœ°æƒ©ç½šè¿™ä¸€ç»“æœã€‚
- en: 'One possible realisation of the cost function that takes into account the above
    argument is as follows: Without loss of generality, assume that we work with the
    binary class labelsÂ "0" andÂ "1", and let y := (*y*[1]*,â€¦,y*[K]) be a vector of
    sample labels (either "0" or "1") from the training dataset. Further, let â„™(ğœƒ)
    := (â„™[1](ğœƒ)*,â€¦,*â„™[K](ğœƒ)) be a vector of QNN estimated probabilities of predicting
    class "1" for the given sample (i.e., the number of quantum circuit runs that
    returned "1" after measurement divided by the total number of quantum circuit
    runs). Then the cost functionÂ *L*(ğœƒ) is given by the following pseudo code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§å¯èƒ½çš„ä»£ä»·å‡½æ•°å®ç°æ–¹å¼ï¼Œè€ƒè™‘åˆ°ä¸Šè¿°å‚æ•°å¦‚ä¸‹ï¼šä¸å¤±ä¸€èˆ¬æ€§ï¼Œå‡è®¾æˆ‘ä»¬ä½¿ç”¨äºŒåˆ†ç±»æ ‡ç­¾"0"å’Œ"1"ï¼Œå¹¶ä¸”è®¾ y := (*y*[1]*,â€¦,y*[K])
    ä¸ºä¸€ä¸ªæ ·æœ¬æ ‡ç­¾å‘é‡ï¼ˆæ ‡ç­¾ä¸º"0"æˆ–"1"ï¼‰ï¼Œæ¥è‡ªè®­ç»ƒæ•°æ®é›†ã€‚è¿›ä¸€æ­¥ï¼Œè®¾ â„™(ğœƒ) := (â„™[1](ğœƒ)*,â€¦,*â„™[K](ğœƒ)) ä¸ºQNNä¼°è®¡çš„æ ·æœ¬é¢„æµ‹ä¸ºç±»åˆ«"1"çš„æ¦‚ç‡å‘é‡ï¼ˆå³é‡å­ç”µè·¯è¿è¡Œä¸­è¿”å›"1"çš„æ¬¡æ•°é™¤ä»¥æ€»è¿è¡Œæ¬¡æ•°ï¼‰ã€‚é‚£ä¹ˆï¼Œä»£ä»·å‡½æ•°
    *L*(ğœƒ) ç”±ä»¥ä¸‹ä¼ªä»£ç ç»™å‡ºï¼š
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This cost function penalises large errors in the class probability estimate
    more than small errors and represents the total error across all samples in the
    training dataset.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ä»£ä»·å‡½æ•°å¯¹ç±»åˆ«æ¦‚ç‡ä¼°è®¡çš„è¾ƒå¤§è¯¯å·®æ¯”å°è¯¯å·®æƒ©ç½šæ›´é‡ï¼Œä¸”è¡¨ç¤ºè®­ç»ƒæ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬çš„æ€»è¯¯å·®ã€‚
- en: 'We can now formulate the QNN training algorithm, which has the following inputs:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥åˆ¶å®šQNNè®­ç»ƒç®—æ³•ï¼Œè¯¥ç®—æ³•çš„è¾“å…¥å¦‚ä¸‹ï¼š
- en: '| Variable | Meaning |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| å˜é‡ | å«ä¹‰ |'
- en: '| X := (X[1]*,â€¦,*X[K]) âˆˆâ„^(MÃ—K) | training dataset of features encoded as |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| X := (X[1]*,â€¦,*X[K]) âˆˆâ„^(MÃ—K) | ç¼–ç ä¸ºç‰¹å¾çš„è®­ç»ƒæ•°æ®é›† |'
- en: '|  | rotation angles on [0*,Ï€*] |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | æ—‹è½¬è§’åº¦èŒƒå›´ä¸º [0*,Ï€*] |'
- en: '| y := (*y*[1]*,â€¦,y*[K]) âˆˆ{0*,*1}^K | vector of binary labels |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| y := (*y*[1]*,â€¦,y*[K]) âˆˆ{0*,*1}^K | äºŒè¿›åˆ¶æ ‡ç­¾å‘é‡ |'
- en: '| *N*[iter] | number of iterations |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| *N*[iter] | è¿­ä»£æ¬¡æ•° |'
- en: '| *N*[runs] | number of quantum circuit runs |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| *N*[runs] | é‡å­ç”µè·¯è¿è¡Œæ¬¡æ•° |'
- en: '| *M* | number of particles (solutions) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| *M* | ç²’å­ï¼ˆè§£ï¼‰çš„æ•°é‡ |'
- en: '| *w* | momentum coefficient |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| *w* | åŠ¨é‡ç³»æ•° |'
- en: '| *c* [1] | particle memory coefficient |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| *c* [1] | ç²’å­è®°å¿†ç³»æ•° |'
- en: '| *c*[2] | swarm influence coefficient |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| *c*[2] | ç¾¤ä½“å½±å“ç³»æ•° |'
- en: '| *n* | number of quantum registers |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| *n* | é‡å­å¯„å­˜å™¨çš„æ•°é‡ |'
- en: '| *l* | number of QNN layers |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| *l* | QNNå±‚æ•° |'
- en: 'TableÂ 8.2: Inputs of the QNN training algorithm'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 8.2ï¼šQNNè®­ç»ƒç®—æ³•çš„è¾“å…¥
- en: 'The algorithm operates on the following objects, where *m* = 1*,â€¦,M* denotes
    the *m*-th particle, and *t* = 0*,â€¦,N*[iter] represents the algorithm iteration
    step:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç®—æ³•ä½œç”¨äºä»¥ä¸‹å¯¹è±¡ï¼Œå…¶ä¸­ *m* = 1*,â€¦,M* è¡¨ç¤ºç¬¬ *m* ä¸ªç²’å­ï¼Œ*t* = 0*,â€¦,N*[iter] è¡¨ç¤ºç®—æ³•çš„è¿­ä»£æ­¥éª¤ï¼š
- en: 'ğœƒ(*t*;*m*) âˆˆâ„³[nl]([âˆ’*Ï€,Ï€*]): position of particleÂ *m* at timeÂ *t*;'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ğœƒ(*t*;*m*) âˆˆâ„³[nl]([âˆ’*Ï€,Ï€*]): ç¬¬ *m* ä¸ªç²’å­åœ¨æ—¶é—´ *t* çš„ä½ç½®ï¼›'
- en: 'v(*t*;*m*) âˆˆâ„³[nl]([âˆ’*Ï€,Ï€*]): velocity of particleÂ *m* at timeÂ *t*;'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'v(*t*;*m*) âˆˆâ„³[nl]([âˆ’*Ï€,Ï€*]): ç¬¬ *m* ä¸ªç²’å­åœ¨æ—¶é—´ *t* çš„é€Ÿåº¦ï¼›'
- en: 'Î(*m*) âˆˆ â„³[nl]([âˆ’*Ï€,Ï€*]): best position found by particleÂ *m* across all iterations;'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Î(*m*) âˆˆ â„³[nl]([âˆ’*Ï€,Ï€*]): ç¬¬ *m* ä¸ªç²’å­åœ¨æ‰€æœ‰è¿­ä»£ä¸­çš„æœ€ä½³ä½ç½®ï¼›'
- en: 'Î¦(*t*) âˆˆ â„³[nl]([âˆ’*Ï€,Ï€*]): the globally best position found by all particles
    at timeÂ *t*;'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Î¦(*t*) âˆˆ â„³[nl]([âˆ’*Ï€,Ï€*]): æ‰€æœ‰ç²’å­åœ¨æ—¶é—´ *t* æ‰¾åˆ°çš„å…¨å±€æœ€ä½³ä½ç½®ï¼›'
- en: '*L*(ğœƒ): value of the cost function for the solutionÂ ğœƒ.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L*(ğœƒ): è§£ ğœƒ çš„ä»£ä»·å‡½æ•°å€¼ã€‚'
- en: '![--------------------------------------------------------------------- -Algorithm---5:-Particle
    Swarm-Optimisation--------------------------- Result: Optimal con figuration of
    adjustable QNN parameters âˆ— ğœƒ := argmin L(ğœƒ). Initialisation and evaluation of
    the first set of solutions (we set Î”t in (8.3.1) equal to 1): for each particle
    m = 1,...,M do | for i = 1,...,n, j = 1,...,l do | | Randomly draw the rotation
    angle ğœƒj(0;m ) from ğ’° ([âˆ’ Ï€,Ï€]). | | i | | Randomly draw the rotation angle vji(0;m
    ) from ğ’° ([âˆ’ Ï€,Ï€]). | end | | | Initialise the individually best solution: | |
    Î(m ) â† ğœƒ(0;m ) | | for k = 1,...,K do | | Run the quantum circuit Nruns times
    with configuration | | | | ğœƒ(0;m ) on sample Xk to estimate the probability â„™k
    of | | reading out "1" on the target qubit. | | end | | Evaluate the cost function
    L(ğœƒ(0;m )) given the probabilities | â„™ := (â„™ ,...,â„™ ). 1 K end Order solutions
    from best (minimal cost function ) to worst (maximal cost function). Î¦ (0) â† configuration
    corresponding to the minimum of the cost function. Initialise the optimal configuration:
    âˆ— ğœƒ â† Î¦ (0) ---------------------------------------------------------------------
    ](img/file792.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- -ç®—æ³•---5:-ç²’å­ç¾¤ä¼˜åŒ–---------------------------
    ç»“æœï¼šå¯è°ƒ QNN å‚æ•°çš„æœ€ä¼˜é…ç½® âˆ— ğœƒ := argmin L(ğœƒ)ã€‚åˆå§‹è§£é›†çš„åˆå§‹åŒ–ä¸è¯„ä¼°ï¼ˆæˆ‘ä»¬åœ¨ï¼ˆ8.3.1ï¼‰ä¸­å°† Î”t è®¾ç½®ä¸º 1ï¼‰ï¼šå¯¹æ¯ä¸ªç²’å­
    m = 1,...,M åš | å¯¹ i = 1,...,n, j = 1,...,l åš | | ä» ğ’° ([âˆ’ Ï€,Ï€]) ä¸­éšæœºæŠ½å–æ—‹è½¬è§’åº¦ ğœƒj(0;m
    )ã€‚ | | i | | ä» ğ’° ([âˆ’ Ï€,Ï€]) ä¸­éšæœºæŠ½å–æ—‹è½¬è§’åº¦ vji(0;m )ã€‚ | ç»“æŸ | | | åˆå§‹åŒ–å•ç‹¬çš„æœ€ä½³è§£ï¼š | | Î(m
    ) â† ğœƒ(0;m ) | | å¯¹ k = 1,...,K åš | | ä½¿ç”¨é…ç½® | | | ğœƒ(0;m ) åœ¨æ ·æœ¬ Xk ä¸Šè¿è¡Œé‡å­ç”µè·¯ Nruns æ¬¡ï¼Œä»¥ä¼°ç®—
    | | è¯»å–ç›®æ ‡é‡å­æ¯”ç‰¹ä¸º "1" çš„æ¦‚ç‡ â„™kã€‚ | | ç»“æŸ | | è¯„ä¼°ç»™å®šæ¦‚ç‡çš„ä»£ä»·å‡½æ•° L(ğœƒ(0;m )) | â„™ := (â„™1,...,â„™K
    ) ç»“æŸ æŒ‰ä»£ä»·å‡½æ•°å€¼ä»æœ€ä¼˜ï¼ˆæœ€å°ä»£ä»·å‡½æ•°ï¼‰åˆ°æœ€å·®ï¼ˆæœ€å¤§ä»£ä»·å‡½æ•°ï¼‰æ’åºè§£ã€‚Î¦ (0) â† å¯¹åº”ä»£ä»·å‡½æ•°æœ€å°å€¼çš„é…ç½®ã€‚åˆå§‹åŒ–æœ€ä¼˜é…ç½®ï¼š âˆ— ğœƒ â† Î¦ (0)
    --------------------------------------------------------------------- ](img/file792.jpg)'
- en: '![--------------------------------------------------------------------- Iterations:
    for t = 1,...,Niter do | | for m = 1,...,M do | | for i = 1,...,n, j = 1,...,l
    do | | | Generate independent random numbers Ï‰ âˆ¼ U [0,1 ] and | | | 1 | | | Ï‰2
    âˆ¼ U[0,1]. | | | momentum â† wvj (t âˆ’ 1;m ) | | | i | | | particle â† c1Ï‰1[Îji(m
    )âˆ’ ğœƒji(tâˆ’ 1;m)] | | | swarm â† c Ï‰ [Î¦j (t âˆ’ 1)âˆ’ ğœƒj(tâˆ’ 1;m )] | | | 2 2 i i | |
    | vji(t;m ) â† momentum + particle+ swarm | | | j j j | | | ğœƒi(t;m ) â† ğœƒi(tâˆ’ 1;m
    )+ vi(t;m ) | | end | | | | for k = 1,...,K do | | | Run the quantum circuit Nruns
    times with configuration | | | ğœƒ(t;m ) on sample X to estimate the probability
    â„™ of | | | k k | | reading out "1" on the target qubit. | | end | | | | Evaluate
    the cost function L(ğœƒ(t;m )) given | | | | â„™ := (â„™1,...,â„™K ). | | if L(ğœƒ(t;m))
    < L(Î (m )) then | | | Î(m ) â† ğœƒ(t;m ) | | | | end | | end | Order solutions from
    best (minimum value of the cost function) | | to worst (maximum value of the cost
    function). | | Î¦ (t) â† con figuration corresponding to the minimum of the cost
    | function. | | if L(ğœƒâˆ—) < L (Î¦(t)) then | ğœƒ âˆ— â† Î¦(t) | | end end ---------------------------------------------------------------------
    ](img/file793.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- è¿­ä»£ï¼šå¯¹äº
    t = 1,...,Niter åš | | å¯¹ m = 1,...,M åš | | å¯¹ i = 1,...,n, j = 1,...,l åš | | | ç”Ÿæˆç‹¬ç«‹çš„éšæœºæ•°
    Ï‰ âˆ¼ U [0,1 ] å’Œ | | | 1 | | | Ï‰2 âˆ¼ U[0,1]ã€‚ | | | åŠ¨é‡ â† wvj (t âˆ’ 1;m ) | | | i |
    | | ç²’å­ â† c1Ï‰1[Îji(m )âˆ’ ğœƒji(tâˆ’ 1;m)] | | | ç¾¤ä½“ â† c Ï‰ [Î¦j (t âˆ’ 1)âˆ’ ğœƒj(tâˆ’ 1;m )] |
    | | 2 2 i i | | | vji(t;m ) â† åŠ¨é‡ + ç²’å­ + ç¾¤ä½“ | | | j j j | | | ğœƒi(t;m ) â† ğœƒi(tâˆ’
    1;m ) + vi(t;m ) | | ç»“æŸ | | | | å¯¹ k = 1,...,K åš | | ä½¿ç”¨é…ç½® | | | ğœƒ(t;m ) åœ¨æ ·æœ¬ X ä¸Šè¿è¡Œé‡å­ç”µè·¯
    Nruns æ¬¡ï¼Œä»¥ä¼°ç®— | | | k k | | è¯»å–ç›®æ ‡é‡å­æ¯”ç‰¹ä¸º "1" çš„æ¦‚ç‡ â„™ã€‚ | | ç»“æŸ | | | | è¯„ä¼°ç»™å®šæ¦‚ç‡çš„ä»£ä»·å‡½æ•° L(ğœƒ(t;m
    )) | | | | â„™ := (â„™1,...,â„™K )ã€‚ | | å¦‚æœ L(ğœƒ(t;m)) < L(Î (m ))ï¼Œåˆ™ | | | Î(m ) â† ğœƒ(t;m
    ) | | | | ç»“æŸ | | ç»“æŸ | æŒ‰ä»£ä»·å‡½æ•°å€¼ä»æœ€ä¼˜ï¼ˆä»£ä»·å‡½æ•°æœ€å°å€¼ï¼‰åˆ°æœ€å·®ï¼ˆä»£ä»·å‡½æ•°æœ€å¤§å€¼ï¼‰æ’åºè§£ã€‚ | | Î¦ (t) â† å¯¹åº”ä»£ä»·å‡½æ•°æœ€å°å€¼çš„é…ç½®ã€‚
    | | å¦‚æœ L(ğœƒâˆ—) < L (Î¦(t))ï¼Œåˆ™ | ğœƒ âˆ— â† Î¦(t) | | ç»“æŸ ç»“æŸ ---------------------------------------------------------------------
    ](img/file793.jpg)'
- en: The non-differentiable learning based on the evolutionary search heuristic works
    well for irregular, non-convex objective functions with many local minima.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè¿›åŒ–æœç´¢å¯å‘å¼çš„ä¸å¯å¾®å­¦ä¹ åœ¨å¤„ç†å…·æœ‰å¤šä¸ªå±€éƒ¨æå°å€¼çš„ä¸è§„åˆ™ã€éå‡¸ç›®æ ‡å‡½æ•°æ—¶æ•ˆæœè‰¯å¥½ã€‚
- en: 8.4 QNN Embedding on NISQ QPU
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4 QNN åœ¨ NISQ QPU ä¸Šçš„åµŒå…¥
- en: Ideally, parameterised quantum circuits should be constructed in a hardware-agnostic
    way, only driven by the characteristics of the problem being solved. This, however,
    would require the existence of large and exceptionally well-connected quantum
    computing systems with very high qubit fidelity and coherence time. In other words,
    we would need QPUs with capabilities that significantly exceed those of existing
    NISQ devices. The time for such powerful quantum computing systems may come sooner
    than one may expect but we still have to find a way of running PQCs efficiently
    on NISQ QPUs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ç†æƒ³æƒ…å†µä¸‹ï¼Œå‚æ•°åŒ–é‡å­ç”µè·¯åº”è¯¥ä»¥ä¸ç¡¬ä»¶æ— å…³çš„æ–¹å¼æ„å»ºï¼Œä»…ç”±å¾…è§£å†³é—®é¢˜çš„ç‰¹å¾é©±åŠ¨ã€‚ç„¶è€Œï¼Œè¿™å°†éœ€è¦å­˜åœ¨å¤§å‹ä¸”è¿æ¥æä¸ºè‰¯å¥½çš„é‡å­è®¡ç®—ç³»ç»Ÿï¼Œå…·æœ‰éå¸¸é«˜çš„é‡å­æ¯”ç‰¹ä¿çœŸåº¦å’Œç›¸å¹²æ—¶é—´ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬éœ€è¦çš„
    QPU åŠŸèƒ½è¿œè¿œè¶…è¿‡ç°æœ‰ NISQ è®¾å¤‡çš„èƒ½åŠ›ã€‚è¿™æ ·çš„å¼ºå¤§é‡å­è®¡ç®—ç³»ç»Ÿçš„åˆ°æ¥å¯èƒ½æ¯”é¢„æœŸçš„æ›´æ—©ï¼Œä½†æˆ‘ä»¬ä»ç„¶éœ€è¦æ‰¾åˆ°ä¸€ç§åœ¨ NISQ QPU ä¸Šé«˜æ•ˆè¿è¡Œ PQC
    çš„æ–¹æ³•ã€‚
- en: 8.4.1 NISQ QPU connectivity
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.1 NISQ QPU è¿æ¥æ€§
- en: 'A typical approach to designing a PQC executable on the NISQ QPU would start
    with observing two main characteristics of the quantum computing systems: the
    graph (qubit connectivity) and the set of native gates. We can illustrate these
    points by looking at Rigettiâ€™s Aspen systemÂ Â [[72](Biblography.xhtml#XCoyle2020)]
    in FigureÂ [8.3](#8.3).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾è®¡å¯åœ¨ NISQ QPU ä¸Šæ‰§è¡Œçš„ PQC çš„å…¸å‹æ–¹æ³•æ˜¯ï¼Œä»è§‚å¯Ÿé‡å­è®¡ç®—ç³»ç»Ÿçš„ä¸¤ä¸ªä¸»è¦ç‰¹å¾å¼€å§‹ï¼šå›¾ï¼ˆé‡å­æ¯”ç‰¹è¿æ¥æ€§ï¼‰å’ŒåŸç”Ÿé—¨é›†ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹å›¾ 8.3
    ä¸­ Rigetti çš„ Aspen ç³»ç»Ÿ [[72](Biblography.xhtml#XCoyle2020)] æ¥è¯´æ˜è¿™äº›è¦ç‚¹ã€‚
- en: '![FigureÂ 8.3: Rigettiâ€™s Aspen system. ](img/file794.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 8.3: Rigetti çš„ Aspen ç³»ç»Ÿã€‚](img/file794.jpg)'
- en: 'FigureÂ 8.3: Rigettiâ€™s Aspen system.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 8.3: Rigetti çš„ Aspen ç³»ç»Ÿã€‚'
- en: As we can see, most qubits are only connected to their nearest neighbours on
    the linear grid, with only four qubits having three connections. These extra connections
    form a bridge between two 8-qubit islands that, otherwise, would be completely
    independent.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œå¤§å¤šæ•°é‡å­æ¯”ç‰¹ä»…ä¸çº¿æ€§ç½‘æ ¼ä¸Šçš„æœ€è¿‘é‚»é‡å­æ¯”ç‰¹è¿æ¥ï¼Œåªæœ‰å››ä¸ªé‡å­æ¯”ç‰¹æœ‰ä¸‰ä¸ªè¿æ¥ã€‚è¿™äº›é¢å¤–çš„è¿æ¥å½¢æˆäº†ä¸¤ä¸ª 8 é‡å­æ¯”ç‰¹å²›ä¹‹é—´çš„æ¡¥æ¢ï¼Œå¦åˆ™å®ƒä»¬å°†æ˜¯å®Œå…¨ç‹¬ç«‹çš„ã€‚
- en: 8.4.2 QNN embedding scheme
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.2 QNN åµŒå…¥æ–¹æ¡ˆ
- en: The shaded qubits in FigureÂ [8.3](#8.3) can be used to construct the 8-qubit
    tree network capable of processing a dataset with up toÂ 16 continuous features
    (two features per quantum register) as shown in FigureÂ [8.4](#8.4). The thick
    lines in FigureÂ [8.3](#8.3) represent qubits connectivity used in constructing
    the QNN. The thin lines represent all other available qubit connections that have
    not been utilised in the QNN ansatz.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.3 ä¸­çš„é˜´å½±é‡å­æ¯”ç‰¹å¯ä»¥ç”¨äºæ„å»º 8 é‡å­æ¯”ç‰¹æ ‘ç½‘ç»œï¼Œèƒ½å¤Ÿå¤„ç†æœ€å¤šå…·æœ‰ 16 ä¸ªè¿ç»­ç‰¹å¾ï¼ˆæ¯ä¸ªé‡å­å¯„å­˜å™¨ä¸¤ä¸ªç‰¹å¾ï¼‰çš„æ•°æ®é›†ï¼Œå¦‚å›¾ 8.4 æ‰€ç¤ºã€‚å›¾
    8.3 ä¸­çš„ç²—çº¿è¡¨ç¤ºç”¨äºæ„å»º QNN çš„é‡å­æ¯”ç‰¹è¿æ¥ã€‚ç»†çº¿è¡¨ç¤ºæ‰€æœ‰å…¶ä»–æœªåœ¨ QNN å‡è®¾ä¸­ä½¿ç”¨çš„é‡å­æ¯”ç‰¹è¿æ¥ã€‚
- en: '![FigureÂ 8.4: QNN for the Aspen system; the gateÂ G is any of the {X,Y,Z} gates.
    ](img/file795.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 8.4: Aspen ç³»ç»Ÿçš„ QNNï¼›é—¨ G æ˜¯ {X,Y,Z} ä»»æ„é—¨ã€‚](img/file795.jpg)'
- en: 'FigureÂ 8.4: QNN for the Aspen system; the gateÂ G is any of the {X,Y,Z} gates.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 8.4: Aspen ç³»ç»Ÿçš„ QNNï¼›é—¨ G æ˜¯ {X,Y,Z} ä»»æ„é—¨ã€‚'
- en: With the limited connectivity of existing QPUs, we need to fully utilise the
    graph structure of the quantum chips to implement the most efficient QNN embedding
    and extract the best possible performance.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºç°æœ‰é‡å­å¤„ç†å•å…ƒï¼ˆQPUï¼‰çš„è¿æ¥æ€§æœ‰é™ï¼Œæˆ‘ä»¬éœ€è¦å……åˆ†åˆ©ç”¨é‡å­èŠ¯ç‰‡çš„å›¾ç»“æ„æ¥å®ç°æœ€æœ‰æ•ˆçš„ QNN åµŒå…¥ï¼Œå¹¶æå–æœ€ä½³æ€§èƒ½ã€‚
- en: 8.5 QNN Trained as a Classifier
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5 QNN ä½œä¸ºåˆ†ç±»å™¨çš„è®­ç»ƒ
- en: We now demonstrate how a binary QNN classifier can be trained on a classical
    credit approval dataset using the non-differentiable learning approach.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨éå¯å¾®åˆ†å­¦ä¹ æ–¹æ³•ï¼Œåœ¨ç»å…¸ä¿¡ç”¨å®¡æ‰¹æ•°æ®é›†ä¸Šè®­ç»ƒäºŒå…ƒ QNN åˆ†ç±»å™¨ã€‚
- en: 8.5.1 The ACA dataset and QNN ansatz
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.5.1 ACA æ•°æ®é›†ä¸ QNN å‡è®¾
- en: One of the most fundamental use cases for a binary classifier in finance is
    credit approval. The UCI Machine Learning Database Â [[241](Biblography.xhtml#XUCI_ACA),Â [242](Biblography.xhtml#XQuinlan1987)]
    holds the Australian Credit Approval (ACA) dataset consisting of 690 samples.
    There are 14 features (binary, integer, continuous) representing various attributes
    of potential borrowers and a binary class label (accept/reject credit application).
    The dataset is reasonably hard for classical classifiers due to the limited predictive
    power of the features and its relatively small size. This makes it ideal for testing
    and benchmarking the QNN performance.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: äºŒåˆ†ç±»å™¨åœ¨é‡‘èé¢†åŸŸæœ€åŸºæœ¬çš„åº”ç”¨ä¹‹ä¸€æ˜¯ä¿¡ç”¨æ‰¹å‡†ã€‚UCIæœºå™¨å­¦ä¹ æ•°æ®åº“[[241](Biblography.xhtml#XUCI_ACA), [242](Biblography.xhtml#XQuinlan1987)]åŒ…å«æ¾³å¤§åˆ©äºšä¿¡ç”¨æ‰¹å‡†ï¼ˆACAï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±690ä¸ªæ ·æœ¬ç»„æˆã€‚æ•°æ®é›†åŒ…å«14ä¸ªç‰¹å¾ï¼ˆåŒ…æ‹¬äºŒè¿›åˆ¶ã€æ•´æ•°ã€è¿ç»­ç‰¹å¾ï¼‰ï¼Œä»£è¡¨æ½œåœ¨å€Ÿæ¬¾äººçš„å„ç§å±æ€§ï¼Œä»¥åŠä¸€ä¸ªäºŒè¿›åˆ¶ç±»åˆ«æ ‡ç­¾ï¼ˆæ¥å—/æ‹’ç»ä¿¡ç”¨ç”³è¯·ï¼‰ã€‚ç”±äºç‰¹å¾çš„é¢„æµ‹èƒ½åŠ›æœ‰é™ä»¥åŠæ•°æ®é›†ç›¸å¯¹è¾ƒå°ï¼Œç»å…¸åˆ†ç±»å™¨å¤„ç†èµ·æ¥ç›¸å½“å›°éš¾ã€‚è¿™ä½¿å¾—å®ƒæˆä¸ºæµ‹è¯•å’ŒåŸºå‡†åŒ–QNNæ€§èƒ½çš„ç†æƒ³é€‰æ‹©ã€‚
- en: We start with the simplest tree network that can be mapped onto Rigettiâ€™s Aspen
    system graph described in the previous section. FigureÂ [8.5](#8.5) shows the full
    quantum circuit consisting of sample encoding and sample processing modulesÂ Â [[171](Biblography.xhtml#XKondratyev2021)].
    The proposed scheme allows us to encode up to two continuous features per quantum
    register with the help of rotations around theÂ *x*- and theÂ *y*-axes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»æœ€ç®€å•çš„æ ‘å½¢ç½‘ç»œå¼€å§‹ï¼Œè¯¥ç½‘ç»œå¯ä»¥æ˜ å°„åˆ°å‰ä¸€èŠ‚ä¸­æè¿°çš„Rigetti Aspenç³»ç»Ÿå›¾ã€‚å›¾[8.5](#8.5)æ˜¾ç¤ºäº†å®Œæ•´çš„é‡å­ç”µè·¯ï¼ŒåŒ…æ‹¬æ ·æœ¬ç¼–ç å’Œæ ·æœ¬å¤„ç†æ¨¡å—[[171](Biblography.xhtml#XKondratyev2021)]ã€‚è¯¥æ–¹æ¡ˆä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨ç»•*x*è½´å’Œ*y*è½´çš„æ—‹è½¬ï¼Œåœ¨æ¯ä¸ªé‡å­å¯„å­˜å™¨ä¸­æœ€å¤šç¼–ç ä¸¤ä¸ªè¿ç»­ç‰¹å¾ã€‚
- en: '![FigureÂ 8.5: PQC for the credit approvals classifier. ](img/file796.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾8.5ï¼šç”¨äºä¿¡ç”¨æ‰¹å‡†åˆ†ç±»å™¨çš„PQCã€‚](img/file796.jpg)'
- en: 'FigureÂ 8.5: PQC for the credit approvals classifier.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.5ï¼šç”¨äºä¿¡ç”¨æ‰¹å‡†åˆ†ç±»å™¨çš„PQCã€‚
- en: The features are encoded as rotation angles *Ï•* âˆˆ [0*,Ï€*] according to the encoding
    scheme described in SectionÂ [7.2](Chapter_7.xhtml#x1-1520002). With all qubits
    initialised as ![|0âŸ©](img/file797.jpg) in the computational basis, this ensures
    the uniqueness of the encoded samples. The sample processing module consists of
    layers of adjustable one-qubit gates (rotations around theÂ *x*- and theÂ *y*-axes)
    and fixed two-qubit gates (CZ). We split the ACA dataset 50:50 into a training
    and a testing dataset using the train_test_split() function provided by the `sklearn.model_selection`
    module. Our objective is to train the QNN and various classical classifiers (classical
    benchmarks) on the training dataset and compare their out-of-sample performance
    on the testing dataset. The classical classifiers have a number of hyperparameters
    that can be fine-tuned to optimise the classifier performance on the given dataset.
    In contrast, the QNN architecture (location and types of one-qubit and two-qubit
    gates) is fixed.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾ä½œä¸ºæ—‹è½¬è§’åº¦*Ï•* âˆˆ [0*,Ï€*]è¿›è¡Œç¼–ç ï¼ŒæŒ‰ç…§ç¬¬[7.2](Chapter_7.xhtml#x1-1520002)èŠ‚ä¸­æè¿°çš„ç¼–ç æ–¹æ¡ˆè¿›è¡Œã€‚æ‰€æœ‰é‡å­æ¯”ç‰¹åˆå§‹åŒ–ä¸ºè®¡ç®—åŸºæ€çš„![|0âŸ©](img/file797.jpg)ï¼Œè¿™ç¡®ä¿äº†ç¼–ç æ ·æœ¬çš„å”¯ä¸€æ€§ã€‚æ ·æœ¬å¤„ç†æ¨¡å—ç”±å¯è°ƒçš„ä¸€é‡å­æ¯”ç‰¹é—¨ï¼ˆç»•*x*è½´å’Œ*y*è½´çš„æ—‹è½¬ï¼‰å’Œå›ºå®šçš„äºŒé‡å­æ¯”ç‰¹é—¨ï¼ˆCZï¼‰ç»„æˆã€‚æˆ‘ä»¬ä½¿ç”¨`sklearn.model_selection`æ¨¡å—æä¾›çš„train_test_split()å‡½æ•°ï¼Œå°†ACAæ•°æ®é›†æŒ‰50:50åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®­ç»ƒQNNå’Œå„ç§ç»å…¸åˆ†ç±»å™¨ï¼ˆç»å…¸åŸºå‡†ï¼‰åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°ï¼Œå¹¶æ¯”è¾ƒå®ƒä»¬åœ¨æµ‹è¯•é›†ä¸Šçš„å¤–æ¨æ€§èƒ½ã€‚ç»å…¸åˆ†ç±»å™¨å…·æœ‰ä¸€äº›è¶…å‚æ•°ï¼Œå¯ä»¥è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¼˜åŒ–åˆ†ç±»å™¨åœ¨ç»™å®šæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒQNNæ¶æ„ï¼ˆå•é‡å­æ¯”ç‰¹å’ŒåŒé‡å­æ¯”ç‰¹é—¨çš„ä½ç½®å’Œç±»å‹ï¼‰æ˜¯å›ºå®šçš„ã€‚
- en: 8.5.2 Training an ACA classifier with the PSO algorithm
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.5.2 ä½¿ç”¨PSOç®—æ³•è®­ç»ƒACAåˆ†ç±»å™¨
- en: We first verify that the QNN can be efficiently trained with the Particle Swarm
    Optimisation algorithmÂ â€“ a non-differentiable learning approach. FigureÂ [8.6](#8.6)
    illustrates PSO convergence for the set of PSO parameters given in TableÂ [8.3](#x1-176001r3).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆéªŒè¯QNNæ˜¯å¦èƒ½å¤Ÿé€šè¿‡ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•ï¼ˆPSOï¼‰é«˜æ•ˆè®­ç»ƒâ€”â€”ä¸€ç§ä¸å¯å¾®åˆ†çš„å­¦ä¹ æ–¹æ³•ã€‚å›¾[8.6](#8.6)å±•ç¤ºäº†PSOç®—æ³•åœ¨è¡¨[8.3](#x1-176001r3)ä¸­ç»™å®šPSOå‚æ•°é›†ä¸‹çš„æ”¶æ•›æƒ…å†µã€‚
- en: '| Parameter | Notation | Value |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| å‚æ•° | ç¬¦å· | å€¼ |'
- en: '| Inertia coefficient | *w* | 0.25 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| æƒ¯æ€§ç³»æ•° | *w* | 0.25 |'
- en: '| Self-confidence coefficient | *c* [1] | 0.25 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| è‡ªä¿¡ç³»æ•° | *c* [1] | 0.25 |'
- en: '| Swarm confidence coefficient | *c* [2] | 0.25 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| ç¾¤ä½“è‡ªä¿¡ç³»æ•° | *c* [2] | 0.25 |'
- en: '| Number of particles | *M* | 10 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| ç²’å­æ•°é‡ | *M* | 10 |'
- en: '| Number of iterations | *N*[iter] | 20 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| è¿­ä»£æ¬¡æ•° | *N*[iter] | 20 |'
- en: '| Number of quantum circuit runs | *N*[runs] | 1000 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| é‡å­ç”µè·¯è¿è¡Œæ¬¡æ•° | *N*[runs] | 1000 |'
- en: 'TableÂ 8.3: PSO parameters.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨8.3ï¼šPSOå‚æ•°ã€‚
- en: The sample algorithm run has reached the minimum of the objective function in
    just four iterations with only ten particles, exploring the search space using
    the `Qiskit` quantum simulator.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: æ ·æœ¬ç®—æ³•è¿è¡Œåœ¨ä»…ç”¨åä¸ªç²’å­çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡`Qiskit`é‡å­æ¨¡æ‹Ÿå™¨æ¢ç´¢æœç´¢ç©ºé—´ï¼Œä»…åœ¨å››æ¬¡è¿­ä»£ä¸­å°±è¾¾åˆ°äº†ç›®æ ‡å‡½æ•°çš„æœ€å°å€¼ã€‚
- en: '![FigureÂ 8.6: Minimum objective function values found by individual particles.
    ](img/file798.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾8.6ï¼šå„ä¸ªç²’å­æ‰¾åˆ°çš„ç›®æ ‡å‡½æ•°æœ€å°å€¼ã€‚](img/file798.jpg)'
- en: 'FigureÂ 8.6: Minimum objective function values found by individual particles.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.6ï¼šå„ä¸ªç²’å­æ‰¾åˆ°çš„ç›®æ ‡å‡½æ•°æœ€å°å€¼ã€‚
- en: The configuration of adjustable parameters (rotations) that corresponds to the
    minimum of the objective function found by the PSO algorithm is given byÂ ([8.5.2](#x1-176003r2)).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: PSOç®—æ³•æ‰¾åˆ°çš„å¯¹åº”äºç›®æ ‡å‡½æ•°æœ€å°å€¼çš„å¯è°ƒå‚æ•°ï¼ˆæ—‹è½¬ï¼‰çš„é…ç½®ç”±([8.5.2](#x1-176003r2))ç»™å‡ºã€‚
- en: '| ![ âŒŠ âŒ‹ 0.16Ï€ &#124; &#124; &#124;&#124;âˆ’ 0.55Ï€ 0.66Ï€ &#124;&#124; &#124;&#124;âˆ’
    0.13Ï€ &#124;&#124; &#124;&#124; &#124;&#124; &#124;&#124; 0.08Ï€ 0.72Ï€ 0.02Ï€ &#124;&#124;
    ğœƒ = &#124;&#124; 0.33Ï€ &#124;&#124; . &#124; &#124; &#124;&#124; 0.06Ï€ 0.95Ï€ &#124;&#124;
    &#124;&#124; 0.48Ï€ &#124;&#124; âŒˆ âŒ‰ 0.19Ï€ âˆ’ 0.91Ï€ âˆ’ 0.83Ï€ 0.59Ï€ ](img/file799.jpg)
    |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| ![ âŒŠ âŒ‹ 0.16Ï€ &#124; &#124; &#124;&#124;âˆ’ 0.55Ï€ 0.66Ï€ &#124;&#124; &#124;&#124;âˆ’
    0.13Ï€ &#124;&#124; &#124;&#124; &#124;&#124; &#124;&#124; 0.08Ï€ 0.72Ï€ 0.02Ï€ &#124;&#124;
    ğœƒ = &#124;&#124; 0.33Ï€ &#124;&#124; . &#124; &#124; &#124;&#124; 0.06Ï€ 0.95Ï€ &#124;&#124;
    &#124;&#124; 0.48Ï€ &#124;&#124; âŒˆ âŒ‰ 0.19Ï€ âˆ’ 0.91Ï€ âˆ’ 0.83Ï€ 0.59Ï€ ](img/file799.jpg)
    |  |'
- en: FigureÂ [8.7](#8.7) displays the in- and out-of-sample confusion matrices for
    the QNN classifier obtained with the Qiskit quantum simulator assuming that ClassÂ 0
    is the positive class.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[8.7](#8.7)æ˜¾ç¤ºäº†é€šè¿‡Qiskité‡å­æ¨¡æ‹Ÿå™¨è·å¾—çš„QNNåˆ†ç±»å™¨çš„å†…å¤–æ ·æœ¬æ··æ·†çŸ©é˜µï¼Œå‡è®¾ç±»0ä¸ºæ­£ç±»ã€‚
- en: '![FigureÂ 8.7: Confusion matrix for the QNN classifier (ACA dataset). ](img/file800.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾8.7ï¼šQNNåˆ†ç±»å™¨çš„æ··æ·†çŸ©é˜µï¼ˆACAæ•°æ®é›†ï¼‰ã€‚](img/file800.jpg)'
- en: 'FigureÂ 8.7: Confusion matrix for the QNN classifier (ACA dataset).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.7ï¼šQNNåˆ†ç±»å™¨çš„æ··æ·†çŸ©é˜µï¼ˆACAæ•°æ®é›†ï¼‰ã€‚
- en: The results are robust with an in-sample accuracy of 0.86 and an out-of-sample
    accuracy of 0.85\. Interestingly, the in-sample and out-of-sample results are
    very close, indicating that the QNN provides strong regularisation. The question
    of quantum and classical neural networks regularisation will be tackled in ChapterÂ [12](Chapter_12.xhtml#x1-22500012).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœå…·æœ‰é²æ£’æ€§ï¼Œå†…æ ·æœ¬å‡†ç¡®ç‡ä¸º0.86ï¼Œå¤–æ ·æœ¬å‡†ç¡®ç‡ä¸º0.85ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå†…æ ·æœ¬å’Œå¤–æ ·æœ¬ç»“æœéå¸¸æ¥è¿‘ï¼Œè¡¨æ˜QNNæä¾›äº†å¼ºæœ‰åŠ›çš„æ­£åˆ™åŒ–ã€‚é‡å­ä¸ç»å…¸ç¥ç»ç½‘ç»œæ­£åˆ™åŒ–çš„é—®é¢˜å°†åœ¨ç¬¬[12](Chapter_12.xhtml#x1-22500012)ç« ä¸­è®¨è®ºã€‚
- en: 8.6 Classical Benchmarks
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6 ç»å…¸åŸºå‡†
- en: 'In ChapterÂ [4](Chapter_4.xhtml#x1-820004), we introduced two classical classifiers:
    a feedforward artificial neural network (Multi-Layer Perceptron) and a decision
    tree algorithm. We now expand the range of classical benchmark classifiers by
    adding Support Vector Machine (SVM)Â Â [[70](Biblography.xhtml#XCortes1995)], Logistic
    RegressionÂ Â [[31](Biblography.xhtml#XBerkson1944)], and Random ForestÂ Â [[136](Biblography.xhtml#XHo1995)].
    The SVM approach based on the *kernel method* is covered in ChapterÂ [13](Chapter_13.xhtml#x1-23600013).
    Here, we briefly explain the main principles of logistic regression and random
    forest classifiers.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬[4](Chapter_4.xhtml#x1-820004)ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§ç»å…¸åˆ†ç±»å™¨ï¼šå‰é¦ˆäººå·¥ç¥ç»ç½‘ç»œï¼ˆå¤šå±‚æ„ŸçŸ¥å™¨ï¼‰å’Œå†³ç­–æ ‘ç®—æ³•ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬é€šè¿‡æ·»åŠ æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰[[70](Biblography.xhtml#XCortes1995)]ã€é€»è¾‘å›å½’[[31](Biblography.xhtml#XBerkson1944)]å’Œéšæœºæ£®æ—[[136](Biblography.xhtml#XHo1995)]æ¥æ‰©å±•ç»å…¸åŸºå‡†åˆ†ç±»å™¨çš„èŒƒå›´ã€‚åŸºäº*æ ¸æ–¹æ³•*çš„SVMæ–¹æ³•å°†åœ¨ç¬¬[13](Chapter_13.xhtml#x1-23600013)ç« ä¸­è®¨è®ºã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ç®€è¦è§£é‡Šé€»è¾‘å›å½’å’Œéšæœºæ£®æ—åˆ†ç±»å™¨çš„ä¸»è¦åŸç†ã€‚
- en: 8.6.1 Logistic Regression and Random Forest
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.6.1 é€»è¾‘å›å½’ä¸éšæœºæ£®æ—
- en: Logistic regression can be seen as a special case of a feedforward neural network
    with a single hidden layer consisting of an activation unit with the logistic
    activation function. The model operates as shown in FigureÂ [4.3](Chapter_4.xhtml#4.3)
    with
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªå…·æœ‰å•ä¸ªéšè—å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œçš„ç‰¹ä¾‹ï¼Œéšè—å±‚ç”±å…·æœ‰é€»è¾‘æ¿€æ´»å‡½æ•°çš„æ¿€æ´»å•å…ƒç»„æˆã€‚è¯¥æ¨¡å‹çš„è¿ä½œå¦‚å›¾[4.3](Chapter_4.xhtml#4.3)æ‰€ç¤ºã€‚
- en: '![ ( ) y(s) = 1 + eâˆ’s âˆ’1 . ](img/file801.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![ ( ) y(s) = 1 + eâˆ’s âˆ’1 . ](img/file801.jpg)'
- en: The standard logistic regression model is a *linear classifier* because the
    outcome always depends on the sum of the (weighted) inputs. Therefore, logistic
    regression performs well when working with a dataset where the classes are more
    or less linearly separable.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†çš„é€»è¾‘å›å½’æ¨¡å‹æ˜¯ä¸€ä¸ª*çº¿æ€§åˆ†ç±»å™¨*ï¼Œå› ä¸ºç»“æœå§‹ç»ˆä¾èµ–äºï¼ˆåŠ æƒï¼‰è¾“å…¥çš„æ€»å’Œã€‚å› æ­¤ï¼Œé€»è¾‘å›å½’åœ¨å¤„ç†ç±»ä¹‹é—´æˆ–å¤šæˆ–å°‘æ˜¯çº¿æ€§å¯åˆ†çš„æ•°æ®é›†æ—¶è¡¨ç°è‰¯å¥½ã€‚
- en: Random forest is an *ensemble learning* model and, as the name suggests, is
    based on combining the classification results of multiple decision trees. The
    ensemble technique used by random forest is known as *bootstrap aggregation*,
    or *bagging*, by choosing random subsets from the dataset. Hence, each decision
    tree is generated from samples drawn from the original dataset with replacement
    (row sampling). This step of row sampling with replacement is called the *bootstrap*.
    Each decision tree is trained independently. The final output for the given samples
    is based on *majority voting* after combining the results of all individual decision
    trees. This is the *aggregation* step.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºæ£®æ—æ˜¯ä¸€ç§*é›†æˆå­¦ä¹ *æ¨¡å‹ï¼Œé¡¾åæ€ä¹‰ï¼Œå®ƒåŸºäºç»“åˆå¤šä¸ªå†³ç­–æ ‘çš„åˆ†ç±»ç»“æœã€‚éšæœºæ£®æ—ä½¿ç”¨çš„é›†æˆæŠ€æœ¯ç§°ä¸º*è‡ªåŠ©èšåˆ*ï¼ˆ*bootstrap aggregation*ï¼‰ï¼Œæˆ–*bagging*ï¼Œé€šè¿‡ä»æ•°æ®é›†ä¸­é€‰æ‹©éšæœºå­é›†ã€‚å› æ­¤ï¼Œæ¯æ£µå†³ç­–æ ‘éƒ½æ˜¯ä»åŸå§‹æ•°æ®é›†çš„æ ·æœ¬ä¸­é€šè¿‡æœ‰æ”¾å›æŠ½æ ·ç”Ÿæˆçš„ï¼ˆè¡ŒæŠ½æ ·ï¼‰ã€‚è¿™ç§æœ‰æ”¾å›çš„è¡ŒæŠ½æ ·æ­¥éª¤è¢«ç§°ä¸º*è‡ªåŠ©æ³•*ï¼ˆ*bootstrap*ï¼‰ã€‚æ¯æ£µå†³ç­–æ ‘éƒ½æ˜¯ç‹¬ç«‹è®­ç»ƒçš„ã€‚ç»™å®šæ ·æœ¬çš„æœ€ç»ˆè¾“å‡ºæ˜¯åŸºäºæ‰€æœ‰å•ç‹¬å†³ç­–æ ‘ç»“æœçš„*å¤šæ•°æŠ•ç¥¨*ï¼Œè¿™æ˜¯*èšåˆ*æ­¥éª¤ã€‚
- en: 8.6.2 Benchmarking against standard classical classifiers
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.6.2 ä¸æ ‡å‡†ç»å…¸åˆ†ç±»å™¨çš„åŸºå‡†æµ‹è¯•
- en: 'The classical benchmarking can be done by training several popular `scikit-learn`
    models. TableÂ [8.4](#x1-179001r4) provides classical benchmarking results in terms
    of out-of-sample *F*[1] scores for the following (weakly) optimised scikit-learn
    classifiers:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ç»å…¸åŸºå‡†æµ‹è¯•å¯ä»¥é€šè¿‡è®­ç»ƒå‡ ä¸ªæµè¡Œçš„`scikit-learn`æ¨¡å‹æ¥å®Œæˆã€‚è¡¨[8.4](#x1-179001r4)æä¾›äº†å‡ ä¸ªï¼ˆå¼±ä¼˜åŒ–çš„ï¼‰`scikit-learn`åˆ†ç±»å™¨çš„æ ·æœ¬å¤–*F*[1]åˆ†æ•°çš„ç»å…¸åŸºå‡†æµ‹è¯•ç»“æœï¼š
- en: 'a feedforward neural network (MLP) classifier: `neural_network.MLPClassifier`'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆMLPï¼‰åˆ†ç±»å™¨ï¼š`neural_network.MLPClassifier`
- en: 'a support vector machine classifier: `svm.SVC`'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ”¯æŒå‘é‡æœºåˆ†ç±»å™¨ï¼š`svm.SVC`
- en: 'an ensemble learning model: `ensemble.RandomForestClassifier`'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç§é›†æˆå­¦ä¹ æ¨¡å‹ï¼š`ensemble.RandomForestClassifier`
- en: 'a logistic regression classifier: `linear_model.LogisticRegression`'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’åˆ†ç±»å™¨ï¼š`linear_model.LogisticRegression`
- en: 'The *F*[1] score is a harmonic average of two performance metrics, precision
    and recall:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*F*[1] åˆ†æ•°æ˜¯ä¸¤ä¸ªæ€§èƒ½æŒ‡æ ‡â€”â€”ç²¾åº¦å’Œå¬å›ç‡â€”â€”çš„è°ƒå’Œå¹³å‡å€¼ï¼š'
- en: '| ![ Precision Ã— Recall F1 := 2 -----------------, Precision + Recall ](img/file802.jpg)
    |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| ![ ç²¾åº¦ Ã— å¬å›ç‡ F1 := 2 -----------------, ç²¾åº¦ + å¬å›ç‡ ](img/file802.jpg) |  |'
- en: both introduced in ChapterÂ [4](Chapter_4.xhtml#x1-820004). In the context of
    credit approvals, optimising for recall helps with minimising the chance of approving
    a credit application that should be rejected. However, this comes at the cost
    of not approving credit applications for some high-quality borrowers. If we optimise
    for precision, then we improve the overall correctness of our decisions at the
    cost of approving some applicants with bad credits. The *F*[1] score is used to
    balance the positives and negatives in optimising precision and recall.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤è€…éƒ½åœ¨ç¬¬[4](Chapter_4.xhtml#x1-820004)ç« ä¸­ä»‹ç»ã€‚åœ¨ä¿¡ç”¨å®¡æ‰¹çš„èƒŒæ™¯ä¸‹ï¼Œä¼˜åŒ–å¬å›ç‡æœ‰åŠ©äºæœ€å°åŒ–é”™è¯¯æ‰¹å‡†åº”è¢«æ‹’ç»çš„ä¿¡ç”¨ç”³è¯·çš„æœºä¼šã€‚ç„¶è€Œï¼Œè¿™ä¼šå¯¼è‡´ä¸€äº›é«˜è´¨é‡å€Ÿæ¬¾äººçš„ä¿¡ç”¨ç”³è¯·æœªèƒ½è·å¾—æ‰¹å‡†ã€‚å¦‚æœæˆ‘ä»¬ä¼˜åŒ–ç²¾åº¦ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¼šæé«˜å†³ç­–çš„æ•´ä½“æ­£ç¡®æ€§ï¼Œä½†è¿™ä¹Ÿå¯èƒ½å¯¼è‡´ä¸€äº›ä¿¡ç”¨ä¸è‰¯çš„ç”³è¯·è€…è¢«æ‰¹å‡†ã€‚*F*[1]
    åˆ†æ•°ç”¨äºå¹³è¡¡ç²¾åº¦å’Œå¬å›ç‡ä¼˜åŒ–ä¸­çš„æ­£è´Ÿæ•ˆæœã€‚
- en: '| Classifier | Average *F*[1] score |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| åˆ†ç±»å™¨ | å¹³å‡ *F*[1] åˆ†æ•° |'
- en: '| Logistic Regression Classifier | 0.88 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| é€»è¾‘å›å½’åˆ†ç±»å™¨ | 0.88 |'
- en: '| Random Forest Classifier | 0.87 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| éšæœºæ£®æ—åˆ†ç±»å™¨ | 0.87 |'
- en: '| MLP Classifier | 0.86 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| MLP åˆ†ç±»å™¨ | 0.86 |'
- en: '| QNN Classifier | 0.85 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| QNN åˆ†ç±»å™¨ | 0.85 |'
- en: '| Support Vector Classifier | 0.84 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| æ”¯æŒå‘é‡åˆ†ç±»å™¨ | 0.84 |'
- en: 'TableÂ 8.4: Out-of-sample F[1] scores for the classical and QNN classifiers
    trained on the ACA dataset.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨8.4ï¼šåœ¨ACAæ•°æ®é›†ä¸Šè®­ç»ƒçš„ç»å…¸å’ŒQNNåˆ†ç±»å™¨çš„æ ·æœ¬å¤–*F*[1]åˆ†æ•°ã€‚
- en: The QNN classifier performance, as measured by the average *F*[1] score for
    ClassÂ 0 and ClassÂ 1, falls somewhere in the middle of the range of out-of-sample
    *F*[1] scores for the chosen classical benchmarks. This is encouraging since the
    QNN ansatz was fixed and we did not optimise the QNN hyperparameters â€“ the placement
    and types of the two-qubit gates. The classifier performance can be further improved
    by deploying the standard ensemble learning techniques, as explained in the following
    section.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: QNNåˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œä¾æ®ç±»åˆ«0å’Œç±»åˆ«1çš„å¹³å‡*F*[1]åˆ†æ•°æ¥è¡¡é‡ï¼Œä½äºæ‰€é€‰ç»å…¸åŸºå‡†æµ‹è¯•æ ·æœ¬å¤–*F*[1]åˆ†æ•°çš„èŒƒå›´ä¸­é—´ã€‚è¿™æ˜¯ä»¤äººé¼“èˆçš„ï¼Œå› ä¸ºQNNçš„åˆå§‹è®¾ç½®æ˜¯å›ºå®šçš„ï¼Œæˆ‘ä»¬æ²¡æœ‰ä¼˜åŒ–QNNçš„è¶…å‚æ•°â€”â€”å³ä¸¤é‡å­æ¯”ç‰¹é—¨çš„æ”¾ç½®å’Œç±»å‹ã€‚é€šè¿‡éƒ¨ç½²æ ‡å‡†çš„é›†æˆå­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚ä¸‹ä¸€èŠ‚æ‰€è¿°ï¼‰ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚
- en: QNNs can be productively used for classification tasks on classical finance-related
    datasets.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: QNN å¯æœ‰æ•ˆç”¨äºç»å…¸é‡‘èç›¸å…³æ•°æ®é›†çš„åˆ†ç±»ä»»åŠ¡ã€‚
- en: 8.7 Improving Performance with Ensemble Learning
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7 ä½¿ç”¨é›†æˆå­¦ä¹ æå‡æ€§èƒ½
- en: The ensemble learning methods combine different weak classifiers into a strong
    classifier that has better generalisation capabilities than each individual standalone
    classifier. In ChapterÂ [4](Chapter_4.xhtml#x1-820004), we saw how the principles
    of ensemble learning can be used in combination with the methods of quantum annealing.
    Here, we look at them from the QNN perspective.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: é›†æˆå­¦ä¹ æ–¹æ³•å°†ä¸åŒçš„å¼±åˆ†ç±»å™¨ç»„åˆæˆä¸€ä¸ªå¼ºåˆ†ç±»å™¨ï¼Œå…¶æ³›åŒ–èƒ½åŠ›ä¼˜äºæ¯ä¸ªå•ç‹¬çš„ç‹¬ç«‹åˆ†ç±»å™¨ã€‚åœ¨ç¬¬[4ç« ](Chapter_4.xhtml#x1-820004)ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°é›†æˆå­¦ä¹ çš„åŸç†å¦‚ä½•ä¸é‡å­é€€ç«æ–¹æ³•ç»“åˆä½¿ç”¨ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä»QNNçš„è§’åº¦æ¥çœ‹å¾…è¿™äº›æ–¹æ³•ã€‚
- en: 8.7.1 Majority voting
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.7.1 å¤šæ•°æŠ•ç¥¨
- en: 'The popular ensemble learning methods are majority voting (binary classification)
    and plurality voting (multiclass classification). Majority voting means what it
    says: the class label for the given sample is the one that receives more than
    half of the individual votes. Plurality voting chooses the class that receives
    the largest number of votes (the mode).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¸è§çš„é›†æˆå­¦ä¹ æ–¹æ³•åŒ…æ‹¬å¤šæ•°æŠ•ç¥¨ï¼ˆç”¨äºäºŒåˆ†ç±»ï¼‰å’Œå¤šæ•°æŠ•ç¥¨ï¼ˆç”¨äºå¤šç±»åˆ†ç±»ï¼‰ã€‚å¤šæ•°æŠ•ç¥¨çš„æ„æ€å°±æ˜¯ï¼šç»™å®šæ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾æ˜¯è·å¾—è¶…è¿‡ä¸€åŠä¸ªä½“æŠ•ç¥¨çš„ç±»åˆ«ã€‚å¤šæ•°æŠ•ç¥¨é€‰æ‹©è·å¾—æœ€å¤šæŠ•ç¥¨çš„ç±»åˆ«ï¼ˆå³ä¼—æ•°ï¼‰ã€‚
- en: The ensemble of the individual classifiers can be built from different classification
    algorithms. For example, by combining neural network classifiers, support vector
    machines, decision trees, etc. On the other hand, the same basic classification
    algorithm can be used to produce multiple classifiers by choosing different configurations
    of hyperparameters and different subsets of the training dataset. The random forest
    classifier, which combines different decision tree classifiers, illustrates the
    latter approach.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ªåˆ«åˆ†ç±»å™¨çš„é›†æˆå¯ä»¥é€šè¿‡ä¸åŒçš„åˆ†ç±»ç®—æ³•æ¥æ„å»ºã€‚ä¾‹å¦‚ï¼Œé€šè¿‡ç»„åˆç¥ç»ç½‘ç»œåˆ†ç±»å™¨ã€æ”¯æŒå‘é‡æœºã€å†³ç­–æ ‘ç­‰ã€‚å¦ä¸€æ–¹é¢ï¼Œå¯ä»¥é€šè¿‡é€‰æ‹©ä¸åŒçš„è¶…å‚æ•°é…ç½®å’Œä¸åŒçš„è®­ç»ƒæ•°æ®é›†å­é›†ï¼Œä½¿ç”¨ç›¸åŒçš„åŸºæœ¬åˆ†ç±»ç®—æ³•æ¥ç”Ÿæˆå¤šä¸ªåˆ†ç±»å™¨ã€‚éšæœºæ£®æ—åˆ†ç±»å™¨å°±æ˜¯é€šè¿‡ç»“åˆä¸åŒçš„å†³ç­–æ ‘åˆ†ç±»å™¨æ¥è¯´æ˜åä¸€ç§æ–¹æ³•ã€‚
- en: With these considerations in mind, we build a strong classifier from several
    individual QNN classifiers by changing the QNN ansatz within the restrictions
    imposed by the QPU qubit connectivity. In order to test the majority voting approach,
    we build two new QNN classifiers by adding a few more two-qubit CZ gates to the
    baseline parameterised quantum circuit, as shown in FiguresÂ [8.8](#8.8) andÂ [8.9](#8.9).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ°è¿™äº›å› ç´ ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨QPUé‡å­æ¯”ç‰¹è¿æ¥æ€§æ‰€é™åˆ¶çš„èŒƒå›´å†…æ”¹å˜QNN ansatzï¼Œæ„å»ºäº†ä¸€ä¸ªç”±å¤šä¸ªç‹¬ç«‹QNNåˆ†ç±»å™¨ç»„æˆçš„å¼ºåˆ†ç±»å™¨ã€‚ä¸ºäº†æµ‹è¯•å¤šæ•°æŠ•ç¥¨æ–¹æ³•ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨åŸºå‡†å‚æ•°åŒ–é‡å­ç”µè·¯ä¸­æ·»åŠ ä¸€äº›é¢å¤–çš„ä¸¤é‡å­æ¯”ç‰¹CZé—¨ï¼Œæ„å»ºäº†ä¸¤ä¸ªæ–°çš„QNNåˆ†ç±»å™¨ï¼Œå¦‚å›¾[8.8](#8.8)å’Œ[8.9](#8.9)æ‰€ç¤ºã€‚
- en: In the case of PQCÂ #2, we add two extraÂ CZ gates, exploiting the "bridge" structure
    of the Aspen system (FigureÂ [8.3](#8.3)). This improves the overall system entaglement
    and allows for a richer set of achievable quantum states. PQCÂ #3 has three extraÂ CZ
    gates in comparison with the baseline circuit. The new classifiers can be trained
    with the same algorithm (PSO) on the same training dataset but will have different
    optimal configurations of the adjustable parameters and will make slightly different
    classification decisions on the testing dataset.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨PQC #2çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¢åŠ äº†ä¸¤ä¸ªé¢å¤–çš„CZé—¨ï¼Œåˆ©ç”¨äº†Aspenç³»ç»Ÿçš„â€œæ¡¥æ¥â€ç»“æ„ï¼ˆå›¾[8.3](#8.3)ï¼‰ã€‚è¿™æ”¹å–„äº†æ•´ä¸ªç³»ç»Ÿçš„çº ç¼ ï¼Œå¹¶å…è®¸å®ç°æ›´ä¸°å¯Œçš„é‡å­æ€ã€‚ç›¸æ¯”åŸºå‡†ç”µè·¯ï¼ŒPQC
    #3å¢åŠ äº†ä¸‰ä¸ªé¢å¤–çš„CZé—¨ã€‚è¿™äº›æ–°çš„åˆ†ç±»å™¨å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ç®—æ³•ï¼ˆPSOï¼‰åœ¨ç›¸åŒçš„è®­ç»ƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†ä¼šæœ‰ä¸åŒçš„å¯è°ƒå‚æ•°çš„æœ€ä½³é…ç½®ï¼Œå¹¶åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šåšå‡ºç•¥æœ‰ä¸åŒçš„åˆ†ç±»å†³ç­–ã€‚'
- en: With three QNN classifiers, the majority voting leads to either a unanimous
    or a 2:1 decision. Performance on the ACA dataset improves marginally with all
    three classifiers generally in full agreement with each other. There are only
    a handful of instances where majority voting adds value, but this improves the
    average out-of-sample *F*[1] score from 0.85 to 0.87Â â€“ on par with the random
    forest classifier trained on the same dataset.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸‰ä¸ªQNNåˆ†ç±»å™¨æ—¶ï¼Œå¤šæ•°æŠ•ç¥¨ä¼šå¯¼è‡´ä¸€è‡´çš„æˆ–2:1çš„å†³ç­–ã€‚å½“ä¸‰ä¸ªåˆ†ç±»å™¨å®Œå…¨ä¸€è‡´æ—¶ï¼ŒACAæ•°æ®é›†çš„è¡¨ç°ç•¥æœ‰æå‡ã€‚å¤šæ•°æŠ•ç¥¨çš„å¢å€¼ä½œç”¨ä»…ä½“ç°åœ¨å°‘æ•°å‡ ä¸ªå®ä¾‹ä¸Šï¼Œä½†è¿™ä½¿å¾—å¹³å‡å¤–æ ·æœ¬*F*[1]å¾—åˆ†ä»0.85æé«˜åˆ°0.87â€”â€”ä¸åœ¨ç›¸åŒæ•°æ®é›†ä¸Šè®­ç»ƒçš„éšæœºæ£®æ—åˆ†ç±»å™¨æŒå¹³ã€‚
- en: '![FigureÂ 8.8: PQCÂ #2 for the credit approvals classifier. New fixed 2-qubit
    gates are shaded grey. ](img/file803.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾8.8ï¼šç”¨äºä¿¡ç”¨å®¡æ‰¹åˆ†ç±»å™¨çš„PQC #2ã€‚æ–°çš„å›ºå®š2é‡å­æ¯”ç‰¹é—¨ä»¥ç°è‰²é˜´å½±æ˜¾ç¤ºã€‚](img/file803.jpg)'
- en: 'FigureÂ 8.8: PQCÂ #2 for the credit approvals classifier. New fixed 2-qubit gates
    are shaded grey.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾8.8ï¼šç”¨äºä¿¡ç”¨å®¡æ‰¹åˆ†ç±»å™¨çš„PQC #2ã€‚æ–°çš„å›ºå®š2é‡å­æ¯”ç‰¹é—¨ä»¥ç°è‰²é˜´å½±æ˜¾ç¤ºã€‚'
- en: '![FigureÂ 8.9: PQCÂ #3 for the credit approvals classifier. New fixed 2-qubit
    gates are shaded grey. ](img/file804.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 8.9ï¼šç”¨äºä¿¡ç”¨å®¡æ‰¹åˆ†ç±»å™¨çš„ PQC #3ã€‚æ–°çš„å›ºå®š 2 é‡å­æ¯”ç‰¹é—¨ä»¥ç°è‰²é˜´å½±æ˜¾ç¤ºã€‚](img/file804.jpg)'
- en: 'FigureÂ 8.9: PQCÂ #3 for the credit approvals classifier. New fixed 2-qubit gates
    are shaded grey.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 8.9ï¼šç”¨äºä¿¡ç”¨å®¡æ‰¹åˆ†ç±»å™¨çš„ PQC #3ã€‚æ–°çš„å›ºå®š 2 é‡å­æ¯”ç‰¹é—¨ä»¥ç°è‰²é˜´å½±æ˜¾ç¤ºã€‚'
- en: Similar results can be achieved with the original QNN classifier trained on
    different subsets of the training dataset. These subsets are produced by drawing
    the bootstrap samples â€“ random samples with replacement â€“ from the original training
    dataset. The differently trained QNN classifiers can then be combined into a single
    strong classifier using the majority voting approach as described above.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å¯¹ä¸åŒå­é›†çš„è®­ç»ƒæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥å®ç°ç±»ä¼¼çš„ç»“æœã€‚è¿™äº›å­é›†æ˜¯é€šè¿‡ä»åŸå§‹è®­ç»ƒæ•°æ®é›†ä¸­æŠ½å–è‡ªåŠ©æ ·æœ¬ï¼ˆå¸¦æœ‰æ›¿æ¢çš„éšæœºæ ·æœ¬ï¼‰äº§ç”Ÿçš„ã€‚ç„¶åï¼Œå¯ä»¥å°†è¿™äº›ä¸åŒè®­ç»ƒçš„
    QNN åˆ†ç±»å™¨ç»“åˆæˆä¸€ä¸ªå¼ºå¤§çš„å•ä¸€åˆ†ç±»å™¨ï¼Œä½¿ç”¨ä¸Šè¿°çš„å¤šæ•°æŠ•ç¥¨æ–¹æ³•ã€‚
- en: 8.7.2 Quantum boosting
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.7.2 é‡å­æå‡
- en: 'We started by introducing the concept of ensemble learning where predictions
    produced by various QNNs are combined into a more robust unified prediction via
    a classical majority voting method. However, we can take a different approach
    to ensemble learning: predictions of various classical classifiers can be treated
    as an input into the QNN that performs their aggregation and comes up with a unified
    prediction. In other words, the QNN operates as a quantum booster similar to the
    QUBO-based QBoost model introduced in ChapterÂ [4](Chapter_4.xhtml#x1-820004).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆä»‹ç»äº†é›†æˆå­¦ä¹ çš„æ¦‚å¿µï¼Œå…¶ä¸­é€šè¿‡ç»å…¸çš„å¤šæ•°æŠ•ç¥¨æ–¹æ³•å°†ç”±å¤šä¸ª QNN äº§ç”Ÿçš„é¢„æµ‹ç»“æœåˆå¹¶æˆä¸€ä¸ªæ›´ä¸ºç¨³å¥çš„ç»Ÿä¸€é¢„æµ‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é‡‡å–å¦ä¸€ç§é›†æˆå­¦ä¹ çš„æ–¹æ³•ï¼šå°†å¤šä¸ªç»å…¸åˆ†ç±»å™¨çš„é¢„æµ‹ç»“æœä½œä¸ºè¾“å…¥ä¼ é€’ç»™
    QNNï¼ŒQNN ä¼šå¯¹è¿™äº›ç»“æœè¿›è¡Œèšåˆå¹¶ç»™å‡ºç»Ÿä¸€çš„é¢„æµ‹ã€‚æ¢å¥è¯è¯´ï¼ŒQNN ä½œä¸ºé‡å­å¢å¼ºå™¨ï¼Œç±»ä¼¼äºç¬¬ [4](Chapter_4.xhtml#x1-820004)
    ç« ä¸­ä»‹ç»çš„åŸºäº QUBO çš„ QBoost æ¨¡å‹ã€‚
- en: Let us come back to the classical benchmarks used in SectionÂ [8.5](#x1-1740005).
    There are four different machine learning models performing binary classifications.
    Their outputs ("0" for ClassÂ 0 and "1" for ClassÂ 1) are inputs into a 4-qubit
    QNN classifier. Since all quantum registers are initialised as ![|0âŸ©](img/file805.jpg),
    the outputs of individual classifiers can be encoded by either doing nothing for
    ClassÂ 0 output (which is equivalent to applying an identity operatorÂ I) or by
    applying a NOT gateÂ X for ClassÂ 1 output.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›åˆ°ç¬¬ [8.5](#x1-1740005) èŠ‚ä¸­ä½¿ç”¨çš„ç»å…¸åŸºå‡†ã€‚è¿™é‡Œæœ‰å››ç§ä¸åŒçš„æœºå™¨å­¦ä¹ æ¨¡å‹æ‰§è¡ŒäºŒå…ƒåˆ†ç±»ã€‚å®ƒä»¬çš„è¾“å‡ºï¼ˆâ€œ0â€è¡¨ç¤ºç±»åˆ« 0ï¼Œâ€œ1â€è¡¨ç¤ºç±»åˆ«
    1ï¼‰ä½œä¸ºè¾“å…¥ä¼ é€’ç»™ 4 é‡å­æ¯”ç‰¹ QNN åˆ†ç±»å™¨ã€‚ç”±äºæ‰€æœ‰é‡å­å¯„å­˜å™¨éƒ½åˆå§‹åŒ–ä¸º ![|0âŸ©](img/file805.jpg)ï¼Œæ‰€ä»¥å„ä¸ªåˆ†ç±»å™¨çš„è¾“å‡ºå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿›è¡Œç¼–ç ï¼šå¯¹äºç±»åˆ«
    0 è¾“å‡ºä¸åšä»»ä½•æ“ä½œï¼ˆç­‰åŒäºåº”ç”¨æ’ç­‰æ“ä½œ Iï¼‰ï¼Œæˆ–è€…å¯¹äºç±»åˆ« 1 è¾“å‡ºåº”ç”¨ NOT é—¨ Xã€‚
- en: '![FigureÂ 8.10: Embedding of a 4-qubit QNN onto the bridge section of Rigettiâ€™s
    Aspen system. ](img/file806.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 8.10ï¼šå°† 4 é‡å­æ¯”ç‰¹ QNN åµŒå…¥åˆ° Rigetti çš„ Aspen ç³»ç»Ÿçš„æ¡¥æ¥éƒ¨åˆ†ã€‚](img/file806.jpg)'
- en: 'FigureÂ 8.10: Embedding of a 4-qubit QNN onto the bridge section of Rigettiâ€™s
    Aspen system.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.10ï¼šå°† 4 é‡å­æ¯”ç‰¹ QNN åµŒå…¥åˆ° Rigetti çš„ Aspen ç³»ç»Ÿçš„æ¡¥æ¥éƒ¨åˆ†ã€‚
- en: FigureÂ [8.10](#8.10) shows how a 4-qubit QNN can be efficiently embedded on
    the QPU and FigureÂ [8.11](#8.11) shows the corresponding parameterised quantum
    circuit with adjustable one-qubit gates (R[X]*,*R[Y]) and fixed two-qubit gates
    (CZ).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ [8.10](#8.10) æ˜¾ç¤ºäº†å¦‚ä½•å°† 4 é‡å­æ¯”ç‰¹ QNN é«˜æ•ˆåœ°åµŒå…¥åˆ° QPU ä¸­ï¼Œå›¾ [8.11](#8.11) æ˜¾ç¤ºäº†å¯¹åº”çš„å‚æ•°åŒ–é‡å­ç”µè·¯ï¼Œå…·æœ‰å¯è°ƒçš„å•é‡å­æ¯”ç‰¹é—¨ï¼ˆR[X]*,*R[Y]ï¼‰å’Œå›ºå®šçš„ä¸¤é‡å­æ¯”ç‰¹é—¨ï¼ˆCZï¼‰ã€‚
- en: '![FigureÂ 8.11: QBoost circuit. The sample encoding gateÂ G is either an identity
    gateÂ I if the input isÂ "0", or a NOT gateÂ X if the input isÂ "1". ](img/file807.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 8.11ï¼šQBoost ç”µè·¯ã€‚æ ·æœ¬ç¼–ç é—¨ G å¦‚æœè¾“å…¥ä¸ºâ€œ0â€åˆ™ä¸ºæ’ç­‰é—¨ Iï¼Œå¦‚æœè¾“å…¥ä¸ºâ€œ1â€åˆ™ä¸º NOT é—¨ Xã€‚](img/file807.jpg)'
- en: 'FigureÂ 8.11: QBoost circuit. The sample encoding gateÂ G is either an identity
    gateÂ I if the input isÂ "0", or a NOT gateÂ X if the input isÂ "1".'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.11ï¼šQBoost ç”µè·¯ã€‚æ ·æœ¬ç¼–ç é—¨ G å¦‚æœè¾“å…¥ä¸ºâ€œ0â€åˆ™ä¸ºæ’ç­‰é—¨ Iï¼Œå¦‚æœè¾“å…¥ä¸ºâ€œ1â€åˆ™ä¸º NOT é—¨ Xã€‚
- en: Ensemble learning can improve QNN performance in the same way it improves performance
    of the classical weak learners.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: é›†æˆå­¦ä¹ å¯ä»¥åƒæé«˜ç»å…¸å¼±å­¦ä¹ è€…çš„æ€§èƒ½ä¸€æ ·ï¼Œæé«˜ QNN çš„æ€§èƒ½ã€‚
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: 'In this chapter, we introduced the concept of a quantum neural network as a
    parameterised quantum circuit trained as a classifier. We considered two approaches
    to training QNNs: differentiable (gradient descent) and non-differentiable (Particle
    Swarm Optimisation) methods. Gradient descent is generally faster but can face
    the problem of barren plateaus (vanishing gradients). The evolutionary search
    heuristics may be slower but can handle the presence of multiple local minima
    and strike the right balance between exploration and exploitation.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç« ï¼Œæˆ‘ä»¬ä»‹ç»äº†é‡å­ç¥ç»ç½‘ç»œçš„æ¦‚å¿µï¼Œå®ƒæ˜¯ä¸€ä¸ªä½œä¸ºåˆ†ç±»å™¨è®­ç»ƒçš„å‚æ•°åŒ–é‡å­ç”µè·¯ã€‚æˆ‘ä»¬è€ƒè™‘äº†ä¸¤ç§è®­ç»ƒ QNN çš„æ–¹æ³•ï¼šå¯å¾®ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰å’Œä¸å¯å¾®ï¼ˆç²’å­ç¾¤ä¼˜åŒ–ï¼‰æ–¹æ³•ã€‚æ¢¯åº¦ä¸‹é™é€šå¸¸æ›´å¿«ï¼Œä½†å¯èƒ½ä¼šé‡åˆ°è’æ¼ é«˜åŸï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰é—®é¢˜ã€‚è¿›åŒ–æœç´¢å¯å‘å¼æ–¹æ³•å¯èƒ½è¾ƒæ…¢ï¼Œä½†èƒ½å¤Ÿå¤„ç†å¤šä¸ªå±€éƒ¨æœ€å°å€¼ï¼Œå¹¶åœ¨æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´æ‰¾åˆ°åˆé€‚çš„å¹³è¡¡ã€‚
- en: We also explored the embedding of QNNs on the NISQ QPUs with limited connectivity
    between the qubits. As an example, we considered Rigettiâ€™s Aspen system and proposed
    an efficient embedding scheme that mirrors the "tree structure" architecture of
    the QNN.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æ¢è®¨äº†åœ¨å…·æœ‰æœ‰é™é‡å­æ¯”ç‰¹è¿æ¥æ€§çš„ NISQ QPU ä¸ŠåµŒå…¥ QNN çš„é—®é¢˜ã€‚ä½œä¸ºç¤ºä¾‹ï¼Œæˆ‘ä»¬è€ƒè™‘äº† Rigetti çš„ Aspen ç³»ç»Ÿï¼Œå¹¶æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åµŒå…¥æ–¹æ¡ˆï¼Œé•œåƒäº†
    QNN çš„â€œæ ‘ç»“æ„â€æ¶æ„ã€‚
- en: Once our QNN was fully specified and embedded into a QPU graph, we investigated
    its performance on a real-world dataset of credit approvals and provided comparisons
    with several standard classical classifiers.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬çš„é‡å­ç¥ç»ç½‘ç»œï¼ˆQNNï¼‰å®Œå…¨å®šä¹‰å¹¶åµŒå…¥åˆ°é‡å­å¤„ç†å•å…ƒï¼ˆQPUï¼‰å›¾ä¸­ï¼Œæˆ‘ä»¬å°±ä¼šç ”ç©¶å…¶åœ¨å®é™…ä¿¡ç”¨å®¡æ‰¹æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå¹¶ä¸å‡ ç§æ ‡å‡†çš„ç»å…¸åˆ†ç±»å™¨è¿›è¡Œæ¯”è¾ƒã€‚
- en: Finally, we looked at several ensemble learning techniques that assist in improving
    QNN performance in the context of a hybrid quantum-classical protocol.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†å‡ ç§é›†æˆå­¦ä¹ æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯æœ‰åŠ©äºåœ¨æ··åˆé‡å­ç»å…¸åè®®çš„èƒŒæ™¯ä¸‹æå‡ QNN çš„è¡¨ç°ã€‚
- en: In the next chapter, we will study a powerful generative QML model â€“ the Quantum
    Circuit Born Machine â€“ which is a direct quantum counterpart of the classical
    Restricted Boltzmann Machine we considered in ChapterÂ [5](Chapter_5.xhtml#x1-960005).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ä¸€ä¸ªå¼ºå¤§çš„ç”Ÿæˆå‹ QML æ¨¡å‹â€”â€”é‡å­ç”µè·¯ Born æœºï¼Œè¿™æ˜¯ç»å…¸å—é™ç»å°”å…¹æ›¼æœºï¼ˆRestricted Boltzmann Machineï¼‰åœ¨é‡å­é¢†åŸŸçš„ç›´æ¥å¯¹åº”ç‰©ï¼Œæˆ‘ä»¬åœ¨ç¬¬[5](Chapter_5.xhtml#x1-960005)ç« ä¸­è®¨è®ºè¿‡è¯¥ç»å…¸æ¨¡å‹ã€‚
- en: Join our bookâ€™s Discord space
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬ä¹¦ç±çš„ Discord ç¤¾åŒº
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/quantum](https://packt.link/quantum)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬çš„ Discord ç¤¾åŒºï¼Œç»“è¯†å¿—åŒé“åˆçš„äººï¼Œå¹¶ä¸è¶…è¿‡ 2000 åæˆå‘˜ä¸€èµ·å­¦ä¹ ï¼Œåœ°å€ä¸ºï¼š[https://packt.link/quantum](https://packt.link/quantum)
- en: '![PIC](img/file1.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file1.png)'
