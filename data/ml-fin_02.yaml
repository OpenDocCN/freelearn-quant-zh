- en: Chapter 2. Applying Machine Learning to Structured Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章：将机器学习应用于结构化数据
- en: Structured data is a term used for any data that resides in a fixed field within
    a record or file, two such examples being relational databases and spreadsheets.
    Usually, structured data is presented in a table in which each column presents
    a type of value, and each row represents a new entry. Its structured format means
    that this type of data lends itself to classical statistical analysis, which is
    also why most data science and analysis work is done on structured data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据是指任何存在于记录或文件中固定字段内的数据，其中两个典型的例子是关系型数据库和电子表格。通常，结构化数据以表格的形式呈现，其中每一列表示一种数据类型，每一行代表一个新条目。其结构化格式意味着这种类型的数据适合进行经典的统计分析，这也是为什么大多数数据科学和分析工作都集中在结构化数据上的原因。
- en: In day-to-day life, structured data is also the most common type of data available
    to businesses, and most machine learning problems that need to be solved in finance
    deal with structured data in one way or another. The fundamentals of any modern
    company's day-to-day running is built around structured data, including, transactions,
    order books, option prices, and suppliers, which are all examples of information
    usually collected in spreadsheets or databases.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在日常生活中，结构化数据也是企业最常见的数据类型，而且大多数需要解决的金融领域机器学习问题都涉及以某种方式处理结构化数据。任何现代企业的日常运营都建立在结构化数据的基础上，包括交易、订单簿、期权价格和供应商，这些通常是以电子表格或数据库的形式收集的信息。
- en: This chapter will walk you through a structured data problem involving credit
    card fraud, where we will use feature engineering to identify the fraudulent transaction
    from a dataset successfully. We'll also introduce the basics of an **end-to-end**
    (**E2E**) approach so that we can solve common financial problems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将带你解决一个涉及信用卡欺诈的结构化数据问题，我们将通过特征工程成功地从数据集中识别出欺诈交易。同时，我们还将介绍**端到端**（**E2E**）方法的基础知识，以便我们可以解决常见的金融问题。
- en: Fraud is an unfortunate reality that all financial institutions have to deal
    with. It's a constant race between companies trying to protect their systems and
    fraudsters who are trying to defeat the protection in place. For a long time,
    fraud detection has relied on simple heuristics. For example, a large transaction
    made while you're in a country you usually don't live in will likely result in
    that transaction being flagged.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈是所有金融机构不得不应对的一个不幸现实。这是企业保护其系统与欺诈者试图突破现有保护措施之间的持续博弈。长期以来，欺诈检测依赖于简单的启发式方法。例如，在你通常不居住的国家发生的大额交易，很可能会导致该交易被标记为可疑。
- en: Yet, as fraudsters continue to understand and circumvent the rules, credit card
    providers are deploying increasingly sophisticated machine learning systems to
    counter this.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着欺诈者不断理解并规避规则，信用卡提供商正在部署越来越复杂的机器学习系统来应对这一挑战。
- en: In this chapter, we'll look at how a real bank might tackle the problem of fraud.
    It's a real-world exploration of how a team of data scientists starts with a heuristic
    baseline, then develops an understanding of its features, and from that, builds
    increasingly sophisticated machine learning models that can detect fraud. While
    the data we will use is synthetic, the process of development and tools that we'll
    use to tackle fraud are similar to the tools and processes that are used every
    day by international retail banks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将探讨一家真实银行如何处理欺诈问题。这是一个现实世界的案例，展示了一个数据科学团队如何从启发式基线开始，逐步理解其特征，并从中构建越来越复杂的机器学习模型来检测欺诈行为。尽管我们使用的数据是合成的，但我们将采用的开发过程和工具与国际零售银行每天使用的工具和流程类似。
- en: So where do you start? To put it in the words of one anonymous fraud detection
    expert that I spoke to, *"I keep thinking about how I would steal from my employer,
    and then I create some features that would catch my heist. To catch a fraudster,
    think like a fraudster."* Yet, even the most ingenious feature engineers are not
    able to pick up on all the subtle and sometimes counterintuitive signs of fraud,
    which is why the industry is slowly shifting toward entirely E2E-trained systems.
    These systems, in addition to machine learning, are both focuses of this chapter
    where we will explore several commonly used approaches to flag fraud.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你从哪里开始呢？用我曾经与一位匿名欺诈检测专家交谈时的话来说，*“我不断思考如果我要从雇主那里偷钱，我会怎么做，然后我创造一些特征来捕捉我的抢劫行为。要抓住一个欺诈者，就要像欺诈者一样思考。”*
    然而，即使是最有创造力的特征工程师，也未必能发现所有细微且有时是反直觉的欺诈迹象，这就是为什么业界正逐渐转向完全端到端（E2E）训练的系统。这些系统除了机器学习外，还将是本章的重点，我们将在本章中探讨几种常用的欺诈标记方法。
- en: This chapter will act as an important baseline to [Chapter 6](ch06.xhtml "Chapter 6. Using
    Generative Models"), *Using Generative Models*, where we will again be revisiting
    the credit card fraud problem for a full E2E model using auto-encoders.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将作为[第6章](ch06.xhtml "第6章 使用生成模型")的一个重要基础，*使用生成模型*，在该章中我们将再次回顾信用卡欺诈问题，构建一个完整的端到端（E2E）模型，使用自编码器。
- en: The data
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: The dataset we will work with is a synthetic dataset of transactions generated
    by a payment simulator. The goal of this case study and the focus of this chapter
    is to find fraudulent transactions within a dataset, a classic machine learning
    problem many financial institutions deal with.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集是一个由支付模拟器生成的合成交易数据集。本案例研究的目标以及本章的重点是从数据集中找出欺诈交易，这是许多金融机构面临的经典机器学习问题。
- en: Note
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: Before we go further, a digital copy of the code, as well as an interactive
    notebook for this chapter are accessible online, via the following two links:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：在继续之前，本章的数字版代码以及交互式笔记本可以通过以下两个链接在线访问：'
- en: An interactive notebook containing the code for this chapter can be found under
    [https://www.kaggle.com/jannesklaas/structured-data-code](https://www.kaggle.com/jannesklaas/structured-data-code)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下网址找到交互式笔记本：[https://www.kaggle.com/jannesklaas/structured-data-code](https://www.kaggle.com/jannesklaas/structured-data-code)
- en: 'The code can also be found on GitHub, in this book''s repository: [https://github.com/PacktPublishing/Machine-Learning-for-Finance](https://github.com/PacktPublishing/Machine-Learning-for-Finance)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 代码也可以在GitHub上找到，在本书的代码库中：[https://github.com/PacktPublishing/Machine-Learning-for-Finance](https://github.com/PacktPublishing/Machine-Learning-for-Finance)
- en: 'The dataset we''re using stems from the paper *PaySim: A financial mobile money
    simulator for fraud detection*, by E. A. Lopez-Rojas, A. Elmir, and S. Axelsson.
    The dataset can be found on Kaggle under this URL: [https://www.kaggle.com/ntnu-testimon/paysim1](https://www.kaggle.com/ntnu-testimon/paysim1).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用的数据集来源于论文*PaySim: A financial mobile money simulator for fraud detection*，作者为E.
    A. Lopez-Rojas、A. Elmir和S. Axelsson。该数据集可以在Kaggle上找到，网址为：[https://www.kaggle.com/ntnu-testimon/paysim1](https://www.kaggle.com/ntnu-testimon/paysim1)。'
- en: Before we break it down on the next page, let's take a minute to look at the
    dataset that we'll be using in this chapter. Remember, you can download the data
    with the preceding link.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们下一页进行详细讲解之前，先花点时间看看我们将在本章中使用的数据集。请记住，你可以通过前面的链接下载数据。
- en: '| step | type | amount | nameOrig | oldBalance Orig | newBalance Orig | nameDest
    | oldBalance Dest | newBalance Dest | isFraud | isFlagged Fraud |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 | 类型 | 金额 | nameOrig | 原始旧余额 | 原始新余额 | nameDest | 目的地旧余额 | 目的地新余额 | 是否欺诈
    | 是否标记为欺诈 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | PAYMENT | 9839.64 | C1231006815 | 170136.0 | 160296.36 | M1979787155
    | 0.0 | 0.0 | 0 | 0 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 1 | PAYMENT | 9839.64 | C1231006815 | 170136.0 | 160296.36 | M1979787155
    | 0.0 | 0.0 | 0 | 0 |'
- en: '| 1 | PAYMENT | 1864.28 | C1666544295 | 21249.0 | 19384.72 | M2044282225 |
    0.0 | 0.0 | 0 | 0 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 1 | PAYMENT | 1864.28 | C1666544295 | 21249.0 | 19384.72 | M2044282225 |
    0.0 | 0.0 | 0 | 0 |'
- en: '| 1 | TRANSFER | 181.0 | C1305486145 | 181.0 | 0.0 | C553264065 | 0.0 | 0.0
    | 1 | 0 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 1 | TRANSFER | 181.0 | C1305486145 | 181.0 | 0.0 | C553264065 | 0.0 | 0.0
    | 1 | 0 |'
- en: '| 1 | CASH_OUT | 181.0 | C840083671 | 181.0 | 0.0 | C38997010 | 21182.0 | 0.0
    | 1 | 0 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 1 | CASH_OUT | 181.0 | C840083671 | 181.0 | 0.0 | C38997010 | 21182.0 | 0.0
    | 1 | 0 |'
- en: '| 1 | PAYMENT | 11668.14 | C2048537720 | 41554.0 | 29885.86 | M1230701703 |
    0.0 | 0.0 | 0 | 0 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 1 | PAYMENT | 11668.14 | C2048537720 | 41554.0 | 29885.86 | M1230701703 |
    0.0 | 0.0 | 0 | 0 |'
- en: '| 1 | PAYMENT | 7817.71 | C90045638 | 53860.0 | 46042.29 | M573487274 | 0.0
    | 0.0 | 0 | 0 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 1 | PAYMENT | 7817.71 | C90045638 | 53860.0 | 46042.29 | M573487274 | 0.0
    | 0.0 | 0 | 0 |'
- en: '| 1 | PAYMENT | 7107.77 | C154988899 | 183195.0 | 176087.23 | M408069119 |
    0.0 | 0.0 | 0 | 0 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 支付 | 7107.77 | C154988899 | 183195.0 | 176087.23 | M408069119 | 0.0 |
    0.0 | 0 | 0 |'
- en: '| 1 | PAYMENT | 7861.64 | C1912850431 | 176087.23 | 168225.59 | M633326333
    | 0.0 | 0.0 | 0 | 0 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 支付 | 7861.64 | C1912850431 | 176087.23 | 168225.59 | M633326333 | 0.0
    | 0.0 | 0 | 0 |'
- en: '| 1 | PAYMENT | 4024.36 | C1265012928 | 2671.0 | 0.0 | M1176932104 | 0.0 |
    0.0 | 0 | 0 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 支付 | 4024.36 | C1265012928 | 2671.0 | 0.0 | M1176932104 | 0.0 | 0.0 |
    0 | 0 |'
- en: '| 1 | DEBIT | 5337.77 | C712410124 | 41720.0 | 36382.23 | C195600860 | 41898.0
    | 40348.79 | 0 | 0 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 借记 | 5337.77 | C712410124 | 41720.0 | 36382.23 | C195600860 | 41898.0
    | 40348.79 | 0 | 0 |'
- en: 'As seen in the first row, the dataset has 11 columns. Let''s explain what each
    one represents before we move on:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如第一行所示，数据集有11列。在继续之前，让我们先解释一下每一列代表什么：
- en: '**step**: Maps time, with each step corresponding to one hour.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**step**: 映射时间，每一步对应一个小时。'
- en: '**type**: The type of the transaction, which can be CASH_IN, CASH_OUT, DEBIT,
    PAYMENT, or TRANSFER.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**type**: 交易类型，可以是CASH_IN、CASH_OUT、DEBIT、PAYMENT或TRANSFER。'
- en: '**amount**: The amount of the transaction.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**amount**: 交易金额。'
- en: '**nameOrig**: The origin account that started the transaction. C relates to
    customer accounts, while M is the account of merchants.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nameOrig**: 发起交易的来源账户。C表示客户账户，而M表示商户账户。'
- en: '**oldbalanceOrig**: The old balance of the origin account.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**oldbalanceOrig**: 来源账户的旧余额。'
- en: '**newbalanceOrig**: The new balance of the origin account after the transaction
    amount has been added.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**newbalanceOrig**: 交易后来源账户的最新余额。'
- en: '**nameDest**: The destination account.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nameDest**: 目标账户。'
- en: '**oldbalanceDest**: The old balance of the destination account. This information
    is not available for merchant accounts whose names start with M.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**oldbalanceDest**: 目标账户的旧余额。对于商户账户（账户名以M开头），此信息不可用。'
- en: '**newbalanceDest**: The new balance of the destination account. This information
    is not available for merchant accounts.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**newbalanceDest**: 目标账户的最新余额。商户账户没有此信息。'
- en: '**isFraud**: Whether the transaction was fraudulent.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**isFraud**: 交易是否为欺诈交易。'
- en: '**isFlaggedFraud**: Whether the old system has flagged the transaction as fraud.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**isFlaggedFraud**: 系统是否已将交易标记为欺诈。'
- en: In the preceding table, we can see 10 rows of data. It's worth noting that there
    are about 6.3 million transactions in our total dataset, so what we've seen is
    a small fraction of the total amount. As the fraud we're looking at only occurs
    in transactions marked as either TRANSFER or CASH_OUT, all other transactions
    can be dropped, leaving us with around 2.8 million examples to work with.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的表格中，我们可以看到10行数据。值得注意的是，我们的总数据集中约有630万个交易记录，所以我们所看到的只是总数的一小部分。由于我们关注的欺诈仅发生在标记为TRANSFER或CASH_OUT的交易中，因此其他所有交易都可以去除，这样我们剩下的约有280万个样本可供处理。
- en: Heuristic, feature-based, and E2E models
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启发式、特征基础和E2E模型
- en: Before we dive into developing models to detect fraud, let's take a second to
    pause and ponder over the different kinds of models we could build.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始开发用于检测欺诈的模型之前，先花一点时间暂停一下，思考一下我们可以构建的不同类型的模型。
- en: A heuristic-based model is a simple "rule of thumb" developed purely by humans.
    Usually, the heuristic model stems from having an expert knowledge of the problem.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于启发式的模型是一种简单的“经验法则”，完全由人类开发。通常，启发式模型源于对问题的专家知识。
- en: A feature-based model relies heavily on humans modifying the data to create
    new and meaningful features, which are then fed into a (simple) machine learning
    algorithm. This approach mixes expert knowledge with learning from data.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于特征的模型在很大程度上依赖于人类修改数据，以创建新的、有意义的特征，然后将这些特征输入到（简单的）机器学习算法中。这种方法将专家知识与数据学习相结合。
- en: An E2E model learns purely from raw data. No human expertise is used, and the
    model learns everything directly from observations.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: E2E模型完全通过原始数据进行学习。没有使用人类的专业知识，模型直接从观察中学习一切。
- en: In our case, a heuristic-based model could be created to mark all transactions
    with the TRANSFER transaction type and an amount over $200,000 as fraudulent.
    Heuristic-based models have the advantage that they are both fast to develop and
    easy to implement; however, this comes with a pay-off, their performance is often
    poor, and fraudsters can easily play the system. Let's imagine that we went with
    the preceding heuristic-based model, fraudsters transferring only $199,999, under
    the fraudulent limit, would evade detection.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，可以创建一个基于启发式的模型，将所有类型为 TRANSFER 且金额超过 20 万美元的交易标记为欺诈。基于启发式的模型具有开发速度快、实施简便的优势；然而，这也有代价，它们的性能通常较差，欺诈者可以轻松地利用系统漏洞。假设我们采用了前述的启发式模型，欺诈者将金额设置为
    199,999 美元，低于欺诈限制，就可以避免检测。
- en: An important heuristic in the field of trading is the momentum strategy. Momentum
    strategies involve betting that a stock that's on the rise will continue to rise,
    with people then buying that stock. While this strategy sounds too simple to be
    any good, it is in fact, a reasonably successful strategy that many high-frequency
    trading and quantitative outlets are using today.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在交易领域，一个重要的启发式方法是动量策略。动量策略是指押注某只正在上涨的股票将继续上涨，人们会购买该股票。虽然这一策略看起来过于简单，似乎并不怎么样，但事实上，它是一种相当成功的策略，许多高频交易和量化交易公司如今都在使用。
- en: To create features, experts craft indicators that can distinguish fraudulent
    transactions from those that are genuine. This is often done using statistical
    data analysis, and when compared to the heuristic-based model that we proposed
    early on, it will take longer, but with the benefit of better results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建特征，专家们设计出能够区分欺诈交易和真实交易的指标。这通常通过统计数据分析来完成，与我们之前提出的基于启发式的模型相比，虽然时间较长，但能够带来更好的结果。
- en: Feature engineering-based models are a midway between data and humans shaping
    rules, where human knowledge and creativity are exploited to craft good features,
    and data and machine learning are used to create a model from those features.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征工程的模型处于数据与人工规则之间的中间地带，人类的知识和创造力被用来设计好的特征，而数据和机器学习则用于从这些特征中创建模型。
- en: E2E models learn purely from collected data without using expert knowledge.
    As discussed before, this often yields much better results, but at the cost of
    taking a lot of time to complete. This method also has some additional elements
    worth considering. For instance, collecting the large amount of data that will
    be needed is an expensive task, as humans have to label millions of records.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: E2E 模型完全依赖收集到的数据进行学习，而不使用专家知识。如前所述，这种方法通常能产生更好的结果，但代价是完成时间较长。该方法还包含一些值得考虑的额外因素。例如，收集所需的大量数据是一项昂贵的任务，因为需要人工标注数百万条记录。
- en: Though for many people in the industry right now, they take the view that shipping
    a poor model is often better than not shipping anything at all. After all, having
    some protection against fraud is better than simply having none.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对于许多业内人士来说，他们认为发布一个差的模型通常比什么都不发布要好。毕竟，有一些防欺诈保护总比没有要强。
- en: Using a heuristic approach that lets through half of all fraudulent transactions
    is better than having no fraud detection at all. The graph shows us the performance
    of the three models we introduced earlier on, against the time taken to implement
    them.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一种启发式方法，哪怕它能漏过一半的欺诈交易，也总比完全没有欺诈检测要好。图表展示了我们之前介绍的三种模型的表现，以及它们实现所需的时间。
- en: '![Heuristic, feature-based, and E2E models](img/B10354_02_01.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![启发式、基于特征和 E2E 模型](img/B10354_02_01.jpg)'
- en: The methods used and the performance of the system during development
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中使用的方法和系统的性能
- en: The best method is to use a combination of all three. If we deploy a heuristic
    model that meets the basic requirements of the task that it set out to achieve,
    then it can be shipped. By employing this method, the heuristic then becomes the
    baseline that any other approach has to beat. Once your heuristic model is deployed,
    then all your efforts should then be directed toward building a feature-based
    model, which as soon as it beats the initially deployed heuristic model, can then
    be deployed while you continue to refine the model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的方法是将三者结合使用。如果我们部署一个符合任务基本要求的启发式模型，它就可以投入使用。通过这种方法，启发式模型成为任何其他方法必须超越的基准。一旦启发式模型部署完成，所有的努力应当转向构建基于特征的模型，当它超越最初部署的启发式模型后，就可以投入使用，同时继续优化模型。
- en: As we've discussed before, feature-based models often deliver pretty decent
    performance on structured data tasks; this gives companies the time to undertake
    the lengthy and expensive task of building an E2E model, which can be shipped
    once it beats the feature-based model. Now that we understand the type of models
    we're going to build, let's look at the software we need to build them.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论过的，基于特征的模型通常在结构化数据任务中表现得相当不错；这使得公司有时间进行耗时且昂贵的构建端到端（E2E）模型的工作，一旦新模型在性能上超越基于特征的模型，就可以投入使用。现在我们理解了要构建的模型类型，让我们来看一下构建这些模型所需的软件。
- en: The machine learning software stack
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习软件栈
- en: 'In this chapter, we will be using a range of different libraries that are commonly
    used in machine learning. Let''s take a minute to look at our stack, which consists
    of the following software:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一系列常见的机器学习库。我们先花点时间看一下我们的软件栈，它包括以下软件：
- en: '**Keras**: A neural network library that can act as a simplified interface
    to TensorFlow.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keras**：一个神经网络库，可以作为 TensorFlow 的简化接口。'
- en: '**NumPy**: Adds support for large, multidimensional arrays as well as an extensive
    collection of mathematical functions.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NumPy**：为大型、多维数组提供支持，并包含广泛的数学函数库。'
- en: '**Pandas**: A library for data manipulation and analysis. It''s similar to Microsoft''s
    Excel but in Python, as it offers data structures to handle tables and the tools
    to manipulate them.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pandas**：一个用于数据处理和分析的库。它类似于微软的 Excel，但在 Python 中，提供了用于处理表格的结构和操作这些表格的工具。'
- en: '**Scikit-learn**: A machine learning library offering a wide range of algorithms
    and utilities.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Scikit-learn**：一个机器学习库，提供了广泛的算法和工具。'
- en: '**TensorFlow**: A dataflow programming library that facilitates working with
    neural networks.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow**：一个数据流编程库，有助于处理神经网络。'
- en: '**Matplotlib**: A plotting library.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Matplotlib**：一个绘图库。'
- en: '**Jupyter**: A development environment. All of the code examples in this book
    are available in Jupyter Notebooks.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jupyter**：一个开发环境。本书中的所有代码示例都可以在 Jupyter Notebooks 中找到。'
- en: The majority of this book is dedicated to working with the Keras library, while
    this chapter makes extensive use of the other libraries mentioned. The goal here
    is less about teaching you all the tips and tricks of all the different libraries,
    but more about showing you how they are integrated into the process of creating
    a predictive model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的大部分内容都专注于使用 Keras 库，而本章则广泛使用了其他提到的库。这里的目标并不是教你所有不同库的技巧和窍门，而是展示它们如何集成到创建预测模型的过程中。
- en: Note
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: All of the libraries needed for this chapter are installed on Kaggle
    kernels by default. If you are running this code locally, please refer to the
    setup instructions in [Chapter 1](ch01.xhtml "Chapter 1. Neural Networks and Gradient-Based
    Optimization"), *Neural Networks and Gradient-Based Optimization*, and install
    all of the libraries needed.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：本章所需的所有库在 Kaggle 内核中默认安装。如果你在本地运行此代码，请参考 [第1章](ch01.xhtml "第1章. 神经网络与基于梯度的优化")，*神经网络与基于梯度的优化*，并安装所需的所有库。'
- en: The heuristic approach
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启发式方法
- en: Earlier in this chapter, we introduced the three models that we will be using
    to detect fraud, now it's time to explore each of them in more detail. We're going
    to start with the heuristic approach.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面，我们介绍了将用于检测欺诈的三个模型，现在是时候更详细地探索它们了。我们将从启发式方法开始。
- en: Let's start by defining a simple heuristic model and measuring how well it does
    at measuring fraud rates.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义一个简单的启发式模型开始，并衡量它在衡量欺诈率方面的效果。
- en: Making predictions using the heuristic model
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用启发式模型进行预测
- en: We will be making our predictions using the heuristic approach over the entire
    training data set in order to get an idea of how well this heuristic model does
    at predicting fraudulent transactions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用启发式方法对整个训练数据集进行预测，以了解这个启发式模型在预测欺诈交易方面的效果。
- en: 'The following code will create a new column, `Fraud_Heuristic`, and in turn
    assigns a value of `1` in rows where the type is `TRANSFER`, and the amount is
    more than $200,000:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将创建一个新列 `Fraud_Heuristic`，并在类型为 `TRANSFER` 且金额超过 $200,000 的行中分配值 `1`：
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With just two lines of code, it's easy to see how such a simple metric can be
    easy to write, and quick to deploy.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 只需两行代码，你就可以轻松看出这样一个简单的度量指标如何轻松编写，并且迅速部署。
- en: The F1 score
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: F1 分数
- en: One important thing we must consider is the need for a common metric on which
    we can evaluate all of our models on. In [Chapter 1](ch01.xhtml "Chapter 1. Neural
    Networks and Gradient-Based Optimization"), *Neural Networks and Gradient-Based
    Optimization*, we used accuracy as our emulation tool. However, as we've seen,
    there are far fewer fraudulent transactions than there are genuine ones. Therefore
    a model that classifies all the transactions as genuine can have a very high level
    of accuracy.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须考虑的一件重要事情是需要一个通用的度量标准，以便对我们的所有模型进行评估。在[第 1 章](ch01.xhtml "第 1 章 神经网络与基于梯度的优化")中，*神经网络与基于梯度的优化*，我们使用准确率作为我们的模拟工具。然而，正如我们所见，欺诈交易远少于真实交易。因此，一个将所有交易分类为真实交易的模型，可能会有一个非常高的准确率。
- en: 'One such metric that is designed to deal with such a skewed distribution is
    the F1 score, which considers true and false positives and negatives, as you can
    see in this chart:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这种偏斜分布问题的一种度量是 F1 分数，它考虑了真正阳性和假阳性以及假阴性，如下图所示：
- en: '|   | Predicted Negative | Predicted Positive |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|   | 预测为负 | 预测为正 |'
- en: '| --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Actual Negative | **True Negative** (**TN**) | **False Positive** (**FP**)
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 实际负例 | **真正阴性** (**TN**) | **假阳性** (**FP**) |'
- en: '| Actual Positive | **False Negative** (**FN**) | **True Positive** (**TP**)
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 实际正例 | **假阴性** (**FN**) | **真正阳性** (**TP**) |'
- en: 'We can first compute the precision of our model, which specifies the share
    of predicted positives that were positives, using the following formula:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以首先计算模型的精准度，精准度指定了预测为正的样本中，实际为正的比例，公式如下：
- en: '![The F1 score](img/B10354_02_001.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![F1 分数](img/B10354_02_001.jpg)'
- en: 'Recall measures the share of predicted positives over the actual number of
    positives, as seen in this formula:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率衡量的是预测为正例的样本占实际正例的比例，公式如下：
- en: '![The F1 score](img/B10354_02_002.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![F1 分数](img/B10354_02_002.jpg)'
- en: 'The F1 score is then calculated from the harmonic mean, an average, of the
    two measures, which can be seen in the following formula:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数是通过计算两种度量的调和平均值来得出的，具体公式如下：
- en: '![The F1 score](img/B10354_02_003.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![F1 分数](img/B10354_02_003.jpg)'
- en: 'To compute this metric in Python, we can use the `metrics` module of scikit-learn,
    or sklearn for short:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中计算此度量时，我们可以使用 scikit-learn 的 `metrics` 模块，简称 sklearn：
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Given the predictions we''ve made, we can now easily compute the F1 score using
    the following command:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 给定我们所做的预测，现在我们可以使用以下命令轻松计算 F1 分数：
- en: '[PRE2]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You'll see that the preceding command outputs a number–starting 0.013131315…-What
    this number means exactly is that our heuristic model is not doing too well, as
    the best possible F1 score is 1, and the worst is 0\. In our case, this number
    represents the harmonic mean of the share of correctly caught frauds over everything
    labeled as fraud and the share of correctly caught frauds over all frauds.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到前面的命令输出了一个数字——从 0.013131315 开始——这个数字的意思是我们的启发式模型表现不太好，因为最好的 F1 分数是 1，最差的是
    0。在我们的例子中，这个数字代表了被标记为欺诈的所有样本中，正确识别出欺诈行为的比例的调和平均值。
- en: Evaluating with a confusion matrix
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用混淆矩阵进行评估
- en: A more qualitative and interpretable way of evaluating a model is with a confusion
    matrix. As the name suggests, the matrix shows how our classifier confuses classes.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更具定性且易于解释的评估模型的方式是使用混淆矩阵。顾名思义，混淆矩阵展示了我们的分类器如何混淆各类。
- en: 'Firstly, let''s study the code appendix for the `plot_confusion_matrix` function:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们研究一下 `plot_confusion_matrix` 函数的代码附录：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Which, when we run, produces the following graphic:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行时，产生以下图形：
- en: '![Evaluating with a confusion matrix](img/B10354_02_02.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![使用混淆矩阵进行评估](img/B10354_02_02.jpg)'
- en: A confusion matrix for a heuristic model
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式模型的混淆矩阵
- en: So, just how accurate was that model? As you can see in our confusion matrix,
    from our dataset of 2,770,409 examples, 2,355,826 were correctly classified as
    genuine, while 406,370 were falsely classified as fraud. In fact, only 2,740 examples
    were correctly classified as fraud.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这个模型的准确度如何呢？从我们的混淆矩阵中可以看到，在我们有 2,770,409 个样本的数据集中，2,355,826 个被正确分类为真实交易，而
    406,370 个被错误分类为欺诈。实际上，只有 2,740 个样本被正确分类为欺诈。
- en: When our heuristic model classified a transaction as fraudulent, it was genuine
    in 99.3% of those cases. Only 34.2% of the total frauds got caught. All this information
    is incorporated into the F1 score we formulated. However, as we saw, it is easier
    to read this from the generated confusion matrix graphic. The reason we used both
    the heuristic model and the F1 score is that it is good practice to have a single
    number that tells us which model is better, and also a more graphical insight
    into how that model is better.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的启发式模型将某笔交易分类为欺诈时，99.3%的情况下是准确的。总共只有34.2%的欺诈被捕获。所有这些信息都被纳入了我们制定的F1分数中。然而，正如我们所见，从生成的混淆矩阵图形中更容易看出这一点。我们同时使用启发式模型和F1分数的原因是，良好的实践是拥有一个可以告诉我们哪个模型更好的单一数字，同时也提供一个更具图形化的视角来展示该模型的优势。
- en: To put it frankly, our heuristic model has performed quite poorly, detecting
    only 34.2% of fraud, which is not good enough. So, using the other two methods
    in the following sections, we're going to see whether we can do better.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 直白地说，我们的启发式模型表现得相当糟糕，仅检测到34.2%的欺诈，这是远远不够的。因此，在接下来的部分中，我们将使用其他两种方法，看看是否能够做得更好。
- en: The feature engineering approach
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程方法
- en: 'The objective of feature engineering is to exploit the qualitative insight
    of humans in order to create better machine learning models. A human engineer
    usually uses three types of insight: *intuition*, *expert domain knowledge*, and
    *statistical analysis*. Quite often, it''s possible to come up with features for
    a problem just from intuition.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的目标是利用人类的定性洞察力来创建更好的机器学习模型。人类工程师通常使用三种类型的洞察力：*直觉*、*专家领域知识*和*统计分析*。很多时候，仅凭直觉就能为某个问题提出特征。
- en: As an example, in our fraud case, it seems intuitive that fraudsters will create
    new accounts for their fraudulent schemes and won't be using the same bank account
    that they pay for their groceries with.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的欺诈案件中，直觉上认为欺诈者会为他们的欺诈计划创建新账户，而不会使用他们支付日常开销的同一银行账户。
- en: Domain experts are able to use their extensive knowledge of a problem in order
    to come up with other such examples of intuition. They'll know more about how
    fraudsters behave and can craft features that indicate such behavior. All of these
    intuitions are then usually confirmed by statistical analysis, something that
    can even be used to open the possibilities of discovering new features.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 领域专家能够利用他们对问题的深入了解，提出其他类似的直觉例子。他们会更了解欺诈者的行为，并能够设计出能够指示这种行为的特征。所有这些直觉通常都会通过统计分析得到验证，统计分析甚至可以用来开启发现新特征的可能性。
- en: Statistical analysis can sometimes turn up quirks that can be turned into predictive
    features. However, with this method, engineers must beware of the **data trap**.
    Predictive features found in the data might only exist in that data because any
    dataset will spit out a predictive feature if it's wrangled with for long enough.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 统计分析有时能发现一些奇特的模式，这些模式可以转化为预测特征。然而，在使用这种方法时，工程师必须警惕**数据陷阱**。数据中找到的预测特征可能仅仅存在于该数据中，因为任何数据集只要经过足够长时间的处理，都会产生一个预测特征。
- en: A data trap refers to engineers digging within the data for features forever,
    and never questioning whether those features they are searching for are relevant.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 数据陷阱指的是工程师在数据中无限挖掘特征，却从不质疑他们所寻找的特征是否相关。
- en: Data scientists stuck in to the data trap keep euphorically finding features,
    only to realize later that their model, with all those features, does not work
    well. Finding strong predictive features in the training set is like a drug for
    data science teams. Yes, there's an immediate reward, a quick win that feels like
    a validation of one's skills. However, as with many drugs, the data trap can lead
    to an after-effect in which teams find that weeks' or months' worth of work in
    finding those features was actually, useless.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 陷入数据陷阱的数据科学家们不停地发现特征，然而过后才意识到，带有这些特征的模型实际上效果并不好。在训练集找到强预测特征对数据科学团队来说就像是毒品。是的，它有一个即时的回报，一个让人感觉自己技能被验证的快速胜利。然而，像许多毒品一样，数据陷阱可能导致后遗症，团队发现几周或几个月的工作寻找这些特征，实际上是无用的。
- en: Take a minute to ask yourself, are you in that position? If you ever find yourself
    applying analysis after analysis, transforming data in every possible way, chasing
    correlation values, you might very well be stuck in a data trap.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 花一分钟问问自己，你是否处于这种状态？如果你发现自己一直在做分析、转换数据、追逐相关性值，那么你很可能已经陷入了数据陷阱。
- en: To avoid the data trap, it is important to establish a **qualitative rationale**
    as to why this statistical predictive feature exists and should exist outside
    of the dataset as well. By establishing this rationale, you will keep both yourself
    and your team alert to avoiding crafting features that represent noise. The data
    trap is the human form of overfitting and finding patterns in noise, which is
    a problem for models as well.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免数据陷阱，建立一个**定性理由**来解释为何这个统计预测特征存在，以及它为什么也应该存在于数据集之外是非常重要的。通过建立这个理由，你将保持自己和团队的警觉，避免创造代表噪声的特征。数据陷阱是人类形式的过拟合和在噪声中寻找模式的问题，这对模型也是一个问题。
- en: Humans can use their qualitative reasoning skills to avoid fitting noise, which
    is a big advantage humans have over machines. If you're a data scientist, you
    should use this skill to create more generalizable models.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 人类可以利用他们的定性推理能力来避免拟合噪声，这是人类相较于机器的一个巨大优势。如果你是数据科学家，你应该运用这一技能来创建更具普适性的模型。
- en: The goal of this section was not to showcase all the features that feature engineering
    could perform on this dataset, but just to highlight the three approaches and
    how they can be turned into features.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目标不是展示特征工程可以在此数据集上执行的所有特征，而只是突出这三种方法以及它们如何转化为特征。
- en: A feature from intuition – fraudsters don't sleep
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自直觉的一个特点——欺诈者不睡觉
- en: Without knowing much about fraud, we can intuitively describe fraudsters as
    shady people that operate in the dark. In most cases, genuine transactions happen
    during the day, as people sleep at night.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对欺诈行为了解不多，我们也能凭直觉描述欺诈者为在黑暗中活动的可疑人物。在大多数情况下，真实的交易发生在白天，因为人们在夜晚休息。
- en: 'The time steps in our dataset represent one hour. Therefore, we can generate
    the time of the day by simply taking the remainder of a division by 24, as seen
    in this code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的时间步骤代表一个小时。因此，我们只需通过 24 取余，就能生成一天中的时间，正如这段代码所示：
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'From there, we can then count the number of fraudulent and genuine transactions
    at different times. To calculate this, we must run the following code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，我们可以计算不同时间的欺诈和真实交易数量。为了计算这个，我们必须运行以下代码：
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then finally, we can plot the share of genuine and fraudulent transactions
    over the course of the day into a chart. To do this, we must run the following
    code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，最后我们可以将一天中每个小时内的真实和欺诈交易的比例绘制成图表。为此，我们必须运行以下代码：
- en: '[PRE7]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![A feature from intuition – fraudsters don''t sleep](img/B10354_02_03.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![来自直觉的一个特点——欺诈者不睡觉](img/B10354_02_03.jpg)'
- en: The share of fraudulent and genuine transactions conducted throughout each hour
    of the day
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一天中每小时进行的欺诈和真实交易的比例
- en: 'As we can see in the preceding chart, there are much fewer genuine transactions
    at night, while fraudulent behavior continues over the day. To be sure that night
    is a time when we can hope to catch fraud, we can also plot the number of fraudulent
    transactions as a share of all transactions. To do this, we must run the following
    command:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前面的图表中所见，夜间的真实交易显著减少，而欺诈行为则持续发生。为了确认夜间是我们有可能抓到欺诈的时段，我们还可以将欺诈交易数量作为所有交易的比例进行绘制。为此，我们必须运行以下命令：
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![A feature from intuition – fraudsters don''t sleep](img/B10354_02_04.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![来自直觉的一个特点——欺诈者不睡觉](img/B10354_02_04.jpg)'
- en: The share of transactions that are fraudulent per hour of the day
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 每小时欺诈交易的比例
- en: Once we run that code, we can see that at around 5 AM, over 60% of all transactions
    seem to be fraudulent, which appears to make this a great time of the day to catch
    fraud.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行那段代码，就能看到大约在早上 5 点时，超过 60% 的所有交易似乎是欺诈性的，这表明这是一天中抓住欺诈行为的最佳时机。
- en: Expert insight – transfer, then cash out
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专家见解——先转账，再提现
- en: The description of the dataset came with another description that explained
    the expected behavior of fraudsters. First, they transfer money to a bank account
    they control. Then, they cash out that money from an ATM.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的描述伴随着另一个解释，解释了欺诈者的预期行为。首先，他们将钱转到他们控制的银行账户。然后，他们从 ATM 提现。
- en: 'We can check whether there are fraudulent transfer destination accounts that
    are the origin of the fraudulent cash outs by running the following code:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下代码，检查是否有欺诈转账目标账户是欺诈提现的来源：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'According to the output, there seems to be no fraudulent transfers that are
    the origin of fraudulent cash outs. The behavior expected by the experts is not
    visible in our data. This could mean two things: firstly, it could mean that the
    fraudsters behave differently now, or secondly that our data does not capture
    their behavior. Either way, we cannot use this insight for predictive modeling
    here.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输出，似乎没有欺诈转账是欺诈性现金提取的源头。专家预期的行为在我们的数据中没有显示出来。这可能意味着两件事：首先，可能是欺诈者现在的行为方式不同；或者是我们的数据没有捕捉到他们的行为。无论哪种情况，我们无法在这里利用这一发现进行预测建模。
- en: Statistical quirks – errors in balances
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计学上的怪异现象 —— 余额错误
- en: A closer examination of the data shows that there are some transactions where
    the old and new balances of the destination account is zero, although the transaction
    amount is not zero. This is odd, or more so a quirk, and so we want to investigate
    whether this type of oddity yields predictive power.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据的进一步检查显示，有些交易中，目标账户的旧余额和新余额都是零，尽管交易金额不为零。这很奇怪，或者说是一种怪异现象，因此我们需要调查这种异常是否具有预测能力。
- en: 'To begin with, we can calculate the share of fraudulent transactions with this
    property by running the following code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以通过运行以下代码来计算具有此属性的欺诈交易的比例：
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, the share of fraudulent transactions stands at 70%, so this
    quirk seems to be a good feature at detecting fraud in transactions. However,
    it is important to ask ourselves how this quirk got into our data in the first
    place. One possibility could be that the transactions never come through.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，欺诈交易的比例为 70%，因此这种怪异现象似乎是检测交易欺诈的一个好特征。然而，我们需要问自己，这种怪异现象最初是如何进入我们数据的。一种可能性是这些交易根本没有通过。
- en: This could happen for a number of reasons including that there might be another
    fraud prevention system in place that blocks the transactions, or that the origin
    account for the transaction has insufficient funds.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况可能由多种原因引起，包括可能存在另一个欺诈防范系统拦截了这些交易，或者交易的原始账户资金不足。
- en: 'While we have no way of verifying if there''s another fraud prevention system
    in place, we can check to see if the origin accounts have insufficient funds.
    To do this, we have to run the following code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们无法验证是否有其他欺诈防范系统在运行，但我们可以检查原始账户是否资金不足。为此，我们需要运行以下代码：
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As we can see in the output, close to 90% of the odd transactions have insufficient
    funds in their origin accounts. From this, we can now construct a rationale in
    which fraudsters try to drain a bank account of all its funds more often than
    regular people do.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在输出中看到的，接近 90% 的异常交易在其原始账户中资金不足。由此，我们可以构建一个推理，即欺诈者比普通人更频繁地试图将银行账户中的所有资金取光。
- en: We need this rationale to avoid the data trap. Once established, the rationale
    must be constantly scrutinized. In our case, it has failed to explain 10% of the
    odd transactions, and if this number rises, it could end up hurting the performance
    of our model in production.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要这个推理来避免数据陷阱。一旦建立，推理必须不断地被审视。在我们的案例中，它未能解释 10% 的异常交易，如果这个比例上升，可能会影响我们模型在生产中的表现。
- en: Preparing the data for the Keras library
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备 Keras 库的数据
- en: In [Chapter 1](ch01.xhtml "Chapter 1. Neural Networks and Gradient-Based Optimization"),
    *Neural Networks and Gradient-Based Optimization*, we saw that neural networks
    would only take numbers as inputs. The issue for us in our dataset is that not
    all of the information in our table is numbers, some of it is presented as characters.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第一章](ch01.xhtml "第一章：神经网络与基于梯度的优化")，*神经网络与基于梯度的优化*中，我们看到神经网络只接受数字作为输入。我们数据集中的问题是，表格中的并非所有信息都是数字，有些是以字符的形式呈现的。
- en: Therefore, in this section, we're going to work on preparing the data for Keras
    so that we can meaningfully work with it.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这一部分，我们将专注于为 Keras 准备数据，以便我们能够有意义地处理它。
- en: 'Before we start, let''s look at the three types of data, *Nominal*, *Ordinal*,
    and *Numerical*:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，先来看一下三种数据类型：*名义型*、*有序型*和*数值型*：
- en: '**Nominal data**: This comes in discrete categories that cannot be ordered.
    In our case, the type of transfer is a nominal variable. There are four discrete
    types, but it does not make sense to put them in any order. For instance, TRANSFER
    cannot be more than CASH_OUT, so instead, they are just separate categories.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**名义数据**：这种数据是离散的类别，不能进行排序。在我们的例子中，转账类型是一个名义变量。有四种离散类型，但没有意义对它们进行排序。例如，TRANSFER不能比CASH_OUT大，因此，它们只是独立的类别。'
- en: '**Ordinal data**: This also comes in discrete categories, but unlike nominal
    data, it can be ordered. For example, if coffee comes in large, medium, and small
    sizes, those are distinct categories because they can be compared. The large size
    contains more coffee than the small size.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序数据**：这种数据也有离散的类别，但与名义数据不同，它是可以排序的。例如，如果咖啡有大、中、小三种尺寸，这些是不同的类别，因为它们是可以比较的。大杯比小杯装的咖啡更多。'
- en: '**Numerical data**: This can be ordered, but we can also perform mathematical
    operations on it. An example in our data is the number of funds, as we can both
    compare the amounts, and also subtract or add them up.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值数据**：这种数据是可以排序的，而且我们还可以对其进行数学运算。例如，在我们的数据中，资金数量是一个数值数据，我们不仅可以比较数量，还可以进行加减操作。'
- en: Both nominal and ordinal data are **categorical data**, as they describe discrete
    categories. While numerical data works fine with neural networks, only out of
    the box, categorical data needs special treatment.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 名义数据和顺序数据都是**类别数据**，因为它们描述的是离散类别。虽然数值数据与神经网络的兼容性很好，但类别数据需要特别处理。
- en: One-hot encoding
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独热编码
- en: The most commonly used method to encode categorical data is called **one-hot
    encoding**. In one-hot encoding, we create a new variable, a so-called **dummy
    variable** for each category. We then set the dummy variable to 1 if the transaction
    is a member of a certain category and to zero otherwise.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 编码类别数据最常用的方法叫做**独热编码**。在独热编码中，我们为每个类别创建一个新的变量，即所谓的**虚拟变量**。然后，如果交易属于某个特定类别，我们就将虚拟变量设置为1，否则设置为0。
- en: 'An example of how we could apply this to our data set can be seen as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此方法应用到我们的数据集中的一个示例如下：
- en: 'So, this is what the categorical data would look like before one-hot encoding:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在进行独热编码之前，类别数据看起来是这样的：
- en: '| Transaction | Type |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Transaction | Type |'
- en: '| --- | --- |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | TRANSFER |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 1 | TRANSFER |'
- en: '| 2 | CASH_OUT |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 2 | CASH_OUT |'
- en: '| 3 | TRANSFER |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 3 | TRANSFER |'
- en: 'This is what the data would look like after one-hot encoding:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这是独热编码后数据的样子：
- en: '| Transaction | Type_TRANSFER | Type_CASH_OUT |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Transaction | Type_TRANSFER | Type_CASH_OUT |'
- en: '| --- | --- | --- |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 1 | 0 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 |'
- en: '| 2 | 0 | 1 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 1 |'
- en: '| 3 | 1 | 0 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 | 0 |'
- en: The Pandas software library offers a function that allows us to create dummy
    variables out of the box. Before doing so, however, it makes sense to add `Type_`
    in front of all actual transaction types. The dummy variables will be named after
    the category. By adding `Type_` to the beginning, we know that these dummy variables
    indicate the type.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas软件库提供了一个函数，允许我们直接创建虚拟变量。然而，在这样做之前，为所有实际的交易类型添加`Type_`前缀是有意义的。虚拟变量将以该类别命名。通过在前面添加`Type_`，我们知道这些虚拟变量表示的是类型。
- en: 'The following line of code does three things. Firstly, `df[''type''].astype(str)`
    converts all the entries in the **Type** column to strings. Secondly, the `Type_`
    prefix is added as a result of combining the strings. Thirdly, the new column
    of combined strings then replaces the original **Type** column:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码行做了三件事。首先，`df['type'].astype(str)`将**Type**列中的所有条目转换为字符串。其次，`Type_`前缀被添加到字符串中。第三，新的组合字符串列将替代原有的**Type**列：
- en: '[PRE15]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can now get the dummy variables by running the following code:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过运行以下代码获取虚拟变量：
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We should note that the `get_dummies()` function creates a new data frame.
    Next we attach this data frame to the main data frame, which can be done by running:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到，`get_dummies()`函数会创建一个新的数据框。接下来，我们将这个数据框附加到主数据框中，这可以通过运行以下代码来实现：
- en: '[PRE17]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `concat()` method, as seen in the preceding code, concatenates two data
    frames. We concatenate along axis 1 to add the data frame as new columns. Now
    that the dummy variables are in our main data frame, we can remove the original
    column by running this:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`concat()`方法，如前面的代码所示，将两个数据框连接起来。我们沿着轴1进行连接，将数据框作为新列添加。现在虚拟变量已经添加到主数据框中，我们可以通过运行以下代码删除原有列：'
- en: '[PRE18]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: And, voilà! We have turned our categorical variable into something a neural
    network will be able to work with.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们已经将类别变量转换为神经网络可以使用的形式。
- en: Entity embeddings
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实体嵌入
- en: In this section, we're going to walk through making use of both embeddings and
    the Keras functional API, showing you the general workflow. Both of these topics
    get introduced and explored fully in [Chapter 5](ch05.xhtml "Chapter 5. Parsing
    Textual Data with Natural Language Processing"), *Parsing Textual Data with Natural
    Language Processing*, where we will go beyond the general ideas presented here
    and where we'll begin discussing topics like implementation.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何同时使用嵌入和 Keras 函数式 API，向您展示一般的工作流程。关于这两个主题，我们在[第5章](ch05.xhtml "第5章.
    使用自然语言处理解析文本数据")，*使用自然语言处理解析文本数据*中会做详细介绍，在那里我们将超越这里呈现的基本概念，并开始讨论像实现这样的话题。
- en: It's fine if you do not understand everything that is going on just now; this
    is an advanced section after all. If you want to use both of these techniques,
    you will be well prepared after reading this book, as we explain different elements
    of both methods throughout the book.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在还不完全理解所有的内容也没关系；毕竟这是一个高级章节。如果你希望使用这两种技术，在阅读完本书后，你将为使用它们做好充分的准备，因为我们在书中详细解释了这两种方法的不同要素。
- en: In this section, we will be creating embedding vectors for categorical data.
    Before we start, we need to understand that embedding vectors are vectors representing
    categorical values. We use embedding vectors as inputs for neural networks. We
    train embeddings together with a neural network, so that we can, over time, obtain
    more useful embeddings. Embeddings are an extremely useful tool to have at our
    disposal.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为类别数据创建嵌入向量。在开始之前，我们需要理解嵌入向量是表示类别值的向量。我们将嵌入向量作为神经网络的输入。我们与神经网络一起训练嵌入向量，这样随着时间的推移，我们可以获得更有用的嵌入。嵌入是我们手头非常有用的工具。
- en: Why are embeddings so useful? Not only do embeddings reduce the number of dimensions
    needed for encoding over one-hot encoding and thus decrease memory usage, but
    they also reduce sparsity in input activations, which helps reduce overfitting,
    and they can encode semantic meanings as vectors. The same advantages that made
    embeddings useful for text, [Chapter 5](ch05.xhtml "Chapter 5. Parsing Textual
    Data with Natural Language Processing"), *Parsing Textual Data with Natural Language
    Processing*, also make them useful for categorical data.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么嵌入如此有用？嵌入不仅减少了与独热编码相比所需的编码维度数量，从而减少了内存使用，而且它们还减少了输入激活的稀疏性，这有助于减少过拟合，并且它们能够将语义含义编码为向量。正是这些优势使得嵌入对于文本非常有用，[第5章](ch05.xhtml
    "第5章. 使用自然语言处理解析文本数据")，*使用自然语言处理解析文本数据*中同样让它们对于类别数据非常有用。
- en: Tokenizing categories
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类别的分词
- en: 'Just as with text, we have to tokenize the inputs before feeding them into
    the embeddings layer. To do this, we have to create a mapping dictionary that
    maps categories to a token. We can achieve this by running:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与文本一样，我们必须在将输入数据传递到嵌入层之前进行分词处理。为此，我们需要创建一个映射字典，将类别映射到令牌。我们可以通过运行以下代码来实现：
- en: '[PRE19]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This code loops over all the unique type categories while counting upward.
    The first category gets token 0, the second 1, and so on. Our `map_dict` looks
    like this:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码会遍历所有独特的类别类型，并按顺序递增计数。第一个类别得到令牌0，第二个得到令牌1，依此类推。我们的`map_dict`看起来是这样的：
- en: '[PRE20]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can now apply this mapping to our data frame:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将此映射应用于我们的数据框：
- en: '[PRE21]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As a result, all types will now be replaced by their tokens.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，所有类型现在将被它们的令牌替代。
- en: 'We have to deal with the non-categorical values in our data frame separately.
    We can create a list of columns that are not the type and not the target like
    this:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要单独处理数据框中的非类别值。我们可以像这样创建一个不属于类型且不是目标的列列表：
- en: '[PRE22]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Creating input models
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建输入模型
- en: 'The model we are creating will have two inputs: one for the types with an embedding
    layer, and one for all other, non-categorical variables. To combine them with
    more ease at a later point, we''re going to keep track of their inputs and outputs
    with two arrays:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在创建的模型将有两个输入：一个是带有嵌入层的类型输入，另一个是所有其他非类别变量输入。为了便于以后将它们组合，我们将使用两个数组跟踪它们的输入和输出：
- en: '[PRE23]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The model that acts as an input for the type receives a one-dimensional input
    and parses it through an embedding layer. The outputs of the embedding layer are
    then reshaped into flat arrays, as we can see in this code:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 作为类型输入的模型接收一维输入，并通过嵌入层进行解析。嵌入层的输出随后会被重塑为平坦数组，正如我们在以下代码中看到的那样：
- en: '[PRE24]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `type` embeddings have three layers here. This is an arbitrary choice, and
    experimentation with different numbers of dimensions could improve the results.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`type`嵌入层有三层。这是一个任意的选择，尝试不同维度的数量可能会提高结果。'
- en: 'For all the other inputs, we create another input that has as many dimensions
    as there are non-categorical variables and consists of a single dense layer with
    no activation function. The dense layer is optional; the inputs could also be
    directly passed into the head model. More layers could also be added, including
    these:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有其他输入，我们创建一个新的输入，该输入的维度与非类别变量的数量相同，并由一个没有激活函数的单一全连接层组成。全连接层是可选的；输入也可以直接传递到头模型中。也可以添加更多的层，包括以下几种：
- en: '[PRE25]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now that we have created the two input models, we can concatenate them. On
    top of the two concatenated inputs, we will also build our head model. To begin
    this process, we must first run the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经创建了两个输入模型，我们可以将它们连接起来。在这两个连接的输入上，我们还将构建我们的头模型。为了开始这个过程，我们首先必须运行以下代码：
- en: '[PRE26]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, by running the following code, we can build and compile the overall model:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过运行以下代码，我们可以构建并编译整个模型：
- en: '[PRE27]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Training the model
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'In this section we''re going to train a model with multiple inputs. To do this,
    we need to provide a list of *X* values for each input. So, firstly we must split
    up our data frame. We can do this by running the following code:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将训练一个具有多个输入的模型。为此，我们需要为每个输入提供一个*X*值的列表。因此，首先我们必须拆分我们的数据框。我们可以通过运行以下代码来实现：
- en: '[PRE28]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, we can train the model by providing a list of the two inputs and the
    target, as we can see in the following code:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过提供两个输入和目标的列表来训练模型，正如我们在以下代码中所看到的：
- en: '[PRE29]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Creating predictive models with Keras
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 创建预测模型
- en: 'Our data now contains the following columns:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据现在包含以下列：
- en: '[PRE31]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now that we've got the columns, our data is prepared, and we can use it to create
    a model.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了列，我们的数据已经准备好了，可以用它来创建一个模型。
- en: Extracting the target
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取目标
- en: 'To train the model, a neural network needs a target. In our case, `isFraud`
    is the target, so we have to separate it from the rest of the data. We can do
    this by running:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，神经网络需要一个目标。在我们的案例中，`isFraud`就是目标，因此我们必须将其从其他数据中分离出来。我们可以通过运行以下代码来实现：
- en: '[PRE32]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The first step only returns the `isFraud` column and assigns it to `y_df`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步只返回`isFraud`列，并将其赋值给`y_df`。
- en: The second step returns all columns except `isFraud` and assigns them to `x_df`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步返回除了`isFraud`之外的所有列，并将其赋值给`x_df`。
- en: 'We also need to convert our data from a pandas `DataFrame` to NumPy arrays.
    The pandas `DataFrame` is built on top of NumPy arrays but comes with lots of
    extra bells and whistles that make all the preprocessing we did earlier possible.
    To train a neural network, however, we just need the underlying data, which we
    can get by simply running the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要将数据从 pandas `DataFrame` 转换为 NumPy 数组。pandas `DataFrame` 是建立在 NumPy 数组之上的，但它包含许多额外的功能，使得我们之前的所有预处理工作得以实现。然而，要训练神经网络，我们只需要底层数据，我们可以通过简单地运行以下代码来获取：
- en: '[PRE33]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Creating a test set
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建测试集
- en: When we train our model, we run the risk of **overfitting**. Overfitting means
    that our model memorizes the *x* and *y* mapping in our training dataset but does
    not find the function that describes the true relationship between *x* and *y*.
    This is problematic because once we run our model **out of sample** – that is,
    on data not in our training set, it might do very poorly. To prevent this, we're
    going to create a so-called **test set**.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练模型时，我们面临**过拟合**的风险。过拟合意味着我们的模型记住了训练数据集中*x*和*y*之间的映射关系，但没有找到描述*x*和*y*之间真实关系的函数。这是有问题的，因为一旦我们将模型应用到**样本外**的数据上，即在训练集之外的数据，它可能会表现得非常差。为了防止这种情况发生，我们将创建一个所谓的**测试集**。
- en: 'A test set is a holdout dataset, which we only use to evaluate our model once
    we think it is doing fairly well in order to see how well it performs on data
    it has not seen yet. A test set is usually randomly sampled from the complete
    data. Scikit-learn offers a convenient function to do this, as we can see in the
    following code:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集是一个保留数据集，我们只在认为模型已经表现得相当不错时，用它来评估模型的表现，以查看模型在尚未见过的数据上的表现如何。测试集通常是从完整数据中随机抽样得到的。Scikit-learn
    提供了一个方便的函数来执行这一操作，正如我们在以下代码中所看到的：
- en: '[PRE34]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The element, `train_test_split` will randomly assign rows to either the train
    or test set. You can specify `test_size`, the share of data that goes into the
    test set (which in our case is 33%), as well as a random state. Assigning `random_state`
    makes sure that although the process is pseudo-random, it will always return the
    same split, which makes our work more reproducible. Note that the actual choice
    of number (for example, `42`) does not really matter. What matters is that the
    same number is used in all experiments.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 元素`train_test_split`会随机地将行分配到训练集或测试集中。你可以指定`test_size`，即进入测试集的数据比例（在我们的例子中是33%），以及随机状态。指定`random_state`可以确保尽管过程是伪随机的，但它总是返回相同的拆分，这使得我们的工作更具可重现性。请注意，数字的实际选择（例如`42`）并不重要。重要的是在所有实验中使用相同的数字。
- en: Creating a validation set
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建验证集
- en: 'Now you might be tempted to just try out a lot of different models until you
    get a really high performance on the test set. However, ask yourself this: how
    would you know that you have not selected a model that by chance works well on
    the test set but does not work in real life?'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能会忍不住尝试不同的模型，直到在测试集上获得非常高的表现。然而，问问自己：你怎么知道你没有选择一个偶然在测试集上表现良好，但在实际生活中无法应用的模型呢？
- en: The answer is that every time you evaluate on the test set, you incur a bit
    of "information leakage," that is, information from the test set leaks into your
    model by influencing your choice of model. Gradually, the test set becomes less
    valuable. The validation set is a sort of a "dirty test set" that you can use
    to frequently test your models out of sample performance without worrying. Though
    it's key to note that we don't want to use the test set too often, but it is still
    used to measure out-of-sample performance frequently.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是，每次你在测试集上进行评估时，你都会产生一点“信息泄露”，也就是说，测试集的信息通过影响你选择的模型泄漏到模型中。逐渐地，测试集的价值会降低。验证集是一种“脏测试集”，你可以用它频繁地测试模型的外部样本表现，而不必担心。尽管需要注意的是，我们不希望过于频繁地使用测试集，但它仍然会被用来衡量外部样本表现。
- en: To this end, we'll create a "validation set," also known as a development set.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将创建一个“验证集”，也叫做开发集。
- en: 'We can do this the same way we created the test set, by just splitting the
    training data again, as we can see in the following code:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像创建测试集一样通过再次拆分训练数据来完成此操作，如下代码所示：
- en: '[PRE35]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Oversampling the training data
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过采样训练数据
- en: Remember that in our dataset, only a tiny fraction of transactions were fraudulent,
    and that a model that is always classifying transactions as genuine would have
    a very high level of accuracy. To make sure we train our model on true relationships,
    we can **oversample** the training data.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在我们的数据集中，只有极小一部分交易是欺诈性的，而一个总是将交易分类为真实交易的模型将会有非常高的准确率。为了确保我们在真实关系上训练我们的模型，我们可以**过采样**训练数据。
- en: This means that we would add data that would be fraudulent to our dataset until
    we have the same amount of fraudulent transactions as genuine transactions.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们会将一些欺诈性数据添加到我们的数据集中，直到我们拥有与真实交易相等数量的欺诈交易。
- en: Note
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: A useful library for this kind of task is `imblearn`, which includes
    a SMOTE function. See, [http://contrib.scikitlearn.org/imbalanced-learn/](http://contrib.scikitlearn.org/imbalanced-learn/).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：一个用于这种任务的有用库是`imblearn`，它包含了SMOTE函数。请参见，[http://contrib.scikitlearn.org/imbalanced-learn/](http://contrib.scikitlearn.org/imbalanced-learn/)。'
- en: '**Synthetic Minority Over-sampling Technique** (**SMOTE**) is a clever way
    of oversampling. This method tries to create new samples while maintaining the
    same decision boundaries for the classes. We can oversample with SMOTE by simply
    running:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**合成少数类过采样技术**（**SMOTE**）是一种巧妙的过采样方法。这种方法试图在保持类别决策边界不变的情况下创建新样本。我们可以通过运行以下代码轻松地使用SMOTE进行过采样：'
- en: '[PRE36]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Building the model
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'We''ve successfully addressed several key learning points, and so it''s now
    finally time to build a neural network! As in [Chapter 1](ch01.xhtml "Chapter 1. Neural
    Networks and Gradient-Based Optimization"), *Neural Networks and Gradient-Based
    Optimization*, we need to import the required Keras modules using the following
    code:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功解决了几个关键的学习点，所以现在终于是时候构建神经网络了！正如在[第1章](ch01.xhtml "第1章：神经网络与基于梯度的优化")中所述，*神经网络与基于梯度的优化*，我们需要使用以下代码导入所需的Keras模块：
- en: '[PRE37]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In practice, many structured data problems require very low learning rates.
    To set the learning rate for the gradient descent optimizer, we also need to import
    the optimizer. We can do this by running:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，许多结构化数据问题需要非常低的学习率。为了设置梯度下降优化器的学习率，我们还需要导入优化器。我们可以通过运行以下代码实现：
- en: '[PRE38]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Creating a simple baseline
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个简单的基准模型
- en: Before we dive into more advanced models, it is wise to start with a simple
    logistic regression baseline. This is to make sure that our model can actually
    train successfully.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究更复杂的模型之前，最好从一个简单的逻辑回归基准开始。这是为了确保我们的模型能够成功训练。
- en: 'To create a simple baseline, we need to run the following code:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个简单的基准模型，我们需要运行以下代码：
- en: '[PRE39]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You can see here a logistic regressor, which is the same as a one-layer neural
    network:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这里的逻辑回归模型，它与单层神经网络是相同的：
- en: '[PRE40]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Here, we will compile the model. Instead of just passing `SGD` to specify the
    optimizer for Stochastic Gradient Descent, we''ll create a custom instance of
    SGD in which we set the learning rate to 0.00001\. In this example, tracking accuracy
    is not needed since we evaluate our models using the F1 score. Still, it still
    reveals some interesting behavior, as you can see in the following code:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将编译模型。我们不仅仅是传递`SGD`来指定随机梯度下降（Stochastic Gradient Descent）的优化器，而是创建一个自定义的SGD实例，在其中将学习率设置为0.00001。在这个例子中，由于我们使用F1分数来评估模型，所以不需要跟踪准确率。尽管如此，它仍然揭示了一些有趣的行为，正如你在以下代码中看到的：
- en: '[PRE41]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Notice how we have passed the validation data into Keras by creating a tuple
    in which we store data and labels. We will train this model for 5 epochs:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们通过创建一个元组将验证数据传递给Keras，在其中存储数据和标签。我们将训练这个模型5个周期：
- en: '[PRE42]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Notice a few things here: first, we have trained on about 3.3 million samples,
    which is more data than we initially had. The sudden increase comes from the oversampling
    that we did earlier on in this chapter. Secondly, the training set''s accuracy
    is significantly lower than the validation set''s accuracy. This is because the
    training set is balanced, while the validation set is not.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里注意几点：首先，我们训练了大约330万样本，这比我们最初拥有的数据还要多。这个突然的增加来自于我们在本章早些时候进行的过采样。其次，训练集的准确率明显低于验证集的准确率。这是因为训练集是平衡的，而验证集则不是。
- en: We oversampled the data by adding more fraud cases to the training set than
    there are in real life, which as we discussed, helped our model detect fraud better.
    If we did not oversample, our model would be inclined to classify all transactions
    as genuine since the vast majority of samples in the training set are genuine.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过向训练集添加更多的欺诈案例来对数据进行过采样，欺诈案例的数量比实际生活中更多，正如我们之前讨论的，这有助于我们的模型更好地检测欺诈。如果我们没有进行过采样，我们的模型可能会倾向于将所有交易分类为真实的，因为训练集中的大多数样本都是正常的。
- en: By adding fraud cases, we are forcing the model to learn what distinguishes
    a fraud case. Yet, we want to validate our model on realistic data. Therefore,
    our validation set does not artificially contain many fraud cases.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加欺诈案例，我们迫使模型学习欺诈案例的区别。然而，我们希望在真实数据上验证我们的模型。因此，我们的验证集不会人为地包含许多欺诈案例。
- en: A model classifying everything as genuine would have over 99% accuracy on the
    validation set, but just 50% accuracy on the training set. Accuracy is a flawed
    metric for such imbalanced datasets. It is a half-decent proxy and more interpretable
    than just a loss, which is why we keep track of it in Keras.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将所有样本分类为真实的模型在验证集上的准确率将超过99%，但在训练集上的准确率只有50%。准确率对于这样不平衡的数据集来说是一个有缺陷的指标。它是一个相对不错的代理，并且比仅仅使用损失函数更具可解释性，这就是为什么我们在Keras中仍然追踪准确率的原因。
- en: To evaluate our model, we should use the F1 score that we discussed at the beginning
    of this chapter. However, Keras is unable to directly track the F1 score in training
    since the calculation of an F1 score is somewhat slow and would end up slowing
    down the training of our model.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的模型，我们应该使用本章开头讨论的F1分数。然而，由于计算F1分数的过程相对较慢，并且会导致训练速度变慢，Keras无法在训练过程中直接跟踪F1分数。
- en: Note
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: Remember that accuracy on an imbalanced dataset can be very high,
    even if the model is performing poorly.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：请记住，在不平衡数据集上的准确率可能非常高，即使模型表现不佳。'
- en: If the model exhibits a higher degree of accuracy on an imbalanced validation
    set than compared to that seen with a balanced training set, then it says little
    about the model performing well.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型在不平衡验证集上的准确度高于在平衡训练集上的准确度，那么这并不能说明模型表现良好。
- en: Compare the training set's performance against the previous training set's performance,
    and likewise the validation set's performance against the previous validation
    set's performance. However, be careful when comparing the training set's performance
    to that of the validation set's performance on highly imbalanced data. However,
    if your data is equally balanced, then comparing the validation set and the training
    set is a good way to gauge overfitting.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 比较训练集的表现与之前训练集的表现，验证集的表现与之前验证集的表现。同样，比较训练集和验证集的表现时要小心，尤其是在数据极度不平衡的情况下。然而，如果数据是平衡的，那么比较训练集和验证集的表现是判断过拟合的好方法。
- en: 'We are now in a position where we can make predictions on our test set in order
    to evaluate the baseline. We start by using `model.predict` to make predictions
    on the test set:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以对测试集进行预测，以评估基准模型。我们首先使用 `model.predict` 对测试集进行预测：
- en: '[PRE43]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Before evaluating our baseline, we need to turn the probabilities given by
    our model into absolute predictions. In our example, we''ll classify everything
    that has a fraud probability above 50% as fraud. To do this, we need to run the
    following code:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估我们的基准模型之前，我们需要将模型给出的概率转换为绝对预测。在我们的示例中，我们将所有欺诈概率超过 50% 的内容分类为欺诈。为此，我们需要运行以下代码：
- en: '[PRE44]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Our F1 score is already significantly better than it was for the heuristic
    model, which if you go back, you''ll see that it only achieved a rate of 0.013131315551742895:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 F1 分数已经比启发式模型好得多，如果你回头看，会发现它仅获得了 0.013131315551742895 的分数：
- en: '[PRE45]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'By plotting the confusion matrix, we''re able to see that our feature-based
    model has indeed improved on the heuristic model:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制混淆矩阵，我们可以看到基于特征的模型确实比启发式模型有所改进：
- en: '[PRE47]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This code should produce the following confusion matrix:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码应该会生成以下混淆矩阵：
- en: '![Creating a simple baseline](img/B10354_02_05.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![创建一个简单的基准模型](img/B10354_02_05.jpg)'
- en: A confusion matrix for a simple Keras model
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单 Keras 模型的混淆矩阵
- en: But what if we wanted to build more complex models that can express more subtle
    relationships, than the one that we've just built? Let's now do that!
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们想构建能够表达更微妙关系的复杂模型，而不是刚才构建的那个呢？现在我们来做这个吧！
- en: Building more complex models
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建更复杂的模型
- en: 'After we have created a simple baseline, we can go on to more complex models.
    The following code is an example of a two-layer network:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了简单的基准模型之后，我们可以继续构建更复杂的模型。以下代码是一个二层网络的示例：
- en: '[PRE48]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'After running that code, we''ll then again benchmark with the F1 score:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完那段代码后，我们将再次使用 F1 分数进行基准测试：
- en: '[PRE49]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In this case, the more complex model does better than the simple baseline created
    earlier. It seems as though the function mapping transaction data to fraud is
    complex and can be approximated better by a deeper network.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，复杂模型的表现优于之前创建的简单基准模型。看起来将交易数据映射到欺诈的函数是复杂的，且可以通过更深的网络来更好地逼近。
- en: In this section we have built and evaluated both simple and complex neural network
    models for fraud detection. We have been careful to use the validation set to
    gauge the initial out-of-sample performance.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们构建并评估了简单和复杂的神经网络模型用于欺诈检测。我们小心地使用验证集来衡量初始的样本外表现。
- en: 'With all of that, we can build much more complex neural networks (and we will).
    But first we will have a look at the workhorse of modern enterprise-ready machine
    learning: tree-based methods.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们可以构建更复杂的神经网络（我们会这样做）。但首先，我们将看看现代企业级机器学习的核心：基于树的方法。
- en: A brief primer on tree-based methods
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于树的方法简要介绍
- en: No chapter on structured data would be complete without mentioning tree-based
    methods, such as random forests or XGBoost.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 任何关于结构化数据的章节，如果没有提到基于树的方法，比如随机森林或 XGBoost，都不算完整。
- en: It is worth knowing about them because, in the realm of predictive modeling
    for structured data, tree-based methods are very successful. However, they do
    not perform as well on more advanced tasks, such as image recognition or sequence-to-sequence
    modeling. This is the reason why the rest of the book does not deal with tree-based
    methods.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这些方法是很有价值的，因为在结构化数据的预测建模领域，基于树的方法非常成功。然而，它们在更高级的任务中表现不佳，比如图像识别或序列到序列的建模。这也是为什么本书其余部分没有涉及基于树的方法的原因。
- en: Note
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: For a deeper dive into XGBoost, check out the tutorials on the XGBoost
    documentation page: [http://xgboost.readthedocs.io](http://xgboost.readthedocs.io).
    There is a nice explanation of how tree-based methods and gradient boosting work
    in theory and practice under the **Tutorials** section of the website.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：如果你想深入了解 XGBoost，可以查看 XGBoost 文档页面上的教程：[http://xgboost.readthedocs.io](http://xgboost.readthedocs.io)。在网站的**教程**部分，有关于树形方法和梯度提升如何在理论和实践中工作的详细解释。'
- en: A simple decision tree
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个简单的决策树
- en: The basic idea behind tree-based methods is the decision tree. A decision tree
    splits up data to create the maximum difference in outcomes.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的算法的基本理念就是决策树。决策树通过将数据分割开来，最大化结果之间的差异。
- en: Let's assume for a second that our `isNight` feature is the greatest predictor
    of fraud. A decision tree would split our dataset according to whether the transactions
    happened at night or not. It would look at all the night-time transactions, looking
    for the next best predictor of fraud, and it would do the same for all day-time
    transactions.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一下，我们的`isNight`特征是预测欺诈的最佳特征。决策树会根据交易是否发生在夜间来分割我们的数据集。它会查看所有夜间的交易，寻找下一个最好的欺诈预测特征，然后对所有白天的交易做同样的处理。
- en: 'Scikit-learn has a handy decision tree module. We can create one for our data
    by simply running the following code:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 提供了一个方便的决策树模块。我们可以通过运行以下代码来为我们的数据创建一个决策树：
- en: '[PRE51]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The resulting tree will look like this:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 结果树将是这样的：
- en: '![A simple decision tree](img/B10354_02_06.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![一个简单的决策树](img/B10354_02_06.jpg)'
- en: A decision tree for fraud detection
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 用于欺诈检测的决策树
- en: Simple decision trees, like the one we've produced, can give a lot of insight
    into data. For example, in our decision tree, the most important feature seems
    to be the old balance of the origin account, given that it is the first node in
    the tree.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 像我们创建的简单决策树，可以为数据提供大量的洞察。例如，在我们的决策树中，最重要的特征似乎是原始账户的旧余额，因为它是树中的第一个节点。
- en: A random forest
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个随机森林
- en: A more advanced version of a simple decision tree is a random forest, which
    is a collection of decision trees. A forest is trained by taking subsets of the
    training data and training decision trees on those subsets.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 简单决策树的一个更高级版本是随机森林，随机森林是决策树的集合。通过从训练数据中取子集并在这些子集上训练决策树来训练森林。
- en: Often, those subsets do not include every feature of the training data. By doing
    it this way, the different decision trees can fit different aspects of the data
    and capture more information on aggregate. After a number of trees have been created,
    their predictions are averaged to create the final prediction.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些子集并不包括训练数据的每个特征。通过这种方式，不同的决策树可以拟合数据的不同方面，从而整体上捕获更多的信息。创建了足够多的树之后，它们的预测结果会被平均化，最终得出预测结果。
- en: 'The idea is that the errors presented by the trees are not correlated, and
    so by using multiple trees you cancel out the error. You can create and train
    a random forest classifier like this:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这个理念是，树形结构所呈现的错误是无关的，因此通过使用多棵树，你可以抵消这些错误。你可以像这样创建和训练一个随机森林分类器：
- en: '[PRE52]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: You'll notice that with the code we've just generated, random forests have far
    fewer knobs to tune than neural networks. In this case, we just specify the number
    of estimators, that is, the number of trees we would like our forest to have.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，在我们刚刚生成的代码中，随机森林比神经网络需要调节的参数要少得多。在这种情况下，我们只需要指定估计器的数量，也就是我们希望森林中包含的树的数量。
- en: 'The `n_jobs` argument tells the random forest how many trees we would like
    to train in parallel. Note that `-1` stands for "as many as there are CPU cores":'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_jobs`参数告诉随机森林我们希望并行训练多少棵树。请注意，`-1`表示“使用所有 CPU 核心”：'
- en: '[PRE53]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The random forest does an order of magnitude better than the neural network
    as its F1 score is close to 1, which is the maximum score. Its confusion plot,
    seen as follows, shows that the random forest significantly reduced the number
    of false positives:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 F1 分数接近 1（这是最高分），随机森林的表现比神经网络好一个数量级。其混淆图如下所示，显示随机森林显著减少了假阳性的数量：
- en: '![A random forest](img/B10354_02_07.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![一个随机森林](img/B10354_02_07.jpg)'
- en: A confusion matrix for the random forest
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的混淆矩阵
- en: A shallow learning approach, such as a random forest, often does better than
    deep learning on relatively simple problems. The reason for this is that simple
    relationships with low-dimensional data can be hard to learn for a deep learning
    model, which has to fit multiple parameters exactly in order to match the simple
    function.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 浅层学习方法，如随机森林，通常在相对简单的问题上表现得比深度学习更好。原因在于，简单的低维数据关系对于深度学习模型来说可能很难学习，因为深度学习模型需要精确拟合多个参数，才能匹配简单的函数。
- en: As we will see in later chapters of this book, as soon as relationships do get
    more complex, deep learning gets to shine.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本书后续章节中看到的那样，一旦关系变得更加复杂，深度学习就能展现出它的优势。
- en: XGBoost
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost
- en: '**XGBoost** stands for **eXtreme Gradient Boosting**. The idea behind gradient
    boosting is to train a decision tree, and then to train a second decision tree
    on the errors that the first decision tree made.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '**XGBoost**代表**极限梯度提升**。梯度提升的核心思想是先训练一个决策树，然后基于第一个决策树的错误训练第二个决策树。'
- en: Through this method, multiple layers of decision trees can be added, which slowly
    reduces the total number of model errors. XGBoost is a popular library that implements
    gradient boosting very efficiently.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方法，可以添加多个层次的决策树，这样可以逐渐减少模型的总错误数。XGBoost是一个流行的库，它非常高效地实现了梯度提升。
- en: Note
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: XGBoost is installed on Kaggle kernels by default. If you are running
    these examples locally, see the XGBoost manual for installation instructions and
    more information: [http://xgboost.readthedocs.io/](http://xgboost.readthedocs.io/).'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：XGBoost默认已安装在Kaggle的内核环境中。如果你在本地运行这些示例，请参阅XGBoost手册以获取安装说明和更多信息：[http://xgboost.readthedocs.io/](http://xgboost.readthedocs.io/)。'
- en: 'Gradient boosting classifiers can be created and trained just like random forests
    from `sklearn`, as can be seen in the following code:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升分类器可以像`sklearn`中的随机森林一样创建和训练，以下代码可以看到这一点：
- en: '[PRE55]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The gradient booster performs at almost the same level as a random forest on
    this task. A common approach that is used is to take both a random forest and
    a gradient booster and to average the predictions in order to get an even better
    model.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项任务中，梯度提升器的表现几乎与随机森林相同。一种常用的方法是同时使用随机森林和梯度提升器，然后对预测结果进行平均，以获得更好的模型。
- en: The bulk of machine learning jobs in business today are done on relatively simple structured
    data. The methods we have learned today, random forests and gradient boosting,
    are therefore the standard tools that most practitioners use in the real world.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 当今大多数商业中的机器学习工作都基于相对简单的结构化数据。我们今天学到的方法，随机森林和梯度提升，因此是大多数从业者在现实世界中使用的标准工具。
- en: In most enterprise machine learning applications, value creation does not come
    from carefully tweaking a model or coming up with cool architectures, but from
    massaging data and creating good features. However, as tasks get more complex
    and more semantic understanding of unstructured data is needed, these tools begin to
    fail.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数企业机器学习应用中，价值的创造并不来自精心调整模型或设计出酷炫的架构，而是来自于调整数据和创建良好的特征。然而，随着任务变得更加复杂并且对非结构化数据的语义理解需求增加时，这些工具开始失效。
- en: E2E modeling
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: E2E建模
- en: Our current approach relies on engineered features. As we discussed at the start
    of this chapter, an alternative method is E2E modeling. In E2E modeling, both
    raw and unstructured data about a transaction is used. This could include the
    description text of a transfer, video feeds from cameras monitoring a cash machine,
    or other sources of data. E2E is often more successful than feature engineering,
    provided that you have enough data available.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的方法依赖于工程化特征。正如我们在本章开始时讨论的，另一种方法是E2E建模。在E2E建模中，使用的是关于事务的原始和非结构化数据。这可能包括转账的描述文本、监控现金机的摄像头视频流或其他数据来源。只要有足够的数据，E2E通常比特征工程更成功。
- en: To get valid results, and to successfully train the data with an E2E model it
    can take millions of examples. Yet, often this is the only way to gain an acceptable
    result, especially when it is hard to codify the rules for something. Humans can
    recognize things in images well, but it is hard to come up with exact rules that
    distinguish things, which is where E2E shines.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得有效的结果，并成功地用E2E模型训练数据，可能需要数百万个示例。然而，这通常是获得可接受结果的唯一方法，特别是当某些内容很难用规则编码时。人类可以很好地识别图像中的事物，但很难提出明确的规则来区分这些事物，这正是E2E的优势所在。
- en: In the dataset used for this chapter, we do not have access to more data, but
    the rest of the chapters of this book demonstrate various E2E models.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章使用的数据集里，我们无法访问更多的数据，但本书的其他章节展示了各种端到端（E2E）模型。
- en: Exercises
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: If you visit [https://kaggle.com](https://kaggle.com), search for a competition
    that has structured data. One example is the Titanic competition. Here you can
    create a new kernel, do some feature engineering, and try to build a predictive
    model.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你访问[https://kaggle.com](https://kaggle.com)，搜索一个包含结构化数据的竞赛。例如，泰坦尼克号竞赛就是一个例子。在这里，你可以创建一个新的内核，进行特征工程，并尝试构建一个预测模型。
- en: How much can you improve it by investing time in feature engineering versus
    model tweaking? Is there an E2E approach to the problem?
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在特征工程和模型调优上投资时间，你能提高多少性能？是否有一种端到端（E2E）的方法来解决这个问题？
- en: Summary
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have taken a structured data problem from raw data to strong
    and reliable predictive models. We have learned about heuristic, feature engineering,
    and E2E modeling. We have also seen the value of clear evaluation metrics and
    baselines.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将一个结构化数据问题从原始数据处理到强大且可靠的预测模型。我们学习了启发式方法、特征工程和端到端（E2E）建模。我们还了解了清晰的评估指标和基准的重要性。
- en: In the next chapter, we will look into a field where deep learning truly shines,
    computer vision. Here, we will discover the computer vision pipeline, from working
    with simple models to very deep networks augmented with powerful preprocessing
    software. The ability to "see" empowers computers to enter completely new domains.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨深度学习真正发挥作用的领域——计算机视觉。在这里，我们将发现计算机视觉的工作流程，从使用简单模型到深度神经网络，并辅以强大的预处理软件。计算机“看见”的能力使得它们能够进入全新的领域。
