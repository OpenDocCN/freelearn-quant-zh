- en: '5'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '5'
- en: Quantum Boltzmann Machine
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­ç»å°”å…¹æ›¼æœº
- en: 'As we saw in ChaptersÂ [3](Chapter_3.xhtml#x1-630003) andÂ [4](Chapter_4.xhtml#x1-820004),
    quantum annealing can be used to solve hard optimisation problems. However, the
    range of possible applications of quantum annealing is much wider than that. In
    this chapter, we will consider two distinct but related use cases that go beyond
    solving optimisation problems: sampling and training deep neural networks. Specifically,
    we will focus on the Quantum Boltzmann Machine (QBM) â€“ a generative model that
    is a direct quantum annealing counterpart of the classical Restricted Boltzmann
    Machine (RBM), and the Deep Boltzmann Machine (DBM) â€“ a class of deep neural networks
    composed of multiple layers of latent variables with connections between the layers
    but not between units within each layer.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬[3](Chapter_3.xhtml#x1-630003)ç« å’Œç¬¬[4](Chapter_4.xhtml#x1-820004)ç« ä¸­çœ‹åˆ°çš„ï¼Œé‡å­é€€ç«å¯ä»¥ç”¨æ¥è§£å†³éš¾åº¦è¾ƒå¤§çš„ä¼˜åŒ–é—®é¢˜ã€‚ç„¶è€Œï¼Œé‡å­é€€ç«çš„åº”ç”¨èŒƒå›´è¿œä¸æ­¢äºæ­¤ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸¤ä¸ªä¸åŒä½†ç›¸å…³çš„åº”ç”¨æ¡ˆä¾‹ï¼Œè¿™äº›åº”ç”¨è¶…å‡ºäº†ä¼˜åŒ–é—®é¢˜çš„è§£å†³ï¼šé‡‡æ ·å’Œè®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†é‡ç‚¹ä»‹ç»é‡å­ç»å°”å…¹æ›¼æœºï¼ˆQBMï¼‰â€”â€”ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ç»å…¸é™åˆ¶ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰å’Œæ·±åº¦ç»å°”å…¹æ›¼æœºï¼ˆDBMï¼‰çš„ç›´æ¥é‡å­é€€ç«å¯¹åº”ç‰©â€”â€”ä»¥åŠæ·±åº¦ç»å°”å…¹æ›¼æœºï¼ˆDBMï¼‰â€”â€”ä¸€ç§ç”±å¤šå±‚æ½œåœ¨å˜é‡ç»„æˆçš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå±‚ä¸å±‚ä¹‹é—´æœ‰è¿æ¥ï¼Œä½†å±‚å†…å•å…ƒä¹‹é—´æ²¡æœ‰è¿æ¥ã€‚
- en: We start by providing detailed descriptions of the classical RBM, including
    the corresponding training algorithm. Due to the fact that an RBM operates on
    stochastic binary activation units, one can establish the correspondence between
    the RBM graph and the QUBO graph embedded onto the quantum chip. This provides
    the main motivation for performing Boltzmann sampling (the key stage in training
    RBMs and DBMs) using quantum annealing.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆæä¾›ç»å…¸RBMçš„è¯¦ç»†æè¿°ï¼ŒåŒ…æ‹¬ç›¸åº”çš„è®­ç»ƒç®—æ³•ã€‚ç”±äºRBMåœ¨éšæœºäºŒè¿›åˆ¶æ¿€æ´»å•å…ƒä¸Šæ“ä½œï¼Œå› æ­¤å¯ä»¥å»ºç«‹RBMå›¾ä¸åµŒå…¥åˆ°é‡å­èŠ¯ç‰‡ä¸Šçš„QUBOå›¾ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚è¿™ä¸ºä½¿ç”¨é‡å­é€€ç«è¿›è¡Œç»å°”å…¹æ›¼é‡‡æ ·ï¼ˆRBMå’ŒDBMè®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®é˜¶æ®µï¼‰æä¾›äº†ä¸»è¦åŠ¨åŠ›ã€‚
- en: DBMs can be trained as both generative and discriminative models. In both cases,
    since a DBM can be constructed by stacking together layers of RBMs, efficient
    Boltzmann sampling is the key element of the training process. Quantum annealing,
    which can be integrated into the hybrid quantum-classical training routine, has
    the potential to improve speed and accuracy. Quantum speedup is an especially
    appealing element of the envisaged quantum advantage since it can be achieved
    not only during the RBM training stage but also during the process of generating
    new samples.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: DBMå¯ä»¥ä½œä¸ºç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œç”±äºDBMå¯ä»¥é€šè¿‡å †å RBMå±‚æ¥æ„å»ºï¼Œå› æ­¤é«˜æ•ˆçš„ç»å°”å…¹æ›¼é‡‡æ ·æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®å…ƒç´ ã€‚é‡å­é€€ç«å¯ä»¥é›†æˆåˆ°æ··åˆé‡å­ç»å…¸è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå…·æœ‰æé«˜é€Ÿåº¦å’Œå‡†ç¡®åº¦çš„æ½œåŠ›ã€‚é‡å­åŠ é€Ÿæ˜¯é¢„æœŸé‡å­ä¼˜åŠ¿ä¸­ç‰¹åˆ«æœ‰å¸å¼•åŠ›çš„å…ƒç´ ï¼Œå› ä¸ºå®ƒä¸ä»…å¯ä»¥åœ¨RBMè®­ç»ƒé˜¶æ®µå®ç°ï¼Œè¿˜å¯ä»¥åœ¨ç”Ÿæˆæ–°æ ·æœ¬çš„è¿‡ç¨‹ä¸­å®ç°ã€‚
- en: 5.1 From Graph Theory to Boltzmann Machines
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 ä»å›¾è®ºåˆ°ç»å°”å…¹æ›¼æœº
- en: We provide here a short self-contained review of graph theory in order to introduce
    Boltzmann machines (or energy-based models), which one can view as particular
    types of connected graphs or networks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æ­¤æä¾›ä¸€ä¸ªç®€çŸ­çš„å›¾è®ºè‡ªåŒ…å«å›é¡¾ï¼Œä»¥ä»‹ç»ç»å°”å…¹æ›¼æœºï¼ˆæˆ–èƒ½é‡æ¨¡å‹ï¼‰ï¼Œå®ƒä»¬å¯ä»¥è§†ä¸ºç‰¹å®šç±»å‹çš„è¿æ¥å›¾æˆ–ç½‘ç»œã€‚
- en: A *graph* is a set of vertices (points or nodes) and edges that connect the
    vertices. A *directed graph* is a type of graph that contains ordered pairs of
    vertices while an *undirected graph* is a type of graph that contains unordered
    pairs of vertices.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾* æ˜¯ä¸€ç»„é¡¶ç‚¹ï¼ˆç‚¹æˆ–èŠ‚ç‚¹ï¼‰å’Œè¿æ¥é¡¶ç‚¹çš„è¾¹ã€‚*æœ‰å‘å›¾* æ˜¯ä¸€ç§åŒ…å«æœ‰åºé¡¶ç‚¹å¯¹çš„å›¾ï¼Œè€Œ*æ— å‘å›¾* æ˜¯ä¸€ç§åŒ…å«æ— åºé¡¶ç‚¹å¯¹çš„å›¾ã€‚'
- en: We consider a graph ğ’¢ = (ğ’±*,*â„°) characterised by a finite number of vertices
    ğ’± and undirected edgesÂ â„°. For a given vertexÂ *v* âˆˆğ’±, its neighbourhood is defined
    as the set of all vertices connected to it by some edge, or
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªå›¾ ğ’¢ = (ğ’±*,*â„°)ï¼Œå…¶ä¸­ğ’±è¡¨ç¤ºæœ‰é™ä¸ªé¡¶ç‚¹ï¼Œâ„°è¡¨ç¤ºæ— å‘è¾¹ã€‚å¯¹äºç»™å®šçš„é¡¶ç‚¹ *v* âˆˆğ’±ï¼Œå…¶é‚»åŸŸè¢«å®šä¹‰ä¸ºä¸å®ƒé€šè¿‡æŸæ¡è¾¹ç›¸è¿çš„æ‰€æœ‰é¡¶ç‚¹çš„é›†åˆï¼Œæˆ–è€…
- en: '![ğ’© (v) := {w âˆˆ ğ’± : {v,w} âˆˆ â„°}. ](img/file388.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![ğ’© (v) := {w âˆˆ ğ’± : {v,w} âˆˆ â„°}. ](img/file388.jpg)'
- en: Finally, a *clique*Â ğ’ is a subset ofÂ ğ’± such that all vertices inÂ ğ’ are pairwise
    connected by some edge inÂ â„°.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œ*å›¢* ğ’æ˜¯ğ’±çš„ä¸€ä¸ªå­é›†ï¼Œæ»¡è¶³ğ’ä¸­çš„æ‰€æœ‰é¡¶ç‚¹é€šè¿‡â„°ä¸­çš„æŸæ¡è¾¹æˆå¯¹ç›¸è¿ã€‚
- en: To each vertexÂ *v* âˆˆğ’±, we associate a random variableÂ *X*[v] taking values in
    some spaceÂ ğ’³. The vectorÂ *X* âˆˆğ’³^(|ğ’±|) is called a Markov random field if
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªé¡¶ç‚¹ *v* âˆˆğ’±ï¼Œæˆ‘ä»¬å°†å…³è”ä¸€ä¸ªéšæœºå˜é‡ *X*[v]ï¼Œå®ƒçš„å–å€¼æ¥è‡ªæŸä¸ªç©ºé—´ ğ’³ã€‚å‘é‡ *X* âˆˆğ’³^(|ğ’±|) è¢«ç§°ä¸ºé©¬å°”å¯å¤«éšæœºåœºï¼Œå¦‚æœ
- en: '![Law (Xv |(Xw ){wâˆˆğ’±âˆ–{v}}) = Law (Xv |(Xw ){wâˆˆğ’© (v)}). ](img/file389.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Law (Xv |(Xw ){wâˆˆğ’±âˆ–{v}}) = Law (Xv |(Xw ){wâˆˆğ’© (v)}). ](img/file389.jpg)'
- en: The following theorem, originally proved by Hammersley and CliffordÂ Â [[125](Biblography.xhtml#XHammersleyClifford)]
    (see alsoÂ Â [[167](Biblography.xhtml#XKollerFriedman),Â Theorem 4.2]), provides
    a way to express the law of Markov random fields over graphs in a convenient form.
    The Markovian property is fundamental here, as dynamics (for example, the passing
    of a signal from a hidden layer to a visible layer of an RBM network) should only
    depend on the current state and not on the whole path followed by the system.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å®šç†æœ€åˆç”±å“ˆé»˜æ–¯åˆ©å’Œå…‹åˆ©ç¦å¾·[[125](Biblography.xhtml#XHammersleyClifford)]ï¼ˆå¦è§[[167](Biblography.xhtml#XKollerFriedman)ï¼Œå®šç†4.2]ï¼‰è¯æ˜ï¼Œæä¾›äº†ä¸€ç§åœ¨å›¾ä¸Šä»¥æ–¹ä¾¿çš„å½¢å¼è¡¨è¾¾é©¬å°”å¯å¤«éšæœºåœºæ³•åˆ™çš„æ–¹æ³•ã€‚é©¬å°”å¯å¤«æ€§è´¨åœ¨æ­¤å¤„è‡³å…³é‡è¦ï¼Œå› ä¸ºåŠ¨æ€ï¼ˆä¾‹å¦‚ï¼Œä»éšè—å±‚åˆ°å¯è§å±‚çš„ä¿¡å·ä¼ é€’ï¼Œåœ¨RBMç½‘ç»œä¸­ï¼‰åº”ä»…ä¾èµ–äºå½“å‰çŠ¶æ€ï¼Œè€Œä¸ä¾èµ–äºç³»ç»Ÿæ‰€ç»è¿‡çš„æ•´ä¸ªè·¯å¾„ã€‚
- en: '**Theorem 8** (Hammersley-Clifford Theorem)**.** *A* *strictly positive distribution
    satisfies the Markov property with respect to an* *undirected graph if and only
    if it factorises over it.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®šç† 8**ï¼ˆå“ˆé»˜æ–¯åˆ©-å…‹åˆ©ç¦å¾·å®šç†ï¼‰**ã€‚** *A* *ä¸¥æ ¼æ­£åˆ†å¸ƒä»…å½“ä¸”ä»…å½“å®ƒåœ¨æ— å‘å›¾ä¸Šè¿›è¡Œå› å¼åˆ†è§£æ—¶ï¼Œæ‰æ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨ã€‚*'
- en: Phrased differently, the theorem says thatÂ *X* is Markovian overÂ ğ’¢ if its distribution
    can be written as
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œå®šç†è¡¨æ˜ï¼Œå¦‚æœå…¶åˆ†å¸ƒå¯ä»¥è¡¨ç¤ºä¸º
- en: '| ![ 1 âˆ â„™X (x) := â„™(X = x) = -- ÏˆC (xC), for all x âˆˆ ğ’³ &#124;ğ’±&#124;, Z Câˆˆğ’
    ](img/file390.jpg) |  |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| ![ 1 âˆ â„™X (x) := â„™(X = x) = -- ÏˆC (xC), for all x âˆˆ ğ’³ &#124;ğ’±&#124;, Z Câˆˆğ’
    ](img/file390.jpg) |  |'
- en: for a setÂ {*Ïˆ*[C]}[Câˆˆğ’] of functions called the potential over all the cliques
    *C* âˆˆğ’ and whereÂ *Z* is a normalisation constant such that the probabilities integrate
    to unity. Here x[C] naturally corresponds to the elements of the vectorÂ x over
    the cliqueÂ *C*. The factorisation is often taken over the so-called *maximal cliques*,
    namely the cliques that are no longer cliques if any node is added. If the distribution
    ofÂ *X* is strictly positive, then so are the functionsÂ {*Ïˆ*[C]}[Câˆˆğ’] and thereforeÂ ([5.1](#x1-97006r1))
    can be written as
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€ä¸ªå‡½æ•°é›†{*Ïˆ*[C]}[Câˆˆğ’]ï¼Œè¿™äº›å‡½æ•°è¢«ç§°ä¸ºæ‰€æœ‰å›¢ä½“*C* âˆˆğ’ä¸Šçš„æ½œåŠ›å‡½æ•°ï¼Œå…¶ä¸­*Z*æ˜¯ä¸€ä¸ªå½’ä¸€åŒ–å¸¸æ•°ï¼Œç¡®ä¿æ¦‚ç‡çš„ç§¯åˆ†ä¸º1ã€‚åœ¨è¿™é‡Œï¼Œx[C]è‡ªç„¶å¯¹åº”äºå‘é‡xåœ¨å›¢ä½“*C*ä¸Šçš„å…ƒç´ ã€‚åˆ†è§£é€šå¸¸æ˜¯åœ¨æ‰€è°“çš„*æœ€å¤§å›¢ä½“*ä¸Šè¿›è¡Œçš„ï¼Œå³é‚£äº›å¦‚æœæ·»åŠ ä»»ä½•èŠ‚ç‚¹å°±ä¸å†æ˜¯å›¢ä½“çš„å›¢ä½“ã€‚å¦‚æœ*X*çš„åˆ†å¸ƒä¸¥æ ¼ä¸ºæ­£ï¼Œé‚£ä¹ˆ{*Ïˆ*[C]}[Câˆˆğ’]ä¹Ÿæ˜¯æ­£çš„ï¼Œå› æ­¤([5.1](#x1-97006r1))å¯ä»¥å†™ä¸º
- en: '| ![ ( ) 1 âˆ‘ 1 â„™X (x) = --exp log(ÏˆC (xC)) =: --eâˆ’E(x), Z Câˆˆğ’ Z ](img/file391.jpg)
    |  |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ) 1 âˆ‘ 1 â„™X (x) = --exp log(ÏˆC (xC)) =: --eâˆ’E(x), Z Câˆˆğ’ Z ](img/file391.jpg)
    |  |'
- en: for all x âˆˆğ’³^(|ğ’±|). The function
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰€æœ‰x âˆˆğ’³^(|ğ’±|)ã€‚è¯¥å‡½æ•°
- en: '![ âˆ‘ E (x) := âˆ’ log(ÏˆC(xC )) Câˆˆğ’ ](img/file392.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![ âˆ‘ E (x) := âˆ’ log(ÏˆC(xC )) Câˆˆğ’ ](img/file392.jpg)'
- en: is called the *energy* function. Because of their uses in statistical physics,
    strictly positive distributions of Markov random fields, taking the formÂ ([5.1](#x1-97006r1)),
    are also called Boltzmann or Gibbs distributions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°ç§°ä¸º*èƒ½é‡*å‡½æ•°ã€‚ç”±äºå®ƒä»¬åœ¨ç»Ÿè®¡ç‰©ç†å­¦ä¸­çš„åº”ç”¨ï¼Œé©¬å°”å¯å¤«éšæœºåœºçš„ä¸¥æ ¼æ­£åˆ†å¸ƒï¼Œå½¢å¼ä¸º([5.1](#x1-97006r1))ï¼Œä¹Ÿè¢«ç§°ä¸ºç»å°”å…¹æ›¼åˆ†å¸ƒæˆ–å‰å¸ƒæ–¯åˆ†å¸ƒã€‚
- en: Energy-based models are generative models that discover data dependencies by
    applying a measure of compatibility (scalar energy) to each configuration of the
    observed and latent variables. The inference consists of finding the values of
    latent variables that minimise the energy given the values of the observed variables.
    Energy-based models posses many useful properties (simplicity, stability, flexibility,
    compositionality)Â â€“ this makes them models of choice for learning complex multivariate
    probability distributions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºèƒ½é‡çš„æ¨¡å‹æ˜¯ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡å¯¹è§‚å¯Ÿåˆ°çš„å’Œæ½œåœ¨å˜é‡çš„æ¯ç§é…ç½®åº”ç”¨å…¼å®¹æ€§åº¦é‡ï¼ˆæ ‡é‡èƒ½é‡ï¼‰æ¥å‘ç°æ•°æ®ä¾èµ–æ€§ã€‚æ¨ç†çš„ç›®æ ‡æ˜¯å¯»æ‰¾æ½œåœ¨å˜é‡çš„å€¼ï¼Œåœ¨ç»™å®šè§‚å¯Ÿå˜é‡å€¼çš„æƒ…å†µä¸‹ï¼Œä½¿å¾—èƒ½é‡æœ€å°åŒ–ã€‚åŸºäºèƒ½é‡çš„æ¨¡å‹å…·æœ‰è®¸å¤šæœ‰ç”¨çš„å±æ€§ï¼ˆç®€æ´æ€§ã€ç¨³å®šæ€§ã€çµæ´»æ€§ã€å¯ç»„åˆæ€§ï¼‰â€”â€”è¿™ä½¿å¾—å®ƒä»¬æˆä¸ºå­¦ä¹ å¤æ‚å¤šå˜é‡æ¦‚ç‡åˆ†å¸ƒçš„é¦–é€‰æ¨¡å‹ã€‚
- en: 5.2 Restricted Boltzmann Machine
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 é™åˆ¶ç»å°”å…¹æ›¼æœº
- en: 5.2.1 The RBM as an energy-based model
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 RBMä½œä¸ºä¸€ç§åŸºäºèƒ½é‡çš„æ¨¡å‹
- en: The RBM corresponds to a special structure of such a graph, called bipartite,
    where the setÂ ğ’± of vertices can be split into two groups of visible verticesÂ ğ’±[V]
    and hidden verticesÂ ğ’±[H] such that the setÂ â„° of edges only consists of elements
    of the form {*v,h*}âˆˆğ’±[V] Ã—ğ’±[H]. FigureÂ [5.1](#5.1) provides a schematic representation
    of the RBM that implements the bipartite graph structure. This in particular implies
    that cliques can only be of size one (all the singleton nodes) or two (all the
    pairs (*v,h*) in ğ’±[V] Ã—ğ’±[H]). For simplicity, we shall denoteÂ v an element of
    ğ’³^(|ğ’±[V] |) andÂ h an element of ğ’³^(|ğ’±[H]|), and identify the random variableÂ *X*
    with the vertices. The following lemma gives us the general form of the energy
    functionÂ ([5.1](#x1-97006r1)) for RBMs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: RBM å¯¹åº”äºè¿™ç§å›¾çš„ç‰¹æ®Šç»“æ„ï¼Œç§°ä¸ºäºŒåˆ†å›¾ï¼Œå…¶ä¸­é¡¶ç‚¹é›† ğ’± å¯ä»¥è¢«æ‹†åˆ†æˆä¸¤ä¸ªç»„ï¼Œåˆ†åˆ«æ˜¯å¯è§é¡¶ç‚¹ ğ’±[V] å’Œéšè—é¡¶ç‚¹ ğ’±[H]ï¼Œä½¿å¾—è¾¹é›† â„° ä»…ç”±å½¢å¼ä¸º
    {*v,h*}âˆˆğ’±[V] Ã—ğ’±[H] çš„å…ƒç´ ç»„æˆã€‚å›¾  [5.1](#5.1) æä¾›äº†å®ç°äºŒåˆ†å›¾ç»“æ„çš„ RBM çš„ç¤ºæ„å›¾ã€‚è¿™ç‰¹åˆ«æ„å‘³ç€ï¼Œå›¢åªèƒ½æ˜¯å¤§å°ä¸ºä¸€ï¼ˆæ‰€æœ‰å•ç‚¹èŠ‚ç‚¹ï¼‰æˆ–äºŒï¼ˆæ‰€æœ‰
    (*v,h*) å¯¹ï¼‰åœ¨ ğ’±[V] Ã—ğ’±[H] ä¸­ã€‚ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å°† v è¡¨ç¤ºä¸º ğ’³^(|ğ’±[V] |) çš„ä¸€ä¸ªå…ƒç´ ï¼Œh è¡¨ç¤ºä¸º ğ’³^(|ğ’±[H]|) çš„ä¸€ä¸ªå…ƒç´ ï¼Œå¹¶å°†éšæœºå˜é‡
    *X* ä¸é¡¶ç‚¹å¯¹åº”ã€‚ä»¥ä¸‹å¼•ç†ç»™å‡ºäº† RBM çš„èƒ½é‡å‡½æ•°çš„ä¸€èˆ¬å½¢å¼ï¼ˆ[5.1](#x1-97006r1)ï¼‰ã€‚
- en: '**Lemma 6** (RBM Energy Lemma)**.** *In a Restricted Boltzmann Machine, the*
    *energy function takes the form*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¼•ç† 6**ï¼ˆRBM èƒ½é‡å¼•ç†ï¼‰**.** *åœ¨é™åˆ¶ç»å°”å…¹æ›¼æœºä¸­ï¼Œèƒ½é‡å‡½æ•°çš„å½¢å¼ä¸º*'
- en: '| ![ âˆ‘N âˆ‘M âˆ‘N âˆ‘M E (v,h ) = Ev(vi)+ Eh (hj)+ Ev,h(vi,hj), i=1 j=1 i=1j=1 ](img/file393.jpg)
    |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘N âˆ‘M âˆ‘N âˆ‘M E (v,h ) = Ev(vi)+ Eh (hj)+ Ev,h(vi,hj), i=1 j=1 i=1j=1 ](img/file393.jpg)
    |  |'
- en: '*for any* v := (*v*[1]*,â€¦,v*[N]) âˆˆğ’³^(|ğ’±[V] |)*,* h := (*h*[1]*,â€¦,h*[M]) âˆˆğ’³^(|ğ’±[H]|)*.
    Here,* *N* *is the number* *of visible vertices and* *M* *is the number of hidden
    vertices.*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¯¹äºä»»ä½•* v := (*v*[1]*,â€¦,v*[N]) âˆˆğ’³^(|ğ’±[V] |)*,* h := (*h*[1]*,â€¦,h*[M]) âˆˆğ’³^(|ğ’±[H]|)*.
    è¿™é‡Œï¼Œ*N* æ˜¯å¯è§é¡¶ç‚¹çš„æ•°é‡ï¼Œ*M* æ˜¯éšè—é¡¶ç‚¹çš„æ•°é‡ã€‚*'
- en: '*Proof.* By the Hammersley-Clifford theorem, for any v âˆˆğ’³^(|ğ’±[V] |), h âˆˆğ’³^(|ğ’±[H]|),
    we have the factorisation'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¯æ˜ã€‚* æ ¹æ®å“ˆé»˜æ–¯åˆ©-å…‹åˆ©ç¦å¾·å®šç†ï¼Œå¯¹äºä»»ä½• v âˆˆğ’³^(|ğ’±[V] |)ï¼Œh âˆˆğ’³^(|ğ’±[H]|)ï¼Œæˆ‘ä»¬æœ‰ä»¥ä¸‹åˆ†è§£å¼ï¼š'
- en: '| â„™(v*,*h) | = ![-1 Z](img/file394.jpg)âˆ [Câˆˆğ’]*Ïˆ*[C]((v[C]*,*h[C]) âˆˆ *C*) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| â„™(v*,*h) | = ![-1 Z](img/file394.jpg)âˆ [Câˆˆğ’]*Ïˆ*[C]((v[C]*,*h[C]) âˆˆ *C*) |'
- en: '|  | = ![-1 Z](img/file395.jpg)âˆ [{{v}:vâˆˆğ’±[V] }]*Ïˆ*[{v}](*v*)âˆ [{{h}:hâˆˆğ’±[H]}]*Ïˆ*[{h}](*h*)âˆ
    [{{v,h}âˆˆğ’±[V] Ã—ğ’±[H]}]*Ïˆ*[{v,h}](*v,h*) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | = ![-1 Z](img/file395.jpg)âˆ [{{v}:vâˆˆğ’±[V] }]*Ïˆ*[{v}](*v*)âˆ [{{h}:hâˆˆğ’±[H]}]*Ïˆ*[{h}](*h*)âˆ
    [{{v,h}âˆˆğ’±[V] Ã—ğ’±[H]}]*Ïˆ*[{v,h}](*v,h*) |'
- en: '|  | = ![-1 Z](img/file396.jpg)exp![{âˆ’ E (v,h)}](img/file397.jpg)*,* |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | = ![-1 Z](img/file396.jpg)exp![{âˆ’ E (v,h)}](img/file397.jpg)*,* |'
- en: over all singletons (cliques of size one) and couples (cliques of size two),
    where the term âˆ’*E*(v*,*h) reads
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰€æœ‰å•ç‚¹é›†ï¼ˆå¤§å°ä¸ºä¸€çš„å›¢ï¼‰å’ŒäºŒç‚¹é›†ï¼ˆå¤§å°ä¸ºäºŒçš„å›¢ï¼‰ä¸Šï¼Œå…¶ä¸­é¡¹ âˆ’*E*(v*,*h) è¡¨ç¤º
- en: '| âˆ’ *E*(v*,*h) | = log ![( ) âˆ âˆ âˆ ( Ïˆ {v}(v) Ïˆ{h}(h ) Ïˆ {v,h}(v,h)) {{v}:vâˆˆğ’±V}
    {{h}:hâˆˆğ’±H} {{v,h}âˆˆ ğ’±VÃ—ğ’±H}](img/file398.jpg) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| âˆ’ *E*(v*,*h) | = log ![( ) âˆ âˆ âˆ ( Ïˆ {v}(v) Ïˆ{h}(h ) Ïˆ {v,h}(v,h)) {{v}:vâˆˆğ’±V}
    {{h}:hâˆˆğ’±H} {{v,h}âˆˆ ğ’±VÃ—ğ’±H}](img/file398.jpg) |'
- en: '|  | = log ![( ) ( âˆ ) Ïˆ {v}(v) {{v}:vâˆˆğ’±V}](img/file399.jpg) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | = log ![( ) ( âˆ ) Ïˆ {v}(v) {{v}:vâˆˆğ’±V}](img/file399.jpg) |'
- en: '|  | + log ![( ) âˆ ( Ïˆ {h}(h)) {{h}:hâˆˆğ’±H}](img/file400.jpg) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | + log ![( ) âˆ ( Ïˆ {h}(h)) {{h}:hâˆˆğ’±H}](img/file400.jpg) |'
- en: '|  | + log ![( ) âˆ ( Ïˆ{v,h}(v,h)) {{v,h}âˆˆğ’±VÃ—ğ’±H }](img/file401.jpg) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | + log ![( ) âˆ ( Ïˆ{v,h}(v,h)) {{v,h}âˆˆğ’±VÃ—ğ’±H }](img/file401.jpg) |'
- en: '|  | = âˆ‘ [{{v}:vâˆˆğ’±[V] }]log ![( ) Ïˆ{v}(v)](img/file402.jpg) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | = âˆ‘ [{{v}:vâˆˆğ’±[V] }]log ![( ) Ïˆ{v}(v)](img/file402.jpg) |'
- en: '|  | + âˆ‘ [{{h}:hâˆˆğ’±[H]}]log ![(Ïˆ{h}(h))](img/file403.jpg) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | + âˆ‘ [{{h}:hâˆˆğ’±[H]}]log ![(Ïˆ{h}(h))](img/file403.jpg) |'
- en: '|  | + âˆ‘ [{{v,h}âˆˆğ’±[V] Ã—ğ’±[H]}]log ![( ) Ïˆ{v,h}(v,h)](img/file404.jpg) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | + âˆ‘ [{{v,h}âˆˆğ’±[V] Ã—ğ’±[H]}]log ![( ) Ïˆ{v,h}(v,h)](img/file404.jpg) |'
- en: '|  | = âˆ’âˆ‘ [i=1]^N*E* [v](*v*[i]) âˆ’âˆ‘ [j=1]^M*E* [h](*h*[j]) âˆ’âˆ‘ [i=1]^N âˆ‘ [j=1]^M*E*
    [v,h](*v*[i]*,h*[j])*,* |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | = âˆ’âˆ‘ [i=1]^N*E* [v](*v*[i]) âˆ’âˆ‘ [j=1]^M*E* [h](*h*[j]) âˆ’âˆ‘ [i=1]^N âˆ‘ [j=1]^M*E*
    [v,h](*v*[i]*,h*[j])*,* |'
- en: which concludes the proof of the lemma. â–¡
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±å®Œæˆäº†å¼•ç†çš„è¯æ˜ã€‚â–¡
- en: The standard example of an RBM is when the random variables follow Bernoulli
    distribution, i.e., with ğ’³ = {0*,*1}^(|ğ’±|). In this case, their energies read
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: RBM çš„æ ‡å‡†ç¤ºä¾‹æ˜¯å½“éšæœºå˜é‡æœä»ä¼¯åŠªåˆ©åˆ†å¸ƒæ—¶ï¼Œå³ ğ’³ = {0*,*1}^(|ğ’±|)ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒä»¬çš„èƒ½é‡ä¸º
- en: '| ![Ev (vi) = âˆ’ aivi, Eh(hj) = âˆ’ bjhj, Ev,h(vi,hj) = âˆ’ wijvihj, ](img/file405.jpg)
    |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| ![Ev (vi) = âˆ’ aivi, Eh(hj) = âˆ’ bjhj, Ev,h(vi,hj) = âˆ’ wijvihj, ](img/file405.jpg)
    |  |'
- en: for some parameters *a*[i], *b*[j], *w*[ij], *i* = 1*,â€¦,N*, *j* = 1*,â€¦,M*. In
    particular, for a given *v*[i], we can write, using Bayesâ€™ formula,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸäº›å‚æ•° *a*[i]ï¼Œ*b*[j]ï¼Œ*w*[ij]ï¼Œ*i* = 1*,â€¦,N*ï¼Œ*j* = 1*,â€¦,M*ã€‚ç‰¹åˆ«åœ°ï¼Œå¯¹äºç»™å®šçš„ *v*[i]ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è´å¶æ–¯å…¬å¼è¡¨ç¤ºï¼š
- en: '| â„™(*v*[i] = 1&#124;v[v[i]]*,*h) | = ![---------â„™(vi =-1,vvi,h)------- â„™ (vi
    = 1,vvi,h )+ â„™(vi = 0,vvi,h)](img/file406.jpg) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| â„™(*v*[i] = 1&#124;v[v[i]]*,*h) | = ![---------â„™(vi =-1,vvi,h)------- â„™ (vi
    = 1,vvi,h )+ â„™(vi = 0,vvi,h)](img/file406.jpg) |'
- en: '|  | = ![ exp (âˆ’ E (v = 1,v ,h)) ----------------------i-----vi---------------
    exp (âˆ’ E (vi = 1,vvi,h))+ exp (âˆ’ E (vi = 0,vvi,h ))](img/file407.jpg)*.* | (5.2.1)
    |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | = ![ exp (âˆ’ E (v = 1,v ,h)) ----------------------i-----vi---------------
    exp (âˆ’ E (vi = 1,vvi,h))+ exp (âˆ’ E (vi = 0,vvi,h ))](img/file407.jpg)*.* | (5.2.1)
    |  |'
- en: where we denote v[v[i]] the states of all the nodes in ğ’±âˆ–{*v*[i]}. Now, using
    the RBM energy lemma, we can single out the energy arising from the particular
    nodeÂ *v* usingÂ ([5.2.1](#x1-99002r1)) as
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æˆ‘ä»¬è¡¨ç¤º v[v[i]] ä¸º ğ’±âˆ–{*v*[i]} ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„çŠ¶æ€ã€‚ç°åœ¨ï¼Œä½¿ç”¨ RBM èƒ½é‡å¼•ç†ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨([5.2.1](#x1-99002r1))å°†æ¥è‡ªç‰¹å®šèŠ‚ç‚¹
    *v* çš„èƒ½é‡å•ç‹¬æå–å‡ºæ¥ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![E(vi,vvi,h) = âˆ’ Î¦v(vi)âˆ’ Î¨v (vvi,h), ](img/file408.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![E(vi,vvi,h) = âˆ’ Î¦v(vi)âˆ’ Î¨v (vvi,h), ](img/file408.jpg)'
- en: where
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: '| ![ âŒŠ âŒ‹ âˆ‘M Mâˆ‘ Î¦v (vi) := aivi + wijvihj = âŒˆai + wijhjâŒ‰ vi, j=1 j=1 âˆ‘N âˆ‘M âˆ‘N
    âˆ‘M Î¨v (vvi,h) := akvk + bjhj + wkjvkhj. k=1(kâ„=i) j=1 k=1(kâ„=i)j=1 ](img/file409.jpg)
    |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| ![ âŒŠ âŒ‹ âˆ‘M Mâˆ‘ Î¦v (vi) := aivi + wijvihj = âŒˆai + wijhjâŒ‰ vi, j=1 j=1 âˆ‘N âˆ‘M âˆ‘N
    âˆ‘M Î¨v (vvi,h) := akvk + bjhj + wkjvkhj. k=1(kâ„=i) j=1 k=1(kâ„=i)j=1 ](img/file409.jpg)
    |  |'
- en: Plugging this intoÂ ([5.2.1](#x1-99003r1)) then yields
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤ä»£å…¥([5.2.1](#x1-99003r1))åå¾—åˆ°
- en: '| ![ exp (Î¦v (vi = 1)+ Î¨v (vvi,h)) â„™(vi = 1&#124;vvi,h ) =-------------------------------------------------------
    exp (Î¦v(vi = 1)+ Î¨v (vvi,h))+ exp (Î¦v(vi = 0)+ Î¨v (vvi,h)) = --exp-(Î¦v(vi =-1))-
    exp (Î¦v(vi = 1)) + 1 = Ïƒ (Î¦v(vi = 1)), ](img/file410.jpg) |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| ![ exp (Î¦v (vi = 1)+ Î¨v (vvi,h)) â„™(vi = 1&#124;vvi,h ) =-------------------------------------------------------
    exp (Î¦v(vi = 1)+ Î¨v (vvi,h))+ exp (Î¦v(vi = 0)+ Î¨v (vvi,h)) = --exp-(Î¦v(vi =-1))-
    exp (Î¦v(vi = 1)) + 1 = Ïƒ (Î¦v(vi = 1)), ](img/file410.jpg) |  |'
- en: since Î¦[v](*v*[i] = 0) = 0, where
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸º Î¦[v](*v*[i] = 0) = 0ï¼Œå…¶ä¸­
- en: '| ![Ïƒ(x) :=---1--- 1 + eâˆ’x ](img/file411.jpg) |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ![Ïƒ(x) :=---1--- 1 + eâˆ’x ](img/file411.jpg) |  |'
- en: is the sigmoid function.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯ Sigmoid å‡½æ•°ã€‚
- en: 'Similarly, we can single out the contribution of the energy on a given hidden
    nodeÂ *h*[j], using the RBM energy lemma:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ RBM èƒ½é‡å¼•ç†å°†ç»™å®šéšè—èŠ‚ç‚¹ *h*[j] ä¸Šçš„èƒ½é‡è´¡çŒ®å•ç‹¬æå–å‡ºæ¥ï¼š
- en: '![E (v,hj,hhj) = âˆ’ Î¦h(hj)âˆ’ Î¨h (v,hhj), ](img/file412.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![E (v,hj,hhj) = âˆ’ Î¦h(hj)âˆ’ Î¨h (v,hhj), ](img/file412.jpg)'
- en: where
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: '| ![ âˆ‘N [ âˆ‘N ] Î¦h (hj) := bjhj + wijvihj = bj + wijvi hj, i=1 i=1 N M N M Î¨
    (v,h ) := âˆ‘ a v + âˆ‘ b h + âˆ‘ âˆ‘ w vh . h hj i i k k ik i k i=1 k=1(kâ„=j) i=1k=1(kâ„=j)
    ](img/file413.jpg) |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘N [ âˆ‘N ] Î¦h (hj) := bjhj + wijvihj = bj + wijvi hj, i=1 i=1 N M N M Î¨
    (v,h ) := âˆ‘ a v + âˆ‘ b h + âˆ‘ âˆ‘ w vh . h hj i i k k ik i k i=1 k=1(kâ„=j) i=1k=1(kâ„=j)
    ](img/file413.jpg) |  |'
- en: Plugging this intoÂ ([5.2.1](#x1-99003r1)) then yields
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤ä»£å…¥([5.2.1](#x1-99003r1))åå¾—åˆ°
- en: '| ![ ( ( )) â„™(h = 1&#124;v,h ) = ----(----------exp--Î¦(h(hj-=))1)+-Î¨h(-v,hhj----------(----))-
    j hj exp Î¦h(hj = 1)+ Î¨h v,hhj + exp Î¦h (hj = 0) + Î¨h v,hhj = --exp(Î¦h-(hj =-1))-
    exp (Î¦h(hj = 1)) + 1 = Ïƒ (Î¦ (h = 1 )) , h j ](img/file414.jpg) |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( ( )) â„™(h = 1&#124;v,h ) = ----(----------exp--Î¦(h(hj-=))1)+-Î¨h(-v,hhj----------(----))-
    j hj exp Î¦h(hj = 1)+ Î¨h v,hhj + exp Î¦h (hj = 0) + Î¨h v,hhj = --exp(Î¦h-(hj =-1))-
    exp (Î¦h(hj = 1)) + 1 = Ïƒ (Î¦ (h = 1 )) , h j ](img/file414.jpg) |  |'
- en: since again Î¦[h](*h*[j] = 0) = 0\.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºå†æ¬¡æœ‰ Î¦[h](*h*[j] = 0) = 0\ã€‚
- en: 5.2.2 RBM network architecture
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 RBM ç½‘ç»œæ¶æ„
- en: As shown above, an RBM is thus a shallow two-layer neural network that operates
    on stochastic binary activation units. The network forms a bipartite graph connecting
    stochastic binary inputs (visible units) to stochastic binary feature detectors
    (hidden units) with no connections between the units within the same layer, as
    shown in FigureÂ [5.1](#5.1)Â Â [[102](Biblography.xhtml#XFischer2012)].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šæ‰€ç¤ºï¼ŒRBM æ˜¯ä¸€ä¸ªæµ…å±‚çš„ä¸¤å±‚ç¥ç»ç½‘ç»œï¼Œæ“ä½œåœ¨éšæœºäºŒè¿›åˆ¶æ¿€æ´»å•å…ƒä¸Šã€‚è¯¥ç½‘ç»œå½¢æˆä¸€ä¸ªäºŒåˆ†å›¾ï¼Œå°†éšæœºäºŒè¿›åˆ¶è¾“å…¥ï¼ˆå¯è§å•å…ƒï¼‰ä¸éšæœºäºŒè¿›åˆ¶ç‰¹å¾æ£€æµ‹å™¨ï¼ˆéšè—å•å…ƒï¼‰è¿æ¥ï¼Œä¸”åŒä¸€å±‚å†…çš„å•å…ƒä¹‹é—´æ²¡æœ‰è¿æ¥ï¼Œå¦‚å›¾[5.1](#5.1)æ‰€ç¤º
    [[102](Biblography.xhtml#XFischer2012)]ã€‚
- en: '![Figurex1-100001r1: Schematic representation of an RBM with the visible layer
    units (white) and hidden layer units (dark) forming a bipartite graph. ](img/file415.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![Figurex1-100001r1: Schematic representation of an RBM with the visible layer
    units (white) and hidden layer units (dark) forming a bipartite graph. ](img/file415.jpg)'
- en: 'FigureÂ 5.1: Schematic representation of an RBM with the visible layer units
    (white) and hidden layer units (dark) forming a bipartite graph.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.1ï¼šå…·æœ‰å¯è§å±‚å•å…ƒï¼ˆç™½è‰²ï¼‰å’Œéšè—å±‚å•å…ƒï¼ˆæ·±è‰²ï¼‰å½¢æˆäºŒåˆ†å›¾çš„RBMçš„ç¤ºæ„å›¾ã€‚
- en: 'Only the visible layer of the network is exposed to the training dataset and
    its inputs v := (*v*[1]*,â€¦,v*[N]) flow through the network (forward pass) to the
    hidden layer, where they are aggregated and added to the hidden layer biases b
    := (*b*[1]*,â€¦,b*[M]). The hidden layer sigmoid activation functionÂ ([5.2.1](#x1-99003r1))
    converts aggregated inputs into probabilities. Each hidden unit then "fires" randomly
    and outputs a {0*,*1} Bernoulli random variable with the associated probabilities:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰ç½‘ç»œçš„å¯è§å±‚æš´éœ²ç»™è®­ç»ƒæ•°æ®é›†ï¼Œè¾“å…¥ v := (*v*[1]*,â€¦,v*[N]) æµç»ç½‘ç»œï¼ˆå‰å‘ä¼ æ’­ï¼‰åˆ°è¾¾éšè—å±‚ï¼Œåœ¨æ­¤å®ƒä»¬è¢«èšåˆå¹¶æ·»åŠ åˆ°éšè—å±‚åç½®
    b := (*b*[1]*,â€¦,b*[M])ã€‚éšè—å±‚çš„ Sigmoid æ¿€æ´»å‡½æ•°([5.2.1](#x1-99003r1))å°†èšåˆçš„è¾“å…¥è½¬æ¢ä¸ºæ¦‚ç‡ã€‚ç„¶åæ¯ä¸ªéšè—å•å…ƒä»¥éšæœºæ–¹å¼â€œè§¦å‘â€ï¼Œå¹¶è¾“å‡ºä¸€ä¸ª
    {0*,*1} çš„ä¼¯åŠªåˆ©éšæœºå˜é‡ï¼Œå…¶ç›¸å…³æ¦‚ç‡ä¸ºï¼š
- en: '| ![ ( Nâˆ‘ ) ( âˆ‘N ) â„™(hj = 1&#124;v) = Ïƒ bj + wijvi and â„™(hj = 0&#124;v) = 1âˆ’
    Ïƒ bj + wijvi . i=1 i=1 ](img/file416.jpg) |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( Nâˆ‘ ) ( âˆ‘N ) â„™(hj = 1&#124;v) = Ïƒ bj + wijvi and â„™(hj = 0&#124;v) = 1âˆ’
    Ïƒ bj + wijvi . i=1 i=1 ](img/file416.jpg) |  |'
- en: 'The outputs from the hidden layer h := (*h*[1]*,â€¦,h*[M]) then flow back (backward
    pass) to the visible layer, where they are aggregated and added to the visible
    layer biases a := (*a*[1]*,â€¦,a*[N]). Similar to the hidden layer, the visible
    layer sigmoid activation function first translates aggregated inputs into probabilities
    and then into Bernoulli random variables:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªéšè—å±‚çš„è¾“å‡º h := (*h*[1]*,â€¦,h*[M]) éšååå‘ä¼ é€’ï¼ˆåå‘ä¼ æ’­ï¼‰åˆ°å¯è§å±‚ï¼Œåœ¨é‚£é‡Œå®ƒä»¬è¢«èšåˆå¹¶åŠ åˆ°å¯è§å±‚çš„åç½® a := (*a*[1]*,â€¦,a*[N])ã€‚ä¸éšè—å±‚ç±»ä¼¼ï¼Œå¯è§å±‚çš„sigmoidæ¿€æ´»å‡½æ•°é¦–å…ˆå°†èšåˆçš„è¾“å…¥è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œç„¶åè½¬åŒ–ä¸ºä¼¯åŠªåˆ©éšæœºå˜é‡ï¼š
- en: '| ![ ( M ) ( M ) â„™(v = 1&#124;h) = Ïƒ(a + âˆ‘ w h ) and â„™ (v = 0&#124;h) = 1âˆ’
    Ïƒ (a + âˆ‘ w h ) . i i ij j i i ij j j=1 j=1 ](img/file417.jpg) |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| ![ ( M ) ( M ) â„™(v = 1&#124;h) = Ïƒ(a + âˆ‘ w h ) and â„™ (v = 0&#124;h) = 1âˆ’
    Ïƒ (a + âˆ‘ w h ) . i i ij j i i ij j j=1 j=1 ](img/file417.jpg) |  |'
- en: Therefore, every unit communicates at most one bit of information. This is especially
    important for the hidden units since this feature implements the information bottleneck
    structure, which acts as a strong regulariserÂ Â [[134](Biblography.xhtml#XHinton2010)].
    The hidden layer of the network can learn the low-dimensional probabilistic representation
    of the dataset if the network is organised and trained as an autoencoder.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ¯ä¸ªå•å…ƒæœ€å¤šä¼ é€’ä¸€ä¸ªæ¯”ç‰¹çš„ä¿¡æ¯ã€‚è¿™å¯¹äºéšè—å•å…ƒå°¤ä¸ºé‡è¦ï¼Œå› ä¸ºè¿™ä¸ªç‰¹æ€§å®ç°äº†ä¿¡æ¯ç“¶é¢ˆç»“æ„ï¼Œä½œä¸ºä¸€ç§å¼ºæ­£åˆ™åŒ–å™¨[[134](Biblography.xhtml#XHinton2010)]ã€‚å¦‚æœç½‘ç»œè¢«ç»„ç»‡å¹¶è®­ç»ƒæˆè‡ªç¼–ç å™¨ï¼Œç½‘ç»œçš„éšè—å±‚å¯ä»¥å­¦ä¹ æ•°æ®é›†çš„ä½ç»´æ¦‚ç‡è¡¨ç¤ºã€‚
- en: 5.2.3 Sample encoding
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 æ ·æœ¬ç¼–ç 
- en: FigureÂ [5.2](#5.2) illustrates the binary representation of an input signal
    that enters the network through the visible layer. The number of activation units
    in the visible layer is determined by the number of features we have to encode
    and the desired precision of their binary representation. For example, if our
    sample consists of *m* continuous features and each feature is encoded as an *n*-digit
    binary number, the total number of activation units in the visible layer is *m*
    Ã— *n*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[5.2](#5.2)å±•ç¤ºäº†è¾“å…¥ä¿¡å·çš„äºŒè¿›åˆ¶è¡¨ç¤ºï¼Œä¿¡å·é€šè¿‡å¯è§å±‚è¿›å…¥ç½‘ç»œã€‚å¯è§å±‚ä¸­æ¿€æ´»å•å…ƒçš„æ•°é‡ç”±æˆ‘ä»¬éœ€è¦ç¼–ç çš„ç‰¹å¾æ•°å’Œå…¶äºŒè¿›åˆ¶è¡¨ç¤ºçš„æ‰€éœ€ç²¾åº¦å†³å®šã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬çš„æ ·æœ¬ç”±
    *m* ä¸ªè¿ç»­ç‰¹å¾ç»„æˆï¼Œå¹¶ä¸”æ¯ä¸ªç‰¹å¾è¢«ç¼–ç ä¸º *n* ä½äºŒè¿›åˆ¶æ•°ï¼Œåˆ™å¯è§å±‚ä¸­çš„æ¿€æ´»å•å…ƒæ€»æ•°ä¸º *m* Ã— *n*ã€‚
- en: '![Figurex1-101001r2: Schematic binary encoding of continuous variables. ](img/file418.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Figurex1-101001r2: Schematic binary encoding of continuous variables. ](img/file418.jpg)'
- en: 'FigureÂ 5.2: Schematic binary encoding of continuous variables.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.2ï¼šè¿ç»­å˜é‡çš„ç¤ºæ„äºŒè¿›åˆ¶ç¼–ç ã€‚
- en: 5.2.4 Boltzmann distribution
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4 åšå°”å…¹æ›¼åˆ†å¸ƒ
- en: 'The network learns the probability distribution â„™(v*,*h) of the configurations
    of visible and hidden activation units â€“ the Boltzmann distribution â€“ by trying
    to reconstruct the inputs from the training dataset (visible unit values) through
    finding an optimal set of the network weights and biases:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œé€šè¿‡å°è¯•ä»è®­ç»ƒæ•°æ®é›†ï¼ˆå¯è§å•å…ƒå€¼ï¼‰é‡å»ºè¾“å…¥ï¼Œæ¥å­¦ä¹ å¯è§å’Œéšè—æ¿€æ´»å•å…ƒé…ç½®çš„æ¦‚ç‡åˆ†å¸ƒ â„™(v*,*h) â€”â€” å³åšå°”å…¹æ›¼åˆ†å¸ƒ â€”â€” é€šè¿‡æ‰¾åˆ°ç½‘ç»œæƒé‡å’Œåç½®çš„æœ€ä½³é›†ï¼š
- en: '| ![ 1 â„™(v,h) = --eâˆ’E(v,h), Z ](img/file419.jpg) |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| ![ 1 â„™(v,h) = --eâˆ’E(v,h), Z ](img/file419.jpg) |  |'
- en: where the energy function reads
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­èƒ½é‡å‡½æ•°ä¸º
- en: '| ![ N M N M âˆ‘ âˆ‘ âˆ‘ âˆ‘ E (v,h) = âˆ’ aivi âˆ’ bjhj âˆ’ wijvihj. i=1 j=1 i=1 j=1 ](img/file420.jpg)
    |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ![ N M N M âˆ‘ âˆ‘ âˆ‘ âˆ‘ E (v,h) = âˆ’ aivi âˆ’ bjhj âˆ’ wijvihj. i=1 j=1 i=1 j=1 ](img/file420.jpg)
    |  |'
- en: 'Here, *Z* is the partition function:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ*Z* æ˜¯é…åˆ†å‡½æ•°ï¼š
- en: '| ![ âˆ‘ Z = eâˆ’E (v,h). v,h ](img/file421.jpg) |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘ Z = eâˆ’E (v,h). v,h ](img/file421.jpg) |  |'
- en: 'However, we are usually interested either in learning the probability distribution
    of the visible layer configurations if we want to generate new samples that would
    have the same statistical properties as the original training dataset, or in learning
    the probability distribution of the hidden layer configurations if we want to
    build a deep neural network where the RBM layer performs the feature extraction
    and dimensionality reduction function. The probabilities of the visible (hidden)
    states are given by summing over all possible hidden (visible) vectors:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬é€šå¸¸æ„Ÿå…´è¶£çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›ç”Ÿæˆä¸åŸå§‹è®­ç»ƒæ•°æ®é›†å…·æœ‰ç›¸åŒç»Ÿè®¡æ€§è´¨çš„æ–°æ ·æœ¬ï¼Œåˆ™å­¦ä¹ å¯è§å±‚é…ç½®çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæˆ–è€…å¦‚æœæˆ‘ä»¬å¸Œæœ›æ„å»ºä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­RBMå±‚æ‰§è¡Œç‰¹å¾æå–å’Œé™ç»´åŠŸèƒ½ï¼Œåˆ™å­¦ä¹ éšè—å±‚é…ç½®çš„æ¦‚ç‡åˆ†å¸ƒã€‚å¯è§ï¼ˆéšè—ï¼‰çŠ¶æ€çš„æ¦‚ç‡æ˜¯é€šè¿‡å¯¹æ‰€æœ‰å¯èƒ½çš„éšè—ï¼ˆå¯è§ï¼‰å‘é‡æ±‚å’Œå¾—åˆ°çš„ï¼š
- en: '| ![ 1 âˆ‘ 1 âˆ‘ â„™ (v) = -- eâˆ’E(v,h) and â„™(h) = -- eâˆ’E (v,h). Z h Z v ](img/file422.jpg)
    |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ![ 1 âˆ‘ 1 âˆ‘ â„™ (v) = -- eâˆ’E(v,h) and â„™(h) = -- eâˆ’E (v,h). Z h Z v ](img/file422.jpg)
    |  |'
- en: The most popular training algorithm for RBM, *k-step Contrastive Divergence*,
    was proposed by HintonÂ Â [[134](Biblography.xhtml#XHinton2010),Â [133](Biblography.xhtml#XHinton2002)].
    The algorithm aims to maximise the log probability of a training vector, i.e.,
    to find such network weights and biases that the "energy" function *E* is minimised
    for the samples from the training dataset (smaller value of energy corresponds
    to larger probability of a configuration). The *k*-step CD algorithm is fully
    specified in SectionÂ [5.3.2](#x1-1060002) and the interested reader can also find
    an excellent introduction to the training of RBMs in the work by Fischer and IgelÂ Â [[103](Biblography.xhtml#XFischer2014)].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¸¸ç”¨çš„ RBM è®­ç»ƒç®—æ³• *k-step Contrastive Divergence* æ˜¯ç”± Hinton æå‡ºçš„ [[134](Biblography.xhtml#XHinton2010)ï¼Œ[133](Biblography.xhtml#XHinton2002)]ã€‚è¯¥ç®—æ³•æ—¨åœ¨æœ€å¤§åŒ–è®­ç»ƒå‘é‡çš„å¯¹æ•°æ¦‚ç‡ï¼Œå³æ‰¾åˆ°ä¸€ç»„ç½‘ç»œæƒé‡å’Œåç½®ï¼Œä½¿å¾—â€œèƒ½é‡â€å‡½æ•°
    *E* å¯¹è®­ç»ƒæ•°æ®é›†ä¸­çš„æ ·æœ¬æœ€å°åŒ–ï¼ˆèƒ½é‡å€¼è¶Šå°ï¼Œé…ç½®çš„æ¦‚ç‡è¶Šå¤§ï¼‰ã€‚*k*-æ­¥ CD ç®—æ³•åœ¨ç¬¬ [5.3.2](#x1-1060002) èŠ‚ä¸­æœ‰å®Œæ•´è¯´æ˜ï¼Œæ„Ÿå…´è¶£çš„è¯»è€…è¿˜å¯ä»¥é€šè¿‡
    Fischer å’Œ Igel çš„å·¥ä½œ [[103](Biblography.xhtml#XFischer2014)] è·å¾—æœ‰å…³ RBM è®­ç»ƒçš„ç²¾å½©ä»‹ç»ã€‚
- en: 5.2.5 Extensions of the Bernoulli RBM
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.5 ä¼¯åŠªåˆ© RBM çš„æ‰©å±•
- en: The standard Bernoulli RBM setup we considered above restricts the visible layerÂ v
    to a Bernoulli distribution. In fact, as long as the Hammersley-Clifford theorem
    holds, we can consider any distribution or any form of energy function. It was
    shown inÂ Â [[62](Biblography.xhtml#XCho),Â [178](Biblography.xhtml#XKrizhevsky)]
    for example, that a Bernoulli distribution for the hidden layer combined with
    a Gaussian distribution for the visible layer are compatible with an energy function
    of the form
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸Šè¿°è®¨è®ºçš„æ ‡å‡†ä¼¯åŠªåˆ© RBM è®¾ç½®å°†å¯è§å±‚ v é™åˆ¶ä¸ºä¼¯åŠªåˆ©åˆ†å¸ƒã€‚äº‹å®ä¸Šï¼Œåªè¦å“ˆé»˜æ–¯åˆ©-å…‹åˆ©ç¦å¾·å®šç†æˆç«‹ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘ä»»ä½•åˆ†å¸ƒæˆ–ä»»ä½•å½¢å¼çš„èƒ½é‡å‡½æ•°ã€‚ä¾‹å¦‚ï¼Œ[[62](Biblography.xhtml#XCho)ï¼Œ[178](Biblography.xhtml#XKrizhevsky)]
    ä¸­æ›¾è¡¨æ˜ï¼Œéšè—å±‚çš„ä¼¯åŠªåˆ©åˆ†å¸ƒä¸å¯è§å±‚çš„é«˜æ–¯åˆ†å¸ƒæ˜¯ä¸å¦‚ä¸‹å½¢å¼çš„èƒ½é‡å‡½æ•°å…¼å®¹çš„ï¼š
- en: '![ âˆ‘N (vi âˆ’ ai)2 âˆ‘M âˆ‘N âˆ‘M vihj E (v,h) = ---2Ïƒ2---âˆ’ bjhj âˆ’ wijÏƒ2--, i=1 i j=1
    i=1j=1 i ](img/file423.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![ âˆ‘N (vi âˆ’ ai)2 âˆ‘M âˆ‘N âˆ‘M vihj E (v,h) = ---2Ïƒ2---âˆ’ bjhj âˆ’ wijÏƒ2--, i=1 i j=1
    i=1j=1 i ](img/file423.jpg)'
- en: for some parameters *a*[i], *Ïƒ*[i], *b*[j], *w*[ij], *i* = 1*,â€¦,N*, *j* = 1*,â€¦,M*.
    In this case, for any *h*[j], the conditional probabilities â„™(*h*[j] = 1|v) remain
    of sigmoid form and the conditional distribution of the visible layer is Gaussian
    as
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸäº›å‚æ•° *a*[i]ã€*Ïƒ*[i]ã€*b*[j]ã€*w*[ij]ï¼Œ*i* = 1*,â€¦,N*ï¼Œ*j* = 1*,â€¦,M*ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯¹äºä»»ä½•
    *h*[j]ï¼Œæ¡ä»¶æ¦‚ç‡ â„™(*h*[j] = 1|v) ä¿æŒ sigmoid å½¢å¼ï¼Œå¹¶ä¸”å¯è§å±‚çš„æ¡ä»¶åˆ†å¸ƒä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![ ( M ) Law (v|h) = ğ’© (a + âˆ‘ w h ,Ïƒ2 ) , for each i = 1,...,N. i i ij j i
    j=1 ](img/file424.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![ ( M ) Law (v|h) = ğ’© (a + âˆ‘ w h ,Ïƒ2 ) , for each i = 1,...,N. i i ij j i
    j=1 ](img/file424.jpg)'
- en: The RBMs we have considered do not account for time series, i.e., probability
    structures with temporal dependence. By enlarging the corresponding graph, in
    particular adding a conditional layer with directed connections to the classical
    hidden and visible layers, TaylorÂ Â [[280](Biblography.xhtml#XTaylorConditional)]
    showed that such dependence can be accounted for.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è€ƒè™‘çš„ RBM ä¸è€ƒè™‘æ—¶é—´åºåˆ—ï¼Œå³å…·æœ‰æ—¶é—´ä¾èµ–æ€§çš„æ¦‚ç‡ç»“æ„ã€‚é€šè¿‡æ‰©å¤§ç›¸åº”çš„å›¾ï¼Œç‰¹åˆ«æ˜¯æ·»åŠ ä¸€ä¸ªå…·æœ‰æœ‰å‘è¿æ¥çš„æ¡ä»¶å±‚åˆ°ç»å…¸çš„éšè—å±‚å’Œå¯è§å±‚ï¼ŒTaylor
    [[280](Biblography.xhtml#XTaylorConditional)] è¡¨ç¤ºå¯ä»¥è€ƒè™‘è¿™ç§ä¾èµ–æ€§ã€‚
- en: An RBM is a neural network represented by a bipartite graph. Its power is derived
    from operating on stochastic binary activation units. It is a generative model
    that encodes learned probability distribution in its weights and biases and then
    generates new samples that are statistically indistinguishable from the samples
    in the original dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: RBMï¼ˆå—é™ç»å°”å…¹æ›¼æœºï¼‰æ˜¯ä¸€ç§ç”±äºŒåˆ†å›¾è¡¨ç¤ºçš„ç¥ç»ç½‘ç»œã€‚å…¶å¼ºå¤§ä¹‹å¤„åœ¨äºæ“ä½œéšæœºäºŒè¿›åˆ¶æ¿€æ´»å•å…ƒã€‚å®ƒæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡å…¶æƒé‡å’Œåç½®ç¼–ç å­¦ä¹ åˆ°çš„æ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åç”Ÿæˆä¸åŸå§‹æ•°æ®é›†ä¸­çš„æ ·æœ¬åœ¨ç»Ÿè®¡ä¸Šæ— æ³•åŒºåˆ†çš„æ–°æ ·æœ¬ã€‚
- en: If it is organised as an autoencoder with the bottleneck information structure,
    an RBM is able to learn the low-dimensional representation of the dataset. This
    property suggests that an RBM can be used as a feature extraction layer in a machine
    learning pipeline for certain supervised and unsupervised learning problems.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå®ƒè¢«ç»„ç»‡ä¸ºå…·æœ‰ç“¶é¢ˆä¿¡æ¯ç»“æ„çš„è‡ªç¼–ç å™¨ï¼ŒRBM èƒ½å¤Ÿå­¦ä¹ æ•°æ®é›†çš„ä½ç»´è¡¨ç¤ºã€‚è¿™ä¸€ç‰¹æ€§è¡¨æ˜ï¼ŒRBM å¯ä»¥ä½œä¸ºæœºå™¨å­¦ä¹ ç®¡é“ä¸­çš„ç‰¹å¾æå–å±‚ï¼Œç”¨äºæŸäº›ç›‘ç£å’Œæ— ç›‘ç£å­¦ä¹ é—®é¢˜ã€‚
- en: 5.3 Training and Running RBM
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 è®­ç»ƒå’Œè¿è¡Œ RBM
- en: To build a neural network means to specify the network architecture and training
    algorithm. Having described the RBM architecture in the previous section, we now
    outline the training routines.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºç¥ç»ç½‘ç»œæ„å‘³ç€æŒ‡å®šç½‘ç»œæ¶æ„å’Œè®­ç»ƒç®—æ³•ã€‚åœ¨å‰ä¸€èŠ‚ä¸­å·²ç»æè¿°äº† RBM çš„æ¶æ„ï¼Œç°åœ¨æˆ‘ä»¬æ¦‚è¿°è®­ç»ƒæµç¨‹ã€‚
- en: 5.3.1 Training RBM with Boltzmann sampling
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 ä½¿ç”¨ç»å°”å…¹æ›¼é‡‡æ ·è®­ç»ƒ RBM
- en: The goal of RBM training is to estimate the optimal vectorÂ ğœƒ of model parameters
    (weights and biases) so that â„™[ğœƒ](v) = â„™[data](v). For a given training sample
    v := (*v*[1]*,â€¦,v*[N]), the RBM aims at maximising the log-likelihood function,
    namely
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: RBM è®­ç»ƒçš„ç›®æ ‡æ˜¯ä¼°è®¡æ¨¡å‹å‚æ•°ï¼ˆæƒé‡å’Œåç½®ï¼‰çš„æœ€ä¼˜å‘é‡ ğœƒï¼Œä½¿å¾— â„™[ğœƒ](v) = â„™[data](v)ã€‚å¯¹äºç»™å®šçš„è®­ç»ƒæ ·æœ¬ v := (*v*[1]*,â€¦,v*[N])ï¼ŒRBM
    æ—¨åœ¨æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶å‡½æ•°ï¼Œå³
- en: '![ n max âˆ‘ ğ” (ğœƒ|v ), ğœƒ i=1 i ](img/file425.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![ n max âˆ‘ ğ” (ğœƒ|v ), ğœƒ i=1 i ](img/file425.jpg)'
- en: where, for any v,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œå¯¹äºä»»ä½• vï¼Œ
- en: '![ ( ) ( âˆ‘ ) ( âˆ‘ ) âˆ‘ ğ”(ğœƒ|v) = log(â„™(v)) = log -1 eâˆ’E(v,h) = log eâˆ’ E(v,h) âˆ’
    log ( eâˆ’E (v,h)) . Z h h v,h ](img/file426.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![ ( ) ( âˆ‘ ) ( âˆ‘ ) âˆ‘ ğ”(ğœƒ|v) = log(â„™(v)) = log -1 eâˆ’E(v,h) = log eâˆ’ E(v,h) âˆ’
    log ( eâˆ’E (v,h)) . Z h h v,h ](img/file426.jpg)'
- en: The standard optimisation method, as proposed inÂ Â [[133](Biblography.xhtml#XHinton2002)],
    is a standard gradient ascent method, i.e., starting from an initial guessÂ ğœƒâ°,
    we update it as
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚ [[133](Biblography.xhtml#XHinton2002)] æ‰€æå‡ºçš„ï¼Œæ˜¯ä¸€ç§æ ‡å‡†çš„æ¢¯åº¦ä¸Šå‡æ–¹æ³•ï¼Œå³ä»åˆå§‹çŒœæµ‹ ğœƒâ°
    å¼€å§‹ï¼Œæˆ‘ä»¬æŒ‰ä»¥ä¸‹æ–¹å¼æ›´æ–°ï¼š
- en: '![ N ğœƒk+1 = ğœƒk + âˆ‚ âˆ‘ ğ” (ğœƒk|v ) ğœƒ i=1 i ](img/file427.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![ N ğœƒk+1 = ğœƒk + âˆ‚ âˆ‘ ğ” (ğœƒk|v ) ğœƒ i=1 i ](img/file427.jpg)'
- en: until we reach good enough convergence. In order to compute it, one first needs
    to compute the joint probabilities â„™(*v*[i]*,h*[j]), which is classically done
    via Boltzmann (Gibbs) samplingÂ Â [[3](Biblography.xhtml#XAckley)], which is possible
    since we know exactly the conditional distributions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´åˆ°æˆ‘ä»¬è¾¾åˆ°è¶³å¤Ÿå¥½çš„æ”¶æ•›ã€‚ä¸ºäº†è®¡ç®—å®ƒï¼Œé¦–å…ˆéœ€è¦è®¡ç®—è”åˆæ¦‚ç‡ â„™(*v*[i]*,h*[j])ï¼Œè¿™é€šå¸¸é€šè¿‡ç»å°”å…¹æ›¼ï¼ˆå‰å¸ƒæ–¯ï¼‰é‡‡æ ·å®Œæˆ [[3](Biblography.xhtml#XAckley)]ï¼Œå› ä¸ºæˆ‘ä»¬ç¡®åˆ‡çŸ¥é“æ¡ä»¶åˆ†å¸ƒã€‚
- en: 5.3.2 The Contrastive Divergence algorithm
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 å¯¹æ¯”æ•£åº¦ç®—æ³•
- en: While training RBMs can be performed with Boltzmann sampling, this is usually
    prohibitively expensive to run. A more efficient training algorithm, the *k*-step
    Contrastive Divergence (CD) algorithm, was proposed inÂ Â [[134](Biblography.xhtml#XHinton2010)].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒ RBM æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ç»å°”å…¹æ›¼é‡‡æ ·ï¼Œä½†è¿™é€šå¸¸æ˜¯éå¸¸æ˜‚è´µçš„ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæå‡ºäº†ä¸€ç§æ›´é«˜æ•ˆçš„è®­ç»ƒç®—æ³•â€”â€”*k*-æ­¥å¯¹æ¯”æ•£åº¦ï¼ˆCDï¼‰ç®—æ³•ï¼Œå‚è€ƒäº† [[134](Biblography.xhtml#XHinton2010)]ã€‚
- en: '![--------------------------------------------------------------------- -Algorithm---2:-k-
    step-Contrastive-Divergence-------------------------- Result: Weights and biases
    updates. Input: â€¢ Training minibatch S; â€¢ Model parameters ai,bj,wij for i = 1,...,N,j
    = 1,...,M (before update ). Initialisation: for all i,j : Î”wij = Î”ai = Î”bj = 0
    for v âˆˆ S do | v(0) â† v | | for t = 0,...,k âˆ’ 1 do | | for j = 1,...,M do | |
    | (t) (t) | | | sample Bernoulli random variable hj âˆ¼ â„™(hj|v ) | | end | | | |
    for i = 1,...,N do | | | sample Bernoulli random variable v(t+1) âˆ¼ â„™(vi|h(t))
    | | i | end | end | | for i = 1,...,N, j = 1(,...,M do ) | | (0) (0) (k) (k) |
    Î”wij â† Î”wij + Î· â„™(hj = 1|v )vi âˆ’ â„™(hj = 1|v )vi | end | | for i = 1,...,N do(
    ) | | Î”ai â† Î”ai + Î· v(0) âˆ’ v(k) | i i | end | | for j = 1,...,M do( ) | | Î”bj
    â† Î”bj + Î· â„™(hj = 1|v(0)) âˆ’ â„™(hj = 1|v(k)) | end end ---------------------------------------------------------------------
    ](img/file428.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- -ç®—æ³•---2:-k-æ­¥-å¯¹æ¯”-æ•£åº¦--------------------------
    ç»“æœï¼šæƒé‡å’Œåç½®æ›´æ–°ã€‚è¾“å…¥ï¼š â€¢ è®­ç»ƒå°æ‰¹é‡ Sï¼› â€¢ æ¨¡å‹å‚æ•° ai,bj,wijï¼Œå¯¹äº i = 1,...,N,j = 1,...,Mï¼ˆæ›´æ–°å‰ï¼‰ã€‚åˆå§‹åŒ–ï¼šå¯¹æ‰€æœ‰
    i,j : Î”wij = Î”ai = Î”bj = 0 å¯¹äº v âˆˆ S åš | v(0) â† v | | å¯¹ t = 0,...,k âˆ’ 1 åš | | å¯¹
    j = 1,...,M åš | | | (t) (t) | | | é‡‡æ ·ä¼¯åŠªåˆ©éšæœºå˜é‡ hj âˆ¼ â„™(hj|v ) | | ç»“æŸ | | | | å¯¹ i =
    1,...,N åš | | | é‡‡æ ·ä¼¯åŠªåˆ©éšæœºå˜é‡ v(t+1) âˆ¼ â„™(vi|h(t)) | | i | ç»“æŸ | ç»“æŸ | | å¯¹ i = 1,...,N,
    j = 1(,...,M åš ) | | (0) (0) (k) (k) | Î”wij â† Î”wij + Î· â„™(hj = 1|v )vi âˆ’ â„™(hj =
    1|v )vi | ç»“æŸ | | å¯¹ i = 1,...,N åš( ) | | Î”ai â† Î”ai + Î· v(0) âˆ’ v(k) | i i | ç»“æŸ |
    | å¯¹ j = 1,...,M åš( ) | | Î”bj â† Î”bj + Î· â„™(hj = 1|v(0)) âˆ’ â„™(hj = 1|v(k)) | ç»“æŸ ç»“æŸ
    --------------------------------------------------------------------- ](img/file428.jpg)'
- en: 'The choice of *k* balances accuracy and speed. For many practical purposes
    *k* = 1 is an optimal choice, even though the expectations may be biased in this
    case. However, the bias tends to be smallÂ Â [[53](Biblography.xhtml#XCarreira2005)].
    The network is trained through the updates of weights and biases, which increase
    the log probability of a training vector and are given by the following expressions:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* çš„é€‰æ‹©å¹³è¡¡äº†å‡†ç¡®æ€§å’Œé€Ÿåº¦ã€‚å¯¹äºè®¸å¤šå®é™…åº”ç”¨ï¼Œ*k* = 1 æ˜¯ä¸€ä¸ªæœ€ä½³é€‰æ‹©ï¼Œå°½ç®¡åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒæœŸæœ›å€¼å¯èƒ½ä¼šæœ‰åå·®ã€‚ç„¶è€Œï¼Œè¿™ç§åå·®å¾€å¾€æ˜¯å¾ˆå°çš„
    [[53](Biblography.xhtml#XCarreira2005)]ã€‚ç½‘ç»œé€šè¿‡æ›´æ–°æƒé‡å’Œåç½®æ¥è®­ç»ƒï¼Œè¿™äº›æ›´æ–°å¢åŠ äº†è®­ç»ƒå‘é‡çš„å¯¹æ•°æ¦‚ç‡ï¼Œæ›´æ–°å…¬å¼å¦‚ä¸‹ï¼š'
- en: '| ![Î”w = Î·âˆ‚â„™(v)-= Î· (âŸ¨v h âŸ© âˆ’ âŸ¨v h âŸ© ), ij âˆ‚wij i j data i j model ](img/file429.jpg)
    |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ![Î”w = Î·âˆ‚â„™(v)-= Î· (âŸ¨v h âŸ© âˆ’ âŸ¨v h âŸ© ), ij âˆ‚wij i j data i j model ](img/file429.jpg)
    |  |'
- en: '| ![ âˆ‚-â„™(v) Î”ai = Î· âˆ‚ai = Î·(âŸ¨viâŸ©data âˆ’ âŸ¨viâŸ©model), ](img/file430.jpg) |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‚-â„™(v) Î”ai = Î· âˆ‚ai = Î·(âŸ¨viâŸ©data âˆ’ âŸ¨viâŸ©model), ](img/file430.jpg) |  |'
- en: '| ![ âˆ‚â„™ (v) Î”bj = Î·------= Î· (âŸ¨hjâŸ©data âˆ’ âŸ¨hjâŸ©model), âˆ‚bi ](img/file431.jpg)
    |  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‚â„™ (v) Î”bj = Î·------= Î· (âŸ¨hjâŸ©data âˆ’ âŸ¨hjâŸ©model), âˆ‚bi ](img/file431.jpg)
    |  |'
- en: where âŸ¨â‹…âŸ© denote expectations under the distribution specified by the subscript
    and *Î·* is the chosen learning rate. Expectations âŸ¨â‹…âŸ©[data] can be calculated
    directly from the training dataset while getting unbiased samples of âŸ¨â‹…âŸ©[model]
    requires performing alternating sampling from the model Boltzmann distribution
    for a long time (this is needed to achieve the state of thermal equilibrium),
    starting from some randomly initialised state. However, the *k*-step CD method
    can be used to approximate âŸ¨â‹…âŸ©[model] with another, easier-to-calculate expectation,
    as shown in AlgorithmÂ [2](#x1-106003r2).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ âŸ¨â‹…âŸ© è¡¨ç¤ºåœ¨ä¸‹æ ‡æ‰€æŒ‡å®šçš„åˆ†å¸ƒä¸‹çš„æœŸæœ›ï¼Œ*Î·* æ˜¯é€‰æ‹©çš„å­¦ä¹ ç‡ã€‚æœŸæœ› âŸ¨â‹…âŸ©[data] å¯ä»¥ç›´æ¥ä»è®­ç»ƒæ•°æ®é›†è®¡ç®—ï¼Œè€Œè·å¾—æ— åæ ·æœ¬ âŸ¨â‹…âŸ©[model]
    åˆ™éœ€è¦åœ¨æ¨¡å‹çš„ç»å°”å…¹æ›¼åˆ†å¸ƒä¸­è¿›è¡Œäº¤æ›¿é‡‡æ ·ä¸€æ®µè¾ƒé•¿æ—¶é—´ï¼ˆè¿™æ˜¯ä¸ºäº†å®ç°çƒ­å¹³è¡¡çŠ¶æ€ï¼‰ï¼Œå¹¶ä¸”ä»æŸä¸ªéšæœºåˆå§‹åŒ–çš„çŠ¶æ€å¼€å§‹ã€‚ç„¶è€Œï¼Œ*k* æ­¥éª¤çš„CDæ–¹æ³•å¯ä»¥ç”¨æ¥è¿‘ä¼¼ âŸ¨â‹…âŸ©[model]ï¼Œé€šè¿‡å¦ä¸€ä¸ªæ›´æ˜“è®¡ç®—çš„æœŸæœ›ï¼Œå¦‚ç®—æ³•[2](#x1-106003r2)æ‰€ç¤ºã€‚
- en: 5.3.3 Generation of synthetic samples
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3 åˆæˆæ ·æœ¬çš„ç”Ÿæˆ
- en: Once fully trained, the network can be used to generate new samples from the
    learned distribution. For example, the RBM can be used as a market generator that
    produces new market scenarios in the form of the new synthetic samples drawn from
    the multivariate distribution of the market risk factors encoded in the network
    weights and biases.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ç½‘ç»œå®Œå…¨è®­ç»ƒå¥½ï¼Œå°±å¯ä»¥ç”¨æ¥ä»å­¦ä¹ åˆ°çš„åˆ†å¸ƒä¸­ç”Ÿæˆæ–°çš„æ ·æœ¬ã€‚ä¾‹å¦‚ï¼ŒRBMå¯ä»¥ä½œä¸ºä¸€ä¸ªå¸‚åœºç”Ÿæˆå™¨ï¼Œç”Ÿæˆæ–°çš„å¸‚åœºæƒ…æ™¯ï¼Œè¿™äº›æ–°æ ·æœ¬æ¥è‡ªäºç½‘ç»œæƒé‡å’Œåç½®ä¸­ç¼–ç çš„å¸‚åœºé£é™©å› ç´ çš„å¤šå…ƒåˆ†å¸ƒã€‚
- en: 'The first step is the generation of a random input: each visible unit is initialised
    with a randomly generated binary variable. The second step is performing a large
    number of forward and backward passes between the visible and the hidden layers,
    until the system reaches a state of *thermal equilibrium*: a state where the initial
    random vector is transformed into a sample from the learned distribution. The
    number of cycles needed to reach the state of thermal equilibrium is problem dependent
    and is a function of network architecture and network parameters (weights and
    biases). In some cases, the generation of independent samples requires 10Â³ âˆ’ 10â´
    forward and backward passes through the networkÂ Â [[173](Biblography.xhtml#XKS2020)].
    The final step is the readout from the visible layer, which gives us a bitstring,
    encoding the sample from the target distribution.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¯ç”Ÿæˆä¸€ä¸ªéšæœºè¾“å…¥ï¼šæ¯ä¸ªå¯è§å•å…ƒéƒ½è¢«åˆå§‹åŒ–ä¸ºä¸€ä¸ªéšæœºç”Ÿæˆçš„äºŒè¿›åˆ¶å˜é‡ã€‚ç¬¬äºŒæ­¥æ˜¯åœ¨å¯è§å±‚å’Œéšè—å±‚ä¹‹é—´è¿›è¡Œå¤§é‡çš„å‰å‘å’Œåå‘ä¼ æ’­ï¼Œç›´åˆ°ç³»ç»Ÿè¾¾åˆ°*çƒ­å¹³è¡¡*çŠ¶æ€ï¼šå³åˆå§‹çš„éšæœºå‘é‡è¢«è½¬æ¢ä¸ºä»å­¦ä¹ åˆ°çš„åˆ†å¸ƒä¸­æŠ½å–çš„æ ·æœ¬ã€‚è¾¾åˆ°çƒ­å¹³è¡¡çŠ¶æ€æ‰€éœ€çš„å‘¨æœŸæ•°å–å†³äºå…·ä½“é—®é¢˜ï¼Œæ˜¯ç½‘ç»œæ¶æ„å’Œç½‘ç»œå‚æ•°ï¼ˆæƒé‡å’Œåç½®ï¼‰çš„å‡½æ•°ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç”Ÿæˆç‹¬ç«‹æ ·æœ¬éœ€è¦è¿›è¡Œ10Â³
    âˆ’ 10â´æ¬¡å‰å‘å’Œåå‘ä¼ æ’­[[173](Biblography.xhtml#XKS2020)]ã€‚æœ€åä¸€æ­¥æ˜¯ä»å¯è§å±‚è¯»å–è¾“å‡ºï¼Œè¿™å°†ç»™æˆ‘ä»¬ä¸€ä¸ªæ¯”ç‰¹ä¸²ï¼Œç¼–ç äº†æ¥è‡ªç›®æ ‡åˆ†å¸ƒçš„æ ·æœ¬ã€‚
- en: 'FigureÂ [5.3](#5.3) displays the QQ-plots of the samples drawn from the distributions
    of daily returns for two stock indices: German DAX and Brazilian BOVESPA. Recall
    that a quantile-quantile (or QQ) plot is a scatter plot created by plotting two
    sets of quantiles against one another. If both sets come from the same distribution,
    all points should lie close to the diagonal. The dataset consists of 536 samplesÂ â€“Â daily
    index returns observed between 5 January 2009 and 22 February 2011 (UCI Machine
    Learning RepositoryÂ Â [[10](Biblography.xhtml#XAkbilgic2013),Â [9](Biblography.xhtml#XUCI_SP)]).
    The "Normal" distribution models daily returns as Normally distributed with a
    mean and variance that match those from the historical dataset. The "RBM" distribution
    is a dataset of RBM-generated samples that, ideally, should have exactly the same
    statistical properties as the original historical dataset. If the samples drawn
    from two distributions have identical quantiles, the QQ-plots will have all points
    placed on the diagonal and we can conclude that the two distributions are identical.
    FigureÂ [5.3](#5.3) shows that this is indeed the case (with reasonably good accuracy)
    for the samples from the "Data" and "RBM" distributions, while both demonstrate
    much heavier tails in comparison with the fitted Normal distribution.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[5.3](#5.3)å±•ç¤ºäº†ä»ä¸¤ä¸ªè‚¡ç¥¨æŒ‡æ•°çš„æ¯æ—¥å›æŠ¥åˆ†å¸ƒä¸­æŠ½å–çš„æ ·æœ¬çš„QQå›¾ï¼šå¾·å›½DAXå’Œå·´è¥¿BOVESPAã€‚å›æƒ³ä¸€ä¸‹ï¼Œåˆ†ä½æ•°-åˆ†ä½æ•°ï¼ˆæˆ–QQï¼‰å›¾æ˜¯é€šè¿‡å°†ä¸¤ä¸ªåˆ†ä½æ•°é›†ç›¸äº’å¯¹ç…§ï¼Œç»˜åˆ¶æˆçš„æ•£ç‚¹å›¾ã€‚å¦‚æœè¿™ä¸¤ä¸ªé›†æ¥è‡ªåŒä¸€åˆ†å¸ƒï¼Œæ‰€æœ‰ç‚¹åº”è¯¥æ¥è¿‘å¯¹è§’çº¿ã€‚æ•°æ®é›†åŒ…å«536ä¸ªæ ·æœ¬â€“
    ä»2009å¹´1æœˆ5æ—¥åˆ°2011å¹´2æœˆ22æ—¥ä¹‹é—´è§‚å¯Ÿåˆ°çš„æ¯æ—¥æŒ‡æ•°å›æŠ¥ï¼ˆUCIæœºå™¨å­¦ä¹ åº“[[10](Biblography.xhtml#XAkbilgic2013)ï¼Œ[9](Biblography.xhtml#XUCI_SP)]ï¼‰ã€‚"Normal"åˆ†å¸ƒå°†æ¯æ—¥å›æŠ¥å»ºæ¨¡ä¸ºæ­£æ€åˆ†å¸ƒï¼Œå…¶å‡å€¼å’Œæ–¹å·®ä¸å†å²æ•°æ®é›†ä¸­çš„åŒ¹é…ã€‚"RBM"åˆ†å¸ƒæ˜¯RBMç”Ÿæˆçš„æ ·æœ¬æ•°æ®é›†ï¼Œç†æƒ³æƒ…å†µä¸‹åº”å…·æœ‰ä¸åŸå§‹å†å²æ•°æ®é›†å®Œå…¨ç›¸åŒçš„ç»Ÿè®¡ç‰¹æ€§ã€‚å¦‚æœä»ä¸¤ä¸ªåˆ†å¸ƒä¸­æŠ½å–çš„æ ·æœ¬å…·æœ‰ç›¸åŒçš„åˆ†ä½æ•°ï¼ŒQQå›¾ä¸­çš„æ‰€æœ‰ç‚¹éƒ½å°†ä½äºå¯¹è§’çº¿å¤„ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œè¿™ä¸¤ä¸ªåˆ†å¸ƒæ˜¯ç›¸åŒçš„ã€‚å›¾[5.3](#5.3)æ˜¾ç¤ºï¼Œæ¥è‡ª"Data"å’Œ"RBM"åˆ†å¸ƒçš„æ ·æœ¬ç¡®å®ç¬¦åˆè¿™ä¸€ç‚¹ï¼ˆå…·æœ‰ç›¸å½“å¥½çš„å‡†ç¡®æ€§ï¼‰ï¼Œè€Œä¸¤è€…åœ¨ä¸æ‹Ÿåˆçš„æ­£æ€åˆ†å¸ƒç›¸æ¯”æ—¶ï¼Œæ˜¾ç¤ºå‡ºæ›´é‡çš„å°¾éƒ¨ã€‚
- en: 'The results shown in FigureÂ [5.3](#5.3) were obtained with an RBM trained on
    a dataset of daily returns. Each return from the training dataset was converted
    into a 12-digit binary number. Every digit of the binary number was treated as
    a separate binary feature (12 features per index; 24 features in total)Â â€“Â this
    required placing 24 activation units in the visible layer of the RBM network.
    The number of hidden units was set to 16\. Thus, the network was trained as a
    strongly regularised autoencoder. The generated returns (in the binary format)
    were then converted back into their continuous representation. The model was Bernoulli
    RBM (sklearn.neural_network.BernoulliRBM) from the open source `scikit-learn`
    packageÂ Â [[230](Biblography.xhtml#XSL)] with the following set of parameters:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[5.3](#5.3)ä¸­æ˜¾ç¤ºçš„ç»“æœæ˜¯é€šè¿‡åœ¨æ¯æ—¥å›æŠ¥æ•°æ®é›†ä¸Šè®­ç»ƒçš„RBMå¾—åˆ°çš„ã€‚æ¯ä¸ªæ¥è‡ªè®­ç»ƒæ•°æ®é›†çš„å›æŠ¥éƒ½è¢«è½¬æ¢ä¸ºä¸€ä¸ª12ä½äºŒè¿›åˆ¶æ•°ã€‚äºŒè¿›åˆ¶æ•°çš„æ¯ä¸€ä½è¢«å½“ä½œä¸€ä¸ªç‹¬ç«‹çš„äºŒè¿›åˆ¶ç‰¹å¾å¤„ç†ï¼ˆæ¯ä¸ªç´¢å¼•æœ‰12ä¸ªç‰¹å¾ï¼›æ€»å…±æœ‰24ä¸ªç‰¹å¾ï¼‰â€“
    è¿™è¦æ±‚åœ¨RBMç½‘ç»œçš„å¯è§å±‚ä¸­æ”¾ç½®24ä¸ªæ¿€æ´»å•å…ƒã€‚éšè—å•å…ƒçš„æ•°é‡è®¾ç½®ä¸º16ã€‚å› æ­¤ï¼Œç½‘ç»œä½œä¸ºä¸€ä¸ªå¼ºæ­£åˆ™åŒ–çš„è‡ªç¼–ç å™¨è¿›è¡Œè®­ç»ƒã€‚ç”Ÿæˆçš„å›æŠ¥ï¼ˆä»¥äºŒè¿›åˆ¶æ ¼å¼ï¼‰éšåè¢«è½¬æ¢å›å®ƒä»¬çš„è¿ç»­è¡¨ç¤ºã€‚è¯¥æ¨¡å‹æ˜¯æ¥è‡ªå¼€æº`scikit-learn`åŒ…çš„Bernoulli
    RBMï¼ˆsklearn.neural_network.BernoulliRBMï¼‰[[230](Biblography.xhtml#XSL)]ï¼Œå…¶å‚æ•°è®¾ç½®å¦‚ä¸‹ï¼š
- en: n_components = 16 â€“ number of hidden activation units
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n_components = 16 â€“ éšè—æ¿€æ´»å•å…ƒçš„æ•°é‡
- en: learning_rate = 0.0005
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: learning_rate = 0.0005
- en: batch_size = 10 â€“ size of the training minibatches
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: batch_size = 10 â€“ è®­ç»ƒå°æ‰¹é‡çš„å¤§å°
- en: n_iter = 40000 â€“ number of iterations
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n_iter = 40000 â€“ è¿­ä»£æ¬¡æ•°
- en: The synthetic data generation approach can be formulated as AlgorithmÂ [3](#x1-107014r3).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•å¯ä»¥è¡¨è¿°ä¸ºç®—æ³•[3](#x1-107014r3)ã€‚
- en: '![--------------------------------------------------------------------- -Algorithm---3:-Synthetic-Data-Generation-----------------------------
    1: The construction of the binary representation of the original dataset: a) A
    continuous feature can be converted into an equivalent binary representation with
    the required precision. b) An integer feature x âˆˆ {x1,...,xn } can be translated
    into an N -digit binary number through the standard procedure, where Nâˆ’1 N 2 â‰¤
    1mâ‰¤ajxâ‰¤n(xj)âˆ’ 1mâ‰¤ijnâ‰¤n(xj) < 2 . c) A categorical feature can be binarised either
    through the one-hot encoding method or following the same procedure as for the
    integer numbers since categorical values can be enumerated. d) The same applies
    to class labels, both integer and categorical. 2: The training of an RBM on the
    binary representation of the original dataset with the help of a 1-step CD algorithm.
    3: The generation of the required number of new synthetic samples in binary format.
    4: For each synthetic data sample: the conversion of the generated binary features
    into the corresponding categorical, integer, and continuous representations. 5:
    The generated synthetic dataset is ready to be used for the training of various
    classifiers and regressors. ---------------------------------------------------------------------
    ](img/file432.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- -ç®—æ³•---3:-åˆæˆæ•°æ®ç”Ÿæˆ-----------------------------
    1: åŸå§‹æ•°æ®é›†äºŒè¿›åˆ¶è¡¨ç¤ºçš„æ„å»ºï¼ša) è¿ç»­ç‰¹å¾å¯ä»¥è½¬æ¢ä¸ºå…·æœ‰æ‰€éœ€ç²¾åº¦çš„ç­‰æ•ˆäºŒè¿›åˆ¶è¡¨ç¤ºã€‚b) æ•´æ•°ç‰¹å¾ x âˆˆ {x1,...,xn} å¯ä»¥é€šè¿‡æ ‡å‡†ç¨‹åºè½¬æ¢ä¸ºNä½äºŒè¿›åˆ¶æ•°ï¼Œå…¶ä¸­
    Nâˆ’1 N 2 â‰¤ 1mâ‰¤ajxâ‰¤n(xj)âˆ’ 1mâ‰¤ijnâ‰¤n(xj) < 2ã€‚c) ç±»åˆ«ç‰¹å¾å¯ä»¥é€šè¿‡ç‹¬çƒ­ç¼–ç æ–¹æ³•æˆ–ä¸æ•´æ•°ç‰¹å¾ç›¸åŒçš„ç¨‹åºè¿›è¡ŒäºŒå€¼åŒ–ï¼Œå› ä¸ºç±»åˆ«å€¼å¯ä»¥æšä¸¾ã€‚d)
    ç±»åˆ«æ ‡ç­¾åŒæ ·é€‚ç”¨ï¼ŒåŒ…æ‹¬æ•´æ•°å’Œç±»åˆ«ç±»å‹ã€‚2: ä½¿ç”¨1æ­¥CDç®—æ³•åœ¨åŸå§‹æ•°æ®é›†çš„äºŒè¿›åˆ¶è¡¨ç¤ºä¸Šè®­ç»ƒRBMã€‚3: ç”Ÿæˆæ‰€éœ€æ•°é‡çš„æ–°åˆæˆæ ·æœ¬ï¼ˆä»¥äºŒè¿›åˆ¶æ ¼å¼ï¼‰ã€‚4: å¯¹äºæ¯ä¸ªåˆæˆæ•°æ®æ ·æœ¬ï¼šå°†ç”Ÿæˆçš„äºŒè¿›åˆ¶ç‰¹å¾è½¬æ¢ä¸ºç›¸åº”çš„ç±»åˆ«ã€æ•´æ•°å’Œè¿ç»­è¡¨ç¤ºã€‚5:
    ç”Ÿæˆçš„åˆæˆæ•°æ®é›†å·²å‡†å¤‡å¥½ç”¨äºå„ç§åˆ†ç±»å™¨å’Œå›å½’å™¨çš„è®­ç»ƒã€‚ ---------------------------------------------------------------------](img/file432.jpg)'
- en: '![Figurex1-107016r3: QQ-plots of the generated and historical returns. a)-c)Â DAX.
    d)-f)Â BOVESPA. The RBM learns the heavy-tailed empirical distribution of stock
    index returns. ](img/file433.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![Figurex1-107016r3: ç”Ÿæˆçš„å’Œå†å²å›æŠ¥çš„QQå›¾ã€‚a)-c) DAX. d)-f) BOVESPAã€‚RBMå­¦ä¹ äº†è‚¡ç¥¨æŒ‡æ•°å›æŠ¥çš„é‡å°¾ç»éªŒåˆ†å¸ƒã€‚](img/file433.png)'
- en: 'FigureÂ 5.3: QQ-plots of the generated and historical returns. a)-c)Â DAX. d)-f)Â BOVESPA.
    The RBM learns the heavy-tailed empirical distribution of stock index returns.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.3ï¼šç”Ÿæˆçš„å’Œå†å²å›æŠ¥çš„QQå›¾ã€‚a)-c) DAX. d)-f) BOVESPAã€‚RBMå­¦ä¹ äº†è‚¡ç¥¨æŒ‡æ•°å›æŠ¥çš„é‡å°¾ç»éªŒåˆ†å¸ƒã€‚
- en: Kondratyev and SchwarzÂ Â [[173](Biblography.xhtml#XKS2020)] proposed an RBM-based
    market generator and investigated its properties on a dataset of daily spot FX
    log-returns. The time series of four currency pairsâ€™ log-returns covered a 20-year
    time interval (1999-2019), which allowed the RBM to learn the dependence structure
    of the multivariate distribution and successfully reconstruct linear and rank
    correlations as well as joint tail behaviour. Also, it was shown that an RBM can
    be used to perform conditional sampling (e.g., from low-volatility/high-volatility
    regimes) and achieve the desired degree of autocorrelation by varying the thermalisation
    parameter. Other productive applications of RBM-based synthetic data generators
    are data anonymisation, fighting overfitting, and the detection of outliers as
    demonstrated by Kondratyev, Schwarz, and HorvathÂ Â [[174](Biblography.xhtml#XKSH2020)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Kondratyev å’Œ Schwarz [[173](Biblography.xhtml#XKS2020)] æå‡ºäº†åŸºäºRBMçš„å¸‚åœºç”Ÿæˆå™¨ï¼Œå¹¶ç ”ç©¶äº†å…¶åœ¨æ—¥å¸¸å¤–æ±‡å¯¹æ•°å›æŠ¥æ•°æ®é›†ä¸Šçš„å±æ€§ã€‚å››ä¸ªè´§å¸å¯¹çš„å¯¹æ•°å›æŠ¥æ—¶é—´åºåˆ—æ¶µç›–äº†20å¹´çš„æ—¶é—´é—´éš”ï¼ˆ1999-2019ï¼‰ï¼Œè¿™ä½¿å¾—RBMèƒ½å¤Ÿå­¦ä¹ å¤šå˜é‡åˆ†å¸ƒçš„ä¾èµ–ç»“æ„ï¼Œå¹¶æˆåŠŸé‡å»ºçº¿æ€§å’Œç§©ç›¸å…³æ€§ä»¥åŠè”åˆå°¾éƒ¨è¡Œä¸ºã€‚æ­¤å¤–ï¼Œè¿˜è¡¨æ˜ï¼ŒRBMå¯ä»¥ç”¨äºæ‰§è¡Œæ¡ä»¶é‡‡æ ·ï¼ˆä¾‹å¦‚ï¼Œä»ä½æ³¢åŠ¨/é«˜æ³¢åŠ¨çŠ¶æ€ï¼‰ï¼Œå¹¶é€šè¿‡è°ƒæ•´çƒ­åŒ–å‚æ•°æ¥å®ç°æ‰€éœ€çš„è‡ªç›¸å…³ç¨‹åº¦ã€‚RBMåŸºäºçš„åˆæˆæ•°æ®ç”Ÿæˆå™¨çš„å…¶ä»–æœ‰æ•ˆåº”ç”¨åŒ…æ‹¬æ•°æ®åŒ¿ååŒ–ã€å¯¹æŠ—è¿‡æ‹Ÿåˆå’Œå¼‚å¸¸å€¼æ£€æµ‹ï¼Œæ­£å¦‚Kondratyevã€Schwarzå’ŒHorvath
    [[174](Biblography.xhtml#XKSH2020)] æ‰€å±•ç¤ºçš„é‚£æ ·ã€‚
- en: In addition to operating on stochastic binary activation units, the RBM gains
    extra resistance to overfitting through the autoencoder architecture and being
    trained with stochastic gradient ascent. This allows RBMs to learn complex multivariate
    probability distributions from relatively small datasets while avoiding overfitting.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†åœ¨éšæœºäºŒè¿›åˆ¶æ¿€æ´»å•å…ƒä¸Šæ“ä½œå¤–ï¼ŒRBMè¿˜é€šè¿‡è‡ªç¼–ç å™¨æ¶æ„å’Œä½¿ç”¨éšæœºæ¢¯åº¦ä¸Šå‡æ³•è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå¢å¼ºäº†å¯¹è¿‡æ‹Ÿåˆçš„æŠµæŠ—åŠ›ã€‚è¿™ä½¿å¾—RBMèƒ½å¤Ÿä»ç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†ä¸­å­¦ä¹ å¤æ‚çš„å¤šå˜é‡æ¦‚ç‡åˆ†å¸ƒï¼ŒåŒæ—¶é¿å…äº†è¿‡æ‹Ÿåˆã€‚
- en: 5.4 Quantum Annealing and Boltzmann Sampling
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 é‡å­é€€ç«ä¸ç»å°”å…¹æ›¼é‡‡æ ·
- en: The application of quantum annealing to Boltzmann sampling is based on the direct
    correspondence between the RBM energy function given byÂ ([5.2.4](#x1-1020004))
    and the Hamiltonian in quantum annealing. Recall from ChapterÂ [2](Chapter_2.xhtml#x1-480002)
    that quantum annealing is based on the principles of adiabatic evolution from
    the initial state at *t* = 0 given by a HamiltonianÂ â„‹[0] to a final state at *t*
    = *T* given by a HamiltonianÂ â„‹[F] , such that the system Hamiltonian at time *t*
    âˆˆ [0*,T*] is given by
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å°†é‡å­é€€ç«åº”ç”¨äºç»å°”å…¹æ›¼é‡‡æ ·åŸºäºRBMèƒ½é‡å‡½æ•°ï¼ˆè§[5.2.4](#x1-1020004)ï¼‰ä¸é‡å­é€€ç«ä¸­çš„å“ˆå¯†é¡¿é‡ä¹‹é—´çš„ç›´æ¥å¯¹åº”å…³ç³»ã€‚å›é¡¾ç¬¬[2](Chapter_2.xhtml#x1-480002)ç« ï¼Œé‡å­é€€ç«åŸºäºç»çƒ­æ¼”åŒ–çš„åŸåˆ™ï¼Œä»åˆæ€
    *t* = 0å¼€å§‹ï¼Œç”±å“ˆå¯†é¡¿é‡â„‹[0]å®šä¹‰ï¼Œåˆ°æœ€ç»ˆçŠ¶æ€ *t* = *T* ç»“æŸï¼Œç”±å“ˆå¯†é¡¿é‡â„‹[F]å®šä¹‰ï¼Œç³»ç»Ÿåœ¨æ—¶é—´ *t* âˆˆ [0, T]æ—¶çš„å“ˆå¯†é¡¿é‡ä¸ºï¼š
- en: '| ![â„‹ (t) = r(t)â„‹0 + (1âˆ’ r(t))â„‹F , ](img/file434.jpg) |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| ![â„‹ (t) = r(t)â„‹0 + (1âˆ’ r(t))â„‹F , ](img/file434.jpg) |  |'
- en: 'where *r*(*t*) decreases fromÂ 1 toÂ 0 asÂ *t* goes fromÂ 0 toÂ *T*. An ideal adiabatic
    evolution scenario envisages the system always staying in the ground state of
    â„‹(*t*): if the system starts in the ground state ofÂ â„‹[0] and the evolution proceeds
    slowly enough to satisfy the conditions of the quantum adiabatic theorem (ChapterÂ [2](Chapter_2.xhtml#x1-480002)),
    then the system will end up in the ground state ofÂ â„‹[F] .'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *r*(*t*) ä»1é€’å‡åˆ°0ï¼Œéšç€ *t* ä»0åˆ° *T* å˜åŒ–ã€‚ç†æƒ³çš„ç»çƒ­æ¼”åŒ–åœºæ™¯è®¾æƒ³ç³»ç»Ÿå§‹ç»ˆä¿æŒåœ¨â„‹(*t*)çš„åŸºæ€ï¼šå¦‚æœç³»ç»Ÿä»â„‹[0]çš„åŸºæ€å¼€å§‹ï¼Œä¸”æ¼”åŒ–è¿‡ç¨‹è¶³å¤Ÿç¼“æ…¢ä»¥æ»¡è¶³é‡å­ç»çƒ­å®šç†çš„æ¡ä»¶ï¼ˆè§ç¬¬[2](Chapter_2.xhtml#x1-480002)ç« ï¼‰ï¼Œé‚£ä¹ˆç³»ç»Ÿæœ€ç»ˆå°†è¾¾åˆ°â„‹[F]çš„åŸºæ€ã€‚
- en: In practice, existing quantum annealing hardware does not strictly satisfy the
    conditions of the quantum adiabatic theorem. Quantum annealers operate at very
    low temperatures of about 15mKÂ Â [[90](Biblography.xhtml#XDW2020)], but some residual
    thermal noise is still present. There is also some amount of cross-talk between
    the qubits and the chains of physical qubits that represent logical qubits can
    be broken. Cross-talk is the effect of a desired action on one or more qubits
    unintentionally affecting one or more other qubits. In some cases, cross-talk
    is the major source of computational errors. This poses serious issues for quantum
    annealers solving optimisation problems where the main objective is to find an
    exact ground state. But some residual amount of thermal and electromagnetic noise
    is desirable if we want to use a quantum annealer as a sampler.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œç°æœ‰çš„é‡å­é€€ç«ç¡¬ä»¶å¹¶æ²¡æœ‰ä¸¥æ ¼æ»¡è¶³é‡å­ç»çƒ­å®šç†çš„æ¡ä»¶ã€‚é‡å­é€€ç«å™¨åœ¨çº¦15mKçš„éå¸¸ä½æ¸©ä¸‹å·¥ä½œ[[90](Biblography.xhtml#XDW2020)]ï¼Œä½†ä»ç„¶å­˜åœ¨ä¸€äº›æ®‹ä½™çš„çƒ­å™ªå£°ã€‚åŒæ—¶ï¼Œé‡å­æ¯”ç‰¹ä¹‹é—´ä¹Ÿä¼šæœ‰ä¸€äº›ä¸²æ‰°ï¼Œç‰©ç†é‡å­æ¯”ç‰¹çš„é“¾æ¡ä»£è¡¨é€»è¾‘é‡å­æ¯”ç‰¹æ—¶ï¼Œè¿™äº›é“¾æ¡å¯èƒ½ä¼šè¢«ç ´åã€‚ä¸²æ‰°æ˜¯æŒ‡æŸä¸ªæˆ–å¤šä¸ªé‡å­æ¯”ç‰¹çš„æœŸæœ›æ“ä½œä¸å°å¿ƒå½±å“åˆ°å…¶ä»–ä¸€ä¸ªæˆ–å¤šä¸ªé‡å­æ¯”ç‰¹ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¸²æ‰°æ˜¯è®¡ç®—é”™è¯¯çš„ä¸»è¦æ¥æºã€‚è¿™å¯¹é‡å­é€€ç«å™¨è§£å†³ä¼˜åŒ–é—®é¢˜æå‡ºäº†ä¸¥é‡çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å½“ä¸»è¦ç›®æ ‡æ˜¯æ‰¾åˆ°ç²¾ç¡®çš„åŸºæ€æ—¶ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æƒ³å°†é‡å­é€€ç«å™¨ä½œä¸ºé‡‡æ ·å™¨ä½¿ç”¨ï¼Œä¸€å®šé‡çš„æ®‹ä½™çƒ­å™ªå£°å’Œç”µç£å™ªå£°æ˜¯å¯ä»¥æ¥å—çš„ã€‚
- en: 5.4.1 Boltzmann sampling
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1 ç»å°”å…¹æ›¼é‡‡æ ·
- en: 'The quantum annealer as a sampling engine is based on the central proposal
    Â [[4](Biblography.xhtml#XAdachi2015)] that the distribution of excited states
    can be modelled as a Boltzmann distribution:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­é€€ç«å™¨ä½œä¸ºé‡‡æ ·å¼•æ“çš„åŸºç¡€æ˜¯ä¸­å¿ƒæè®®[[4](Biblography.xhtml#XAdachi2015)]ï¼Œå³æ¿€å‘æ€çš„åˆ†å¸ƒå¯ä»¥è¢«å»ºæ¨¡ä¸ºç»å°”å…¹æ›¼åˆ†å¸ƒï¼š
- en: '| ![ 1- â„™(x) = Z exp (âˆ’ Î² â„‹F (x )) , ](img/file435.jpg) |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| ![ 1- â„™(x) = Z exp (âˆ’ Î² â„‹F (x )) , ](img/file435.jpg) |  |'
- en: 'where *Î²* is some parameter (which can be seen as an effective inverse temperature)
    and *Z* is the partition function:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *Î²* æ˜¯æŸä¸ªå‚æ•°ï¼ˆå¯ä»¥è§†ä¸ºæœ‰æ•ˆçš„å€’æ•°æ¸©åº¦ï¼‰ï¼Œ*Z* æ˜¯é…åˆ†å‡½æ•°ï¼š
- en: '| ![ âˆ‘ Z = exp (âˆ’ Î²â„‹F (x)). x ](img/file436.jpg) |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘ Z = exp (âˆ’ Î²â„‹F (x)). x ](img/file436.jpg) |  |'
- en: 'If we define the binary vector x to be the concatenation of the visible node
    vector v and the hidden node vector h:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†äºŒè¿›åˆ¶å‘é‡xå®šä¹‰ä¸ºå¯è§èŠ‚ç‚¹å‘é‡vå’Œéšè—èŠ‚ç‚¹å‘é‡hçš„è¿æ¥ï¼š
- en: '| ![x := (v ,v ,...,v ,h ,h ,...,h ), 1 2 N 1 2 M ](img/file437.jpg) |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| ![x := (v ,v ,...,v ,h ,h ,...,h ), 1 2 N 1 2 M ](img/file437.jpg) |  |'
- en: 'then, by comparingÂ ([5.2.4](#x1-1020004)) andÂ ([5.4.1](#x1-1090001)), we can
    establish a direct correspondence between the energy functionÂ *E* and the HamiltonianÂ â„‹[F]
    . Therefore, we can suggest an alternative way of calculating the expectations
    âŸ¨â‹…âŸ©[model] formulated as in the following algorithmÂ Â [[4](Biblography.xhtml#XAdachi2015)]:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œé€šè¿‡æ¯”è¾ƒ[5.2.4](#x1-1020004)å’Œ[5.4.1](#x1-1090001)ï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹èƒ½é‡å‡½æ•° *E* ä¸å“ˆå¯†é¡¿é‡â„‹[F]ä¹‹é—´çš„ç›´æ¥å¯¹åº”å…³ç³»ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æå‡ºä¸€ç§æ›¿ä»£æ–¹æ³•æ¥è®¡ç®—æœŸæœ›å€¼
    âŸ¨â‹…âŸ©[model]ï¼Œè¯¥æ–¹æ³•æŒ‰ç…§ä»¥ä¸‹ç®—æ³•[[4](Biblography.xhtml#XAdachi2015)]è¿›è¡Œï¼š
- en: '![--------------------------------------------------------------------- -Algorithm---4:-Boltzmann--Sampling-----------------------------------
    1: Use the RBM energy function E as the final Hamiltonian â„‹F . 2: Run quantum
    annealing K times and collect the readout statistics for vi(k ) and hj(k), i =
    1,...,N , j = 1,...,M , k = 1,...,K. 3: Calculate the unbiased expectations: 1
    âˆ‘K âŸ¨vihjâŸ©model :=-- vi(k)hj(k), K k=1 1 âˆ‘K âŸ¨viâŸ©model := K- vi(k), k=1 âˆ‘K âŸ¨hjâŸ©model
    := 1- hj(k). K k=1 ---------------------------------------------------------------------
    ](img/file438.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![--------------------------------------------------------------------- -ç®—æ³•---4:-ç»å°”å…¹æ›¼--é‡‡æ ·-----------------------------------
    1: ä½¿ç”¨RBMèƒ½é‡å‡½æ•°Eä½œä¸ºæœ€ç»ˆçš„å“ˆå¯†é¡¿é‡â„‹Fã€‚ 2: è¿è¡Œé‡å­é€€ç«Kæ¬¡ï¼Œå¹¶æ”¶é›†vi(k)å’Œhj(k)çš„è¯»å–ç»Ÿè®¡é‡ï¼Œi = 1,...,Nï¼Œj = 1,...,Mï¼Œk
    = 1,...,Kã€‚ 3: è®¡ç®—æ— åæœŸæœ›å€¼ï¼š1 âˆ‘K âŸ¨vihjâŸ©model :=-- vi(k)hj(k), K k=1 1 âˆ‘K âŸ¨viâŸ©model :=
    K- vi(k), k=1 âˆ‘K âŸ¨hjâŸ©model := 1- hj(k)ã€‚ K k=1 ---------------------------------------------------------------------
    ](img/file438.jpg)'
- en: 'There are two main motivations for using quantum annealing to perform Boltzmann
    sampling as described in AlgorithmÂ [4](#x1-109006r4). First, it bypasses the need
    for running the Contrastive Divergence algorithm (AlgorithmÂ [2](#x1-106003r2)),
    which only provides approximations to the expectations âŸ¨â‹…âŸ©[model] (even though
    these approximations can be sufficiently accurate). Second, the anneal time needed
    to generate a new sample from the Boltzmann distribution is of the order ofÂ âˆ¼1Â microsecond
    regardless of the graph size. This is not the case with the classical RBM, where
    it is often necessary to perform thousands of forward and backward passes through
    the network before a new independent sample from the Boltzmann distribution encoded
    in the network weights and biases can be read outÂ Â [[173](Biblography.xhtml#XKS2020)].
    For large RBM graphs, it can easily take tens of milliseconds on standard hardware.
    Thus, we have two avenues of exploring the potential quantum advantage offered
    by quantum annealing for Boltzmann sampling: accuracy and speedup.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é‡å­é€€ç«è¿›è¡Œç»å°”å…¹æ›¼é‡‡æ ·æœ‰ä¸¤ä¸ªä¸»è¦çš„åŠ¨æœºï¼Œå¦‚ç®—æ³•[4](#x1-109006r4)æ‰€è¿°ã€‚é¦–å…ˆï¼Œå®ƒç»•è¿‡äº†è¿è¡Œå¯¹æ¯”æ•£åº¦ç®—æ³•ï¼ˆç®—æ³•[2](#x1-106003r2)ï¼‰çš„éœ€è¦ï¼Œè¯¥ç®—æ³•ä»…æä¾›å¯¹æœŸæœ›å€¼âŸ¨â‹…âŸ©[model]çš„è¿‘ä¼¼å€¼ï¼ˆå°½ç®¡è¿™äº›è¿‘ä¼¼å¯ä»¥éå¸¸å‡†ç¡®ï¼‰ã€‚å…¶æ¬¡ï¼Œä»ç»å°”å…¹æ›¼åˆ†å¸ƒç”Ÿæˆæ–°æ ·æœ¬æ‰€éœ€çš„é€€ç«æ—¶é—´çº¦ä¸ºâˆ¼1å¾®ç§’ï¼Œä¸å›¾çš„å¤§å°æ— å…³ã€‚åœ¨ç»å…¸çš„RBMä¸­ï¼Œæƒ…å†µå¹¶éå¦‚æ­¤ï¼Œé€šå¸¸éœ€è¦è¿›è¡Œæ•°åƒæ¬¡çš„å‰å‘å’Œåå‘ä¼ æ’­ï¼Œæ‰èƒ½è¯»å–åˆ°ç½‘ç»œæƒé‡å’Œåç½®ä¸­ç¼–ç çš„ç»å°”å…¹æ›¼åˆ†å¸ƒçš„ç‹¬ç«‹æ–°æ ·æœ¬[[173](Biblography.xhtml#XKS2020)]ã€‚å¯¹äºå¤§çš„RBMå›¾ï¼Œåœ¨æ ‡å‡†ç¡¬ä»¶ä¸Šå¯èƒ½éœ€è¦èŠ±è´¹æ•°åæ¯«ç§’ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä»å‡†ç¡®æ€§å’ŒåŠ é€Ÿä¸¤ä¸ªæ–¹é¢æ¥æ¢ç´¢é‡å­é€€ç«ä¸ºç»å°”å…¹æ›¼é‡‡æ ·å¸¦æ¥çš„æ½œåœ¨é‡å­ä¼˜åŠ¿ã€‚
- en: 5.4.2 Mapping
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2 æ˜ å°„
- en: 'The first step in performing Boltzmann sampling on a quantum annealer is the
    mapping of the RBM onto the quantum annealing hardware graph. We start with writing
    an expression for the RBM energy function *E* in the following form:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é‡å­é€€ç«å™¨ä¸Šæ‰§è¡Œç»å°”å…¹æ›¼é‡‡æ ·çš„ç¬¬ä¸€æ­¥æ˜¯å°†RBMæ˜ å°„åˆ°é‡å­é€€ç«ç¡¬ä»¶å›¾ä¸Šã€‚æˆ‘ä»¬ä»ä¸ºRBMèƒ½é‡å‡½æ•°*E*å†™å‡ºä»¥ä¸‹å½¢å¼çš„è¡¨è¾¾å¼å¼€å§‹ï¼š
- en: '| ![E (v,h) = E(x) = Î²xTQx. ](img/file439.jpg) |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| ![E (v,h) = E(x) = Î²xTQx. ](img/file439.jpg) |  |'
- en: 'Here, *Q* is the (*N* + *M*) Ã— (*N* + *M*) matrix whose elements are RBM weights
    and biases:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ*Q*æ˜¯ä¸€ä¸ª(*N* + *M*) Ã— (*N* + *M*)çŸ©é˜µï¼Œå…¶å…ƒç´ æ˜¯RBMçš„æƒé‡å’Œåç½®ï¼š
- en: '| ![ âŒŠ &#124; âŒ‹ &#124;a1 0 ... 0 &#124;w11 w12 ... w1M &#124; &#124;&#124;
    0 a2 ... 0 &#124;w21 w22 ... w2M &#124;&#124; &#124;&#124; . . . . &#124; . .
    . . &#124;&#124; &#124; .. .. .. .. &#124; .. .. .. .. &#124; &#124;&#124; 0 0
    ... a &#124;w w ... w &#124;&#124; Q = 1-&#124;&#124;-------------N--&#124;--N1---N2--------NM--&#124;&#124;.
    Î² &#124;&#124; 0 0 ... 0 &#124; b1 0 ... 0 &#124;&#124; &#124;&#124; &#124; &#124;&#124;
    &#124;&#124; 0 0 ... 0 &#124; 0 b2 ... 0 &#124;&#124; &#124; ... ... ... ... &#124;
    ... ... ... ... &#124; âŒˆ &#124; âŒ‰ 0 0 ... 0 &#124; 0 0 ... bM ](img/file440.jpg)
    |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| ![ âŒŠ &#124; âŒ‹ &#124;a1 0 ... 0 &#124;w11 w12 ... w1M &#124; &#124;&#124;
    0 a2 ... 0 &#124;w21 w22 ... w2M &#124;&#124; &#124;&#124; . . . . &#124; . .
    . . &#124;&#124; &#124; .. .. .. .. &#124; .. .. .. .. &#124; &#124;&#124; 0 0
    ... a &#124;w w ... w &#124;&#124; Q = 1-&#124;&#124;-------------N--&#124;--N1---N2--------NM--&#124;&#124;.
    Î² &#124;&#124; 0 0 ... 0 &#124; b1 0 ... 0 &#124;&#124; &#124;&#124; &#124; &#124;&#124;
    &#124;&#124; 0 0 ... 0 &#124; 0 b2 ... 0 &#124;&#124; &#124; ... ... ... ... &#124;
    ... ... ... ... &#124; âŒˆ &#124; âŒ‰ 0 0 ... 0 &#124; 0 0 ... bM ](img/file440.jpg)
    |  |'
- en: Quantum annealers operate on spin variables {âˆ’1*,*+1} instead of binary variables
    {0, 1}. The vector of binary variablesÂ x can be transformed into the vector of
    spin variablesÂ s using
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­é€€ç«å™¨æ“ä½œçš„æ˜¯è‡ªæ—‹å˜é‡{âˆ’1*,*+1}ï¼Œè€Œä¸æ˜¯äºŒè¿›åˆ¶å˜é‡{0, 1}ã€‚äºŒè¿›åˆ¶å˜é‡å‘é‡xå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è½¬æ¢ä¸ºè‡ªæ—‹å˜é‡å‘é‡sï¼š
- en: '| ![x âˆ’â†’ s = 2x âˆ’ 1, ](img/file441.jpg) |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| ![x âˆ’â†’ s = 2x âˆ’ 1, ](img/file441.jpg) |  |'
- en: 'and we obtain the following expression for the RBM energy:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¾—åˆ°RBMèƒ½é‡çš„ä»¥ä¸‹è¡¨è¾¾å¼ï¼š
- en: '| ![ Nâˆ‘ N+âˆ‘M âˆ‘N Nâˆ‘+M E = âˆ’ gisi âˆ’ gjsj âˆ’ Jijsisj âˆ’ const = EIsing âˆ’ const,
    i=1 j=N+1 i=1j=N+1 ](img/file442.jpg) |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| ![ Nâˆ‘ N+âˆ‘M âˆ‘N Nâˆ‘+M E = âˆ’ gisi âˆ’ gjsj âˆ’ Jijsisj âˆ’ const = EIsing âˆ’ const,
    i=1 j=N+1 i=1j=N+1 ](img/file442.jpg) |  |'
- en: where, for *i* = 1*,â€¦,N* and *j* = *N* + 1*,â€¦,N* + *M*,
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œå¯¹äº *i* = 1, â€¦, N å’Œ *j* = *N* + 1, â€¦, *N* + *M*ï¼Œ
- en: '| ![ N+M N g := ai+ 1- âˆ‘ w , g := bj + 1-âˆ‘ w , J := 1w , i 2 4 ij j 2 4 i=1
    ij ij 4 ij j=N+1 ](img/file443.jpg) |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| ![ N+M N g := ai+ 1- âˆ‘ w , g := bj + 1-âˆ‘ w , J := 1w , i 2 4 ij j 2 4 i=1
    ij ij 4 ij j=N+1 ](img/file443.jpg) |  |'
- en: and (*s*[i])[i=1,â€¦,N] are spin variables corresponding to the visible nodes
    and (*s*[j])[j=N+1,â€¦,N+M] are spin variables corresponding to the hidden nodes.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸” (*s*[i])[i=1,â€¦,N] æ˜¯å¯¹åº”å¯è§èŠ‚ç‚¹çš„è‡ªæ—‹å˜é‡ï¼Œè€Œ (*s*[j])[j=N+1,â€¦,N+M] æ˜¯å¯¹åº”éšè—èŠ‚ç‚¹çš„è‡ªæ—‹å˜é‡ã€‚
- en: We can ignore the constant term in the RBM energy expressionÂ ([5.4.2](#x1-1100002))
    since the same factor will appear in the numerator and denominator of â„™(v*,*h).
    Thus, we have
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å¿½ç•¥ RBM èƒ½é‡è¡¨è¾¾å¼ä¸­çš„å¸¸æ•°é¡¹ ([5.4.2](#x1-1100002))ï¼Œå› ä¸ºç›¸åŒçš„å› å­å°†åœ¨ â„™(v*,*h) çš„åˆ†å­å’Œåˆ†æ¯ä¸­å‡ºç°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: '| ![âŸ¨vihjâŸ©EmIosidngel = âŸ¨vihjâŸ©Emodel. ](img/file444.jpg) |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| ![âŸ¨vihjâŸ©EmIosidngel = âŸ¨vihjâŸ©Emodel. ](img/file444.jpg) |  |'
- en: 'To express the Ising Hamiltonian using a quantum mechanical description of
    spins, we replace the spin variables with their respective Pauli operators:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿ç”¨é‡å­åŠ›å­¦æè¿°è‡ªæ—‹æ¥è¡¨è¾¾ä¼Šè¾›å“ˆå¯†é¡¿é‡ï¼Œæˆ‘ä»¬ç”¨ç›¸åº”çš„ä¿åˆ©ç®—ç¬¦æ›¿æ¢è‡ªæ—‹å˜é‡ï¼š
- en: '| ![ N N+M N N+M âˆ‘ i âˆ‘ j âˆ‘ âˆ‘ i j â„‹Ising = âˆ’ giÏƒz âˆ’ gjÏƒz âˆ’ JijÏƒzÏƒz, i=1 j=N+1
    i=1j=N+1 ](img/file445.jpg) |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| ![ N N+M N N+M âˆ‘ i âˆ‘ j âˆ‘ âˆ‘ i j â„‹Ising = âˆ’ giÏƒz âˆ’ gjÏƒz âˆ’ JijÏƒzÏƒz, i=1 j=N+1
    i=1j=N+1 ](img/file445.jpg) |  |'
- en: with *Ïƒ*[z]^i being the usual Pauli matrix representation for an Ising quantum
    spin. With the initial Hamiltonian given by
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *Ïƒ*[z]^i æ˜¯è¡¨ç¤ºä¼Šè¾›é‡å­è‡ªæ—‹çš„å¸¸ç”¨ä¿åˆ©çŸ©é˜µã€‚åˆå§‹å“ˆå¯†é¡¿é‡ä¸ºï¼š
- en: '| ![ N+âˆ‘M â„‹0 = Ïƒi, i=1 x ](img/file446.jpg) |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| ![ N+âˆ‘M â„‹0 = Ïƒi, i=1 x ](img/file446.jpg) |  |'
- en: the time-dependent HamiltonianÂ ([5.4](#x1-1080004)) takes the form
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: éšæ—¶é—´å˜åŒ–çš„å“ˆå¯†é¡¿é‡ ([5.4](#x1-1080004)) å½¢å¼ä¸ºï¼š
- en: '| ![â„‹ (t) = r(t)â„‹0 + (1 âˆ’ r(t))â„‹Ising. ](img/file447.jpg) |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| ![â„‹ (t) = r(t)â„‹0 + (1 âˆ’ r(t))â„‹Ising. ](img/file447.jpg) |  |'
- en: 5.4.3 Hardware embedding and parameters optimisation
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3 ç¡¬ä»¶åµŒå…¥å’Œå‚æ•°ä¼˜åŒ–
- en: In the standard programming practices of existing quantum annealers, each spin
    variableÂ *s*[i] should ideally be assigned to a specific chip element, a superconducting
    flux qubit, modelled by a quantum two-level system that could represent the quantum
    Hamiltonian
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç°æœ‰é‡å­é€€ç«å™¨çš„æ ‡å‡†ç¼–ç¨‹å®è·µä¸­ï¼Œæ¯ä¸ªè‡ªæ—‹å˜é‡ *s*[i] ç†æƒ³æƒ…å†µä¸‹åº”åˆ†é…ç»™ä¸€ä¸ªç‰¹å®šçš„èŠ¯ç‰‡å…ƒç´ ï¼Œå³è¶…å¯¼é‡å­é€šé‡æ¯”ç‰¹ï¼Œæ¨¡å‹ä¸ºé‡å­äºŒèƒ½çº§ç³»ç»Ÿï¼Œå¯ä»¥è¡¨ç¤ºé‡å­å“ˆå¯†é¡¿é‡ã€‚
- en: '| ![ âˆ‘ â„‹local = giÏƒiz. i ](img/file448.jpg) |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘ â„‹local = giÏƒiz. i ](img/file448.jpg) |  |'
- en: While each qubit supports the programming of theÂ *g*[i] terms, theÂ *J*[ij] parameters
    can then be implemented energetically through inductive elements, meant to represent
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ¯ä¸ªé‡å­æ¯”ç‰¹æ”¯æŒ *g*[i] é¡¹çš„ç¼–ç¨‹ï¼Œä½† *J*[ij] å‚æ•°å¯ä»¥é€šè¿‡ç”µæ„Ÿå…ƒä»¶ä»¥èƒ½é‡æ–¹å¼å®ç°ï¼Œè¿™äº›å…ƒä»¶æ—¨åœ¨è¡¨ç¤ºã€‚
- en: '| ![ âˆ‘ â„‹couplers = JijÏƒizÏƒjz, ij ](img/file449.jpg) |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| ![ âˆ‘ â„‹couplers = JijÏƒizÏƒjz, ij ](img/file449.jpg) |  |'
- en: if and only if the required circuitry exists between qubitsÂ *i* andÂ *j*, which
    cannot be manufactured too far apart in the spatial layout of the processor due
    to engineering considerationsÂ Â [[296](Biblography.xhtml#XVenturelli2019)]. In
    other words, *J*[ij] = 0 unless (*i,j*) âˆˆ *G*, where *G* is a particular quantum
    annealing graph (e.g., *Chimera* or *Pegasus* graphs in the case of D-Wave quantum
    annealers).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…å½“é‡å­æ¯”ç‰¹ *i* å’Œ *j* ä¹‹é—´å­˜åœ¨æ‰€éœ€çš„ç”µè·¯æ—¶æ‰èƒ½å®ç°ï¼Œå¦åˆ™å®ƒä»¬åœ¨å¤„ç†å™¨çš„ç©ºé—´å¸ƒå±€ä¸­ä¸èƒ½åˆ¶é€ å¾—å¤ªè¿œï¼Œä»¥é¿å…å·¥ç¨‹ä¸Šçš„é—®é¢˜ [[296](Biblography.xhtml#XVenturelli2019)]ã€‚æ¢å¥è¯è¯´ï¼Œé™¤é
    (*i,j*) âˆˆ *G*ï¼Œå¦åˆ™ *J*[ij] = 0ï¼Œå…¶ä¸­ *G* æ˜¯ç‰¹å®šçš„é‡å­é€€ç«å›¾ï¼ˆä¾‹å¦‚ï¼Œåœ¨ D-Wave é‡å­é€€ç«å™¨çš„æƒ…å†µä¸‹ï¼Œ*Chimera*
    æˆ– *Pegasus* å›¾ï¼‰ã€‚
- en: It would be straightforward to embed the final HamiltonianÂ ([5.4.2](#x1-1100002))
    on the quantum chip had all the physical qubits been connected to each other.
    Unfortunately, this is not the case. The existing quantum annealers have rather
    limited qubit connectivity. For example, in the case of the *Chimera* (*Pegasus*)
    graph, a physical qubit is connected with a maximum of six (fifteen) other physical
    qubits.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰€æœ‰ç‰©ç†é‡å­æ¯”ç‰¹éƒ½å·²äº’ç›¸è¿æ¥ï¼Œåˆ™å°†æœ€ç»ˆçš„å“ˆå¯†é¡¿é‡ ([5.4.2](#x1-1100002)) åµŒå…¥åˆ°é‡å­èŠ¯ç‰‡ä¸Šæ˜¯å¾ˆç®€å•çš„ã€‚é—æ†¾çš„æ˜¯ï¼Œå®é™…æƒ…å†µå¹¶éå¦‚æ­¤ã€‚ç°æœ‰çš„é‡å­é€€ç«å™¨çš„é‡å­æ¯”ç‰¹è¿æ¥æ€§éå¸¸æœ‰é™ã€‚ä¾‹å¦‚ï¼Œåœ¨
    *Chimera* (*Pegasus*) å›¾ä¸­ï¼Œä¸€ä¸ªç‰©ç†é‡å­æ¯”ç‰¹æœ€å¤šä¸å…­ä¸ªï¼ˆåäº”ä¸ªï¼‰å…¶ä»–ç‰©ç†é‡å­æ¯”ç‰¹è¿æ¥ã€‚
- en: 'To get around this restriction, the standard procedure is to employ the minor-embedding
    compilation technique for fully connected graphs. By means of this procedure,
    we obtain another Ising form, where qubits are arranged in ordered 1D chains (forming
    the *logical* qubits that represent the spin variables) interlaced on the quantum
    annealer graph:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç»•è¿‡è¿™ä¸€é™åˆ¶ï¼Œæ ‡å‡†çš„åšæ³•æ˜¯é‡‡ç”¨æ¬¡è¦åµŒå…¥ç¼–è¯‘æŠ€æœ¯æ¥å¤„ç†å®Œå…¨è¿æ¥çš„å›¾ã€‚é€šè¿‡è¿™ä¸€è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¾—åˆ°å¦ä¸€ä¸ªä¼Šè¾›æ¨¡å‹å½¢å¼ï¼Œå…¶ä¸­é‡å­æ¯”ç‰¹æŒ‰é¡ºåºæ’åˆ—æˆ1Dé“¾ï¼ˆå½¢æˆä»£è¡¨è‡ªæ—‹å˜é‡çš„*é€»è¾‘*é‡å­æ¯”ç‰¹ï¼‰ï¼Œå¹¶äº¤é”™äºé‡å­é€€ç«å™¨å›¾ä¸Šï¼š
- en: '| ![ Nâˆ‘ [Ncâˆ‘âˆ’1 ] N+âˆ‘M [Ncâˆ‘âˆ’1 ] â„‹Ising = âˆ’ &#124;JF&#124; ÏƒicÏƒi(c+1) âˆ’ &#124;JF
    &#124; ÏƒjcÏƒj(c+1) i=1 c=1 z z j=N+1 c=1 z z ](img/file450.jpg) |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| ![ Nâˆ‘ [Ncâˆ‘âˆ’1 ] N+âˆ‘M [Ncâˆ‘âˆ’1 ] â„‹Ising = âˆ’ &#124;JF&#124; ÏƒicÏƒi(c+1) âˆ’ &#124;JF
    &#124; ÏƒjcÏƒj(c+1) i=1 c=1 z z j=N+1 c=1 z z ](img/file450.jpg) |  |'
- en: '| ![ N [ Nc ] N+M [ Nc ] âˆ’ âˆ‘ gi- âˆ‘ Ïƒic âˆ’ âˆ‘ gj- âˆ‘ Ïƒjc Nc z Nc z i=1 c=1 j=N+1
    c=1 ](img/file451.jpg) |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| ![ N [ Nc ] N+M [ Nc ] âˆ’ âˆ‘ gi- âˆ‘ Ïƒic âˆ’ âˆ‘ gj- âˆ‘ Ïƒjc Nc z Nc z i=1 c=1 j=N+1
    c=1 ](img/file451.jpg) |  |'
- en: '| ![ âŒŠ âŒ‹ âˆ‘N N+âˆ‘M Nâˆ‘c âˆ’ JijâŒˆ Î´Gij(ci,cj)ÏƒizciÏƒjczjâŒ‰ . i=1j=N+1 ci,cj=1 ](img/file452.jpg)
    |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| ![ âŒŠ âŒ‹ âˆ‘N N+âˆ‘M Nâˆ‘c âˆ’ JijâŒˆ Î´Gij(ci,cj)ÏƒizciÏƒjczjâŒ‰ . i=1j=N+1 ci,cj=1 ](img/file452.jpg)
    |  |'
- en: 'InÂ ([5.4.3](#x1-1110003)), we explicitly isolate the encoding of the *logical*
    quantum variable: the classical binary variable *s*[i] is associated with *N*[c]
    Ising spins *Ïƒ*[z]^(ic), ferromagnetically coupled directly by strength *J*[F]
    , forming an ordered 1D chain subgraph of *G*. The value of *J*[F] should be strong
    enough to correlate the value of the magnetisation of each individual spin if
    measured in the computational basis (âŸ¨*Ïƒ*[z]^(ic)âŸ© = âŸ¨*Ïƒ*[z]^(i(c+1))âŸ©).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Â ([5.4.3](#x1-1110003))ä¸­ï¼Œæˆ‘ä»¬æ˜¾å¼åœ°åˆ†ç¦»å‡º*é€»è¾‘*é‡å­å˜é‡çš„ç¼–ç ï¼šç»å…¸äºŒè¿›åˆ¶å˜é‡*s*[i]ä¸*N*[c]ä¸ªä¼Šè¾›è‡ªæ—‹*Ïƒ*[z]^(ic)ç›¸å…³è”ï¼Œé€šè¿‡è€¦åˆå¼ºåº¦*J*[F]
    ç›´æ¥å‘ç”Ÿé“ç£è€¦åˆï¼Œå½¢æˆä¸€ä¸ªæœ‰åºçš„1Dé“¾å­å›¾*G*ã€‚å¦‚æœåœ¨è®¡ç®—åŸºä¸‹æµ‹é‡ï¼Œæ¯ä¸ªè‡ªæ—‹çš„ç£åŒ–å¼ºåº¦å€¼åº”è¶³å¤Ÿå¼ºï¼Œä»¥ä¾¿èƒ½å¤Ÿç›¸å…³è”ï¼ˆâŸ¨*Ïƒ*[z]^(ic)âŸ© = âŸ¨*Ïƒ*[z]^(i(c+1))âŸ©ï¼‰ã€‚
- en: 'InÂ ([5.4.3](#x1-1110003)) andÂ ([5.4.3](#x1-1110003)), we encode the Ising HamiltonianÂ ([5.4.2](#x1-1100002))
    through our extended set of variables: the local field *g*[i] is evenly distributed
    across all qubits belonging to the logical chain *i*, and each coupler *J*[ij]
    is active only between one specific pair of qubits (*Ïƒ*[z]^(ic[i]^â‹†)*,Ïƒ*[z]^(jc[j]^â‹†)),
    which is specified by the adjacency check function *Î´*[ij]^G(*c*[i]*,c*[j]), which
    assumes a unit value only if (*c*[i] = *c*[i]^â‹†) and (*c*[j] = *c*[j]^â‹†), and
    is zero otherwise.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Â ([5.4.3](#x1-1110003))å’ŒÂ ([5.4.3](#x1-1110003))ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ‰©å±•çš„å˜é‡é›†å¯¹ä¼Šè¾›å“ˆå¯†é¡¿é‡Â ([5.4.2](#x1-1100002))è¿›è¡Œç¼–ç ï¼šå±€éƒ¨åœº*g*[i]å‡åŒ€åˆ†å¸ƒåœ¨å±äºé€»è¾‘é“¾*i*çš„æ‰€æœ‰é‡å­æ¯”ç‰¹ä¸Šï¼Œæ¯ä¸ªè€¦åˆå™¨*J*[ij]ä»…åœ¨ä¸€å¯¹ç‰¹å®šçš„é‡å­æ¯”ç‰¹ä¹‹é—´æ¿€æ´»ï¼ˆ*Ïƒ*[z]^(ic[i]^â‹†)*,Ïƒ*[z]^(jc[j]^â‹†)ï¼‰ï¼Œè¯¥å¯¹é‡å­æ¯”ç‰¹ç”±é‚»æ¥æ£€æŸ¥å‡½æ•°*Î´*[ij]^G(*c*[i]*,c*[j])æŒ‡å®šï¼Œåªæœ‰å½“(*c*[i]
    = *c*[i]^â‹†)ä¸”(*c*[j] = *c*[j]^â‹†)æ—¶ï¼Œå‡½æ•°å€¼ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚
- en: Given this particular embedding scheme, we can turn our attention to finding
    an optimal value for the parameterÂ *Î²* inÂ ([5.4.2](#x1-1100002)), which can only
    be done experimentally. Since the final Hamiltonian is programmed on the quantum
    annealer using dimensionless coefficients, the parameterÂ *Î²* cannot be expressed
    in the usual form 1*âˆ•kT*, whereÂ *k* is the Boltzmann constant andÂ *T* the effective
    temperature. Instead, it should be viewed as an empirical parameter that depends
    on the network architecture, the embedding scheme, and the physical characteristics
    of the quantum annealer (such as the operating temperature, the anneal time, the
    energy scale of the superconducting flux qubit system, etc.).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç‰¹å®šçš„åµŒå…¥æ–¹æ¡ˆä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ³¨æ„åŠ›é›†ä¸­åœ¨å®éªŒæ€§åœ°å¯»æ‰¾å‚æ•°*Î²*çš„æœ€ä¼˜å€¼ä¸Šï¼ˆè§Â [5.4.2](#x1-1100002)ï¼‰ï¼Œè¿™ä¸€ç‚¹åªèƒ½é€šè¿‡å®éªŒæ¥å®Œæˆã€‚ç”±äºæœ€ç»ˆçš„å“ˆå¯†é¡¿é‡æ˜¯åœ¨é‡å­é€€ç«å™¨ä¸Šä½¿ç”¨æ— é‡çº²ç³»æ•°ç¼–ç¨‹çš„ï¼Œå› æ­¤å‚æ•°*Î²*ä¸èƒ½ç”¨å¸¸è§„å½¢å¼
    1*âˆ•kT* æ¥è¡¨ç¤ºï¼Œå…¶ä¸­*k*æ˜¯ç»å°”å…¹æ›¼å¸¸æ•°ï¼Œ*T*æ˜¯æœ‰æ•ˆæ¸©åº¦ã€‚ç›¸åï¼Œå®ƒåº”è¯¥è¢«è§†ä¸ºä¸€ä¸ªç»éªŒå‚æ•°ï¼Œä¾èµ–äºç½‘ç»œæ¶æ„ã€åµŒå…¥æ–¹æ¡ˆä»¥åŠé‡å­é€€ç«å™¨çš„ç‰©ç†ç‰¹æ€§ï¼ˆä¾‹å¦‚å·¥ä½œæ¸©åº¦ã€é€€ç«æ—¶é—´ã€è¶…å¯¼æµé‡é‡å­æ¯”ç‰¹ç³»ç»Ÿçš„èƒ½é‡å°ºåº¦ç­‰ï¼‰ã€‚
- en: 'The experimental approach of estimatingÂ *Î²* consists of the following five
    stepsÂ Â [[4](Biblography.xhtml#XAdachi2015)]:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼°ç®—*Î²*çš„å®éªŒæ–¹æ³•åŒ…æ‹¬ä»¥ä¸‹äº”ä¸ªæ­¥éª¤ [[4](Biblography.xhtml#XAdachi2015)]ï¼š
- en: Construct an RBM.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªRBMã€‚
- en: Map the RBM to a final Hamiltonian assuming a particular value ofÂ *Î²* (Alg.Â [4](#x1-109006r4)-Step
    1).
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†RBMæ˜ å°„åˆ°å‡è®¾ç‰¹å®š*Î²*å€¼çš„æœ€ç»ˆå“ˆå¯†é¡¿é‡ï¼ˆç®—æ³•Â [4](#x1-109006r4)-ç¬¬1æ­¥ï¼‰ã€‚
- en: Run quantum annealing (Alg.Â [4](#x1-109006r4)-Step 2).
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿è¡Œé‡å­é€€ç«ï¼ˆç®—æ³•Â [4](#x1-109006r4)-ç¬¬2æ­¥ï¼‰ã€‚
- en: Compute the model expectations using the quantum samples (Alg.Â [4](#x1-109006r4)-Step
    3).
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é‡å­æ ·æœ¬è®¡ç®—æ¨¡å‹æœŸæœ›å€¼ï¼ˆç®—æ³•Â [4](#x1-109006r4)-ç¬¬3æ­¥ï¼‰ã€‚
- en: Compare the resulting expectations with the "correct" benchmark values (e.g.,
    obtained with the classical CD algorithm).
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†ç»“æœæœŸæœ›å€¼ä¸â€œæ­£ç¡®â€çš„åŸºå‡†å€¼è¿›è¡Œæ¯”è¾ƒï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ç»å…¸CDç®—æ³•è·å¾—çš„å€¼ï¼‰ã€‚
- en: This process is repeated for different choices of *Î²*. The value of *Î²* that
    gives the best fit can then be used for the given RBM architecture. As noted inÂ Â [[4](Biblography.xhtml#XAdachi2015)],
    even with the optimal settings for *Î²*, the estimates of the model expectations
    will still have some error. However, in comparison to the noise associated with
    the Boltzmann sampling in the Contrastive Divergence algorithm, this may be sufficient
    to estimate the gradients inÂ ([5.3.2](#x1-106003r2)), ([5.3.2](#x1-106003r2)),
    and ([5.3.2](#x1-106003r2)).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹ä¼šé’ˆå¯¹ä¸åŒçš„*Î²*å€¼è¿›è¡Œé‡å¤ã€‚ç„¶åï¼Œå¯ä»¥ä½¿ç”¨ç»™å®šRBMæ¶æ„çš„æœ€ä½³æ‹Ÿåˆçš„*Î²*å€¼ã€‚å¦‚åœ¨[[4](Biblography.xhtml#XAdachi2015)]ä¸­æ‰€è¿°ï¼Œå³ä½¿å¯¹äº*Î²*çš„æœ€ä¼˜è®¾ç½®ï¼Œæ¨¡å‹æœŸæœ›å€¼çš„ä¼°è®¡ä»ç„¶ä¼šæœ‰ä¸€å®šè¯¯å·®ã€‚ç„¶è€Œï¼Œä¸å¯¹æ¯”æ•£åº¦ç®—æ³•ä¸­ç»å°”å…¹æ›¼é‡‡æ ·ç›¸å…³çš„å™ªå£°ç›¸æ¯”ï¼Œè¿™å¯èƒ½è¶³ä»¥ä¼°è®¡[5.3.2](#x1-106003r2)ä¸­çš„æ¢¯åº¦ï¼Œ[5.3.2](#x1-106003r2)ä¸­çš„æ¢¯åº¦ï¼Œä»¥åŠ[5.3.2](#x1-106003r2)ä¸­çš„æ¢¯åº¦ã€‚
- en: 5.4.4 Generative models
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.4 ç”Ÿæˆæ¨¡å‹
- en: The main application of the Boltzmann sampling weâ€™ve considered so far is in
    providing an unbiased estimate of the model expectations as specified in AlgorithmÂ [4](#x1-109006r4).
    Once fully trained with the help of quantum annealing, an RBM can be used in a
    conventional classical way to generate new synthetic samples from the learned
    probability distribution. In this case, quantum annealing is only used as a subroutine
    in the hybrid quantum-classical training protocol.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬è€ƒè™‘çš„ç»å°”å…¹æ›¼é‡‡æ ·çš„ä¸»è¦åº”ç”¨æ˜¯åœ¨æä¾›æ— åä¼°è®¡æ¨¡å‹æœŸæœ›å€¼ä¸Šï¼Œæ­£å¦‚ç®—æ³•[4](#x1-109006r4)ä¸­æ‰€æŒ‡å®šçš„é‚£æ ·ã€‚ä¸€æ—¦é€šè¿‡é‡å­é€€ç«å®Œæˆå……åˆ†è®­ç»ƒï¼ŒRBMå°±å¯ä»¥ä»¥ä¼ ç»Ÿçš„ç»å…¸æ–¹å¼ï¼Œç”¨äºä»å·²å­¦ä¹ çš„æ¦‚ç‡åˆ†å¸ƒä¸­ç”Ÿæˆæ–°çš„åˆæˆæ ·æœ¬ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé‡å­é€€ç«ä»…ä½œä¸ºæ··åˆé‡å­-ç»å…¸è®­ç»ƒåè®®ä¸­çš„ä¸€ä¸ªå­ç¨‹åºä½¿ç”¨ã€‚
- en: 'However, it is possible to use a quantum annealer as a generator in its own
    right. Rather than assisting in training the classical RBM, a quantum annealer
    can output the binary representation of the continuous samples as per the distribution
    encoded in the final HamiltonianÂ ([5.4.2](#x1-1100002)). The Quantum Variational
    AutoencoderÂ Â [[162](Biblography.xhtml#XKhoshaman2018)] is another example of a
    QBM that can be trained end to end by maximising a well-defined cost function:
    a quantum lower bound to a variational approximation of the log-likelihood.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå®Œå…¨å¯ä»¥å°†é‡å­é€€ç«å™¨æœ¬èº«ä½œä¸ºç”Ÿæˆå™¨ä½¿ç”¨ã€‚é‡å­é€€ç«å™¨ä¸ä»…ä»…ååŠ©è®­ç»ƒç»å…¸çš„RBMï¼Œå®ƒè¿˜å¯ä»¥æ ¹æ®æœ€ç»ˆå“ˆå¯†é¡¿é‡([5.4.2](#x1-1100002))ç¼–ç çš„åˆ†å¸ƒè¾“å‡ºè¿ç»­æ ·æœ¬çš„äºŒè¿›åˆ¶è¡¨ç¤ºã€‚é‡å­å˜åˆ†è‡ªç¼–ç å™¨[[162](Biblography.xhtml#XKhoshaman2018)]æ˜¯å¦ä¸€ä¸ªQBMçš„ä¾‹å­ï¼Œå®ƒå¯ä»¥é€šè¿‡æœ€å¤§åŒ–æ˜ç¡®å®šä¹‰çš„ä»£ä»·å‡½æ•°æ¥ç«¯åˆ°ç«¯åœ°è®­ç»ƒï¼šé‡å­å˜åˆ†ä¸‹ç•Œè¿‘ä¼¼å¯¹æ•°ä¼¼ç„¶ã€‚
- en: Botzmann sampling is the key element of RBM training and the generation of new
    samples. Quantum annealing can provide orders of magnitude speedup by replacing
    classical Boltzmann sampling with quantum sampling.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ç»å°”å…¹æ›¼é‡‡æ ·æ˜¯RBMè®­ç»ƒå’Œæ–°æ ·æœ¬ç”Ÿæˆçš„å…³é”®å…ƒç´ ã€‚é€šè¿‡ç”¨é‡å­é‡‡æ ·æ›¿ä»£ç»å…¸çš„ç»å°”å…¹æ›¼é‡‡æ ·ï¼Œé‡å­é€€ç«å¯ä»¥æä¾›æ•°é‡çº§çš„åŠ é€Ÿã€‚
- en: 5.5 Deep Boltzmann Machine
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 æ·±åº¦ç»å°”å…¹æ›¼æœº
- en: Deep Boltzmann Machines (DBMs) can be constructed from several RBMs where the
    hidden layer of the first RBM becomes the visible layer of the second, and so
    on, as shown in FigureÂ [5.4](#5.4).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦ç»å°”å…¹æ›¼æœºï¼ˆDBMï¼‰å¯ä»¥ç”±å¤šä¸ªRBMæ„æˆï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªRBMçš„éšè—å±‚æˆä¸ºç¬¬äºŒä¸ªRBMçš„å¯è§å±‚ï¼Œä¾æ­¤ç±»æ¨ï¼Œå¦‚å›¾[5.4](#5.4)æ‰€ç¤ºã€‚
- en: '![Figurex1-113002r4: Schematic representation of a DBM. ](img/file453.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾x1-113002r4ï¼šDBMçš„ç¤ºæ„å›¾ã€‚](img/file453.jpg)'
- en: 'FigureÂ 5.4: Schematic representation of a DBM.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.4ï¼šDBMçš„ç¤ºæ„å›¾ã€‚
- en: A DBM can be trained layer by layer, one RBM at a time. This will result in
    a powerful generative model capable of learning complex multivariate distributions
    and dependence structures. However, the generative training of the DBM can be
    used as the first step towards building a discriminative model if the training
    dataset samples are labelled. In this case all DBM weights and biases found with
    the help of either CD or quantum Boltzmann sampling algorithms are seen as initial
    values of the weights and biases of the corresponding feedforward neural network.
    The discriminative model will consist of all the layers of the original DBM with
    an extra output layer performing assignment of the class labels. The discriminative
    model can be fine tuned through the standard backpropagation of the error algorithm.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: DBMå¯ä»¥é€å±‚è®­ç»ƒï¼Œæ¯æ¬¡è®­ç»ƒä¸€ä¸ªRBMã€‚è¿™å°†ç”Ÿæˆä¸€ä¸ªå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„å¤šå˜é‡åˆ†å¸ƒå’Œä¾èµ–ç»“æ„ã€‚ç„¶è€Œï¼ŒDBMçš„ç”Ÿæˆè®­ç»ƒå¯ä»¥ä½œä¸ºæ„å»ºåˆ¤åˆ«æ¨¡å‹çš„ç¬¬ä¸€æ­¥ï¼Œå¦‚æœè®­ç»ƒæ•°æ®é›†çš„æ ·æœ¬æœ‰æ ‡ç­¾ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰€æœ‰é€šè¿‡CDæˆ–é‡å­ç»å°”å…¹æ›¼é‡‡æ ·ç®—æ³•è·å¾—çš„DBMæƒé‡å’Œåç½®ï¼Œéƒ½ä¼šè¢«è§†ä¸ºç›¸åº”å‰é¦ˆç¥ç»ç½‘ç»œæƒé‡å’Œåç½®çš„åˆå§‹å€¼ã€‚åˆ¤åˆ«æ¨¡å‹å°†åŒ…æ‹¬åŸå§‹DBMçš„æ‰€æœ‰å±‚ï¼Œå¹¶å¢åŠ ä¸€ä¸ªé¢å¤–çš„è¾“å‡ºå±‚ï¼Œç”¨äºåˆ†ç±»æ ‡ç­¾çš„åˆ†é…ã€‚é€šè¿‡æ ‡å‡†çš„è¯¯å·®åå‘ä¼ æ’­ç®—æ³•ï¼Œå¯ä»¥å¯¹åˆ¤åˆ«æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
- en: 5.5.1 Training DBMs with quantum annealing
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1 ä½¿ç”¨é‡å­é€€ç«è®­ç»ƒDBM
- en: The generative training of DBMs can be seen as a pre-training of the discriminative
    model. FigureÂ [5.5](#5.5) provides a schematic illustration of the hybrid quantum-classical
    training process.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: DBMçš„ç”Ÿæˆè®­ç»ƒå¯ä»¥çœ‹ä½œæ˜¯åˆ¤åˆ«æ¨¡å‹çš„é¢„è®­ç»ƒã€‚å›¾[5.5](#5.5)æä¾›äº†æ··åˆé‡å­-ç»å…¸è®­ç»ƒè¿‡ç¨‹çš„ç¤ºæ„å›¾ã€‚
- en: '![Figurex1-114002r5: Generative and discriminative training of a DBM. ](img/file454.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![Figurex1-114002r5ï¼šDBMçš„ç”Ÿæˆè®­ç»ƒä¸åˆ¤åˆ«è®­ç»ƒã€‚](img/file454.jpg)'
- en: 'FigureÂ 5.5: Generative and discriminative training of a DBM.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5.5ï¼šDBMçš„ç”Ÿæˆè®­ç»ƒä¸åˆ¤åˆ«è®­ç»ƒã€‚
- en: 'In the DBM training scheme shown in FigureÂ [5.5](#5.5), only Step 1 relies
    on quantum annealing. Steps 2 and 3 are completely classical. Step 3 is optional:
    without it we have a standard machine learning "pipeline" where one or several
    RBMs (Step 1) perform "feature extraction" by building a low-dimensional representation
    of the samples in the dataset, thus helping the discriminative model (Step 2)
    to achieve better classification results.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾[5.5](#5.5)æ‰€ç¤ºçš„DBMè®­ç»ƒæ–¹æ¡ˆä¸­ï¼Œåªæœ‰æ­¥éª¤1ä¾èµ–äºé‡å­é€€ç«ã€‚æ­¥éª¤2å’Œæ­¥éª¤3å®Œå…¨æ˜¯ç»å…¸çš„ã€‚æ­¥éª¤3æ˜¯å¯é€‰çš„ï¼šæ²¡æœ‰å®ƒï¼Œæˆ‘ä»¬å°±æ‹¥æœ‰ä¸€ä¸ªæ ‡å‡†çš„æœºå™¨å­¦ä¹ â€œç®¡é“â€ï¼Œå…¶ä¸­ä¸€ä¸ªæˆ–å¤šä¸ªRBMï¼ˆæ­¥éª¤1ï¼‰é€šè¿‡æ„å»ºæ•°æ®é›†æ ·æœ¬çš„ä½ç»´è¡¨ç¤ºæ¥æ‰§è¡Œâ€œç‰¹å¾æå–â€ï¼Œä»è€Œå¸®åŠ©åˆ¤åˆ«æ¨¡å‹ï¼ˆæ­¥éª¤2ï¼‰å®ç°æ›´å¥½çš„åˆ†ç±»ç»“æœã€‚
- en: 5.5.2 A DBM pipeline example
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2 ä¸€ä¸ªDBMç®¡é“ç¤ºä¾‹
- en: The pipeline approach can be illustrated using the popular "King+Rook vs. King+Pawn"
    dataset from the UCI Machine Learning Repository Â [[262](Biblography.xhtml#XUCI_KRKP),Â [263](Biblography.xhtml#XShapiro1987)].
    The task is to classify the end game positions with the black pawn one move from
    queening and the white side (King+Rook) to move. The possible outcomes are "white
    can win" (Class 1) and "white cannot win" (Class 0). The board is described byÂ 36
    categorical attributes that can be encoded asÂ 38 binary variables. The dataset
    consists of 3,196 samples (white can win inÂ 52% of all cases in the dataset).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ç®¡é“æ–¹æ³•å¯ä»¥ä½¿ç”¨UCIæœºå™¨å­¦ä¹ åº“ä¸­çš„æµè¡Œæ•°æ®é›†â€œå›½ç‹+è½¦ vs. å›½ç‹+å…µâ€æ¥è¯´æ˜[[262](Biblography.xhtml#XUCI_KRKP),
    [263](Biblography.xhtml#XShapiro1987)]ã€‚ä»»åŠ¡æ˜¯åˆ†ç±»æ®‹å±€ï¼Œå…¶ä¸­é»‘è‰²å…µå³å°†å‡å˜ï¼Œè€Œç™½æ–¹ï¼ˆå›½ç‹+è½¦ï¼‰å³å°†èµ°å­ã€‚å¯èƒ½çš„ç»“æœæ˜¯â€œç™½æ–¹å¯ä»¥èµ¢â€ï¼ˆç±»1ï¼‰å’Œâ€œç™½æ–¹ä¸èƒ½èµ¢â€ï¼ˆç±»0ï¼‰ã€‚æ£‹ç›˜ç”±36ä¸ªç±»åˆ«å±æ€§æè¿°ï¼Œå¯ä»¥ç¼–ç ä¸º38ä¸ªäºŒè¿›åˆ¶å˜é‡ã€‚æ•°æ®é›†åŒ…å«3196ä¸ªæ ·æœ¬ï¼ˆåœ¨æ•°æ®é›†çš„æ‰€æœ‰æ¡ˆä¾‹ä¸­ï¼Œç™½æ–¹å¯ä»¥èµ¢52%ï¼‰ã€‚
- en: 'The `scikit-learn` package provides all the necessary components for building
    the classical part of a DBM pipeline. The pipeline itself can be constructed with
    the help of `sklearn.pipeline.make_pipeline`. The DBM is constructed from two
    RBMs implemented with the help of `sklearn.neural_network.BernoulliRBM`. RBMÂ #1
    hasÂ 38 nodes in the visible layer and 30 nodes in the hidden layer; RBMÂ #2 hasÂ 30
    nodes in the visible layer andÂ 20 nodes in the hidden layer. The exact pipeline
    configuration is as follows (all other parameters were set at their default values):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`åŒ…æä¾›äº†æ„å»ºDBMç®¡é“ç»å…¸éƒ¨åˆ†æ‰€éœ€çš„æ‰€æœ‰ç»„ä»¶ã€‚å¯ä»¥å€ŸåŠ©`sklearn.pipeline.make_pipeline`æ„å»ºç®¡é“æœ¬èº«ã€‚DBMæ˜¯ç”±ä¸¤ä¸ªRBMæ„å»ºçš„ï¼Œè¿™äº›RBMæ˜¯é€šè¿‡`sklearn.neural_network.BernoulliRBM`å®ç°çš„ã€‚RBM
    #1åœ¨å¯è§å±‚æœ‰38ä¸ªèŠ‚ç‚¹ï¼Œåœ¨éšè—å±‚æœ‰30ä¸ªèŠ‚ç‚¹ï¼›RBM #2åœ¨å¯è§å±‚æœ‰30ä¸ªèŠ‚ç‚¹ï¼Œåœ¨éšè—å±‚æœ‰20ä¸ªèŠ‚ç‚¹ã€‚ç¡®åˆ‡çš„ç®¡é“é…ç½®å¦‚ä¸‹ï¼ˆæ‰€æœ‰å…¶ä»–å‚æ•°è®¾ç½®ä¸ºé»˜è®¤å€¼ï¼‰ï¼š'
- en: '| RBMÂ #1 | RBMÂ #2 | MLP Classifier |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| RBM #1 | RBM #2 | MLP åˆ†ç±»å™¨ |'
- en: '| n_components = 30 | n_components = 20 | hidden_layer_sizes = (20) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| n_components = 30 | n_components = 20 | hidden_layer_sizes = (20) |'
- en: '| learning_rate = 0.00025 | learning_rate = 0.00025 | activation = â€™tanhâ€™ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| learning_rate = 0.00025 | learning_rate = 0.00025 | activation = ''tanh''
    |'
- en: '| batch_size = 10 | batch_size = 10 | solver = â€™adamâ€™ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| batch_size = 10 | batch_size = 10 | solver = ''adam'' |'
- en: '| n_iter = 100000 | n_iter = 100000 | alpha = 0.1 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| n_iter = 100000 | n_iter = 100000 | alpha = 0.1 |'
- en: '|  |  | max_iter = 5000 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  |  | max_iter = 5000 |'
- en: 'TableÂ 5.1: Configuration of the DBM pipeline for the â€œKing+Rook vs. King+Pawnâ€
    classification problem.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 5.1ï¼šç”¨äºâ€œå›½ç‹+è½¦ vs. å›½ç‹+å…µâ€åˆ†ç±»é—®é¢˜çš„DBMç®¡é“é…ç½®ã€‚
- en: 'Thus, both RBMs are trained as autoencoders: the DBM translates each 38-feature
    sample into its 20-feature low-dimensional representation. These new "extracted"
    features, ideally, should have higher predicting power in comparison with the
    original features, assuming that both RBMs learned the main characteristics and
    dependence structure of the dataset and stripped away the noise or the less important
    characteristics. The discriminator is `sklearn.neural_network.MLPClassifier` withÂ 20
    tanh activation units in its single hidden layer.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸¤ä¸ªRBMéƒ½è¢«è®­ç»ƒä¸ºè‡ªç¼–ç å™¨ï¼šDBMå°†æ¯ä¸ª38ç‰¹å¾çš„æ ·æœ¬è½¬æ¢ä¸ºå…¶20ç‰¹å¾çš„ä½ç»´è¡¨ç¤ºã€‚è¿™äº›æ–°çš„â€œæå–â€ç‰¹å¾ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œåº”è¯¥æ¯”åŸå§‹ç‰¹å¾å…·æœ‰æ›´é«˜çš„é¢„æµ‹èƒ½åŠ›ï¼Œå‰ææ˜¯ä¸¤ä¸ªRBMéƒ½å­¦åˆ°äº†æ•°æ®é›†çš„ä¸»è¦ç‰¹å¾å’Œä¾èµ–ç»“æ„ï¼Œå¹¶å‰”é™¤äº†å™ªå£°æˆ–ä¸å¤ªé‡è¦çš„ç‰¹å¾ã€‚åˆ¤åˆ«å™¨æ˜¯`sklearn.neural_network.MLPClassifier`ï¼Œå…¶å•ä¸€éšè—å±‚ä¸­æœ‰20ä¸ªtanhæ¿€æ´»å•å…ƒã€‚
- en: 'With this setting, the DBM achieves the following out-of-sample classification
    results (with the dataset split 70:30 into the training and testing datasets using
    `sklearn.model_selection.train_test_split`):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè®¾ç½®ä¸‹ï¼ŒDBMåœ¨ä»¥ä¸‹çš„æ ·æœ¬å¤–åˆ†ç±»ç»“æœï¼ˆæ•°æ®é›†æŒ‰70:30æ¯”ä¾‹åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œä½¿ç”¨`sklearn.model_selection.train_test_split`ï¼‰ä¸­å–å¾—äº†ä»¥ä¸‹æˆç»©ï¼š
- en: 'Classification accuracy: 95.2%'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†ç±»å‡†ç¡®ç‡ï¼š95.2%
- en: 'This compares favourably with, for example, an ensemble learning classifier
    such as random forest (`sklearn.ensemble.RandomForestClassifier`). The random
    forest classifier with the number of estimators set at 1,000 and the maximum depth
    set equal to 5 has the following out-of-sample classification results:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ä¾‹å¦‚é›†æˆå­¦ä¹ åˆ†ç±»å™¨å¦‚éšæœºæ£®æ—ï¼ˆ`sklearn.ensemble.RandomForestClassifier`ï¼‰ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚è®¾ç½®1000ä¸ªä¼°è®¡å™¨å’Œæœ€å¤§æ·±åº¦ä¸º5çš„éšæœºæ£®æ—åˆ†ç±»å™¨åœ¨æ ·æœ¬å¤–åˆ†ç±»ç»“æœä¸Šå–å¾—äº†ä»¥ä¸‹æˆç»©ï¼š
- en: 'Classification accuracy: 94.9%'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†ç±»å‡†ç¡®ç‡ï¼š94.9%
- en: The architecture of DBMs allows them to be trained as either generative or discriminative
    models. In both cases, Boltzmann sampling can play an important role in improving
    their performance.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: DBMçš„æ¶æ„ä½¿å…¶å¯ä»¥ä½œä¸ºç”Ÿæˆæ¨¡å‹æˆ–åˆ¤åˆ«æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œç»å°”å…¹æ›¼é‡‡æ ·éƒ½å¯ä»¥åœ¨æé«˜å…¶æ€§èƒ½æ–¹é¢å‘æŒ¥é‡è¦ä½œç”¨ã€‚
- en: Summary
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: In this chapter, we learned about energy-based models â€“ a special class of powerful
    generative models. We learned how to build, train, and run RBMs in order to generate
    synthetic samples that are statistically indistinguishable from the original training
    dataset.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†åŸºäºèƒ½é‡çš„æ¨¡å‹â€”â€”ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹çš„ç‰¹æ®Šç±»å‹ã€‚æˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•æ„å»ºã€è®­ç»ƒå’Œè¿è¡ŒRBMï¼Œä»¥ç”Ÿæˆä¸åŸå§‹è®­ç»ƒæ•°æ®é›†ç»Ÿè®¡ä¸Šæ— æ³•åŒºåˆ†çš„åˆæˆæ ·æœ¬ã€‚
- en: We familiarised ourselves with the Boltzmann sampling and Contrastive Divergence
    algorithms. Boltzmann sampling can be efficiently performed on NISQ-era quantum
    annealers that may improve the quality of the model and achieve orders of magnitude
    of speedup in generating new samples.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç†Ÿæ‚‰äº†ç»å°”å…¹æ›¼é‡‡æ ·å’Œå¯¹æ¯”æ•£åº¦ç®—æ³•ã€‚ç»å°”å…¹æ›¼é‡‡æ ·å¯ä»¥åœ¨NISQæ—¶ä»£çš„é‡å­é€€ç«æœºä¸Šé«˜æ•ˆæ‰§è¡Œï¼Œè¿™å¯èƒ½æé«˜æ¨¡å‹çš„è´¨é‡ï¼Œå¹¶åœ¨ç”Ÿæˆæ–°æ ·æœ¬æ—¶å®ç°æ•°é‡çº§çš„åŠ é€Ÿã€‚
- en: We learned how to combine individual RBMs together to construct a DBM. Quantum
    annealing can be productively applied to the pre-training of a DBM before it is
    fine tuned as a deep feedforward neural network classifier.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•å°†å•ä¸ªRBMç»“åˆèµ·æ¥æ„å»ºDBMã€‚é‡å­é€€ç«å¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºDBMçš„é¢„è®­ç»ƒï¼Œç„¶åå°†å…¶å¾®è°ƒä¸ºæ·±åº¦å‰é¦ˆç¥ç»ç½‘ç»œåˆ†ç±»å™¨ã€‚
- en: Finally, we explored the possibility of using RBMs and DBMs as the first model
    in the machine learning pipeline for denoising and feature extraction.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨RBMå’ŒDBMä½œä¸ºæœºå™¨å­¦ä¹ ç®¡é“ä¸­ç¬¬ä¸€ä¸ªæ¨¡å‹è¿›è¡Œå»å™ªå’Œç‰¹å¾æå–çš„å¯èƒ½æ€§ã€‚
- en: 'In the next chapter, we will shift our attention to gate model quantum computing.
    We will start with the concept of a classical binary digit (bit) and classical
    logic gates before introducing their quantum counterparts: the quantum binary
    digit (qubit) and one-qubit/multi-qubit quantum logic gates and quantum circuits.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†è½¬å‘é—¨æ¨¡å‹é‡å­è®¡ç®—ã€‚æˆ‘ä»¬å°†ä»ç»å…¸äºŒè¿›åˆ¶æ•°å­—ï¼ˆbitï¼‰å’Œç»å…¸é€»è¾‘é—¨çš„æ¦‚å¿µå¼€å§‹ï¼Œç„¶åä»‹ç»å®ƒä»¬çš„é‡å­å¯¹åº”ç‰©ï¼šé‡å­äºŒè¿›åˆ¶æ•°å­—ï¼ˆqubitï¼‰å’Œå•é‡å­æ¯”ç‰¹/å¤šé‡å­æ¯”ç‰¹é‡å­é€»è¾‘é—¨åŠé‡å­ç”µè·¯ã€‚
- en: Join our bookâ€™s Discord space
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬ä¹¦ç±çš„Discordç©ºé—´
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/quantum](https://packt.link/quantum)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬çš„Discordç¤¾åŒºï¼Œç»“è¯†å¿—åŒé“åˆçš„äººï¼Œå¹¶ä¸è¶…è¿‡2000åæˆå‘˜ä¸€èµ·å­¦ä¹ ï¼š [https://packt.link/quantum](https://packt.link/quantum)
- en: '![PIC](img/file1.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
